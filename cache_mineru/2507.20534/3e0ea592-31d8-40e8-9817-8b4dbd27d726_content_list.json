[
    {
        "type": "text",
        "text": "K KIMI K2: OPEN AGENTIC INTELLIGENCE",
        "text_level": 1,
        "bbox": [
            228,
            121,
            766,
            145
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "TECHNICAL REPORT OF KIMI K2 ",
        "bbox": [
            383,
            176,
            614,
            191
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Kimi Team ",
        "text_level": 1,
        "bbox": [
            459,
            218,
            537,
            233
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "ABSTRACT ",
        "text_level": 1,
        "bbox": [
            450,
            267,
            545,
            282
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We introduce Kimi K2,a Mixture-of-Experts (MoE) large language model with 32 bilion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efciency of Muon. Based on MuonClip,K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments. ",
        "bbox": [
            174,
            289,
            825,
            386
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Kimi K2 achieves state-of-the-art performance among open-source non-thinking models,with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench,76.5 on ACEBench (En), 65.8 on SWE-Bench Verified,and 47.3 on SWE-Bench Multilingual— surpassing most open and closed-sourced baselines in non-thinking setings. It also exhibits strong capabilities in coding, mathematics,and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond,and 27.1 on OJBench,all without extended thinking.These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints1 to facilitate future research and applications of agentic intelligence. ",
        "bbox": [
            173,
            388,
            826,
            513
        ],
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/e110bf358f54c8c0155d8c416051855dc74f60ac2c77545d2cc8e62d68131778.jpg",
        "image_caption": [
            "Figure 1: Kimi K2 main results.2 "
        ],
        "image_footnote": [],
        "bbox": [
            169,
            534,
            831,
            820
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1 Introduction ",
        "text_level": 1,
        "bbox": [
            117,
            89,
            251,
            106
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The development of Large Language Models (LLMs) is undergoing a profound paradigm shift towards Agentic Inteligence-the capabilities for models toautonomously perceive,plan,reason,and act within complex and dynamic environments.This transition marks a departure from static imitation learning towards models that actively learn through interactions,acquire new skils beyond their training distribution,and adapt behavior through experiences [63]. It is believed that this approach alows an AI agent to go beyond the limitationof static human-generated data,and acquire superhuman capabilities through its own exploration and exploitation. Agentic intellgence is thus rapidly emerging as a defining capabilityfor the next generation offoundation models, with wide-ranging implications across tool use, software development, and real-world autonomy. ",
        "bbox": [
            116,
            123,
            883,
            234
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Achieving agentic intelligence introduces chalenges in both pre-training and post-training. Pre-training must endow models with broad general-purpose priors under constraints of limited high-quality data, elevating token ffciency—learning signal per token—as a critical scaling coeficient.Post-training must transform those priors into actionable behaviors,yet agenticcapabilities such as multi-step reasoning,long-term planning,and tool use are rare in natural data and costlyto scale. Scalable synthesis ofstructured,high-qualityagentic trajectories,combined with general reinforcement learning (RL) techniques that incorporate preferences and self-critique,are essential to bridge this gap. ",
        "bbox": [
            116,
            241,
            882,
            338
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In this work, we introduce Kimi K2,a1.04 trillon-parameter Mixture-of-Experts (MoE)LLM with 32bilion activated parameters, purposefully designed to address the core challenges and push the boundaries of agentic capability. Our contributions span both the pre-training and post-training frontiers: ",
        "bbox": [
            117,
            344,
            882,
            387
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "· We present MuonClip,a novel optimizer that integrates the token-effcient Muon algorithm with a stabilityenhancing mechanism called QK-Clip.Using MuonClip, we successfully pre-trained Kimi K2 on 15.5 trillion tokens without a single loss spike.   \n· We introduce a large-scale agentic data synthesis pipeline that systematically generates tool-use demonstrations via simulated and real-world environments.This system constructs diverse tools,agents,tasks,and trajectories to create high-fidelity, verifiably correct agentic interactions at scale.   \n· We design a general reinforcement learning framework that combines verifiable rewards (RLVR) with a selfcritique rubric reward mechanism.The model learns not only from externally defined tasks but also from evaluating its own outputs, extending alignment from static into open-ended domains. ",
        "bbox": [
            129,
            401,
            885,
            546
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Kimi K2 demonstrates strong performance acrossa broad spectrum of agentic and frontier benchmarks. It achieves scores of 6.1 on Tau2-bench,76.5on ACEBench (en), 65.8 on SWE-bench Verifed,and 47.3 on SWE-bench Multilingual,outperforming most open-and closed-weight baselines under non-thinking evaluationsetings,closing the gap with Claude 4 Opus and Sonnet. In coding, mathematics,and broader STEM domains, Kimi K2 achieves 53.7 on LiveCodeBench v6,27.1 on OJBench, 49.5 on AIME 2025,and 75.1 on GPQA-Diamond, further highlighting its capabilities in general tasks. On the LMSYS Arena leaderboard (July17,2025)3, Kimi K2 ranks as the top1 open-source model and 5th overall based on over 3,Ooo user votes. ",
        "bbox": [
            116,
            563,
            882,
            659
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "[o spurfurther progress in Agentic Inteligence, we are open-sourcing our base and post-trained checkpoints,enabling :he community to explore, refine, and deploy agentic intelligence at scale. ",
        "bbox": [
            120,
            666,
            882,
            694
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2Pre-training ",
        "text_level": 1,
        "bbox": [
            116,
            718,
            250,
            736
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The base model of Kimi K2 is atrillion-parameter mixture-of-experts (MoE) transformer [72] model, pre-trained on15.5 trilion high-quality tokens. Given the increasingly limited availabilityof high-quality human data, we posit thattoken efciency is emerging as a critical coefficient in the scaling of large language models.To address this, we introduce a suite of pre-training techniques explicitlydesigned for maximizing token efciency. Specifically, we employ the token-effcient Muon optimizer[33,46]and mitigate its training instabilities through the introduction of QK-Clip. Additionally, we incorporate synthetic data generation to further squeeze the intelligence out of available high-quality tokens.The model architecture follows anultra-sparse MoE with multi-head latent attention(MLA) similar to DeepSeek-V3[10],derived from empirical scaling law analysis.The underlying infrastructure is built to optimize both training eficiency and research efficiency. ",
        "bbox": [
            114,
            752,
            883,
            877
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.1MuonClip: Stable Training with Weight Clipping ",
        "text_level": 1,
        "bbox": [
            116,
            90,
            498,
            107
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We train Kimi K2 using the token-eficient Muon optimizer[33], incorporating weight decay and consistent update RMS scaling [46]. Experiments in our previous work Moonlight [46] show that,under the same compute budget and model size—and therefore the same amount of training data—Muon substantially outperforms AdamW [36, 48], making it an effective choice for improving token efficiency in large language model training. ",
        "bbox": [
            114,
            116,
            883,
            172
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Training instability whenscaling MuonDespite its eficiency,scaling up Muon training reveals a challenge: training instability due to exploding attention logits,an isse that occurs more frequently with Muon but less with AdamW in our experiments.Existing mitigation strategies are insuficient.For instance,logit soft-cap[69]directlyclips the attentionlogits,butthe dot products betweenqueries and keys can stillgrow excessively beforecapping is applied.On the other hand,Query-Key Normalization (QK-Norm)[11,81]is not applicable to multi-ead latent atention (MLA), because its Key matrices are not fully materialized during inference. ",
        "bbox": [
            114,
            186,
            883,
            270
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Taming Muon with QK-ClipTo addressthis issue, we propose a novel weight-clipping mechanism QK-Clip to explicitly constrain attention logits. QK-Clip works by rescaling the query and key projection weights post-update to bound the growth of attention logits. ",
        "bbox": [
            114,
            284,
            883,
            327
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Let the inputrepresentationofa transformerlayerbeX.For eachatention head h,its query,key,andvalue projections are computed as ",
        "bbox": [
            117,
            332,
            880,
            359
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/d72557ba8443d6d706c9f6fc0047514c899cd172e021670ae6ca236b198802b8.jpg",
        "bbox": [
            343,
            359,
            655,
            378
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "where Wq,Wk,W are model parameters. The attention output is: ",
        "bbox": [
            112,
            380,
            562,
            395
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/719602b52e44978962407a3d37864a293552ca9f1de3c70276b95ce2dea8fb22.jpg",
        "bbox": [
            377,
            410,
            619,
            445
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We define the max logit, a per-head scalar, as the maximum input to softmax in this batch B: ",
        "bbox": [
            112,
            453,
            722,
            468
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/4e08a5922adc218df6f3da8401a9f8c2b1eac6d48949b9f50edaf7cb79cd7030.jpg",
        "bbox": [
            393,
            474,
            601,
            506
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "where i, j are indices of different tokens in a training sample X. ",
        "bbox": [
            114,
            511,
            534,
            526
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "to determine the strength to control the weight growth. ",
        "bbox": [
            114,
            530,
            883,
            574
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "A naive implementation clips all heads at the same time: ",
        "bbox": [
            114,
            579,
            486,
            594
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/ef605e7aa2d675020b1187c03c3e27e9926762f02f8d939ad796078731a1a93c.jpg",
        "bbox": [
            372,
            599,
            625,
            621
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "whereY= min(1,/Smax)with Smax = maxh Smax, and α is a balancing parameter typicaly settoO.5,applying equal scaling to queries and keys. ",
        "bbox": [
            112,
            626,
            882,
            656
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "However, we observe that in practice,only a small subset of heads exhibit exploding logits. In order to minimize our interventioegeteedaictoSadtaa QK-Clip. Such clipping is straightforward for regular multi-head atention (MHA).For MLA, we apply clippng only on unshared attention head components: ",
        "bbox": [
            116,
            661,
            883,
            718
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "· q and k℃ (head-specific components): each scaled by √Yh · qR (head-specific rotary): scaled by Yh; · kR (shared rotary): left untouched to avoid effect across heads. ",
        "bbox": [
            129,
            729,
            555,
            787
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "MuonClip: The New OptimizerWe integrate Muon with weight decay,consistent RMS matching,and QK-Clir into a single optimizer, which we refer to as MuonClip (see Algorithm 1). ",
        "bbox": [
            114,
            806,
            877,
            837
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We demonstrate the effctiveness of MuonClipfrom several scaling experiments.First, we train a mid-scale 9Bactivated and 53B total parameters Mixture-of-Experts (MoE)model using the vanilla Muon.As shown in Figure 2 (Left), we observe that the maximum attention logits quickly exceed amagnitude of 10o,showing that attentionlogits explosion is already evident in Muon training to this scale. Max logits at this level usuall result in instability during training, including significant loss spikes and occasional divergence. ",
        "bbox": [
            116,
            842,
            883,
            911
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Algorithm 1MuonClip Optimizer ",
        "text_level": 1,
        "bbox": [
            117,
            90,
            343,
            106
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "1: for each training step t do   \n2: // 1. Muon optimizer step   \n3: for each weight W ∈ Rn×m do   \n4: Mt =μMt-1+Gt Mo= O,Gt is the grad of Wt, μ is momentum   \n5: Ot = Newton-Schulz(Mt) : √max(n,m) · 0.2 Match Adam RMS   \n6: Wt = Wt-1-n(Ot +λWt-1) > learning rate η, weight decay 入   \n7: end for   \n8: // 2. QK-Clip   \n9: for each attention head h in every attention layer of the model do   \n10: Obtain Shax alreadycomputed during forward   \n11: 12: if Shax >Tthen /   \n13:   \n15: War←WrY   \n16: end if   \n17: end for   \n18:end for ",
        "bbox": [
            117,
            109,
            885,
            375
        ],
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/8d82493c3ffab724d6b7300272b2694747d50755dd65cc130720554b0d74edae.jpg",
        "image_caption": [
            "Figure 2: Left: During a mid-scale training run,attention logits rapidlyexceed 10o0, which could lead to potential numerical instabilities and even training divergence. Right: Maximum logits for Kimi K2 with MuonClipand T =100 over the entire training run.The max logits rapidly increase to the capped value of 10O,and onlydecay toa stable range after approximately 30% of the training steps, demonstrating the effective regulation effect of QK-Clip. "
        ],
        "image_footnote": [],
        "bbox": [
            119,
            398,
            879,
            566
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Next, we demonstrate that QK-Clip does not degrade model performance and confirm that the MuonClip optimizer preserves the optimization characteristics of Muon without adversely affcting the losstrajectory.A detailed discussion of the experiment designs and findings is provided in the Appendix D. ",
        "bbox": [
            114,
            667,
            883,
            710
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Finally,we train Kimi K2,alarge-scale MoE model,using MuonClip with T =1OOand monitorthe maximum attention logits throughout the training run (Figure 2 (Right). Initially,te logits are capped at 100 due to QK-Clip. Over the courseof training, the maximum logits gradually decay to a typical operating range without requiring any adjustment to T. Importantly,the training lossremains smoth and stable,with no observable spikes, as shown inFigure 3,validating that MuonClip provides robust and scalablecontrol over atention dynamics in large-scale language model training. ",
        "bbox": [
            116,
            715,
            882,
            785
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2.2Pre-training Data: Improving Token Utility with Rephrasing ",
        "text_level": 1,
        "bbox": [
            114,
            811,
            576,
            827
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Token efficiency in pre-training refers to how much performance improvement is achieved for each token consumed during training.Increasing token utility—theefective learning signal each token contributes—enhances the per-token impact on modelupdates,thereby directly improving token eficiency.This is particularly important when the supplyof high-quality tokens is limited and must be maximally leveraged. A naive approach to increasing token utilityis through repeated exposure to the same tokens, which can lead to overfitting and reduced generalization. ",
        "bbox": [
            116,
            842,
            882,
            911
        ],
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/416a12d17164cf967d9e15f7633c89a6edd8d0cd79093a56c9ffcb7a26177dd9.jpg",
        "image_caption": [
            "Figure 3: Per-step training losscurve of Kimi K2, without smoothing or sub-sampling. It shows no spikes throughou the entire training process. Note that we omit the very beginning of training for clarity. "
        ],
        "image_footnote": [],
        "bbox": [
            228,
            87,
            769,
            324
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "A key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction ofa syntheticdata generation strategy to increase token utility. Specifically,acarefulydesignedrephrasing pipelineis employed toamplifythe volume of high-quality tokens without inducing significant overfiting. In this report, we describe two domain-specialized rephrasing techniques—targeted respectively atthe Knowledge and Mathematics domains—that enable this controlled data augmentation. ",
        "bbox": [
            116,
            387,
            882,
            458
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Knowledge Data RephrasingPre-training on natural, knowledge-intensive text presents a trade-of: asingle epoch is insuffcient for comprehensive knowledge absorption, while multi-epoch repetition yields diminishing returns and increases the risk of overfiting.To improve the token utilityof high-quality knowledge tokens, we propose a synthetic rephrasing framework composed of the following key components: ",
        "bbox": [
            116,
            473,
            882,
            530
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "·Style-and perspective-diverse prompting: To enhance linguistic diversity while maintaining factual integrity, we apply a range of carefully engineered prompts. These prompts guide a large language model to generate faithful rephrasings of the original texts in varied styles and from different perspectives.   \nChunk-wise autoregressve generation: To preserve global coherence and avoid information loss in long documents, we adopt a chunk-based autoregressive rewriting strategy. Texts are divided into segments,rephrased individually,and then stitched back together to form complete passges.This method mitigates implicit output length limitations that typically exist with LLMs. An overview of this pipeline is presented in Figure 4.   \n·Fidelity verification: To ensure consistency between original and rewriten content, we perform fidelity checks that compare the semantic alignment of each rephrased passage with its source. This serves as an initial quality control step prior to training. ",
        "bbox": [
            125,
            542,
            885,
            694
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "We compare data rephrasing with multi-epoch repetition by testing their corresponding accuracy on SimpleQA. We experiment with an early checkpointof K2 and evaluate three training strategies: (1)repeating the original dataset for 10 epochs,(2)rephrasing the data once and repeating it for 10 epochs,and(3)rephrasing the data 10 times with a single training pass.As shown in Table 1,the accuracy consistently improves across these strategies,demonstrating the efficacy of ourrephrasing-based augmentation. We extended this method to other large-scale knowledge corpora and observed similarly encouraging results,and each corpora is rephrased at most twice. ",
        "bbox": [
            114,
            707,
            883,
            791
        ],
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/688739e4c4e3310eb971eb48a80dcc6259ed0d73212a9e50f939293195ba944d.jpg",
        "table_caption": [
            "Table 1: SimpleQA Accuracy under three rephrasing-epoch configurations "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td># Rephrasings</td><td>#Epochs</td><td>SimpleQA Accuracy</td></tr><tr><td>0 (raw wiki-text)</td><td>10</td><td>23.76</td></tr><tr><td>1</td><td>10</td><td>27.39</td></tr><tr><td>10</td><td>1</td><td>28.94</td></tr></table>",
        "bbox": [
            315,
            829,
            683,
            900
        ],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/e6ddf3c555f440dd80c5670e68cc2fecb68c1da89104460df1d594aa22c5fd1b.jpg",
        "image_caption": [
            "Figure 4: Auto-regressive chunk-wise rephrasing pipeline for long input excerpts. The input is split into smaller chunks with preserved context, rewriten sequentially,and then concatenated into a full rewritten passage. "
        ],
        "image_footnote": [],
        "bbox": [
            230,
            104,
            767,
            287
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Mathematics Data RephrasingTo enhance mathematical reasoning capabilities,we rewrite high-quality mathematical documents intoa“learning-note”style,following the methodology introduced in SwallowMath[15]. In addition, we increased data diversity by translating high-quality mathematical materials from other languages into English. ",
        "bbox": [
            117,
            375,
            883,
            417
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Although initial experiments with rephrased subsets of our datasets show promising results,the use of synthetic data as a strategy for continued scaling remains an active area of investigation. Key challenges include generalizing the approach to diverse source domains without compromising factual accuracy, minimizing halucinations and unintended toxicity, and ensuring scalability to large-scale datasets. ",
        "bbox": [
            114,
            424,
            882,
            479
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Pre-training Data OverallThe Kimi K2 pre-training corpus comprises 15.5 trillon tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics,and Knowledge. Most data procesing pipelines follow the methodologies outlined in Kimi K1.5 [35].For each domain,we performed rigorous correctness and qualityvalidation and designed targeted data experiments to ensure the curated dataset achieved both high diversity and effectiveness. ",
        "bbox": [
            116,
            496,
            882,
            565
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2.3Model Architecture ",
        "text_level": 1,
        "bbox": [
            117,
            583,
            290,
            598
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Kimi K2 is a1.04trillon-parameter Mixture-of-Experts (MoE)transformer model with 32 billion activated parameters. The architecture follows a similar design to DeepSeek-V3[10],employing Multi-head Latent Atention (MLA)[44] as the attention mechanism, with a model hidden dimension of 7i68 and an MoE expert hidden dimension of 2048.Our scaling law analysis reveals that continued increases in sparsity yield substantial performance improvements, which motivated us to increase the number of experts to 384,compared to 256 in DeepSeek-V3.To reduce computational overhead during inference, we cut the number of attention heads to 64,as opposed to 128 in DeepSeek-V3.Table 2 presents a detailed comparison of architectural parameters between Kimi K2 and DeepSeek-V3. ",
        "bbox": [
            114,
            609,
            883,
            707
        ],
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/6d86c7553fa98a90c298e06af776a5a5200111cca3f66b2f87e1647a659a1f5f.jpg",
        "table_caption": [
            "Table 2: Architectural comparison between Kimi K2 and DeepSeek-V3 "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>DeepSeek-V3</td><td>Kimi K2</td><td>△</td></tr><tr><td>#Layers</td><td>61</td><td>61</td><td>=</td></tr><tr><td>TotalParameters</td><td>671B</td><td>1.04T</td><td>↑54%</td></tr><tr><td>ActivatedParameters</td><td>37B</td><td>32.6B</td><td>↓13%</td></tr><tr><td>Experts (total)</td><td>256</td><td>384</td><td>↑ 50%</td></tr><tr><td>Experts Active per Token</td><td>8</td><td>8</td><td>=</td></tr><tr><td>Shared Experts</td><td>1</td><td>1</td><td>=</td></tr><tr><td>Attention Heads</td><td>128</td><td>64</td><td>↓50%</td></tr><tr><td>Number of Dense Layers</td><td>3</td><td>1</td><td>↓67%</td></tr><tr><td>Expert Grouping</td><td>Yes</td><td>No</td><td>1</td></tr></table>",
        "bbox": [
            274,
            746,
            725,
            898
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Sparsity Scaling LawWe develop a sparsity scaling law tailored for the Mixture-of-Experts (MoE) model family using Muon.Sparsity is defined as theratioof the total number of experts to the number of activated experts.Through carefully controlled small-scale experiments, we observe that—under a fixed number of activated parameters (i.e., constantFLOPs)—increasing the total numberof experts (i.e., increasing sparsity)consistently lowers both the training and validation loss,tereby enhancing overallmodel performance (Figure 5). Concretely,under the compute-optimal sparsity scaling law,achieving the same validation lossof1.5,sparsity48 reduces FLOPs by 1.69x,1.39x,and1.15x compard to sparsity levels 8,16,and 32,respectively.Though increasing sparsity leads to beter performance,this gain comes with increased infrastructure complexity. To balance model performance with cost, we adopt a sparsity of 48 for Kimi K2,activating 8 out of 384 experts per forward pass. ",
        "bbox": [
            114,
            90,
            883,
            215
        ],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/f460627aae4f3db62d682707412ad0dc1a54d3b8cc9779a5d0f4371cf6d773b6.jpg",
        "image_caption": [
            "Figure 5: Sparsity Scaling Law. Increasing sparsity leads to improved model performance. We fixed the number of activated experts to 8 and the number of shared experts to 1,and varied the total number of experts, resulting in models with different sparsity levels. "
        ],
        "image_footnote": [],
        "bbox": [
            122,
            236,
            477,
            439
        ],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/7ac8223ec720e4137141101427729b3f38723f57068fcc273246fc59caf28253.jpg",
        "image_caption": [
            "Figure 6: Scaling curves for models with number of attention heads equals to number of layers and their counterparts with doubled attention heads.Doubling the number of attention heads leads to a reduction in validation loss of approximately 0.5% to 1.2%. "
        ],
        "image_footnote": [],
        "bbox": [
            519,
            236,
            875,
            439
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Number of Attention HeadsDeepSeek-V3[10] sets the number of atention heads to roughly twice the number of model layers tobetterutilize memory bandwidth and enhance computational eficiency.However,as the context length increases,doubling the number of atention heads leads tosignificant inference overhead,reducing eficiencyat longer sequence lengths.This becomes a major limitation in agentic applications, where eficient long context processing is esential.For example,with a sequence length of 128k,increasing the numberof attntion heads from 64 to128,while keeping the total expert count fixed at 384,leads toan 83% increase in inference FLOPs.To evaluate the impact of this design, we conduct controlled experiments comparing configurations where the number of atention heads equals the number of layers against those with double number of heads,under varying training FLOPs. Under iso-token training conditions, we observe that doubling the atention heads yields only modest improvements in validation loss (ranging from 0.5% to 1.2%)across diffrent compute budgets (Figure 6). Given that sparsity48 alreadyoffers strong performance,the marginal gains from doubling atention heads do not justify the inference cost.Therefore we choose to 64 attention heads. ",
        "bbox": [
            114,
            550,
            883,
            717
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "2.4Training Infrastructure ",
        "text_level": 1,
        "bbox": [
            117,
            734,
            318,
            750
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "2.4.1 Compute Cluster ",
        "text_level": 1,
        "bbox": [
            116,
            761,
            285,
            776
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Kimi K2 was trained on a cluster equipped with NVIDIA H800 GPUs. Each node in the H800 cluster contains 2 TB RAM and 8 GPUs connected by NVLink and NVSwitch within nodes.Across diferent nodes,8×400 Gbps RoCE interconnects are utilized to facilitate communications. ",
        "bbox": [
            117,
            785,
            882,
            827
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "2.4.2Parallelism for Model Scaling ",
        "text_level": 1,
        "bbox": [
            117,
            844,
            372,
            859
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Training of large language models often progresses under dynamic resource availability. Instead of optimizing one paraelism strategy that's only applicable under specific amount of resources, we pursue a flexible strategy that allows Kimi K2 to be trained on any number of nodes that is amultiple of 32. Our strategy leverages a combination of 16-way ",
        "bbox": [
            116,
            869,
            882,
            911
        ],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/be86a106675bc2524b88846b22bad1f8925332eda973a2550316a633fe939ce5.jpg",
        "image_caption": [
            "Figure 7: Computation, communication and offloading overlapped in different PP phases. "
        ],
        "image_footnote": [],
        "bbox": [
            132,
            90,
            867,
            199
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Pipeline Paralelism (PP) with virtual stages [28,53,38,57,47,21],16-wayExpertParalelism (EP)[39],and ZeRO-: Data Parallelism [60]. ",
        "bbox": [
            117,
            251,
            877,
            280
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Under this setting,storing the model parameters in BF16 and their gradient accumulation buer inFP32 requires approximately 6 TB of GPU memory,distributed over a model-paralel group of 256 GPUs. Placement of optimizer states depends on the training configurations.When the total number of taining nodes islarge,the optimizer states are distributed,reducing its per-device memory footprint toanegligible level. When the total number oftraining nodes is small(e.g.,32), we can ofload some optimizer states to CPU. ",
        "bbox": [
            116,
            285,
            882,
            354
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "This approach allows us to reuse an identical parallelism configuration for both small- and large-scale experiments, while letting each GPU hold approximately 30 GB of GPU memory for allstates.The rest of the GPU memory are used for activations,as described in Sec.2.4.3.Such aconsistentdesign is important forresearch eficiency,as itsimplifies the system and substantially accelerates experimental iteration. ",
        "bbox": [
            116,
            362,
            882,
            417
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "EP communication overlap with interleaved 1F1B By increasing the number of warm-up micro-batches, we can overlap EP al-to-allcommunication with computation under the standard interleaved 1F1B schedule [21,53].In comparison,DualPipe[10] doubles the memory required for parameters and gradients,necessitating an increase in paralelism to compensate.Increasing PP introduces more bubbles,while increasing EP,as discussd below, incurs higher overhead.Theaditionalcosts are prohibitively high for training alarge model withover1 trilion parameters and thus we opted not to use DualPipe. ",
        "bbox": [
            116,
            438,
            882,
            521
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "However, interleaved 1F1B splits the model into more stages,introducing non-trivial PPcommunicationoverhead.To mitigate this cost, we decouple the weight-gradient computation from each micro-batch's backward pass and execute it in paralel with the corresponding PPcommunication. Consequently,all PPcommunications can be effectively overlapped except for the warm-up phase. ",
        "bbox": [
            116,
            527,
            882,
            583
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "SmallerEP sizeTo ensure fullcomputation-communication overlap during the 1F1B stage,the reduced attention computationtime in K2 (which has 64 atention heads compared to128 heads in DeepSeek-V3) necesstates minimizing the time of EPoperations.This is achieved by adopting the smallest feasible EP parallelization strategy,specifically EP= 16. Utilizing a smaler EP group also relaxes expert-balance constraints,alowing for near-optimal speed to be achieved without further tuning. ",
        "bbox": [
            116,
            602,
            882,
            672
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "2.4.3Activation Reduction ",
        "text_level": 1,
        "bbox": [
            117,
            693,
            313,
            707
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "After reserving space for parameters,gradient bufers,and optimizer states,the remaining GPU memoryon each device is insufcientto hold thefull MoE activations.To ensure theactivation memory fits within theconstraints,especially for the initial pipeline stages that accumulate the largest activations during the 1F1B warm-up phase,the following techniques are employed. ",
        "bbox": [
            116,
            718,
            882,
            773
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Selective recomputationRecomputation is applied to inexpensive, high-footprint stages,including LayerNorm, SwiGLU,and MLAup-projections [1O]. Additionally, MoE down-projectionsarerecomputed during training to further reduce activation memory. While optional, tis recomputation maintains adequate GPU memory, preventing crashes caused by expert imbalance in early training stages. ",
        "bbox": [
            116,
            792,
            883,
            849
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "FP8 storage for insensitive activationsInputs of MoE up-projections and SwiGLU are compressed to FP8-E4M3 in 1× 128 tiles with FP32 scales. Smal-scale experiments show no measurable lossincrease. Due to potential risks of performance degradation that we observed during preliminary study, we do not apply FP8 in computation. ",
        "bbox": [
            116,
            869,
            883,
            911
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Activation CPU offloadAll remaining activations are offoaded to CPU RAM. A copy engine is responsible for streaming the ofoad and onload,overlapping with both computation and communication kernels.During the 1F1B phase,we ofload the forward activations of the previous micro-batch while prefetching the backward activations of the next.The warm-up and cool-down phases are handled similarlyand the overallpatern is shown in Figure 7.Although offloading may slightly affect EPtraffic due to PCIe traffccongestion,our tests showthat EPcommunication remains fully overlapped. ",
        "bbox": [
            116,
            90,
            882,
            174
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "2.5Training recipe ",
        "text_level": 1,
        "bbox": [
            116,
            190,
            261,
            205
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "We pre-trained the model with a 4,096-token context window using the MuonClip optimizer (Algorithm 1) and the WSD learning rate schedule[25],processing atotal of 15.5Ttokens.The first 10T tokens were trained with aconstant learning rate of 2e-4 aftera500-step warm-up,folowed by 5.5Ttokens with a cosine decay from 2e-4 to2e-5.Weight decay was settoO.1 throughout,and the global batch size was held at 67M tokens.The overal training curve is shown in Figure 3. ",
        "bbox": [
            116,
            215,
            882,
            285
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Towards the end of pre-training,we conducted an annealing phase folowed by a long-context activation stage. The batch size was keptconstant at 67M tokens, while the learning rate was decayed from2e-5 to 7e-6. In this phase, the model was trained on 40 billion tokens with a 4k sequence length,followed byanadditional 6O billion tokens with a 32k sequence length. To extend the context window to 128k, we employed the YaRN method [55]. ",
        "bbox": [
            116,
            291,
            882,
            347
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "3Post-Training ",
        "text_level": 1,
        "bbox": [
            116,
            364,
            261,
            382
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "3.1Supervised Fine-Tuning ",
        "text_level": 1,
        "bbox": [
            117,
            395,
            321,
            410
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "We employ the Muon optimizer [33] inour post-training and recommend its use for fine-tuning with K2.This follows fromtheconclusion of our previous work [46] that a Muon-pre-trained checkpoint produces the best performance with Muon fine-tuning. ",
        "bbox": [
            116,
            420,
            882,
            463
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Weconstruct a large-scale instruction-tuning dataset spanning diverse domains, guided by two core principles: maximizing prompt diversity and ensuring high response quality.To this end, we develop asuite of data generation pipelines tailored todiferent task domains,eachutilizing a combination of humanannotation,prompt engineering,and verification proceses. We adopt K1.5[35]and other in-house domain-specialized expert models to generate candidate responses for various tasks,followed by LLMs or human-based judges to perform automated quality evaluation and filtering.Foragentic data,wecreate a datasynthesis pipeline to teach models tool-use capabilities through multi-step, interactive reasoning. ",
        "bbox": [
            116,
            469,
            883,
            566
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "3.1.1Large-Scale Agentic Data Synthesis for Tool Use Learning ",
        "text_level": 1,
        "bbox": [
            116,
            579,
            570,
            594
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "A criticalcapabilityof modernLLMagents is theirability toautonomouslyuseunfamiliar tools,interact with external environments,and iterativelyrefine their actions through reasoning,execution,and error correction.Agentic tooluse capability is essential for solving complex,multi-step tasks that require dynamic interaction with real-world systems. Recent benchmarks suchas ACEBench [6]and7-bench[85]have highlightedthe importance ofcomprehensive tool-use evaluation, while frameworks like ToolLLM [58] and ACEBench [6] have demonstrated the potential of teaching models to use thousands of tools effectively. ",
        "bbox": [
            116,
            603,
            883,
            686
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "However, training such capabilities at scale presents a significant challenge: while real-world environments provide rich and authentic interaction signals, theyare often difficult to construct atscaledue to cost, complexity, privacy and accesibility constraints.Recent work on synthetic data generation (AgentInstruct [51]; Self-Instruct[75]; StableToolBench [20]; ZeroSearch [66]) has shown promising results in creating large-scale data without relying on real-world interactions.Building on these advances and inspired by ACEBench [6]'s comprehensive data synthesis framework, we developed a pipeline that simulates real-world tool-use scenarios at scale,enabling the generation of tens of thousands of diverse and high-quality training examples. ",
        "bbox": [
            116,
            693,
            883,
            790
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "There are three stages in our data synthesis pipeline, depicted in Fig. 8. ",
        "bbox": [
            117,
            796,
            578,
            811
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "· Tool spec generation: we first construct a large repository of tool specs from both real-world tools and LLMsynthetic tools;   \n· Agent and task generation: for each tool-set sampled from the tool repository, we generate an agent to use the toolset and some corresponding tasks;   \n· Trajectory generation: for each agent and task, we generate trajectories where the agent finishes the task by invoking tools. ",
        "bbox": [
            129,
            820,
            885,
            911
        ],
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/42c5fcd325a6a599b88fe5df89980779b23a9f2df3e1ff2b47901e65cc4ff24d.jpg",
        "image_caption": [
            "Figure 8: Data synthesis pipelinefor tooluse. (a)Tool specs are from both real-world tools and LLMs; agents and tasks are the generated from the tool repo. (b)Multi-agent pipeline to generate and filter trajectories with tool calling. "
        ],
        "image_footnote": [],
        "bbox": [
            171,
            88,
            828,
            233
        ],
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/ff75df4ac8cf10c6033da9d18c4a9b832a38059f1282fb86c1ab9719f4a3d250.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            117,
            290,
            483,
            500
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "(a) t-SNE visualization of real MCP tools,colored by their original source categories ",
        "bbox": [
            114,
            507,
            485,
            532
        ],
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/85f0cce3db55afc21006ecbb1753b898b016e0d265a89fed11806831d89ea326.jpg",
        "image_caption": [
            "(b)t-SNE visualization of synthetic tools,colored by pre-defined domain categories "
        ],
        "image_footnote": [],
        "bbox": [
            514,
            286,
            880,
            500
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Figure 9:t-SNE visualizations of tool embeddings.(a)Real-world MCP tools exhibit natural clustering based on their original source categories. (b)Synthetic tools are organized into pre-defined domain categories,providing systematic coverage of the tool space.Together, they ensure comprehensive representation across diffrent tool functionalities. ",
        "bbox": [
            114,
            541,
            883,
            583
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Domain Evolution and Tool Generation.We construct a comprehensive tool repository through two complementary approaches.First,we directly fetch 3000+ real MCP (Model Context Protocol) tools from GitHub repositories, leveraging existing high-qualitytool specs.Second, we systematically evolve[82] synthetic tools through ahierarchical domain generation proces: we begin with key categories (e.g.,financial trading,software applications,robotcontrol), then evolve multiple specific application domains within each category.Specialized tools are then synthesizedfor each domain, with clear interfaces,descriptions,and operational semantics.This evolution proces produces over 20,000 synthetic tools.Figure9 visualizes the diversityofour tool colection through t-SNEembeddings,demonstrating that both MCP and synthetic tools cover complementary regions of the tool space. ",
        "bbox": [
            114,
            612,
            883,
            723
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Agent Diversification. We generate thousands of distinct agents by synthesizing various system prompts and equipping them with diferent combinations of tools from our repository.This creates a diverse population of agents with varied capabilities,areas of expertise,and behavioral paterns,ensuring a broad coverage of potential use cases. ",
        "bbox": [
            116,
            739,
            883,
            781
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Rubric-Based Task Generation.For each agent configuration, we generate tasks that range from simple to complex operations.Each task is paired with an explicit rubric that specifies success criteria, expected tool-use paterns,and evaluation checkpoints.This rubric-based approach ensures a consistent and objective evaluation of agent performance. ",
        "bbox": [
            116,
            797,
            883,
            840
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Multi-turn Trajectory Generation. We simulate realistic tool-use scenarios through several components: ",
        "bbox": [
            122,
            856,
            823,
            871
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "· User Simulation: LLM-generated user personas with distinct communication styles and preferences engage in multi-turn dialogues with agents, creating naturalistic interaction patterns. ",
        "bbox": [
            124,
            883,
            890,
            911
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "·Tool Execution Environment: A sophisticated tool simulator (functionall equivalent to a world model) executes tool cals and provides realistic feedback. The simulator maintains and updates state after each tool execution, enabling complex multi-step interactions with persistent effects. It introduces controlled stochasticity to produce varied outcomes including successes, partial failures, and edge cases. ",
        "bbox": [
            127,
            92,
            883,
            147
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Quality Evaluation and Filtering.An LLM-based judge evaluates each trajectory against the task rubrics. Only trajectories that meet the success criteria are retained for training,ensuring high-qualitydata while alowing natural variation in task-completion strategies. ",
        "bbox": [
            119,
            160,
            882,
            203
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Hybrid Approach with Real Execution Environments.While simulation provides scalability, we acknowledge the inherent limitation of simulation fidelity.To address this,we complement our simulated environments with real execution sandboxes forscenarios where authenticity is crucial,particularly incodingand software enginering tasks. These real sandboxes execute actual code, interact with genuine development environments,and provide ground-truth feedback through objective metrics such as test suite pass rates.This combination ensures that our models learn from both the diversityof simulated scenarios and the authenticity ofreal executions,significantly strengthening practical agent capabilities. ",
        "bbox": [
            116,
            217,
            882,
            314
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "By leveraging this hybrid pipeline that combines scalable simulation with targeted real-world execution, we generate diverse,high-qualitytool-use demonstrations that balance coverage and authenticity.The scale and automation of our synthetic data generation,coupled with the grounding provided byreal execution environments,fectively implements large-scale rejection sampling [26,87]through our qualityfltering process. This high-quality synthetic data, when used forsupervised fine-tuning,has demonstrated significant improvements inthe model'stol-usecapabilities across a wide range of real-world applications. ",
        "bbox": [
            116,
            320,
            882,
            404
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "3.2Reinforcement Learning ",
        "text_level": 1,
        "bbox": [
            117,
            419,
            325,
            434
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Reinforcement learning (RL)is believed to have beter token efciencyand generalization than SFT.Based on the work of K1.5[35], we continue to scale RL in both task diversity and training FLOPs in K2. To support this, we developa Gym-like extensible framework that facilitates RL acrossa wide range of scenarios.We extend the framework with a large number of tasks with verifiable rewards.For tasks that relyon subjective preferences,such as creative writing and open-ended question answering,we introduce a self-critic reward in which the model performs pairwise comparisons to judge its own outputs. This approach allows tasks from various domains to all benefit from the RL paradigm. ",
        "bbox": [
            116,
            444,
            882,
            527
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "3.2.1Verifiable Rewards Gym ",
        "text_level": 1,
        "bbox": [
            116,
            541,
            338,
            556
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Math,STEMand Logical TasksFor math,stem and logical reasoning domains,our RL data preparation folows two key principles, diverse coverage and moderate difficulty. ",
        "bbox": [
            119,
            565,
            879,
            594
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Diverse Coverage. For math and stem tasks, we colect high-quality QA pairs using a combination of expert annotations, internal QA extraction pipelines,and open datasets [41,52]. During the colection process, we leverage atagging system to deliberately increase coverage of under-covered domains.Forlogical tasks,our dataset comprises a variety of formats,including structured data tasks (e.g.,multi-hoptabular reasoning,crosstable aggregation)and logic puzzles (e.g., the 24-game, Sudoku, riddles, cryptarithms, and Morse-code decoding). ",
        "bbox": [
            116,
            599,
            882,
            670
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Moderate Diffculty. TheRL prompt-set should beneither tooeasy nor too hard,bothof which may produce litle signal and reduce learning eficiency. We assessthe difculty of each problem using the SFT model's pass@k accuracy and select only problems with moderate difficulty. ",
        "bbox": [
            116,
            675,
            882,
            718
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Complex Instruction FollowingEffective instruction following requires not only understanding explicit constraints butalso navigating implicit requirements, handling edge cases,and maintaining consistency over extended dialogues. We address these challnges through a hybrid verification framework that combines automated verification with adversarial detection, coupled with a scalable currculum generation pipeline. Our approach employs a dual-path system to ensure both precision and robustness: ",
        "bbox": [
            116,
            731,
            882,
            800
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Hybrid Rule Verification. We implement two verification mechanisms: (1) deterministic evaluation viacode interpreters for instructions with verifiable outputs (e.g.,length,styleconstraints),and (2)LLM-as-judge evaluation for instructions requiring nuanced understanding of constraints.To address potential adversarial behaviors where models might claim instruction fulfllment without actual compliance,we incorporate anadditional hack-check layer that specifically detects such deceptive claims. ",
        "bbox": [
            114,
            806,
            882,
            876
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Multi-Source Instruction Generation. To construct our raining data, we employ three distinct generation strategies to ensure comprehensive coverage: (1) expert-crafted complex conditional prompts and rubrics developed by our data team (2)agentic instruction augmentation inspiredby AutoIF[12],and (3)a fine-tuned model specialized for generating additional instructions that probe specific failure modes or edge cases.This multipronged approach ensures both breadth and depth in instruction coverage. ",
        "bbox": [
            112,
            883,
            885,
            911
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            116,
            90,
            883,
            133
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "FaithfulnessFaithfulness is esential for an agentic model operating in scenarios such as multi-turn tool use,selfgenerated reasoning chains,and open-environment interactions. Inspired by the evaluation framework from FACTS Grounding [3O], we train a sentence-level faithfulnessjudge model to perform automated verification. The judge is effective in detecting sentences that make a factual claim without supporting evidence in context. It serves as areward model to enhance overall faithfulness performance. ",
        "bbox": [
            114,
            148,
            882,
            218
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Coding & Software EngineeringTo enhance our capability in tackling competition-level programming problems, we gather problems and their judges from both open-source datasets [27,83] and synthetic sources. To ensure the diversity of the synthetic data and the correctnessof reward signals, we incorporate high-quality human-written unit tests retrieved from pre-training data. ",
        "bbox": [
            116,
            233,
            882,
            290
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "For software engineering tasks, we collect a vast amount of pull requests and issues from GitHub to build software development environment that consists of user prompts/isues and executable unit tests.This environment was built on a robust sandbox infrastructure,powered by Kubernetes for scalability andsecurity. It supports over 10,O0 concurrent sandbox instances with stable performance,making it ideal for both competitive coding and software engineering tasks. ",
        "bbox": [
            116,
            296,
            882,
            352
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Safety Our work to enhance the safety begins with a human-curated set of seed prompts, manually crafted to encompass prevalent risk categories such as violence,fraud,and discrimination. ",
        "bbox": [
            117,
            367,
            882,
            396
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "To simulate sophisticated jailbreak attempts (e.g.,role-playing,literarynarratives,and academic discourse),we employ an automated prompt evolution pipeline with three key components: ",
        "bbox": [
            114,
            401,
            879,
            431
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "· Attack Model: Iteratively generates adversarial prompts designed to elicit unsafe responses from the target LLM. · Target Model: Produces responses to these prompts,simulating potential vulnerabilities. · Judge Model: Evaluates the interaction to determine if the adversarial prompt successfully bypasses safety mechanisms. ",
        "bbox": [
            127,
            441,
            885,
            510
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Each interaction is assessed using atask-specific rubric,enabling the judge model to provide a binary succes/failure label. ",
        "bbox": [
            114,
            520,
            877,
            549
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "3.2.2Beyond Verification: Self-Critique Rubric Reward ",
        "text_level": 1,
        "bbox": [
            116,
            564,
            516,
            580
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "To extend model alignment beyond tasks with verifiable reward, we introduce aframework for general reinforcement learning from self-critic feedbacks. This approach is designed to align LLMs with nuanced human preferences, including helpfulnes,creativity,depthofreasoning,factuality,andsafety,by extending thecapabilities learned from verifiable scenarios to abroader range of subjective tasks.The framework operates using a Self-Critique Rubric Reward mechanism, where the model evaluates its own outputs to generate preference signals.To bootstrap K2 as a competent judge, we curated a mixture of open-sourceand in-house preference datasets and initialize its critic capability in the SFT stage. ",
        "bbox": [
            116,
            588,
            883,
            686
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Self-Critiqued Policy OptimizationIn the frst core processof the learning loop,the K2 actor generates responses for general prompts that cover a wide range of usecases.The K2 critic then ranks allresults by performing pairwise evaluations against acombination ofrubrics,which incorporates both core rubrics (Appendix.F.1),which represent the fundamental values of our AI assistant that Kimi cherish,prescriptive rubrics (Appendix.F.2)that aim to eliminate reward hacking,and human-annotated rubrics crafted by ourdata team for specific instructional contexts. Although certain rubrics canbe designatedas mandatory,K2 retains the flexibility to weigh them against its internal priors.This capacity enables a dynamic and continuous alignment with its evolving on-policy behavior, ensuring that the model's responses remain coherent with its core identity while adapting to specific instructions. ",
        "bbox": [
            116,
            700,
            882,
            813
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Closed-Loop Critic Refinement and AlignmentDuring RL training,the critic model is refined using verifiable signals.On-policyrolluts generated from verifiable-reward prompts areused tocontinuously update the critic,a crucial step that distils objective performance signals from RLVR directly into its evaluation model. This transfer learning process grounds its more subjective judgments in verifiable data,alowing the performance gains from verifiable tasks to enhance the critic's judgment on complex tasks that lack explicit reward signals.This closed-loop process ensures that the critic continuously recalibrates its evaluation standards in lockstep with the policy's evolution.By grounding subjective evaluation in verifiable data, the framework enables robust and scalable alignment with complex, non-verifiable human objectives. ",
        "bbox": [
            116,
            827,
            882,
            911
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            119,
            90,
            885,
            119
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Consequently, this holistic alignment yields comprehensive performance improvements acro a wide spectrum of domains,including user intent understanding,creative writing,complex reasoning,and nuanced language comprehension. ",
        "bbox": [
            119,
            126,
            885,
            155
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "3.2.3 RL Algorithm ",
        "text_level": 1,
        "bbox": [
            116,
            175,
            266,
            190
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "We adopt the policy optimization algorithm introduced in K1.5[35] as the foundation for K2.For each problem x, we sample K responses {y1,..,yk} from the previous policy Told, and optimize the model π with respect to the following objective: ",
        "bbox": [
            114,
            202,
            887,
            244
        ],
        "page_idx": 12
    },
    {
        "type": "equation",
        "img_path": "images/8c4503a4453095b43edc25a56dc1210fb1ef3f4e18c1384d4fe86fe70499b1bc.jpg",
        "bbox": [
            261,
            257,
            735,
            301
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "RL training to encompass a broader range of tasks in K2,a primary challenge is achieving consistent performance improvements across all domains. To address this,we introduce several additions to the RL algorithm. ",
        "bbox": [
            116,
            314,
            883,
            372
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Budget Control Ithasbeen widely observed thatRLoften results ina substantial increase in the length of modelgeneratedresponses [35,19]. While longer responses can enable the model to utilize additional test-time compute for improved performance on complex reasoning tasks,the benefits often do not justify its inference cost in non-reasoning domains. To encourage the model to properly distribute inference budget, we enforce a per-sample maximum token budget throughout RL training, where the budget is determined based on the type of task. Responses that exceed this token budget are truncated and assigned a penalty, which incentivizes the model to generate solutions within the specifiedlimit.Empiricaly,this approach significantlyenhances the model's token efciency,encouragingconcise yet effective solutions across all domains. ",
        "bbox": [
            114,
            393,
            883,
            506
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "PTX LossTo prevent the potential forgeting of valuable, high-quality data during joint RL training, we curate a datasetcomprising hand-selected,high-qualitysamples and integrate it intotheRLobjective throughanauxiliaryPTX loss[54].This strategy not only leverages the advantages ofhigh-qualitydata,but also mitigates therisk of overfitting to the limited setof tasks explicitly present in thetrainingregime.This augmentation substantiallyimproves the model's generalization across a broader range of domains. ",
        "bbox": [
            114,
            526,
            882,
            597
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Temperature DecayFor tasks such as creative writing and complex reasoning, we find that promoting exploration viaahigh sampling temperature during the initial stages of training is crucial. Ahigh temperature alow the model to generate diverse and inovative responses,therebyfacilitating the discovery of efective strategies andreducing the risk of premature convergence to suboptimal solutions. However,retaining a high temperature in the later stages of training or during evaluation can be detrimental,as it introduces excessive randomnessand compromises the reliability and consistencyof the model'soutputs.To addressthis, we employ atemperature decay schedule,to shift from exploration to exploitation throughout the training.This strategy ensures that the model leverages exploration when it is most beneficial, while ultimately converge on stable and high-quality outputs. ",
        "bbox": [
            114,
            617,
            882,
            729
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "3.3RL Infrastructure ",
        "text_level": 1,
        "bbox": [
            117,
            752,
            281,
            767
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "3.3.1 Colocated Architecture ",
        "text_level": 1,
        "bbox": [
            116,
            780,
            328,
            795
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Similar to K1.5[35], we adopt a hybrid colocated architecture forour synchronized RL training,where the training and inference engines live onthe same workers.When one engine is actively working,the other engine releases orofloads its GPU resources to accommodate. In each iteration of RL training,acentralizedcontrollerfrst calls the inference engine to generate new data for training.It then notifies the training engine to train on the new data,and send updated parameters to the inference engine for the next iteration. ",
        "bbox": [
            114,
            806,
            882,
            877
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Eachengine is heavilyoptimizedforthroughput. Inaddition,as the modelscales tothe sizeof K2,the latencyof engine switching and failure recovery becomes significant. We present our system design considerations in these aspects. ",
        "bbox": [
            112,
            882,
            879,
            911
        ],
        "page_idx": 12
    },
    {
        "type": "image",
        "img_path": "images/f81544226c8349e29c868f4b932ee613f9f0fc4937ab30489e942391116bb503.jpg",
        "image_caption": [
            "Figure 1O: Parameter update utilizing a checkpoint engine "
        ],
        "image_footnote": [],
        "bbox": [
            313,
            92,
            691,
            253
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "3.3.2Efficient Engine Switching ",
        "text_level": 1,
        "bbox": [
            117,
            310,
            351,
            325
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "During rollout,the parameters of the training engine are ofloaded to DRAM. Bringing up the training engine is therefore a simple step of H2D transmission. However, bringing up the inference engine is a bigger challenge,a it must obtain updated parameters from the training engine with a different sharding paradigm. ",
        "bbox": [
            116,
            334,
            883,
            377
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Given the scale of K2 and the vast number of devices involved,using a network file system for resharding and broadcasting parameters is impractical.The aggregate bandwidth required to keep overhead low reaches several petabytes persecond. To address this challenge,we developed a distributed checkpoint engine co-located on training nodes to manage parameter states. To perform a parameter update, each checkpoint engine worker obtains a local copy of parameters from the training engine,then broadcasts the full parameter set across all checkpoint engine workers. Subsequently,the inference engine retrieves only the parameter shard it requires from the checkpoint engine.This processis illustrated in Figure IO.To enable this fora 1T model, updates are performed parameter-by-parameter in a pipelined manner, minimizing memory footprint (see Appendix G). ",
        "bbox": [
            116,
            382,
            882,
            494
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "We optto broadcast the fullparameter set across the entire cluster,regardlessof the specific sharding schemes on each inference worker. While this transfers several times moredata thana theoreticallyoptimal approach,itoffersa simpler system design that is less intrusive to the training and inference engines.We choseto trade off this minor overhead to fully decouple the training engine and the inference engine,significantly simplifying maintenance and testing. ",
        "bbox": [
            116,
            500,
            882,
            556
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Notably,this approach outperforms the transfer-what-you-need method due to reduced synchronization overhead and higher network bandwidth utilization. Our system can complete a full parameter update for Kimi K2 with less than 30 seconds,a negligible duration for a typical RL training iteration. ",
        "bbox": [
            117,
            561,
            882,
            604
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "3.3.3Efficient System Startup ",
        "text_level": 1,
        "bbox": [
            116,
            619,
            336,
            633
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "As large-scale training is prone to system failure,optimizing the startuptime is crucial for models as large as Kimi K2. ",
        "bbox": [
            120,
            643,
            882,
            659
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "To start the training engine, we let each training worker selectively read part or none ofthe parameters from disk,and broadcast necessary parameters to its peers.The design goal is to ensure allworkers collectively read the checkpoint only once, minimizing expensive disk IO. ",
        "bbox": [
            116,
            664,
            883,
            705
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "As the nference engines are independent replicas, we would like to avoid introducing extra synchronization barriers between them.Therefore, we optto reuse checkpoint engine for startup: we let checkpoint engine colectively read the checkpoint from disk,similar to howthe training engine starts.Thenitupdates the state ofthe uninitialized inference engine,using the approach introduced in the previous section.By leveraging the dedicated checkpoint engine,the system also becomes robust to single-point failures,because an inference replica canrestart without communicating with other replicas. ",
        "bbox": [
            116,
            712,
            882,
            795
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "3.3.4Agentic Rollout ",
        "text_level": 1,
        "bbox": [
            117,
            810,
            276,
            825
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Our RLinfrastructure supports the training oflong-horizon,multi-turnagentic tasks.Duringrollout,these tasks present distinct challenges,such as complex environmental interactions and prolonged rollout durations. Here we introduce a few optimizations to alleviate these issues. ",
        "bbox": [
            116,
            835,
            883,
            876
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Due to the diversityof environments,certain interactions may be blocked on waiting for environment fedback (e.g., a virtual machine or a code interpreter),leaving the GPUs idle.We employtwo strategies to maximize GPUutilization: ",
        "bbox": [
            112,
            883,
            885,
            911
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "(i) we deploy heavy environments as dedicated services that can scale up more easily; (i) we employ alarge number of concurrent rollouts to amortize the latency induced by certain expensive interactions. ",
        "bbox": [
            116,
            90,
            885,
            119
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Another challenge inagentic rolout is that individual rollout trajectories can be extremelylong.To preventlong-tail trajectories from blockingthe entire rolout process,weemploythe partial rollout [35]technique.This strategy allows long-tail unfinished tasks to be paused,and resumed in the next RL iteration. ",
        "bbox": [
            116,
            126,
            882,
            167
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "To improve research eficiency, we also design aunified interface inspired by the OpenAI Gym framework [49]to streamline the integration of new environments.We hope to scale our RL infrastructure to more diverse interactive environments in the future. ",
        "bbox": [
            116,
            174,
            882,
            215
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4Evaluations ",
        "text_level": 1,
        "bbox": [
            116,
            237,
            245,
            255
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "This sectionbegins withthe post-training evaluationof Kimi-K2-Instruct,folowedbyabrief overviewofthecapabilities of Kimi-K2-Base. We conclude with a comprehensive safety evaluation. ",
        "bbox": [
            116,
            270,
            880,
            299
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4.1Post-training Evaluations ",
        "text_level": 1,
        "bbox": [
            117,
            318,
            331,
            332
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4.1.1Evaluation Setings ",
        "text_level": 1,
        "bbox": [
            117,
            344,
            302,
            359
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "BenchmarksWe assess Kimi-K2-Instruct across diferent areas.For coding, we adopt LiveCodeBench v6[31](questions from August2024 to May2025),OJBench[77],MultiPL-E[5],SWE-bench Verified[32,84],TerminalBench[71], Multi-SWE-bench [86],SWE-Lancer [50],PaperBench [65],and Aider-Polyglot[16].For tooluse tasks,weevaluate performanceon 72-Bench [3]and AceBench [6], which emphasize multi-turn tool-caling capabilities. Inreasoning, we include a wide range of mathematical,science and logical tasks: AIME 2024/2025,MATH-50o,HMMT 2025, CNMO 2024,PolyMath-en, ZebraLogic [43], AutoLogi [91], GPQA-Diamond [61],SuperGPQA[13],and Humanity's LastExam (Text-Only)[56]. We benchmark the long-context capabilities on: MRCR4 for long-context retrieval,and DROP[14], FRAMES[37]and LongBench v2[2] for long-context reasoning.For factuality, we evaluate FACTS Grounding [30], the Vectara HalucinationLeaderboard[73],andFaithJudge [68].Finally, general capabilities are assessed using MMLU[23],MMLU-Redux[17],MMLU-Pro[76],IFEval[90],Multi-Challenge [64],SimpleQA[78], and LiveBench [80] (as of 2024-11-25). ",
        "bbox": [
            114,
            368,
            883,
            521
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "BaselinesWe benchmark against both open-source and proprietary frontier models,ensuring every candidate is evaluated under its non-thinking configuration to eliminate additional gains from test-time compute. Open-source baselines: DeepSeek-V3-0324 and Qwen3-235B-A22B, with the later run in the vendor-recommended no-thinking regime. Proprietary baselines: Claude Sonnet 4,Claude Opus 4,GPT-4.1,and Gemini 2.5Flash Preview (2025-05-20). Each invoked in its respective non-thinking mode via offcial APIs under unified temperature and top-p setigs. ",
        "bbox": [
            116,
            539,
            883,
            608
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Evaluation Configurations All runs query models in their non-thinking mode.Output token length is capped at 8192 tokens everywhere except SWE-bench Verified (Agentless), which is raised to 16384.For benchmarks with high per-question variance,we adopt repeated sampling k times and average the results to obtain stable scores,denoted as Avg@k.For long-context tasks, we set the context window size to 128K tokens during evaluation,truncating any input that exceeds this limito fit within the window. SWE-bench Verified is evaluated in two modes: Agentless Coding via Single Patch without Test (Acc)and Agentic Coding via bash/editor tools under both Single Atempt (Acc) and Multiple Atempts (Acc)usingbest-of-Nselection with an internal verifier; SWE-bench Multilingual is tested only in the single-attempt agentic seting.Some data points have been omited due to prohibitively expensive evaluation costs. ",
        "bbox": [
            116,
            614,
            882,
            726
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4.1.2Evaluation Results ",
        "text_level": 1,
        "bbox": [
            117,
            742,
            297,
            757
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "A comprehensive evaluation results of Kimi-K2-Instruct is shown in Table 3, with detailed explanation provided in the Appendix C. Below, we highlight key results across four core domains: ",
        "bbox": [
            114,
            767,
            883,
            795
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Agentic and Competitive Coding Kimi-K2-Instruct demonstrates state-of-the-art open-source performance on real-world SWE tasks. It outperforms most baselines on SWE-bench Verified (65.8%,71.6% with multiple attemps), SWE-bench Multilingual (47.3%),and SWE-lancer (39.1%),significantly closing the gap with Claude 4 Opus and Sonnet. On competitive codingbenchmarks (e.g.,LiveCodeBench v6 53.7%,OJBench 27.1%),it also leads amongall models, highlighting its practical coding proficiency across difficulty levels. ",
        "bbox": [
            116,
            813,
            882,
            882
        ],
        "page_idx": 14
    },
    {
        "type": "table",
        "img_path": "images/e43a5ae275002f0e0480347061e514df1f94fbe3526a9abc544f3169a2507a0c.jpg",
        "table_caption": [
            "Table 3: Performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across diverse tasks.Bold denotes the global SOTA; underlined bold indicates the best open-source result.Data points marked with \\* are taken directly from the model's technical report or blog. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td colspan=\"3\">Open Source</td><td colspan=\"4\">Proprietary</td></tr><tr><td>Benchmark</td><td>Kimi-K2- Instruct</td><td>DeepSeek- V3-0324</td><td>Qwen3- 235B-</td><td>Claude Sonnet 4</td><td>Claude Opus 4</td><td>GPT-4.1</td><td>Gemini 2.5 Flash</td></tr><tr><td colspan=\"8\">Coding Tasks</td></tr><tr><td>LiveCodeBench v6 (Pass @ 1)</td><td>53.7</td><td>46.9</td><td>37.0</td><td>48.5</td><td>47.4</td><td>44.7</td><td>44.7</td></tr><tr><td>OJBench (Pass @1)</td><td>27.1</td><td>24.0</td><td>11.3</td><td>15.3</td><td>19.6</td><td>19.5</td><td>19.5</td></tr><tr><td>MultiPL-E (Pass @1)</td><td>85.7</td><td>83.1</td><td>78.2</td><td>88.6</td><td>89.6</td><td>86.7</td><td>85.6</td></tr><tr><td>SWE-bench Verified Agentless-Single-Patch (Pass@1)</td><td>51.8</td><td>36.6</td><td>39.4</td><td>50.2</td><td>53.0</td><td>40.8</td><td>32.6</td></tr><tr><td>SWE-bench Verified</td><td>65.8</td><td>38.8</td><td>34.4</td><td>72.7*</td><td>72.5*</td><td>54.6</td><td>1</td></tr><tr><td>Agentic-Single-Attempt (Pass @1) SWE-bench Verified</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Agentic-Multi-Attempt (Pass @1) SWE-bench Multilingual (Pass @ 1)</td><td>71.6</td><td>一</td><td>1 20.9</td><td>80.2*</td><td>79.4*</td><td>1 31.5</td><td></td></tr><tr><td>Multi-SWE-bench (Pass @ 1)</td><td>47.3 18.3</td><td>25.8 8.0</td><td>9.0</td><td>51.0 29.2</td><td></td><td>11.7</td><td>14.0</td></tr><tr><td>SWE-Lancer (Pass @ 1)</td><td>39.1</td><td>30.5</td><td>24.1</td><td>40.8</td><td></td><td>23.0</td><td>38.5</td></tr><tr><td>Paper Bench Code-Dev (Acc.)</td><td>27.8</td><td>12.2</td><td>13.2</td><td>43.3</td><td></td><td>29.9</td><td>5.7</td></tr><tr><td>Terminal Bench In-House (Acc.)</td><td>30.0</td><td></td><td></td><td>35.5</td><td>43.2</td><td>8.3</td><td></td></tr><tr><td>Terminal Bench Terminus (Acc.)</td><td>25.0</td><td>16.3</td><td>6.6</td><td></td><td></td><td>30.3</td><td>16.8</td></tr><tr><td>Aider-Polyglot (Acc.)</td><td>60.0</td><td>55.1</td><td>61.8</td><td>56.4</td><td>70.7</td><td>52.4</td><td>44.0</td></tr><tr><td colspan=\"8\">Tool Use Tasks</td></tr><tr><td>Tau2 retail (Avg@4)</td><td>70.6</td><td>69.1</td><td>57.0</td><td>75.0</td><td>81.8</td><td>74.8</td><td>64.3</td></tr><tr><td>Tau2 airline (Avg @4)</td><td>56.5</td><td>39.0</td><td>26.5</td><td>55.5</td><td>60.0</td><td>54.5</td><td>42.5</td></tr><tr><td>Tau2 telecom (Avg@4)</td><td>65.8</td><td>32.5</td><td>22.1</td><td>45.2</td><td>57.0</td><td>38.6</td><td>16.9</td></tr><tr><td>AceBench (Acc.)</td><td>76.5</td><td>72.7</td><td>70.5</td><td>76.2</td><td>75.6</td><td>80.1</td><td>74.5</td></tr><tr><td colspan=\"8\">Math&amp; STEM Tasks</td></tr><tr><td>AIME 2024 (Avg@64)</td><td>69.6</td><td>59.4*</td><td>40.1*</td><td>43.4</td><td>48.2</td><td>46.5</td><td>61.3</td></tr><tr><td>AIME 2025 (Avg@64)</td><td>49.5</td><td>46.7</td><td>24.7*</td><td>33.1*</td><td>33.9*</td><td>37.0</td><td>46.6</td></tr><tr><td>MATH-500 (Acc.)</td><td>97.4</td><td>94.0*</td><td>91.2*</td><td>94.0</td><td>94.4</td><td>92.4</td><td>95.4</td></tr><tr><td>HMMT 2025 (Avg@32)</td><td>38.8</td><td>27.5</td><td>11.9</td><td>15.9</td><td>15.9</td><td>19.4</td><td>34.7</td></tr><tr><td>CNMO 2024 (Avg@16)</td><td>74.3</td><td>74.7</td><td>48.6</td><td>60.4</td><td>57.6</td><td>56.6</td><td>75.0</td></tr><tr><td>PolyMath-en (Avg@4)</td><td>65.1</td><td>59.5</td><td>51.9</td><td>52.8</td><td>49.8</td><td>54.0</td><td>49.9</td></tr><tr><td>ZebraLogic (Acc.)</td><td>89.0</td><td>84.0</td><td>37.7*</td><td>79.7</td><td>59.3</td><td>58.5</td><td>57.9</td></tr><tr><td>AutoLogi (Acc.)</td><td>89.5</td><td>88.9</td><td>83.3*</td><td>89.8</td><td>86.1</td><td>88.2</td><td>84.1</td></tr><tr><td>GPQA-Diamond (Avg @ 8)</td><td>75.1</td><td>68.4*</td><td>62.9*</td><td>70.0*</td><td>74.9*</td><td>66.3</td><td>68.2</td></tr><tr><td>SuperGPQA (Acc.)</td><td>57.2</td><td>53.7</td><td>50.2</td><td>55.7</td><td>56.5</td><td>50.8</td><td>49.6</td></tr><tr><td>Humanity&#x27;s Last Exam (Acc.)</td><td>4.7</td><td>5.2</td><td>5.7</td><td>5.8</td><td>7.1</td><td>3.7</td><td>5.6</td></tr><tr><td colspan=\"8\">General Tasks</td></tr><tr><td>MMLU (EM)</td><td>89.5</td><td>89.4</td><td>87.0</td><td>91.5</td><td>92.9</td><td>90.4</td><td>90.1</td></tr><tr><td>MMLU-Redux (EM)</td><td>92.7</td><td>90.5</td><td>89.2*</td><td>93.6</td><td>94.2</td><td>92.4</td><td>90.6</td></tr><tr><td>MMLU-Pro (EM)</td><td>81.1</td><td>81.2*</td><td>77.3</td><td>83.7</td><td>86.6</td><td>81.8</td><td>79.4</td></tr><tr><td>IFEval (Prompt Strict)</td><td>89.8</td><td>81.1</td><td>83.2*</td><td>87.6</td><td>87.4</td><td>88.0</td><td>84.3</td></tr><tr><td>Multi-Challenge (Acc.)</td><td>54.1</td><td>31.4</td><td>34.0</td><td>46.8</td><td>49.0</td><td>36.4</td><td>39.5</td></tr><tr><td>SimpleQA (Correct)</td><td>31.0</td><td>27.7</td><td>13.2</td><td>15.9</td><td>22.8</td><td>42.3</td><td>23.3</td></tr><tr><td>Livebench (Pass @1) Arena Hard v2.0</td><td>76.4</td><td>72.4</td><td>67.6</td><td>74.8</td><td>74.6</td><td>69.8</td><td>67.8</td></tr><tr><td>Hard Prompt (Win rate)</td><td>54.5</td><td>39.9</td><td>39.9</td><td>51.6</td><td>59.7</td><td>51.7</td><td>48.7</td></tr><tr><td>Arena Hard v2.0</td><td>85.0</td><td>59.3</td><td>59.8</td><td>54.6</td><td>68.5</td><td>61.5</td><td>72.8</td></tr><tr><td>Creative Writing (Win rate) FACTS Grounding (Adjusted)</td><td>88.5</td><td>68.3</td><td></td><td></td><td></td><td>79.2</td><td>86.6</td></tr><tr><td>HHEM v2.1(1-Hallu.)</td><td>98.9</td><td>88.9</td><td>68.5 94.5</td><td>83.6 94.5</td><td></td><td>96.7</td><td>97.8</td></tr><tr><td>FaithJudge (1-Hallu.)</td><td>92.6</td><td>83.4</td><td>75.7</td><td>83.0</td><td></td><td>91.0</td><td>93.2</td></tr><tr><td>LongBench v2 (Acc.)</td><td>49.1</td><td>51.1</td><td></td><td>52.5</td><td></td><td>54.3</td><td>55.5</td></tr><tr><td>FRAMES (Acc.)</td><td>77.1</td><td>79.2</td><td></td><td>76.3</td><td></td><td>87.4</td><td>72.9</td></tr><tr><td>MRCR (Acc.)</td><td>55.0</td><td>50.8</td><td></td><td>74.4</td><td></td><td>66.9</td><td>81.7</td></tr><tr><td>DROP (Acc.)</td><td>93.5</td><td>91.2</td><td>84.3</td><td>92.0</td><td></td><td>79.1</td><td>81.7</td></tr></table>",
        "bbox": [
            119,
            133,
            877,
            926
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Agentic Tool UseOn multi-turn tol-use benchmarks,Kimi-K2-Instruct setsanew standard.It achieves 66.1Pass@1 on T2-Bench and 76.5on ACEBench,substantiallyoutperforming allbaselines.These results affrm its strength in grounded, controlled,and agent-driven tool orchestration across domains. ",
        "bbox": [
            116,
            90,
            883,
            132
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "General CapabilitiesKimi-K2-Instruct exhibits strong,balanced performance across general knowledge,math, instruction following,and long-context tasks. Itsurpasss open-source peers on SimpleQA (31.0%),MMLU (89.5%) and MMLU-Redux (92.7%),and leads all models on instruction benchmarks (IFEval: 89.8%,Multi-Challenge:54.1%). In math and STEM,it achieves top-tier scores (AIME 2024: 69.6%,GPQA-Diamond: 75.1%),and remains competitive on long-context factuality and retrieval (DROP: 93.5%,MRCR: 55.0%). These results position Kimi-K2-Instruct as a well-rounded and capable generalist across both short- and long-context settings. ",
        "bbox": [
            116,
            150,
            883,
            234
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Open-Ended EvaluationOn the LMSYS Arena leaderboard (July17,2025), Kimi-K2-Instruct ranks as the top-1 open-source model and 5th overallbased on over 3,O0O user votes.This real-world preference signal-acrossdiverse, blind prompts—underscores Kimi-K2's strengths in generating high-quality responses on open-ended tasks. ",
        "bbox": [
            116,
            251,
            883,
            294
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "4.2Pre-training Evaluations ",
        "text_level": 1,
        "bbox": [
            117,
            313,
            325,
            327
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "4.2.1Evaluation Setings ",
        "text_level": 1,
        "bbox": [
            116,
            338,
            302,
            353
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "BenchmarksWe evaluate Kimi-K2-Base across diverse capability areas.For general capabilities, we assess on MMLU [23], MMLU-Pro [76], MMLU-Redux [17], BBH [67],TriviaQA [34],SuperGPQA[13],SimpleQA[78], HellaSwag [88],AGIEval[89],GPQA-Diamond[61],ARC-Chalenge [8],and WinoGrande[62].Forcodingcapabilities, we employ EvalPlus [45] (averaging HumanEval[7],MBPP[1],HumanEval+,and MBPP+),LiveCodeBench v6 [31], and CRUXEval [18]. For mathematical reasoning,we utilize GSM8K[9], GSM8K-Platinum [74],MATH[24],and CMATH[79]. For Chinese language capabilities, we evaluate on C-Eval [29], CMMLU [40],and CSimpleQA [22]. ",
        "bbox": [
            114,
            363,
            883,
            446
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "BaselinesWe benchmark against leading open-source foundation models: DeepSeek-V3-Base[10], Qwen2.5-72BBase [59](Note that Qwen3-235B-A22B-Base is not open-sourced,and the largest open-sourced base model in the Qwen series is Qwen2.5-72B-Base),and Llama 4-Maverick [70](Llama 4-Behemoth is also not open-sourced).All models are evaluated under identical configurations to ensure fair comparison. ",
        "bbox": [
            116,
            464,
            883,
            520
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Evaluation ConfigurationsWe employ perplexity-based evaluation for MMLU,MMLU-Redux, GPQA-Diamond, HellaSwag, ARC-Challenge, C-Eval,and CMMLU.Generation-based evaluation is used for MMLU-ProSuperGPQA, TriviaQA,BBH, CSimpleQA, MATH, CMATH, GSM8K, GSM8K-Platinum, CRUXEval, LiveCodeBench,and EvalPlus.To mitigate the high variance inherent to GPQA-Diamond,wereport the meanscore acrosseight independent runs. All evaluationsareconducted using our internal framework derived fromLM-Harness-Evaluation[4],ensuring consistent settings across all models. ",
        "bbox": [
            116,
            537,
            883,
            621
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "4.2.2Evaluation Results ",
        "text_level": 1,
        "bbox": [
            116,
            638,
            297,
            652
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Table 4 presents a comprehensive comparison of Kimi-K2-Base against leading open-source foundation models across diverse evaluation benchmarks.The results demonstrate that Kimi-K2-Base achieves state-of-the-art performance across the majority of evaluated tasks, establishing it as a leading foundation model in the open-source landscape. ",
        "bbox": [
            117,
            662,
            882,
            705
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "General Language UnderstandingKimi-K2-Base achieves state-of-the-art performance on 10 outof 12 English language benchmarks.Notable results include MMLU (87.79%), MMLU-Pro (69.17%), MMLU-Redux (90.17%), SuperGPQA (44.67%), and SimpleQA (35.25%), significantly outperforming all baselines. ",
        "bbox": [
            117,
            722,
            883,
            765
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Coding CapabilitiesOn coding benchmarks,Kimi-K2-Base sets new standards with leading performance acrossall metrics. It achieves 74.00% on CRUXEval-I-cot,83.50% on CRUXEval-O-cot,26.29% on LiveCodeBench v6,and 80.33% on EvalPlus,demonstrating superior code generation and comprehension abilities, particularly in scenarios requiring step-by-step reasoning. ",
        "bbox": [
            116,
            781,
            883,
            838
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Mathematical ReasoningKimi-K2-Base exhibits exceptional mathematical capabilities,leading on thr out of four benchmarks: MATH (70.22%), GSM8K (92.12%),and GSM8K-Platinum (94.21%). It maintains competitive performance on CMATH(90.26%), narrowly behind DeepSeek-V3-Base (90.53%).These results highlight the model's robust mathematical problem-solving abilities across varying difficulty levels. ",
        "bbox": [
            116,
            856,
            882,
            911
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Chinese Language UnderstandingThe model demonstrates superior multilingualcapabilities,achieving state-of-theart results across al Chinese language benchmarks: C-Eval (92.50%), CMMLU (90.90%),and CSimpleQA (77.57%). Theseresults establish Kimi-K2-Base as a leading model for Chinese language understanding while maintaining strong performance across other languages. ",
        "bbox": [
            116,
            90,
            883,
            147
        ],
        "page_idx": 17
    },
    {
        "type": "table",
        "img_path": "images/719cf129a0a8014e1a93a0076c52a2f25ec1c0622ebcc27d2542cf1e9b2e45a1.jpg",
        "table_caption": [
            "Table 4: Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"7\">Benchmark (Metric) |#Shots |Kimi-K2-Base|DeepSeek-V3-Base Llama4-Maverick-Base Qwen2.5-72B-Base</td></tr><tr><td></td><td>Architecture # Activated Params # Total Params</td><td>- - 1</td><td>MoE 32B</td><td>MoE 37B</td><td>MoE 17B</td><td>Dense 72B</td></tr><tr><td rowspan=\"9\">English TriviaQA</td><td rowspan=\"9\">MMLU MMLU-pro MMLU-redux SuperGPQA SimpleQA</td><td rowspan=\"9\">5-shots</td><td>1043B 87.79</td><td>671B</td><td>400B</td><td>72B</td></tr><tr><td>|5-shots 5-shots</td><td>87.10</td><td>84.87</td><td>86.08</td></tr><tr><td>69.17 90.17</td><td>60.59 89.53</td><td>63.47</td><td>62.80 87.77</td></tr><tr><td>5-shots</td><td>44.67</td><td></td><td>88.18</td></tr><tr><td>5-shots</td><td>39.20 50.51</td><td>38.84</td><td>34.23</td></tr><tr><td>GPQA-Diamond(avg@8)</td><td>48.11 26.49</td><td>49.43</td><td>40.78</td></tr><tr><td>5-shots 5-shots</td><td>35.25</td><td>23.74</td><td>10.31</td></tr><tr><td>3-shots</td><td>85.09 88.71</td><td>84.11 79.25 88.37</td><td>76.03</td></tr><tr><td>BBH HellaSwag</td><td>94.60</td><td>87.10 89.44 86.02</td><td>84.09</td></tr><tr><td>AGIEval</td><td>5-shots =</td><td>84.23</td><td>81.57</td><td>67.55</td><td>95.27 76.87</td></tr><tr><td>ARC-Challenge</td><td>O-shot</td><td>95.73</td><td>93.77</td><td>94.03</td><td>95.56</td></tr><tr><td>WinoGrande</td><td>5-shots</td><td>85.32</td><td>84.21</td><td>77.58</td><td>84.14</td></tr><tr><td>CRUXEval-I-cot CRUXEval-O-cot</td><td>|0-shots</td><td>74.00</td><td>62.75</td><td>67.13</td><td>61.12</td></tr><tr><td rowspan=\"3\">Code</td><td></td><td>O-shots</td><td>83.50</td><td>75.25 75.88</td><td>66.13</td></tr><tr><td>LiveCodeBench(v6) EvalPlus</td><td>1-shots</td><td>26.29</td><td>24.57</td><td>25.14 22.29</td></tr><tr><td>MATH</td><td>=</td><td>80.33</td><td>65.61</td><td>65.48 66.04</td></tr><tr><td rowspan=\"4\">Math</td><td>GSM8k</td><td>4-shots</td><td>70.22 92.12</td><td>61.70</td><td>63.02</td><td>62.68</td></tr><tr><td>GSM8k-platinum</td><td>8-shots</td><td></td><td>91.66</td><td>86.35</td><td>90.37</td></tr><tr><td>CMATH</td><td>8-shots</td><td>94.21</td><td>93.38</td><td>88.83</td><td>92.47</td></tr><tr><td></td><td>6-shots</td><td>90.26</td><td>90.53</td><td>88.07</td><td>86.98</td></tr><tr><td rowspan=\"4\"></td><td>C-Eval</td><td>|5-shots</td><td>92.50</td><td>90.04</td><td>80.91</td><td>90.86</td></tr><tr><td>Chinese CMMLU</td><td>5-shots</td><td>90.90</td><td>88.84</td><td>81.24</td><td>90.55</td></tr><tr><td>CSimpleQA</td><td>5-shots</td><td>77.57</td><td>72.13</td><td>53.47</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>50.53</td></tr></table>",
        "bbox": [
            125,
            190,
            870,
            573
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "4.3Safety Evaluation ",
        "text_level": 1,
        "bbox": [
            116,
            608,
            279,
            623
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "4.3.1 Experiment Settings ",
        "text_level": 1,
        "bbox": [
            116,
            636,
            308,
            651
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "We conducted red-teaming evaluations on Kimi K2 compare with other open-source LLMs.The evaluation covered a range of atack scenarios—including harmful content, privacy content,and securitycontent,as wellasdiferent attack strategies such as prompt injection and iterative jailbreak. ",
        "bbox": [
            114,
            662,
            883,
            704
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "We choose Promptfoo to generate adversarial prompts and analyze the responses. By this way, we can evaluate modei in a scalable ways. ",
        "bbox": [
            116,
            710,
            879,
            739
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Model Selection We compare Kimi K2 with three other open-source LLMs: DeepSeek-V3, DeepSeek-R1,and Qwen3 ",
        "bbox": [
            114,
            744,
            879,
            761
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Promptfoo Settings Table 5lists plugins and strategies evaluated, with each plugin paired with allstrategies to assess their performance. ",
        "bbox": [
            112,
            765,
            879,
            795
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Test Case Count Given the inherent non-determinism of large language model inference, single-ass outputs may exhibit variability. To account for this, we generated 3 attack prompts per plugin for each strategy. ",
        "bbox": [
            112,
            800,
            879,
            829
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Prompt Language Setings We pre-tested the language compatibility for each plugin-strategy combination. Some plugins support both English and Chinese,while others only support English.For combinations that support both,we generated 3 prompts in each language,resulting in 6 prompts per combination. ",
        "bbox": [
            116,
            835,
            883,
            877
        ],
        "page_idx": 17
    },
    {
        "type": "table",
        "img_path": "images/afc505c0dedaf38b6d3dc79a3e21915fcaccaaae72127a7904bce5521e82bb2f.jpg",
        "table_caption": [
            "Table 5: Enabled Plugins and Strategies "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=5 colspan=1>Plugin</td><td rowspan=1 colspan=1>Harmful</td><td rowspan=1 colspan=1>Graphic Content, Harassment and Bullying,Hate Speech,Insults,Profanity,Radicalization, SelfHarm, Sexual Content, ToxicChat</td></tr><tr><td rowspan=1 colspan=1>Criminal</td><td rowspan=1 colspan=1>Chemical&amp;Biological Weapons, Child Exploitation, Copyright Violations,Cybercrime, IllgalActivities,Illegal Drugs,Indiscriminate Weapons,Intellectual Property Violation,Non-ViolentCrime, Violent Crime,Sex Crimes</td></tr><tr><td rowspan=1 colspan=1>Misinformation</td><td rowspan=1 colspan=1>Competitor Endorsement, Unsupervised Contracts,Excessive Agency,Hallucination,Misin-formation and Disinformation, Specialized Advice,Unsafe Practices,Imitation,Overreliance,Political Opinions,Religious Sensitivity</td></tr><tr><td rowspan=1 colspan=1>Privacy</td><td rowspan=1 colspan=1>Privacy Violation,PII inAPI/Database,DirectPII Exposure,PIIin Session Data,PII via SocialEngineering</td></tr><tr><td rowspan=1 colspan=1>Security</td><td rowspan=1 colspan=1>ASCII Smuggling, CyberSecEval, Harmbench,Debug Access, Divergent Repetition, DoNotAn-swer,Malicious Code,Pliny,Prompt Extraction, Reasoning DoS,Tool Discovery</td></tr><tr><td rowspan=1 colspan=1>Strategy</td><td rowspan=1 colspan=2>Basic,Prompt Injection, Iterative Jailbreak,Crescendo</td></tr></table>",
        "bbox": [
            137,
            112,
            859,
            294
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Manual Review We incorporated human review into the evaluation process. To minimize subjectivity problem, we conducted multiple rounds of review and assigned the same reviewer to evaluate allcases within a given test et to ensure consistency and reduce variability in judgment. ",
        "bbox": [
            116,
            321,
            882,
            364
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "4.3.2Safety Evaluation Results ",
        "text_level": 1,
        "bbox": [
            117,
            381,
            346,
            396
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Table 6 presents the passing rates of diferent models under various plugin-strategy combinations. ",
        "bbox": [
            112,
            405,
            754,
            421
        ],
        "page_idx": 18
    },
    {
        "type": "table",
        "img_path": "images/39819a066575dc90359df54e42c4d08ae11e0a92ae0482442bc7492864e84b34.jpg",
        "table_caption": [
            "Table 6: Safety Evaluation Results "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Plugin Strategy</td><td></td><td colspan=\"4\">Kimi-K2-Instruct|DeepSeek-V3-0324 DeepSeek-R1 ( Qwen3-235B-A22B</td></tr><tr><td rowspan=\"5\">Harmful</td><td>Basic</td><td>98.04</td><td>90.45</td><td>99.02</td><td>98.53</td></tr><tr><td>Base64</td><td>100</td><td>90.20</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>93.14</td><td>100</td><td>95.10</td><td>99.02</td></tr><tr><td>Iterative Jailbreak</td><td>92.16</td><td>66.67</td><td>72.55</td><td>74.51</td></tr><tr><td>Crescendo</td><td>64.71</td><td>64.71</td><td>80.39</td><td>86.27</td></tr><tr><td rowspan=\"5\">Criminal</td><td>Basic</td><td>100</td><td>99.62</td><td>95.45</td><td>99.24</td></tr><tr><td>Base64</td><td>96.97</td><td>89.39</td><td>84.85</td><td>98.48</td></tr><tr><td>Prompt Injection</td><td>75.76</td><td>91.67</td><td>69.70</td><td>98.47</td></tr><tr><td>Iterative Jailbreak</td><td>57.57</td><td>21.21</td><td>25.76</td><td>53.03</td></tr><tr><td>Crescendo</td><td>56.06</td><td>31.81</td><td>42.42</td><td>59.09</td></tr><tr><td rowspan=\"5\">Misinformation</td><td>Basic</td><td>97.28</td><td>92.57</td><td>92.46</td><td>94.84</td></tr><tr><td>Base64</td><td>98.48</td><td>90.48</td><td>96.83</td><td>93.65</td></tr><tr><td>Prompt Injection</td><td>98.39</td><td>86.51</td><td>93.65</td><td>93.65</td></tr><tr><td>Iterative Jailbreak</td><td>63.97</td><td>53.97</td><td>84.13</td><td>69.84</td></tr><tr><td>Crescendo</td><td>85.71</td><td>55.56</td><td>88.89</td><td>84.13</td></tr><tr><td rowspan=\"5\">Privacy</td><td>Basic</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Base64</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>88.33</td><td>98.33</td><td>100</td><td>91.67</td></tr><tr><td>Iterative Jailbreak</td><td>76.67</td><td>100</td><td>93.33</td><td>96.67</td></tr><tr><td>Crescendo</td><td>96.67</td><td>100</td><td>96.67</td><td>100</td></tr><tr><td rowspan=\"5\">Security</td><td>Basic</td><td>77.84</td><td>75.57</td><td>70.46</td><td>90.09</td></tr><tr><td>Base64</td><td>82.93</td><td>82.93</td><td>63.41</td><td>95.12</td></tr><tr><td>Prompt Injection</td><td>87.80</td><td>97.56</td><td>65.85</td><td>84.13</td></tr><tr><td>Iterative Jailbreak</td><td>43.90</td><td>60.97</td><td>43.90</td><td>78.04</td></tr><tr><td>Crescendo</td><td>68.29</td><td>87.80</td><td>68.29</td><td>87.80</td></tr></table>",
        "bbox": [
            165,
            460,
            833,
            832
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Vithout targeted optimization for specific evaluation scenarios,the pasing rate of some complex cases (e.g.,Harmul-Iterative Jailbreak) was relatively higher compared to other models. ",
        "bbox": [
            120,
            848,
            883,
            877
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Across different attck strategies,the models exhibited varying trends. Under the Base64 strategy, passing rates generally approached or reached iO0%,suggesting that encoding transformations had minimal impact on the models' basic robustness Incontrast,the Crescendo strategyledtoa generaldrop inpassingrates,indicating strongeradversarial effectiveness. ",
        "bbox": [
            116,
            883,
            882,
            911
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            90,
            883,
            121
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "In addition,complex atack strategies do not always outperform basic prompts. Some originally adversarial prompts may lose their intended meaning after multiple rounds of transformation,rendering the resulting model outputs less meaningful. ",
        "bbox": [
            116,
            126,
            882,
            167
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Automated Red-teaming Limitations Due to the involvement of human review,the evaluation results inevitably contain adegree ofsubjectivity. Aditionall,certain plugin types involve APImisuse or external toolinvocation, which are more suitable for evaluating agent models with tool-caling capabilities. Inthe context of base LLMs,such ests may have limited relevance. ",
        "bbox": [
            116,
            174,
            882,
            229
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "5Limitations ",
        "text_level": 1,
        "bbox": [
            116,
            248,
            243,
            266
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "In our internal tests, we have identified some limitations in current Kimi K2 models. When dealing with hard reasoning tasks or unclear tool definition, the model may generate excessive tokens,sometimes leading to truncated outputs or incomplete toolcalls.Additionally,performance maydecline on certain tasks if tool use is unnecessarily enabled. When building complete software projects,te success rate ofone-shot prompting is not as good as using K2 under an agentic coding framework. We are working to address these isues in future releases and looking forward to more fedbacks. ",
        "bbox": [
            116,
            280,
            882,
            349
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "6Conclusions ",
        "text_level": 1,
        "bbox": [
            116,
            369,
            248,
            386
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "We introduced Kimi K2,a1T-parameter open-weight MoE model built for agentic inteligence. Leveraging the tokenefficient MuonClip optimizer and a15.5T-token high-quality dataset Kimi K2 achieves stable,scalable pre-training. Post-training combines large-scale synthetic tool-use data with aunified RL framework using both verifiable rewards and self-critic feedbacks.Kimi K2 sets new state-of-the-art on agenticand reasoning benchmarks,establishing itself as the most capable open-weight LLM to date. ",
        "bbox": [
            116,
            400,
            883,
            469
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "7Acknowledgments ",
        "text_level": 1,
        "bbox": [
            117,
            489,
            299,
            506
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "We would like to acknowledge the valuable support provided by the OpenHands and Multi-SWE-bench teams in evaluating the SWE-bench Verified and Multi-SWE-bench experimental results. ",
        "bbox": [
            114,
            520,
            882,
            549
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "bbox": [
            116,
            89,
            209,
            106
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "[1] Jacob Austin et al. Program Synthesis with Large Language Models.2021. arXiv: 2108.07732 [cs.PL]. URL: https://arxiv.org/abs/2108.07732. [2] Yushi Bai et al. LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks. 2025.arXiv: 2412.15204 [cs.CL]. URL: https://arxiv.org/abs/2412.15204.   \n[3] Victor Barres et al. T-Bench: Evaluating Conversational Agents in a Dual-Control Environment. 2025.arXiv: 2506.07982 [cs.AI]. URL: https://arxiv.0rg/abs/2506.07982.   \n[4] Stella Biderman et al. “Lessons from the trenches on reproducible evaluation of language models\". In: arXiv preprint arXiv:2405.14782 (2024). [5] Federico Cassano et al.“MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation\". In: IEEE Transactions on Software Engineering 49.7(2023),pp.3675-3691.DO1: 10.1109/TSE.2023. 3267446. [6] Chen Chen et al.“ACEBench: Who Wins the Match Point in Tool Learning?\" In: arXiv e-prints (2025),arXiv2501. [7] Mark Chen et al.“Evaluating Large Language Models Trained on Code\". In: (2021). arXiv: 2107 .03374 [cs.LG]. [8] Peter Clark et al. “Think you have solved question answering? try arc, the ai2 reasoning challenge\". In: arXiv preprint arXiv:1803.05457 (2018). [9] Karl Cobbe et al. Training Verifiers to Solve Math Word Problems. 2021. arXiv: 2110.14168 [cs.LG]. URL: https://arxiv.org/abs/2110.14168.   \n[10] DeepSeek-A1. DeepSeek-V3 Technical Report. 2024. arXiv: 2412.19437 [cs.CL]. URL: https : //arxiv. org/abs/2412.19437.   \n[11] Mostafa Dehghani et al.“Scaling vision transformers to 22 bilion parameters\". In: International conference on machine learning. PMLR. 2023, pp. 7480-7512.   \n[12] Guanting Dong et al. Self-play with Execution Feedback: Improving Instruction-folowing Capabilities of Large Language Models. 2024. arXiv: 2406.13542 [cs.CL]. URL: https://arxiv.0rg/abs/2406.13542.   \n[13] Xinrun Du et al.“Supergpqa: Scaling lm evaluation across 285 graduate disciplines”. In: arXiv preprint arXiv:2502.14739 (2025).   \n[14] Dheeru Dua et al. “DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\". In: CoRR abs/1903.00161 (2019). arXiv: 1903.00161. URL: http://arxiv.0rg/abs/1903.00161.   \n[15] Kazuki Fuji et al. Rewriting Pre-Training Data Boosts LLM Performance in Math and Code.2O25.arXiv: 2505.02881 [cs.LG]. URL: https://arxiv.org/abs/2505.02881.   \n[16] Paul Gauthier. Aider LLM Leaderboards. https: //aider .chat/docs/leaderboards/.2025.   \n[17] Aryo Pradipta Gema et al.“Are we done with mmlu?\" In: arXiv preprint arXiv:2406.04127 (2024).   \n[18] Alex Gu et al.“Cruxeval: A benchmark for code reasoning, understanding and execution\". In: arXiv preprint arXiv:2401.03065 (2024).   \n[19] Daya Guo et al. “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning\". In: arXiv preprint arXiv:2501.12948 (2025).   \n[20] Zhicheng Guo et al.“StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models\". In: arXiv preprint arXiv:2403.07714 (2025).   \n[21] Aaron Harlap et al. “Pipedream: Fast and efficient pipeline parallel dnn training\". In: arXiv preprint arXiv:1806.03377 (2018).   \n[22] Y He et al.“Chinese simpleqa: A chinese factuality evaluation for large language models, 2O24a\". In: URL https://arxiv. org/abs/2411.07140 ().   \n[23] Dan Hendrycks et al. “Measuring massive multitask language understanding\". In: arXiv preprint arXiv:2009.03300 (2020).   \n[24] Dan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset. 2021.arXiv: 2103. 03874 [cs.LG]. URL: https://arxiv.org/abs/2103.03874.   \n[25] Shengding Hu etal. “Minicpm: Unveiling the potential ofsmalanguage models with scalable training strategies\". In: arXiv preprint arXiv:2404.06395 (2024).   \n[26] Jiaxin Huang et al.“Large language models can self-improve\". In: arXiv preprint arXiv:2210.11610 (2022).   \n[27] Siming Huang et al. OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models.2025.arXiv: 2411.04905 [cs.CL]. URL: https://arxiv.0rg/abs/2411.04905.   \n[28] Yanping Huang et al. “Gpipe: Eficient training of giant neural networks using pipeline parallelism\". In: Advances in neural information processing systems 32 (2019).   \n[29] Yuzhen Huang et al. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. 2023.arXiv: 2305.08322 [cs.CL]. URL: https://arxiv.0rg/abs/2305.08322.   \n[30] Alon Jacovi et al.The FACTS Grounding Leaderboard: Benchmarking LLMs'Ability to Ground Responses to Long-Form Input.2025.arXiv: 2501.03200 [cs.CL]. URL: https://arxiv.0rg/abs/2501.03200.   \n[31] Naman Jain et al. “Livecodebench: Holistic and contamination free evaluation of large language models for code\". In: arXiv preprint arXiv:2403.07974 (2024).   \n[32] Carlos E Jimenez et al.“SWE-bench: Can Language Models Resolve Real-world Github Issues?” In: The Twelfth International Conference on Learning Representations. 2024. URL: https: //openreview.net/forum?id= VTF8yNQM66.   \n[33] Keller Jordan et al. Muon: An optimizer for hidden layers in neural networks. 2024. URL: https : / / kellerjordan.github.io/posts/muon/.   \n[34] Mandar Joshi et al. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. 2017.arXiv: 1705.03551 [cs.CL]. URL: https://arxiv.0rg/abs/1705.03551.   \n[35] Kimi Team.“Kimi k1.5: Scaling reinforcement learning with llms\". In: arXiv preprint arXiv:2501.12599 (2025).   \n[36] Diederik P. Kingma and Jimmy Ba.“Adam: A Method for Stochastic Optimization”. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,May 7-9,2015, Conference Track Proceedings. Ed.by Yoshua Bengio and Yann LeCun. 2015. URL: http: //arxiv.org/abs/1412.6980.   \n[37] Satyapriya Krishna et al. Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation. 2025.arXiv: 2409.12941 [cs.CL]. URL: https://arxiv.org/abs/2409.12941.   \n[38] Joel Lamy-Poirier.“Breadth-first pipeline paralelism\". In: Proceedings of Machine Learning and Systems 5 (2023), pp. 48-67.   \n[39] Dmitry Lepikhin et al.“Gshard: Scaling giant models with conditional computation and automatic sharding\". In: arXiv preprint arXiv:2006.16668 (2020).   \n[40] Haonan Li et al. CMMLU: Measuring massive multitask language understanding in Chinese. 2O24. arXiv: 2306.09212 [cs.CL].URL: https://arxiv.org/abs/2306.09212.   \n[41] JiaLiet al.“Numinamath: The largest public dataset in ai4maths with 86Ok pairs of competition math problems and solutions\". In: Hugging Face repository 13.9 (2024), p. 9.   \n[42] Tianle Li etal.“From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline\". In: arXiv preprint arXiv:2406.11939 (2024).   \n[43] Bill Yuchen Lin et al. ZebraLogic: On the Scaling Limits ofLLMs for Logical Reasoning. 2025.arXiv: 2502. 01100 [cs.AI].URL: https://arxiv.org/abs/2502.01100.   \n[44] Aixin Liu et al.“Deepseek-v2: A strong, economical, and eficient mixture-of-experts language model\". In: arXiv preprint arXiv:2405.04434 (2024).   \n[45] Jiawei Liu et al.“Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation\". In: Advances in Neural Information Processing Systems 36 (2023), pp. 21558-21572.   \n[46] Jingyuan Liu et al. “Muon is scalable for LLM training\". In: arXiv preprint arXiv:2502.16982 (2025).   \n[47] Ziming Liu et al. “Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced Large Model Training Effciency. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis.SC 23. ACM, Nov. 2023, pp.1-13. DOI: 10.1145/3581784.3607073. URL: http: //dx.doi.0rg/10.1145/3581784.3607073.   \n[48] Ilya Loshchilov and Frank Hutter.“Decoupled Weight Decay Regularization\". In: International Conference on Learning Representations. 2019. URL: https: //openreview .net/forum?id=Bkg6RiCqY7.   \n[49] Jan Ludziejewski et al. OpenAIGym. 2025. arXiv: 2502.05172 [cs.LG]. URL: https: //arxiv.org/abs/ 2502.05172.   \n[50] Samuel Miserendino et al.“SWE-Lancer: Can Frontier LLMs Earn \\$1 Million from Real-World Freelance Software Engineering?\" In: arXiv preprint arXiv:2502.12115 (025).   \n[51] Arindam Mitra et al.“Agentinstruct: Toward generative teaching with agentic flows\". In: arXiv preprint arXiv:2407.03502 (2024).   \n[52] Ivan Moshkov et al.\"Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset\". In: arXiv preprint arXiv:2504.16891 (2025).   \n[53] Deepak Narayanan et al.“Eficient large-scale language model training on gpu clusters using megatron-lm\". In: Proceedings ofthe international conference for high performance computing, networking,storage and analysis. 2021, pp. 1-15. neural information processing systems 35 (2022), pp. 27730-27744.   \n[55] Bowen Peng et al. “Yarn: Efficient context window extension of large language models\". In: arXiv preprint arXiv:2309.00071 (2023).   \n[56] Long Phan et al. Humanity's Last Exam. 2025.arXiv: 2501.14249 [cs.LG]. URL: https: //arxiv .org/ abs/2501.14249.   \n[57] Penghui Qi et al.“Zero bubble pipeline parallelism\". In: arXiv preprint arXiv:2401.10241 (2023).   \n[58] Yujia Qin et al.“Toollm: Facilitating large language models to master 160oO+ real-world apis\". In: arXiv preprint arXiv:2307.16789 (2023).   \n[59] Qwen et al. Qwen2.5 Technical Report.2025.arXiv: 2412.15115 [cs.CL]. URL: https://arxiv.org/abs/ 2412.15115.   \n[60] Samyam Rajbhandari et al.“Zero: Memory optimizations toward training trilion parameter models\". In: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE. 2020, pp. 1-16.   \n[61] David Rein et al.\"Gpqa: A graduate-level google-proof q&a benchmark\". In: First Conference on Language Modeling. 2024.   \n[62] Keisuke Sakaguchi et al. “Winogrande: An adversarial winograd schema challnge at scale\". In: Communications of the ACM 64.9 (2021), pp. 99-106.   \n[63] David Silver and Richard S Sutton.“Welcome to the era of experience\". In: Google AI 1 (2025).   \n[64] Ved Sirdeshmukh et al. MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs. 2025.arXiv: 2501.17399 [cs .CL]. URL: https: //arxiv.0rg/abs/2501.17399.   \n[65] Giulio Starace et al. “PaperBench: Evaluating AI's Ability to Replicate AI Research\". In: arXiv preprint arXiv:2504.01848 (2025).   \n[66] Hao Sun et al. ZeroSearch: Incentivize the Search Capability of LLMs without Searching. 2O25.arXiv: 2505. 04588 [cs.CL].URL: https://arxiv.org/abs/2505.04588.   \n[67] Mirac Suzgun etal. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.2022.arXiv: 2210.09261 [cs.CL]. URL: https://arxiv.org/abs/2210.09261.   \n[68] Manveer Singh Tamber et al.“Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards\". In: arXiv preprint arXiv:2505.04847 (2025).   \n[69] Gemma Team et al.“Gemma 2: Improving open language models at a practical size\". In: arXiv preprint arXiv:2408.00118 (2024).   \n[70] LlaMA Team. The Llama 4 herd: The beginning of a new era of natively multimodal Al innovation-ai.meta.com. https://ai.meta.com/blog/llama-4-multimodal-intelligence/.[Accessed 15-07-2025].   \n[71] The Terminal-Bench Team.Terminal-Bench: A Benchmark forAI Agents in Terminal Environments.Apr.2025. URL: https://github.com/laude-institute/terminal-bench.   \n[72] Ashish Vaswani et al.“Attention is All you Need\". In: Advances in Neural Information Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. URL: https: //proceedings. neurips cc/ paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper .pdf.   \n[73] Vectara. Hallucination Evaluation Model (Revision 7437011). 2024. URL: https : / /huggingface . co / vectara/hallucination_evaluation_model.   \n[74] Joshua Vendrow et al.“Do large language model benchmarks test reliability?” In: arXiv preprint arXiv:2502.03461 (2025).   \n[75] Yizhong Wang et al. “Self-instruct: Aligning language models with self-generated instructions”. In: arXiv preprint arXiv:2212.10560 (2022).   \n[76] Yubo Wang et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. 2024.arXiv: 2406.01574 [cs.CL]. URL: https://arxiv.0rg/abs/2406.01574.   \n[77] Zhexu Wang et al. OJBench: A Competition Level Code Benchmark For Large Language Models. 2025. arXiv: 2506.16395 [cs.CL].URL: https://arxiv.org/abs/2506.16395.   \n[78] Jason Wei et al.“Measuring short-form factuality in large language models”. In: arXiv preprint arXiv:241.04368 (2024).   \n[79] Tianwen Wei et al. CMATH: Can Your Language Model Pass Chinese Elementary School Math Test? 2023. arXiv: 2306.16636 [cs.CL].URL: https://arxiv.org/abs/2306.16636.   \n[80] Colin White et al.“LiveBench: A Challenging, Contamination-Free LLM Benchmark\". In: The Thirteenth International Conference on Learning Representations.2025.   \n[81] Mitchell Wortsman et al.“Small-scale proxies for large-scale transformer training instabilities,2023\". In: URL https://arxiv. org/abs/2309.14322 ().   \n[82] Can Xu et al. WizardLM: Empowering large pre-trained language models to follow complex instructions. 2025. arXiv: 2304.12244 [cs.CL]. URL: https://arxiv.org/abs/2304.12244.   \n[83] Zhangchen Xu et al. KodCode: A Diverse, Chalenging, and Verifiable Synthetic Dataset for Coding.2025.arXiv: 2503.02951 [cs.LG].URL: https://arxiv.org/abs/2503.02951.   \n[84] John Yang et al. SWE-smith: Scaling Data for Sofware Engineering Agents.2025.arXiv: 2504.21798 [cs.SE]. URL: https://arxiv.org/abs/2504.21798.   \n[85] Shunyu Yao et al.“tau-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains\". In: arXiv preprint arXiv:2406.12045 (2024).   \n[86] Daoguang Zan et al.“Multi-swe-bench: A multilingual benchmark for issue resolving\". In: arXiv preprint arXiv:2504.02605 (025).   \n[87] Eric Zelikman et al. “Star: Bootstrapping reasoning with reasoning\". In: Advances in Neural Information Processing Systems 35 (2022), pp. 15476-15488.   \n[88] Rowan Zelers et al. “Hellaswag: Can a machine reall finish your sentence?\" In: arXiv preprint arXiv:1905.07830 (2019).   \n[89] Wanjun Zhong et al.“Agieval: A human-centric benchmark for evaluating foundation models\".In: arXiv preprint arXiv:2304.06364 (2023).   \n[90] Jeffrey Zhou et al.“Instruction-Following Evaluation for Large Language Models”. In: ArXiv abs/2311.07911 (2023). URL: https://arxiv.org/abs/2311.07911.   \n[91] Qin Zhu et al. AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models. 2025.arXiv: 2502.16906 [cs.CL]. URL: https://arxiv.org/abs/2502.16906. ",
        "bbox": [
            112,
            122,
            887,
            905
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            69,
            888,
            911
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            103,
            890,
            904
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            90,
            885,
            421
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Appendix ",
        "text_level": 1,
        "bbox": [
            117,
            89,
            199,
            107
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "AContributions ",
        "text_level": 1,
        "bbox": [
            117,
            122,
            267,
            138
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "The listing of authors is in alphabetical order based on their last names.Names marked with an asterisk (\\*) indicate people who are no longer part of our team. ",
        "bbox": [
            114,
            154,
            879,
            181
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Yifan Bai   \nYiping Bao   \nGuanduo Chen   \nJiahao Chen   \nNingxin Chen   \nRuijue Chen   \nYanru Chen   \nYuankun Chen   \nYutian Chen   \nZhuofu Chen\\*   \nJialei Cui   \nHao Ding   \nMengnan Dong   \nAng'ang Du   \nChenzhuang Du   \nDikang Du   \nYulun Du   \nYu Fan   \nYichen Feng   \nKelin Fu   \nBofei Gao   \nHongcheng Gao   \nPeizhong Gao   \nTong Gao   \nXinran Gu   \nLongyu Guan   \nHaiqing Guo\\*   \nJianhang Guo   \nHao Hu   \nXiaoru Hao   \nTianhong He   \nWeiran He   \nWenyang He   \nChao Hong   \nYangyang Hu   \nZhenxing Hu   \nWeixiao Huang   \nZhiqi Huang   \nZihao Huang   \nTao Jiang   \nZhejun Jiang   \nXinyi Jin   \nYongsheng Kang   \nGuokun Lai   \nCheng Li   \nFangLi   \nHaoyang Li   \nMingLi   \nWentao Li   \nYanhao Li   \nYiweiLi   \nZhaowei Li   \nZheming Li   \nHongzhan Lin\\*   \nXiaohan Lin   \nZongyu Lin   \nChengyin Liu   \nChenyu Liu   \nHongzhang Liu   \nJingyuan Liu\\*   \nJunqi Liu   \nLiang Liu   \nShaowei Liu   \nT.Y. Liu   \nTianwei Liu   \nWeizhou Liu   \nYangyang Liu   \nYiboLiu   \nYiping Liu   \nYue Liu   \nZhengying Liu   \nEnzheLu   \nLijun Lu   \nShengling Ma   \nXinyu Ma   \nYingwei Ma   \nShaoguang Mao   \nJie Mei   \nXin Men   \nYibo Miao   \nSiyuan Pan   \nYebo Peng   \nRuoyu Qin   \nBowen Qu   \nZeyu Shang   \nLidong Shi   \nShengyuan Shi   \nFeifan Song   \nJianlin Su   \nZhengyuan Su   \nXinjie Sun\\*   \nFlood Sung   \nHeyi Tang   \nJiawen Tao   \nQifeng Teng   \nChensi Wang   \nDinglu Wang   \nFeng Wang   \nHaiming Wang   \nJianzhou Wang\\*   \nJiaxing Wang   \nJinhong Wang   \nShengjie Wang   \nShuyi Wang   \nYao Wang   \nYejie Wang   \nYiqin Wang   \nYuxin Wang   \nYuzhi Wang   \nZhaoji Wang   \nZhengtao Wang   \nZhexu Wang   \nChu Wei   \nQianqian Wei   \nWenhao Wu   \nXingzhe Wu   \nYuxin Wu   \nChenjun Xiao   \nXiaotong Xie   \nWeimin Xiong\\*   \nBoyu Xu   \nJing Xu\\*   \nJinjing Xu   \nL.H. Xu   \nLin Xu   \nSuting Xu   \nWeixin Xu   \nXinran Xu   \nVangohunn V\"   \nZiyao Xu   \nJunjie Yan   \nYuzi Yan   \nXiaofei Yang   \nYing Yang   \nZhen Yang   \nZhilin Yang   \nZonghan Yang   \nHaotian Yao   \nXingcheng Yao   \nWenjie Ye   \nZhuorui Ye   \nBohong Yin   \nLonghui Yu   \nEnming Yuan   \nHongbang Yuan\\*   \nMengjie Yuan   \nHaobing Zhan   \nDehao Zhang   \nHao Zhang   \nWanlu Zhang   \nXiaobin Zhang   \nYangkun Zhang   \nYizhi Zhang   \nYongting Zhang   \nYu Zhang   \nYutao Zhang   \nYutong Zhang   \nZheng Zhang   \nHaotian Zhao   \nYikai Zhao   \nHuabin Zheng   \nShaojie Zheng   \nJianren Zhou   \nXinyu Zhou   \nZaida Zhou   \nZhen Zhu   \nWeiyu Zhuang   \nXinxing Zu   \nKimi K2 ",
        "bbox": [
            112,
            194,
            227,
            792
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            307,
            193,
            419,
            792
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            501,
            193,
            616,
            781
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            696,
            193,
            820,
            751
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "BToken Template of Tool Calling ",
        "text_level": 1,
        "bbox": [
            117,
            88,
            413,
            107
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "There are three components in the token structure for tool-calling: ",
        "bbox": [
            114,
            121,
            547,
            136
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "· Tool declaration message: defines the list of available tools and the schema of the arguments;   \n· Tool invoking section in assistant message: encodes the model's request to invoke tools;   \n· Tool result message: encapsulates the invoked tool's execution result. ",
        "bbox": [
            129,
            145,
            764,
            199
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "The raw tokens of the tool declaration message are formatted as follows: ",
        "bbox": [
            117,
            209,
            589,
            224
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "<lim_begin|>   \ntool_declare   \n<lim_middle|>   \n# Tools   \n{{ tool declaration content }}   \n<lim_end|> ",
        "bbox": [
            305,
            237,
            692,
            332
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "The blue highlighted marks represent special tokens,and the green part,quoted bybrackets,is the tool declaration content.We use TypeScript to express the tool declaration content, since TypeScript is a concise language with a comprehensive type system,able to expressthe types and constraints of tool parameters with brief text.The code 1 shows an example fortwo simple tools in JSON formatcompatible with OpenAI's chat completion API,asacomparison, the same tools defined in TypeScript (listed in Code 2)is much shorter.To improvecompatibility,part of our training data also uses JSON as the tool declaration language,so that 3rd-party frameworks need not aditional development to support our tool calling scheme. ",
        "bbox": [
            114,
            343,
            883,
            440
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Listing 1: Tool definition with JSON in OpenAI compatible API [{ \"type\": \"function\", \"function\":{ \"name\": \"get_weather\", \"description\": \"Get weather for a location and date\", \"parameters\":{ \"type\":\"object\", \"properties\":{ \"location\":{ \"type\":\"string\", \"description\": \"City and country e.g. Beijing， China\" }， \"date\":{ \"type\": \"string\", \"description\": \"Date to query， format in ‘%Y-%m-%d'\" } }， \"required\":[ \"location\" ] } 了 \"type\": \"function\", \"function\":{ \"name\": \"Calculator\", \"description\": \"Simple calculator\", \"parameters\":{ \"properties\":{ \"expr\":{ \"type\": \"string\", \"description\": \"Arithmetic expression in javascript\" } }， ",
        "bbox": [
            285,
            450,
            710,
            467
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            470,
            696,
            912
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "\"type\": \"object\" } } }] ",
        "bbox": [
            116,
            92,
            318,
            143
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Listing 2: Tool definition in TypeScript ",
        "text_level": 1,
        "bbox": [
            369,
            161,
            627,
            176
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "namespace functions{   \n// Get weather for a location and date   \ntype get_weather = （_:{ // City and country e.g. Beijing，China location: string, // Date to query，format in ‘%Y-%m-%d' date?: string   \n}） => any;   \n// Simple calculator   \ntype Calculator = （_:{ // Arithmetic expression in javascript expr?: string   \n}）=> any;   \n} ",
        "bbox": [
            116,
            181,
            495,
            356
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "The token template of the tool invoking section in the model's response messages is listed as follows: ",
        "bbox": [
            109,
            371,
            774,
            386
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "<tool_call_section_beginl>   \n<|tool_call_begin|>   \n// call_id part   \nfunctions.{{tool name}}:{{counter}}   \n<|tool_arguments_begin|>   \n{{ json serialized call arguments }}   \n<|tool_call_end|>   \n<|tool_call_beginl>   \n// more tool calls   \n<|tool_call_end|>   \n<|tool_call_section_end|> ",
        "bbox": [
            166,
            397,
            828,
            541
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "As shown in the template, we support paralel tool caling by placing multiple tool calls in asingle response turn.Each toolcall hasauniquecallid,formattedas functions.{tool-name}:{counter},where tool-name is the nameof the tool, and counter is an auto-increasing counter of all tool cals starting from O in the dialog. ",
        "bbox": [
            116,
            553,
            883,
            594
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "During inference,the model may ocasionaly generate unexpected tokens,leading to format errors when parsing atool call. To solve this isse,we developed aconstrained decoding module named enforcer,inspired byIm-format-enforcer6. When a<tool_call_section_begin|> token is generated,itensures that the upcoming tool-related tokens follow the predefined template, and the JSON argument string follows the declared schema. ",
        "bbox": [
            114,
            601,
            885,
            656
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "The tool result message is simply a text message encoded with the tool's callid and the corresponding results. ",
        "bbox": [
            111,
            661,
            833,
            678
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "<lim_begin|>   \ntool   \n<lim_middlel>   \n## Results of {{call_id}}   \n{{ execution result content }}   \n<lim_end|> ",
        "bbox": [
            305,
            689,
            692,
            770
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "CEvaluation Details ",
        "text_level": 1,
        "bbox": [
            116,
            786,
            303,
            803
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Coding Tasks.We evaluate Kimi-K2-Instruct's capabilities on competitive coding benchmarks,LiveCodeBench and OJBench, where Kimi-K2-Instruct atains superior performance with scores of 53.7% and 27.1%,respectively.This excellence spans both medium-level coding chalenges,such as LeetCode and AtCoder,and hard-level contests like NOI and ICPC,outperforming leading open-source and proprietary models. For multilingual programming proficiency, we employ MultiPL-E,coveringlanguages including C++, C#,Java, JavaScript, PHP,Go, Kimi-K2-Instruct surpasss top open-source models with an accuracy of 85.7%,compared with 83.1% for DeepSeek-V3-0324 and 78.2%for Qwen3- 235B-A22B.Insoftware engineering tasks, Kimi-K2-Instruct demonstrates robust performance onSWE-bench Verified (Python),SWE-lancer(Python),SWE-bench Multilingual,and Multi-SWE-bench datasets.Itsignificantlyoutperforms open-source counterparts in resolving real-world code repository isues and notably narrows the performance gap with proprietary models. For example: ",
        "bbox": [
            116,
            818,
            883,
            887
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            116,
            90,
            882,
            161
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "·SWE-bench Verified (multiple atempts): 71.6% (Kimi-K2-Instruct) vs. 80.2% (Claude 4 Sonnet) · SWE-bench Multilingual: 47.3% (Kimi-K2-Instruct) vs. 51.0% (Claude 4 Sonnet) · SWE-lancer: 39.1% (Kimi-K2-Instruct) vs. 40.8% (Claude 4 Sonnet) ",
        "bbox": [
            129,
            172,
            784,
            227
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "On PaperBench, Kimi-K2-Instruct achieves an accuracy of 27.8%,closely matching GPT-4.1 and outperforming DeepSeek-V3-0324 (12.2%)and Qwen3-235B-A22B (8.2%)by a substantial margin. In terminal interaction tasks measured by TerminalBench, Kimi-K2-Instruct atains 25.O% using the default Terminus framework and rises to 30% within Moonshot's in-house agentic framework,underscoring its capabilities in real-world agentic programming scenarios.Moreover,on the Aider-Polyglot benchmark, Kimi-K2-Instruct atains a 60.0% accuracy while employing rigorous decontamination procedures,further ilustrating its strength and reliabilityacrossdiverse coding environments. ",
        "bbox": [
            116,
            238,
            885,
            321
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Tool Use Tasks.We evaluate multi-turn tooluse with two complementary suites: T2-Bench and ACEBench.72-Bench extends the original T-bench single-control setup to a dual-control environment in which both the agent andan LLMsimulated user have constrained tool affordances over ashared state,adding arealistic Telecom troubleshooting domain alongside the prior Airline/Retail TAU tasks and enabling analysis of coordination vs. pure reasoning. ACEBench is a large bilingual (En/Zh)API-grounded benchmark (4.5K APIs across 8domains; 2Kannotated eval items)partitioned into NORMAL (basic/personalized/atomic),SPECIAL (imperfect or out-of-scope inputs),and AGENT(scenario-driven multi-turn, multi-step sandbox)tracks with automated grading of cals and outcomes.Al models run in non-thinking mode; we set the temperature to O.0,use deterministic tool adapters,score r² Airline/Retail/Telecom under Avg@4 seeds with Pass@1/4,and report overall on ACEBench English. Kimi-K2-Instruct averages 66.1 micro Pass@1 across T2 vs DeepSeek-V3-0324 48.8 /Qwen3-235B-A22B 37.3. On ACEBench Overall Kimi-K2-Instruct scores 76.5 vs DeepSeek 72.7 / Qwen 70.5 and remains competitive with GPT-4.1 (80.1). ",
        "bbox": [
            114,
            337,
            883,
            489
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Math & STEM & Logical Tasks.For Math tasks, Kimi-K2-Instruct achieves consistently strong performance, averaging over Geimini-2.5-Flash by5.3 percentage points,over DeepSeek-V3-0324 by5.5 points and over GPT4.1by 15.8 points. For example,on AIME 2024, Kimi-K2-Instruct scores 69.6%,outperforminganother two top open-source models by a large margin,DeepSeek-V3-0324 by 10.2 points and Qwen3-235B-A22B by 29.5 points.In STEM evaluations, Kimi-K2-Instruct achieves 75.1%on GPQA-Diamond,outperforming DeepSeek-V3-0324 (68.4%)and all non-thinking baselines by at least 5 percentage points. On SuperGPQA, it also exceeds the previous best open-source model,DeepSeek-V3-0324,by3.5 points.Kimi-K2-Instruct also surpasses the other two leading models inlogical reasoning. It achieves 89.0% on ZebraLogic and 89.5% on AutoLogi, exceeding DeepSeek-V3-0324 (84.0%,88.9%) and substantially outperforming Qwen3-235B-A22B (37.7%,83.3%). ",
        "bbox": [
            116,
            505,
            883,
            631
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "General Tasks.Kimi-K2-Instruct ties DeepSeek-V3-0324 on MMLUand MMLU-Pro,and takes the lead on MMLURedux with a 92.7EM score—slightly ahead of GPT-4.1 (92.4) and just 1.5 points behind Claude-Opus-4. Beyond multiple-choice tasks,the model achieves 31.0% accuracy on the short-answer SimpleQA-3.3 points above DepSeekV3-0324 and more than twice that of Qwen3-235B-A22B—though stillbelow GPT-4.1(42.3%). On the adversarial free-response LiveBench (2024-11-25 snapshot), it reaches 76.4%,surpassing Claude-Sonnet 4 (74.8%)and leading Gemini 2.5 Flash Preview by8.6 points.Acros this chalenging triad measuring breadth,depth,and robustness of world knowledge, Kimi-K2-Instruct secures atop-tier position among open-source models. We evaluate instruction-following with IFEval and Multi-Challnge.On IFEval, Kimi-K2-Instruct scores 89.8%,higher than DeepSeek-V3-0324 (81.1%) and GPT-4.1 (88.0%). On Multi-Challenge, which involves multi-turn dialogues with conflicting instructions,itachieves 54.1%,outperforming DeepSeek-V3-0324 (31.4%), GPT-4.1 (36.4%),and Claude-Opus-4 (49.0%). These results demonstrate that Kimi-K2-Instruct integrates strong factual knowledge with consistent instruction adherence across both single- and multi-turn settings, supporting robust and reliable real-world deployment. ",
        "bbox": [
            114,
            645,
            883,
            813
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Long Context and Factuality Tasks.To evaluate the factualityof Kimi-K2-Instruct, we employ three benchmarks: FACTS Grounding, which measures adherence to provided documents using the proprietary models GPT-4o, Gemini 1.5 Pro and Claude 3.5 Sonnet; HHEM, which aesss summarization quality via the open-source HHEM-2.1-Open judge; andFaithJudge, which analyzes faithfulnessinRAG tasks with o3-mini as the judge. Kimi-K2-Instruct scores 88.5 on FACTS Grounding,substantially outperforming all open-source rivals and even surpassing the closed-source Gemini 2.5Flash.With HHEM-2.1-Open it achievesa hallucinationrate of1.1%,reported in the tables as 1 minus the rate,i.e.98.9. OnFaithJudge'sRAG tasks the hallcinationrate is7.4 %,likewise present as 92.6fortableconsistency. For long-contextcapabilities,Kimi-K2-Instruct outperforms allopensource and proprietary models on DROP(93.5%), and exceeds DeepSeek-V3-0324 on retrieval task MRCR (55.0% vs 50.8%).For long-context reasoning tasks FRAMES and LongBench v2, Kimi-K2-Instruct (77.1%,49.1%) lags slightly behind DeepSeek-V3-0324 by around 2%. ",
        "bbox": [
            116,
            827,
            882,
            911
        ],
        "page_idx": 27
    },
    {
        "type": "image",
        "img_path": "images/607d4cd37123a83ad4440f56635076076be2a06386d5116f3886a6c86ee87729.jpg",
        "image_caption": [
            "Figure 11: Chinese in-house benchmark evaluation. "
        ],
        "image_footnote": [],
        "bbox": [
            194,
            92,
            802,
            319
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            114,
            372,
            883,
            429
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Open-Ended EvaluationBeyond static,closed-ended benchmarks, we evaluate the model's performance on openended, nuanced tasks that more closely resemble real-world usage. ",
        "bbox": [
            117,
            444,
            882,
            472
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "ForEnglish scenarios,we leverage the Arena-Hard-Auto v2.0 benchmark, which use LLM-as-a-judge protocols to assess generation quality across diverse,open-ended prompts [42]. These evaluations cover a wide range of highdifficulty promptsandare widelyrecognized intheresearch community.OnArena-Hard-Autov2.0,Kimi-K2-Instruct achieves state-of-the-art win-rate onboth hard prompts (54.5%)and creative writing tasks (85.0%),outperforming all open-source models and rivaling top proprietary systems such as GPT-4.1 and Claude Sonnet. These results underscore the model's strength in handling complex reasoning and nuanced generation under diverse, unconstrained setings. ",
        "bbox": [
            116,
            478,
            883,
            563
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "However, Arena-Hard-Auto provides limited coverage of Chinese-specific tasks.To addressthis gap, we developed an in-house held-out benchmark grounded in authentic user queries.To safeguard the integrityof the evaluation, the benchmark data is access-restricted, thereby eliminating the risk of overfitting. ",
        "bbox": [
            116,
            568,
            883,
            609
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "As shown in Figure 11, Kimi-K2-Instruct shows strong performance across all comparisons on Chinese in-house benchmarks.It outperforms ChatGPT-4o-latest with a 65.4% win rate,Claude Sonnet4 with 64.6%,and DeepSeek-V3- 0324 with 59.6%.In allcases,theloss ratestays low (around17%),indicating thatKimi-K2-Instructrarelyfals behind. The high win rates and consistent margins demonstrate its strong ability on open-ended Chinese tasks. ",
        "bbox": [
            114,
            616,
            883,
            672
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "In addition tocontrolled evaluations, we also consider real-world user preference through public human assessments. As of July17,2025, Kimi-K2-Instruct ranked as the topopen-source model and fifth overallon the LMSYS Arena leaderboard7,based on over 3,0 blind votes fromreal users. Unlike LLM-as-a-judge protocols,this leaderboard reflects direct human preference on diverse, user-submitted prompts,providing acomplementary perspective on practical model performance. ",
        "bbox": [
            116,
            678,
            883,
            748
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "The results on Arena-Hard-Auto,our in-house benchmark and votes from LMSYS Arena collectively offer a comprehensive viewof Kimi-K2-Instruct's open-ended capabilities,showing that it is a highly preferred model inreal-world user experience across English and Chinese. ",
        "bbox": [
            116,
            753,
            883,
            796
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "DQK-Clip Does Not Impair Model Quality ",
        "text_level": 1,
        "bbox": [
            114,
            815,
            493,
            834
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "The QK-Clip design folows a minimal intervention principle: it activates only when necessary,and deactivates after training stabilizes. Empirical evidence and analysis converge on its negligible impact on model quality. ",
        "bbox": [
            114,
            848,
            882,
            877
        ],
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/e6aaa30e08c45ac21122f8f079cfc44725bec9ab6c4ed58935d5d40316ed7c6a.jpg",
        "image_caption": [
            "Figure 12: Applying QK-Clip to Muon in a small-scale setting with an aggresive threshold (T = 3O) has negligible impact on loss, indicating that it is a safe and effective method for constraining attention logits. "
        ],
        "image_footnote": [],
        "bbox": [
            316,
            90,
            679,
            257
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Small-Scale AblationsWe train two smal-scale 0.5B activated and 3B total parameters MoE models,one with vanilla Muon and the other with MuonClip using a low clipping threshold (T = 30). As shown in Figure 12,applying MuonClip has negligible effcts on thelosscurve,indicating that even aggessiveclipping does not impairconvergence ortraining dynamics with MuonClip.This demonstrates that MuonClip is a safe and effective method for bounding attntion logits without degrading model performance. Furthermore,evaluation on downstream tasks reveals no statistically significant degradation in performance.These results collectively demonstrate that MuonClip is a safe and effective method for bounding attention logits without compromising model quality. ",
        "bbox": [
            114,
            348,
            883,
            446
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Self-deactivationIn Kimi K2, QK-Clip was only transiently active: ",
        "bbox": [
            114,
            460,
            571,
            477
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "·Initial 70o0 steps:12.7% of atention heads triggered QK-Clip for at least once, clamping Smax to 100.   \n· Post-70o00 steps: Allheads at some point reduced their Smax below 1O0,rendering QK-Clip inactive. ",
        "bbox": [
            129,
            487,
            834,
            522
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "When QK-Clip is active,itis applied per-head (rather than per-layer)to minimize potential over-regularization on other heads.After training stabilizes,QK-clip is deactivated and has no effect at all. ",
        "bbox": [
            117,
            532,
            883,
            561
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "EWhy Muon is More Prone to Logit Explosion ",
        "text_level": 1,
        "bbox": [
            114,
            579,
            526,
            598
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Logit explosion occurs when the largest pre-softmax attention score ",
        "bbox": [
            116,
            612,
            560,
            626
        ],
        "page_idx": 29
    },
    {
        "type": "equation",
        "img_path": "images/9dc7aa700528d3651abff50e88c2613fabc682802c8e2efe204a7733a012cbd9.jpg",
        "bbox": [
            428,
            635,
            570,
            659
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "grows unboundedly during training. Since ",
        "bbox": [
            117,
            666,
            393,
            681
        ],
        "page_idx": 29
    },
    {
        "type": "equation",
        "img_path": "images/ab2e6eccd1d02ac41075578bdadd28e2bea45600592e30bcbb52c7abef4eb927.jpg",
        "bbox": [
            346,
            688,
            651,
            707
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "ind RMS-Norm keeps|lxillxj|lbounded, the phenomenon is primarilydrivenby the growing spectral-normof Wqor Wk.Empiricall, we found that Muon is more susceptible to logit explosion. We give our hypothesis below. ",
        "bbox": [
            120,
            712,
            890,
            741
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Structural difference in updatesMuon produces a weight update coming from the msign operation; as a result, all singular values of the update matrix are equal—its effectiverank is full Incontrast,a typical update matrix produced by Adamexhibits askewed spectrum: afew large singular values dominate,and the effctive rank is low.This low-rank assumption for Adam is not new; higher-order muP makes the same assumption. ",
        "bbox": [
            114,
            755,
            883,
            811
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Such phenomenon is verified on the 16B Moonlight model, which shows weights trained with Muon exhibit higher singular-valueentropy (i.e.higher effectiverank) thanthose trained with Adam,corrborating the theoretical intuition. ",
        "bbox": [
            117,
            816,
            885,
            847
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "SVD formulation Let the parameter matrix at step t -1 have the singular value decomposition ",
        "bbox": [
            111,
            859,
            753,
            876
        ],
        "page_idx": 29
    },
    {
        "type": "equation",
        "img_path": "images/35658fefd6b8c9953e98ceddcaf09656c70aed33475800fe4d83df5493614e62.jpg",
        "bbox": [
            426,
            882,
            568,
            915
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "We write the update matrices as ",
        "bbox": [
            116,
            92,
            325,
            106
        ],
        "page_idx": 30
    },
    {
        "type": "equation",
        "img_path": "images/2c6b5dc765977f162b620aebb181655b5c9df248b1046a419620b5692be144ea.jpg",
        "bbox": [
            429,
            111,
            566,
            145
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "The next parameter update is therefore ",
        "bbox": [
            116,
            151,
            369,
            166
        ],
        "page_idx": 30
    },
    {
        "type": "equation",
        "img_path": "images/44fca4cf97c4b40ad052921fbc011c8cf97a65c1b1535b373f75fb592ee9117c.jpg",
        "bbox": [
            387,
            171,
            609,
            205
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "In Muon,as both the weights and the updates have a higher efective rank than Adam, we hypothesize there is a higher probability for singular-vector pair uiu to align with üjuJ.This could cause thecorresponding singular value of Wt to increase additively. ",
        "bbox": [
            114,
            218,
            885,
            261
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Attention-specific amplification Attention logits are computed via the bilinear form ",
        "bbox": [
            111,
            275,
            684,
            290
        ],
        "page_idx": 30
    },
    {
        "type": "equation",
        "img_path": "images/7633db73f97cbaffb093d1d74ae0355d52fba67bd2bceddcf34817c922a96960.jpg",
        "bbox": [
            403,
            295,
            593,
            314
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "The product WqWk squares the spectral norm, soany singular-value increase in either matrix is compounded. Muon's tendency to enlarge singular values therefore translates into a higher risk of logit explosion. ",
        "bbox": [
            116,
            319,
            885,
            348
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "FK2 Critic Rubrics for General RL ",
        "text_level": 1,
        "bbox": [
            116,
            367,
            431,
            385
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "F.1 Core Rubrics ",
        "text_level": 1,
        "bbox": [
            116,
            397,
            248,
            412
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Clarity and Relevance: Assesss the extent to which the response is succinct while fully addressing the user's intent.The focus is on eliminating unnecessary detail,staying aligned with thecentral query,and using efficient formats such as brief paragraphs or compact ists.Unlessspecifically required,long itemizations should be avoided. When a choice is expected, the response should clearly offer a single, well-defined answer. ",
        "bbox": [
            129,
            424,
            883,
            479
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "ConversationalFluencyand Engagement: Evaluates the response's contribution to anatural,flowing dialogue that extends beyond simple question-answering. This includes maintaining coherence,showing appropriate engagement with the topic,ofering relevant observations or insights, potentially guiding the conversation constructively when appropriate, using follow-up questions judiciously, handling hypothetical or personal-analogy queries gracefully, and adapting tone effectively to suit the conversational context (e.g. empathetic, formal, casual). ",
        "bbox": [
            129,
            483,
            883,
            553
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "· Objective and Grounded Interaction: Assesses the response's ability to maintain an objective and grounded tone,focusing squarely on the substance of the user's request. It evaluates the avoidance of both metacommentary (analyzing the query's structure,topic combination, perceived oddity,or the nature of the interaction itself)and unwarranted flatery or excessive praise directed at the user or their input. Excellent responses interact respectfully but neutraly, prioritizing direct, task-focused asistance over commentary on the conversational dynamics or attempts to curry favor through compliments. ",
        "bbox": [
            130,
            558,
            883,
            640
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "F.2Prescriptive Rubrics ",
        "text_level": 1,
        "bbox": [
            116,
            656,
            297,
            671
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Initial Praise: Responses must not begin with compliments directed at the user or the question (e.g,“That's a beautiful question”,“Good question!\").   \n· Explicit Justification: Any sentence or clause that explains why the response is good or how it successfully fulfilled the user's request. This is different from simply describing the content. ",
        "bbox": [
            129,
            681,
            883,
            743
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "F.3 Limitations ",
        "text_level": 1,
        "bbox": [
            116,
            758,
            235,
            773
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "One potential side efectof this evaluation framework is thatit mayfavorresponses that appear confident and assertive. even in contexts involving ambiguity or subjectivity. This stems from two key constraints in the current rubric: ",
        "bbox": [
            119,
            784,
            880,
            813
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "· Avoidance of Self-Qualification: The prescriptive rules prohibit self-assessments, explicit disclaimers,or hedging language (e.g.,“this may not be accurate”,“Imight be wrong\"). While these phrasescan reflect epistemic humility, they are often penalized as non-informative or performative.   \n· Preference for Clarity and Singularity: The rubric reward direct, decisive answers when users ask for a recommendation or explanation. In complex or open-ended scenarios,this may disincentivize appropriately cautious or multi-perspective responses. ",
        "bbox": [
            127,
            823,
            885,
            911
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "As a result, the model may occasionally overstate certainty in areas where ambiguity, nuance, or epistemic modesty would be more appropriate.Future terations ofthe framework may incorporate more fine-grained handling of calibrated uncertainty. ",
        "bbox": [
            114,
            92,
            883,
            133
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "GEngine Switching Pipeline for RL Training ",
        "text_level": 1,
        "bbox": [
            114,
            151,
            511,
            170
        ],
        "page_idx": 31
    },
    {
        "type": "image",
        "img_path": "images/a438c69f65017c41e5d17ba56ce85fb1be1661130137e006fc263f7e11e22174.jpg",
        "image_caption": [
            "Figure 13: pipeline for RL weight update "
        ],
        "image_footnote": [],
        "bbox": [
            210,
            198,
            790,
            571
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "The checkpoint engine manages three equal-size device buffers on each GPU: an H2Dbuer forloading the offloaded model parameters,and two IPC buffers for GPU-to-GPU broadcast. The IPC bufers are shared to inference engines, allowing it to directly accesste same physical memory. These three buers allow us to arrange the tree steps in a pipeline. ",
        "bbox": [
            114,
            608,
            883,
            665
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Theoretical three-stage pipeline.As ilustrated inFigure 13a,a thre-stage pipeline is introduced. (1) H2D: a shard of thelatest weights is copied into the H2D buffer asynchronously. (2) Broadcast: Once thecopycompletes,theshard will be copied toone IPC buffersand broadcast to alldevices. (3)Reload: Inference engines simultaneously load parameters from the other IPC buffer. ",
        "bbox": [
            114,
            679,
            883,
            736
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Two-stage pipeline due to PCIe saturation.On NVIDIA H8OO clusters,concurrent H2D and broadcast saturate the shared PCIe fabric,collapsing the three stages into a sequential procedure (Figure 13b). We therefore adopt a simpler, two-stage scheme (Figure 13c): (1) Alldevices perform a single,synchronous H2D transfer. (2)The broadcast and reload proceed in parallel. ",
        "bbox": [
            114,
            750,
            882,
            806
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "The two-stage pipeline willbe bound by multiple synchronous H2Dcopy operations.But in large scale devices, model will be split into smallshards,the entire parameter set fits into the H2D buffer in one transfer,the overhead wil disappear. ",
        "bbox": [
            116,
            811,
            883,
            854
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "By overlapping H2D,Broadcast,and Reload weights,we can obtain a high bandwidth to reshard the weights from trair engines to all inference engines. ",
        "bbox": [
            114,
            859,
            877,
            888
        ],
        "page_idx": 31
    }
]