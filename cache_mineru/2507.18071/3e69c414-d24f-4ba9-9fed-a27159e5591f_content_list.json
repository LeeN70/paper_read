[
    {
        "type": "text",
        "text": "Group Sequence Policy Optimization ",
        "text_level": 1,
        "bbox": [
            290,
            80,
            707,
            101
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Chujie Zheng\\*Shixuan LiuMingze LiXiong-Hui ChenBowen Yu\\* Chang GaoKai DangYuqiong LiuRui Men‘ An YangJingren Zhou Junyang Lin ",
        "bbox": [
            193,
            126,
            805,
            167
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Qwen Team, Alibaba Inc. ",
        "bbox": [
            406,
            171,
            593,
            184
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "bbox": [
            460,
            222,
            537,
            237
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixtureof-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models. ",
        "bbox": [
            178,
            253,
            823,
            372
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1Introduction ",
        "text_level": 1,
        "bbox": [
            117,
            398,
            265,
            413
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Reinforcement learning (RL) has emerged as a pivotal paradigm for scaling language models (OpenAI, 2024; DeepSeek-AI,2025; Qwen,2025b;a). Through large-scale RL,language models develop the capability to tackle sophisticated problems, such as competition-level mathematics and programming, by undertaking deeper and longer reasoning processes. ",
        "bbox": [
            117,
            428,
            882,
            481
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "To successfull scale RL with greater computational investment, the foremost prerequisite is maintaining stable and robust training dynamics. However, current state-of-the-art RL algorithms, exemplified by GRPO (Shao et al., 2024), exhibit severe stability issues when training gigantic language models, often resulting in catastrophic and irreversible model collapse (Qwen, 2025a; MiniMax,2025). This instability hinders eforts to push the capability boundaries of language models through continued RL training. ",
        "bbox": [
            117,
            488,
            882,
            555
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In this paper, we identify that the instability of GRPO stems from the fundamental misapplication and invalidation of importance sampling weights in its algorithmic design. This introduces high-variance training noise that progressively accumulates with increased response length and is further amplified by the clipping mechanism, ultimately precipitating model collapse. ",
        "bbox": [
            117,
            561,
            882,
            615
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "To address these core limitations, we propose Group Sequence Policy Optimization (GSPO), a new RL algorithm for training large language models. The key innovation of GSPO lies in its theoretically grounded definition of importance ratio based onsequence likelihood (Zheng et al., 2023),aligning with the basic principle of importance sampling. Additionaly, GSPO computes the normalized rewards as the advantages of multiple responses to a query, ensuring the alignment between sequence-level rewarding and optimization. ",
        "bbox": [
            117,
            621,
            882,
            700
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Our empirical evaluation demonstrates the significant superiority of GSPO over GRPO in training stability, eficiency,and performance. Criticaly,GSPOhas inherently resolved thestabilitychalenges in the RL training of large Mixture-of-Experts (MoE) models, eliminating the need for complex stabilization strategies,and shows the potential for simplifying RL infrastructure. These merits of GSPO ultimately contributed to the exceptional performance improvements in the latest Qwen3 models. We envision GSPO as a robust and scalable algorithmic foundation that willenable the continued advancement of large-scale RL training with language models. ",
        "bbox": [
            115,
            707,
            882,
            800
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "2 Preliminaries ",
        "text_level": 1,
        "bbox": [
            117,
            819,
            273,
            835
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Notation In this paper, an autoregressive language model parameterized by θ is defined as a policy π0. We use x to denote a query and D as the query set. Given a response y to a query x,its likelihood under the policyθ isdenotedasTe(y|x)=I1π(ytxy<t)where|ydenotes thenumberof tokens iny.A query-response pair (x,y) can be scored by a verifier r, resulting in a reward r(x,y) ∈ [0,1]. ",
        "bbox": [
            115,
            851,
            880,
            878
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            115,
            83,
            885,
            116
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Proximal Policy Optimization (PPO)Using samples generated from the old policy π0old, PPO (Schulman et al., 2017) constrains the policy update within a proximal region of the old policy through the clipping mechanism. Specifically, PPO employs the following objective for policy optimization (we omit the KL regularization term hereinafter for brevity, as it is not the focus of this paper): ",
        "bbox": [
            115,
            129,
            884,
            185
        ],
        "page_idx": 1
    },
    {
        "type": "equation",
        "img_path": "images/ef30c2e23a2f78725de8812788130d61c8c540a8c67bc1aac6c413d6971a4ac1.jpg",
        "bbox": [
            196,
            191,
            801,
            231
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "wheretheimpariofthetoytsefiswt(tadtaAtfyi estimated by another value model, and ε is the clipping range of importance ratios. ",
        "bbox": [
            115,
            237,
            882,
            275
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The core challenge of PPO in practice lies in its heavy reliance on the value model. Specifically, the value model usually has a similar size to the policy model, introducing a considerable memory and computational burden. Furthermore, the algorithmic effectiveness hinges on the reliability of its value estimate. While acquiring a reliable value model is inherently challenging, ensuring its scalability to longer responses and more complex tasks presents an even greater challenge. ",
        "bbox": [
            115,
            280,
            882,
            348
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Group Relative Policy Optimization (GRPO) GRPO (Shao et al., 2024) bypasses the need for the value model by computing the relative advantage of each response within a group of responses to the same query. Specifically, GRPO optimizes the following objective: ",
        "bbox": [
            115,
            361,
            882,
            404
        ],
        "page_idx": 1
    },
    {
        "type": "equation",
        "img_path": "images/539f310a5fbce64e9518f08bed6cbb35acc102b3c086b2d4736a7a16a08ab956.jpg",
        "bbox": [
            124,
            410,
            847,
            449
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "where G is the number of generated responses to each query x (i.e., the group size),and the importance ratio Wi,t(0) and advantage Ai,t of token Yi,t are: ",
        "bbox": [
            114,
            454,
            884,
            486
        ],
        "page_idx": 1
    },
    {
        "type": "equation",
        "img_path": "images/255e92b0eac3c5cfdb2cbbedf7906f7f23cf191549c06b727d14d427d0531803.jpg",
        "bbox": [
            220,
            492,
            776,
            529
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "respectively, where al the tokens in yi share the same advantage as Ai. ",
        "bbox": [
            119,
            535,
            640,
            550
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3Motivation ",
        "text_level": 1,
        "bbox": [
            115,
            569,
            252,
            586
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The growth in model size, sparsity(e.g., in Mixture-of-Experts models),and response length necesitates a large rollout batch size to maximize hardware utilization during RL. To improve sample effciency, it is standard practice to partition a large batch of rollout data into multiple mini-batches for gradient updates. This procedure inevitably introduces an off-policy learning seting, where responses y are sampled from an old policy πθold rather than the current policy π being optimized. This also explains the necessity of the clipping mechanism in PPO and GRPO, which prevents overly \"off-policy\" samples from being involved in gradient estimation. ",
        "bbox": [
            115,
            599,
            884,
            694
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "While mechanisms like clipping aim to manage this off-policy discrepancy, we identify a more fundamental issue in GRPO: its objective is il-posed. This problem becomes particularly acute when training large models on long-response tasks,leading to catastrophic model collapse. The il-osed nature of the GRPO objective stems from a misapplication of importance sampling weights. The principle of importance sampling is to estimate the expectation of a function f under a target distribution Ttar by re-weighting samples drawn from a behavior distribution Tbeh: ",
        "bbox": [
            115,
            699,
            884,
            781
        ],
        "page_idx": 1
    },
    {
        "type": "equation",
        "img_path": "images/81fe3eede57bd870fc7362b5513d69e1e784a485d67f05e0ad8b1927e8d568da.jpg",
        "bbox": [
            347,
            785,
            650,
            819
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Crucially, this relies on averaging over multiple samples (N >1) from the behavior distribution Tbeh for theimportance weight πteh(2) to effectively correct for the distributional mismatch. ",
        "bbox": [
            115,
            822,
            892,
            859
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "IncontrastGc is based on a single sample yi,t from each next-token distribution π0old(:|x,yi<t), it fails to perform the intended distribution-correction role. Instead, it introduces high-variance noise into the training gradients, which accumulates over long sequences and is exacerbated by the clipping mechanism. We have empirically observed that this can lead to model colapse that is often irreversible. Once the collapse occurs,resuming training is unavailing, even when reverting to a previous checkpoint and meticulously tuning hyperparameters (e.g. the clipping ranges), extending generation length, or switching the RL queries. ",
        "bbox": [
            115,
            866,
            882,
            920
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            117,
            84,
            882,
            151
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "The above observation suggests a fundamental issue in GRPO's design. The failure of the token-level importance weight points to a core principle: the unit of optimization objective should match the unit of reward. Since the reward is granted to the entire sequence, applying of-policy correction at the token level appears problematic. This motivates us to forego the token-level objective and explore utilizing importance weights and performing optimization directly at the sequence level. ",
        "bbox": [
            117,
            156,
            882,
            224
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "4Algorithm ",
        "text_level": 1,
        "bbox": [
            117,
            242,
            245,
            260
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "4.1 GSPO: Group Sequence Policy Optimization ",
        "text_level": 1,
        "bbox": [
            117,
            273,
            500,
            290
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "While the token-level importance weight T0oldyi,tx,yi<t） π(yit|x,yi<t） is problematic in GRPO,we observe that in the context of language generation, the sequence-level importance weight e(ylx) has a clear theoretical πld（yx) meaning: it reflects how far the response y sampled from Tπ0old (\\*|x) deviates from πθ(\\*|x), which naturally aligns with the sequence-level reward and can also serve as a meaningful indicator of the clipping mechanism. ",
        "bbox": [
            115,
            300,
            884,
            391
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Based on this straightforward observation, we propose the Group Sequence Policy Optimization (GSPO) algorithm. GSPO employs the following sequence-level optimization objective: ",
        "bbox": [
            110,
            404,
            882,
            434
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/fbb41c5f31b577b94687526ee73bde32ece617d8bba3949f44f73565d3db46ec.jpg",
        "bbox": [
            186,
            439,
            813,
            479
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "where we adopt the group-based advantage estimation: ",
        "bbox": [
            115,
            485,
            532,
            499
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/244682dadacc6a80db62f3db8393166e1df89bcb65df09ffecc1bb08e9001ebd.jpg",
        "bbox": [
            361,
            506,
            638,
            543
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "and define the importance ratio si(0) based on sequence likelihood (Zheng et al., 2023): ",
        "bbox": [
            114,
            549,
            761,
            564
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/57f2594f48f6a6866d364d07e4ab9b11c6b305434f1bead7719aab27963305f7.jpg",
        "bbox": [
            255,
            570,
            741,
            612
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Therefore, GSPO applies clipping to entire responses instead of individual tokens to exclude the overly \"off-policy\" samples from gradient estimation, which matches both the sequence-level rewarding and optimization. Note that we adopt length normalization in si(0) to reduce the variance and to control si(0)within a unified numerical range. Otherwise,the likelihood changes of a few tokens can result in dramatic fluctuations of the sequence-level importance ratio,and the importance ratios of responses with different lengths willrequire varying clipping ranges. We also note that the clipping ranges in GSPO and in previous algorithms (e.g., GRPO) typically differ in order of magnitude due to the distinct definitions of importance ratios. ",
        "bbox": [
            115,
            617,
            884,
            722
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "4.2Gradient Analysis ",
        "text_level": 1,
        "bbox": [
            117,
            739,
            297,
            753
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We can derive the gradient of the GSPO objective as folows (clipping is omited for brevity): ",
        "bbox": [
            117,
            763,
            801,
            780
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/2ff9a260240f1e0cf653a6e2587c5af573a45327ee612742ddd0643a3320d8f2.jpg",
        "bbox": [
            226,
            827,
            613,
            865
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/8259d7d9d8faa401d03ef62bcebca8b7c3493437e95b4bf64dec142f90ea42e6.jpg",
        "bbox": [
            220,
            868,
            843,
            908
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "For comparison, the gradient of the GRPO objective is as follows (note that Ai,t = Ai): ",
        "bbox": [
            115,
            82,
            754,
            99
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/c8f9f9f3c393351046df08df3bd9d88ecb7694bfb133adf9cb96a63e4da5efe8.jpg",
        "bbox": [
            124,
            105,
            842,
            189
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Therefore,the fundamental distinction between GSPO and GRPO lies in how they weight the gradients of the log likelihoods of tokens. In GRPO,the tokens are weighted according to their respective \"importance weight\"eo( [1 -ε,+)(for Ai < O),are not negligible,and their impact can accumulate and lead to unpredictable consequences as training progresses. In contrast, GSPO weights all the tokens in a response equally, eliminating this instability factor of GRPO. ",
        "bbox": [
            115,
            192,
            885,
            288
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4.3GSPO-token: A Token-level Objective Variant ",
        "text_level": 1,
        "bbox": [
            117,
            303,
            510,
            318
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "In scenarios like multi-turn RL, we may desire a finer-grained advantage adjustment than the sequence level. To this end, we introduce a token-level objective variant of GSPO, namely GSPO-token, to allow token-wise advantage customization: ",
        "bbox": [
            117,
            328,
            882,
            369
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/a5b6a90fe30773919b0035960c75d3638f6b0963d0ceffd5bba7a19e4ed8983d.jpg",
        "bbox": [
            124,
            376,
            872,
            417
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "where ",
        "bbox": [
            115,
            436,
            166,
            450
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/b25f22f3cc9616f7f876f1969dcc10cd9a6ce3b0c6cac7a0d8e8fbb3da7ccacf.jpg",
        "bbox": [
            352,
            454,
            645,
            487
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "and sgl-] denotes only taking the numerical value but stopping the gradient, corresponding to the detach operation in PyTorch. The gradient of GSPO-token can be derived as: ",
        "bbox": [
            119,
            492,
            882,
            520
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/4de6c0eb24250406c3363d24d27856a7d160a48dc0683500a6c0e5c4649986b4.jpg",
        "bbox": [
            117,
            527,
            636,
            567
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/9474897c4fd62039415e5cb34f04a0b8f2d142a09261f252fd1313479876f871.jpg",
        "bbox": [
            119,
            567,
            882,
            652
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Note that the term π0(yi,t|x,yi<t） has a numerical value of 1, so si,t(0) is numerically equal to si(0). sg[πe(yit|xyi<t)]   \nComparing Equation (5) and (13),and Equation (10) and (17), GSPO-token and GSPO are numerically identical in the optimization objective,clipping condition,and theoretical gradient when we set the advantages of allthe tokens in the response yi to the same value (i.e., Ai,t = Ai), while GSPO-token enjoys the higher flexibility of adjusting the advantages per token. ",
        "bbox": [
            115,
            671,
            884,
            752
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "5Experiments and Discussion ",
        "text_level": 1,
        "bbox": [
            117,
            770,
            410,
            787
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "5.1Empirical Results ",
        "text_level": 1,
        "bbox": [
            117,
            801,
            294,
            815
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "We experiment with a cold-start model fine-tuned from Qwen3-30B-A3B-Base, and report the training reward curves as well as the model performance curves on the AIME'24 (average Pass@1 over 32 samplings), LiveCodeBench (202410-202502, average Pass@1 over 8 samplings),and CodeForces (Elo Rating)benchmarks.During the RL training,each batch of rollout data is partitioned into four minibatches for gradient updates. In GSPO, we set the left and right clipping ranges in Equation (5) to 3e-4 and 4e-4,respectively. We compare against GRPO as the baseline and set the left and right clipping ranges in Equation (2) to 0.2 and 0.27, respectively, which we have carefully tuned to ensure afair comparison. ",
        "bbox": [
            115,
            826,
            884,
            920
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Note that GRPO necessitates the Routing Replay training strategy for the normal convergence of MoE RL,whichweilladdiioallydscussinS5.3,hile Gsbiated thenedfortissategy ",
        "bbox": [
            117,
            84,
            880,
            111
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Figure 1 shows that the training with GSPO proceeds stably throughout. We observe that GSPO can deliver continuous performance improvement through increasing the training compute, regularly updating the query set, and extending the generation length. Moreover, GSPO also demonstrates superior training eficiency over GRPO,achieving beter training accuracy and benchmark performance under the same training compute and consumed queries. Finally, we have successfully applied GSPO to the RL training of the latest Qwen3 models, strongly proving the eficacy of GSPO in unleashing the power of RL scaling for large language models. ",
        "bbox": [
            115,
            117,
            882,
            211
        ],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/88583472fb23ac1587c24e21b5f48bf560871b6381453fc39bd1b64ea558b40d.jpg",
        "image_caption": [
            "Figure 1: Training curves of a cold-start model fine-tuned from Qwen3-30B-A3B-Base. GSPO possesses remarkably higher training efficiency than GRPO. "
        ],
        "image_footnote": [],
        "bbox": [
            115,
            222,
            882,
            532
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "5.2Curious Observation on Clipping Fractions ",
        "text_level": 1,
        "bbox": [
            119,
            592,
            489,
            607
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "A key distinction of GSPO compared to GRPO is its practice of clipping entire responses rather than individual tokens.Particularly,as shown in Figure2,we observea difference of two orders of magnitude in the fractions of clipped tokens between GSPO and GRPO (while adjusting the clipping ranges does not alter the disparity in magnitude). However, despite clipping significantly more tokens and consequently using fewer for training (or gradient estimation), GSPO still achieves higher training eficiency than GRPO.This counter-intuitive finding -that clipping a much larger fraction of tokens leads to superior training efciency-further indicates that GRPO's token-level gradient estimates are inherently noisy and inefficient for sample exploitation. In contrast, GSPO's sequence-level approach provides a more reliable and effective learning signal. ",
        "bbox": [
            115,
            618,
            884,
            738
        ],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/6790fcb8b5b72545db397218b6a4b32fe3a1f66733a381f18ae3128fb9cb7d2f.jpg",
        "image_caption": [
            "Figure 2: Average fractions of clipped tokens over the RL training of GSPO and GRPO. "
        ],
        "image_footnote": [],
        "bbox": [
            154,
            750,
            842,
            877
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "5.3Benefit of GSPO for MoE Training ",
        "text_level": 1,
        "bbox": [
            119,
            84,
            420,
            98
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Background Compared to the RL training of dense models,the sparse activation nature of MoE models introduces unique stability challenges. In particular, we found that when adopting the GRPO algorithm, the expert-actioation volatility of MoE models can prevent RL training from converging properly. To be specific, after one or more gradient updates, the experts activated for the same response can change significantly.For example, with the 48-layer Qwen3-30B-A3B-Base model,after each RL gradient update and for the same rollut sample, there are roughly 10% of the experts activated under the new policy Tt that are different from those under the old policyTt0old.This phenomenon,which becomesmore prominentindeepermodesaksetken-lptaceatis() fluctuate drastically and further invalidates them, as discussed in S 3 and 4.2,consequently hindering the normal convergence of RL training. ",
        "bbox": [
            115,
            109,
            884,
            255
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Our Previous ApproachTo tackle this challenge, we previously employed the Routing Replay training strategy. Specificall, we cache the activated experts inπ0old and \"replay\" these routing modes in πθ when computing the importance ratios Wi,t (0) = Ty.Itsfoeactokeyit,(yxyt) Tldyixyi<t)   \nand π0old(yi,t|x,yi,<t) share the same activated network,so that we can restore the stability of the tokenlevel importance ratios and ensure optimization of the consistent activated network across gradient updates. Figure 3 demonstrates that Routing Replay serves as an essential technique in the normal convergence of the GRPO training of MoE models. ",
        "bbox": [
            115,
            268,
            884,
            376
        ],
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/bfa83dcb19d87ab9646676c73be8cb5507d529a2b55798d77a8998ac25e9a77f.jpg",
        "image_caption": [
            "Figure 3: The Routing Replay strategy plays a critical rolein the normal convergence of the GRPO training of MoE models. "
        ],
        "image_footnote": [],
        "bbox": [
            115,
            390,
            882,
            563
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Benefit of GSPO. Although Routing Replay enables the GRPO training of MoE models to converge properly, its practice of reusing routing modes incurs additional memory and communication overhead and can also limit the actual capacity of the MoE model. In contrast, as shown in Figure 1, GSPO eliminates the dependency on Routing Replay and is fully capable of computing the importance ratios si(0) conventionally, converging normally,and optimizing stably. The key insight is that GSPO focuses only on the sequence likelihood (i.e.,πθ(yilx))and is not sensitive to the individual token likelihood (i.e., πθ(yit|x,yi,<t)). Since the MoE model always maintains its language modeling capability, the sequence likelihood will not fluctuate drastically. In summary, GSPO fundamentally resolves the expert-activation volatility issue in MoE models,obviating the need for complex workarounds like Routing Replay. This not only simplifies and stabilizes the training process but also allows the model to leverage its full capacity without artificial constraints. ",
        "bbox": [
            115,
            621,
            884,
            766
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5.4Benefit of GSPO for RL Infrastructure ",
        "text_level": 1,
        "bbox": [
            117,
            783,
            447,
            797
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Given the precision discrepancies between training engines (e.g., Megatron) and inference engines (e.g., SGLang and vLLM), in practice, we typically use the training engine to recompute the likelihoods of sampled responses under the old policy π0old: However, GSPO uses only sequence-level, rather than token-level,likelihoods for optimization,and intuitively, the former is much more tolerant of precision discrepancies. Hence, GSPO makes it possible to directly use the likelihoods returned by the inference engine for optimization, thereby avoiding the need for recomputation with the training engine. This can be especially beneficial in scenarios like partial rolout and multi-turn RL and in the training-inference disaggregated frameworks. ",
        "bbox": [
            117,
            808,
            884,
            915
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "6Conclusion ",
        "text_level": 1,
        "bbox": [
            115,
            83,
            253,
            98
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "We propose Group Sequence Policy Optimization (GSPO), a new reinforcement learning algorithm for training large language models. Following the basic principle of importance sampling, GSPO defines importance ratios based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. GSPO demonstrates notably superior training stability, efficiency,and performance compared to GRPO and exhibits particular effcacy for the large-scale RL training of MoE models, laying the foundation for the exceptional improvements in the latest Qwen3 models. With GSPO as a scalable algorithmic cornerstone, we willcontinue to scale RL and look forward to the resulting fundamental advances in intelligence. ",
        "bbox": [
            115,
            114,
            882,
            219
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "bbox": [
            117,
            239,
            218,
            254
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ar Xiv preprint arXiv:2501.12948,2025.   \nMiniMax. Minimax-m1: Scaling test-time compute effciently with lightning attention. arXiv preprint arXio:2506.13585,2025.   \nOpenAI. Learning to reason with LLMs， 2024. URL https://openai.com/index/ learning-to-reason-with-llms/.   \nTeam Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388,2025a.   \nTeam Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https : //qwenlm.github.io/blog/qwq-32b/.   \nJohn Schulman, Filip_ Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \nZhihong Shao, Peiyi Wang Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \nChujie Zheng, Pei Ke, Zheng Zhang,and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning.In Findings of the Association for Computational Linguistics: ACL 2023, 2023. URL https://aclanthology.org/2023.findings-acl.65/. ",
        "bbox": [
            114,
            261,
            887,
            552
        ],
        "page_idx": 6
    }
]