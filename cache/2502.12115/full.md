# SWE-Lancer: Can Frontier LLMs Earn \$1 Million from Real-World Freelance Software Engineering?

Samuel Miserendinol\* Michele Wang1\* Tejal Patwardhan’ Johannes Heidecke 1

# Abstract

We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \$1 million USD total in realworld payouts. SWE-Lancer encompasses both independent engineering tasks — ranging from \$50 bug fixes to \$32,OoO feature implementations — and managerial tasks,where models choose between technical implementation proposals. Independent tasks are graded with endto-end tests triple-verified by experienced software engineers,while managerial decisions are assessed against the choices of the original hired engineering managers.We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research,we open-source a unified Docker image and a public evaluation split, S WELancer Diamond (https://github.com/ openai/SwELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.

# 1. Introduction

In just two years, language models have advanced from solving basic textbook computer science problems to winning gold medals in international programming competitions (OpenAI, 2024a; Quan et al., 2025). When OpenAI announced SWE-Bench Verified in August 2024, GPT-4o scored 33%; today, their o3 reasoning model achieves SOTA at 72% (OpenAI, 2024b), highlighting the need for unsaturated evaluations that reflect the complexity of real-world software engineering. As AI research and development continues to accelerate, benchmarks that rigorously evaluate and forecast software engineering (SWE) capabilities will be crucial in assessing economic impacts and agentic safety (OpenAI,2023; Anthropic,2024; DeepMind, 2024).

Prior coding benchmarks have focused on self-contained tasks like program synthesis (Chen et al., 2021) and competitive programming (Hendrycks et al., 2O21). The most realistic benchmarks are currently SWE-Bench (Jimenez et al.,2024) and SWE-Bench Verified (Chowdhury et al., 2024),which focus on isolated,self-contained tasks.In the real world, however, software engineers operate across the full technology stack and must reason about complex inter-codebase interactions and tradeoffs.

To better measure the real-world software engineering capabilities and impact of AI models,we introduce SWELancer. Our benchmark evaluates frontier language models on 1,488 freelance software engineering jobs from Upwork, collectively worth \$1,0oo,00o USD in payouts. The evaluation includes Individual Contributor (IC) SWE tasks, where models generate code patches to resolve real-world issues,and SWE Manager tasks,where models act as technical leads by selecting the best implementation proposal for a given problem.

· IC SWE tasks range in difficulty from 15-minute bug fixes to new feature requests that took weeks to close out.Unlike prior benchmarks,which grade model performance with unit tests,SWE-Lancer evaluates IC SWE tasks using end-to-end tests created by a team of professional software engineers.These end-to-end tests use browser automation to verify application behavior and mirror the real-world freelance review process,and have been triple-verified for quality by experienced software engineers.

· SWE Manager tasks direct models to review competing proposals submitted by freelancers in response to job postings,and select the best one.They are then assessed against the choices of the original engineering managers. These tasks require a deep technical understanding of both the issue and the proposals; it is often the case that multiple proposals are technically correct, and identifying the winning proposal requires considering context from the entire repository.

![](images/49432e27d505424c735406639813e52d0c2995a43af044dcdd438464c141e065.jpg)  
Figure 1. Evaluation flow for IC SWE tasks; the model only earns the payout if all applicable tests pass.

![](images/8db5df7a490e6eb326a4f499919cc2321f5ee2608fab495e1ab3395afdfa02e0.jpg)  
Figure 2.EvaluationflowforSWE Managertasks; during proposal selection,themodelhas theabilitytobrowsethecodebase.

SWE-Lancer offers several advantages over existing SWE benchmarks:

· Real-world payouts: All 1,488 tasks represent real payouts to freelance engineers and provide a natural market-derived difficulty gradient (ranging from \$250 to \$32,Ooo). Notably, the economic payout associated with each task is not an estimate;rather,it reflects the actual amount paid to the freelancer who completed it.Monetary payouts are non-trivial: within the opensourced SWE-Lancer Diamond set, 35% of tasks are worth more than \$1,OoO,and 34% are worth from \$500 to \$1,000.

· Management assessment: Large-scale software engineering in the real-world requires capable technical management.Our benchmark evaluates models’ability to function as a SWE manager using real-world data.In SWE management tasks, the model acts as a technical lead, weighing the actual implementation proposals that were submitted by freelancers when the issue was originally posted.Previous works have investigated models’ ability to generate research ideas (Si et al.,2O24), but their capacity to evaluate commercial engineering proposals remains unexplored.

· Advanced full-stack engineering: Prior evaluations have largely focused on issues in narrow, developerfacing repositories (e.g. open source utilities to facilitate plotting or PDF generation). In contrast, SWELancer is more representative of real-world software engineering,as tasks come from a user-facing product with millions of real customers.SWE-Lancer tasks frequently require whole-codebase context. They involve engineering on both mobile and web, interaction with APIs,browsers,and external apps,and validation and reproduction of complex issues.Example tasks include a \$25O reliability improvement (fixing a double-triggered API call), \$1,Oo0 bug fix (resolving permissions discrepancies),and \$16,Ooo feature implementation (adding support for in-app video playback in web, iOS,Android,and desktop).

· Improved grading via end-to-end (E2E) tests: Existing coding benchmarks often rely on unit tests to evaluate isolated functions or components; however, these tests are highly susceptible to grader hacking (see Appendix A7) and insufficient for evaluating fullstack engineering work (Brar & Kaur,2O15; Paul et al.,

2006).SWE-Lancer is the first benchmark to use E2E tests created by professional engineers,offering a more comprehensive, real-world evaluation that is harder to exploit.For example,in one \$1,OoO task (Expensify, 2024a),a bug caused the avatar on the “Share Code" page to differ from the profile page.While unit tests might confirm each function works independently, the end-to-end test we created for this task simulates the entire user workflow - including logging in, uploading a profile picture,and interacting with a second account - and catches the bug. Tests underwent three rounds of validation by software engineers for quality, coverage, and fairness,supplemented by an automated system ensuring each test passed in the fixed state and failed in the broken state.

· Domain diversity: SWE-Lancer tasks span many categories (Figure 3). In the open-sourced Diamond set, 74% of IC SWE tasks and 76% of SWE Management tasks involve Application Logic, while 17% of IC SWE tasks and 18% of SWE Management tasks involve UI/UX development. These categories extend beyond styling to include core full-stack concepts like frontend event handling,DOM interactions,and application behavior. Most tasks (88% of IC SWE tasks and 94% of SWE Management tasks)are bug fixes. See Appendix A12 for more details on dataset composition.

·Difficulty: SWE-Lancer tasks are challenging. The average task in the Diamond set takes 26 days to resolve on Github and has 47 comments,which include discussion of multiple proposals. On average, IC SWE tasks in the Diamond set require modifying 2 files and 69 lines of code, while SWE Manager tasks require the model to choose between 4-5 proposals.

· Unbiased data collection: The data collection process for prior benchmarks,such as SWE-Bench and SWEBench Verified, involved searching for pull requests on GitHub and selecting ones that had unit tests.As a result of this selection process,these datasets are biased towards code and problems that are easily testable. Instead of fltering out the vast majority of problems to find ones that volunteers chose to write tests for, we selected a representative sample of tasks from Upwork, and paid a team of 1Oo professional software engineers to write and verify end-to-end tests for all of them.

We also perform large-scale evaluations of state-of-the-art models to better understand the capabilities of autonomous SWE agents,including running ablations on test-time compute and tool affordances. We find that the best performing model- Claude 3.5 Sonnet - scores 26.2% on IC SWE tasks and 44.9% on SWE Management tasks, earning a total of \$208,050 out of \$500,800 possible on the SWE-Lancer Diamond set. On the full SWE-Lancer dataset, Claude 3.5 Sonnet earns over \$400,000 out of \$1,000,000 possible.

To advance research in automated software engineering, agentic safety,and economic impacts of automated coding models, we release a public evaluation split - SWE-Lancer Diamond - containing \$50o,8OO worth of tasks. To avoid contamination via training or search (e.g.,for models that can browse the internet),we keep the remaining set as a private holdout. The full dataset is available upon request.

# 1.1. Related Work

Researchers have evaluated LLMs’ coding skills in scientific programming (Tian et al., 2024), text analysis (Zhong et al., 2023; Lam et al., 2024),and repository-level completion (Zhang et al.,2024; Liu et al.,2023),with benchmarks such as HumanEval (Chen et al., 2O21),HumanEvalPack (Muennighoff et al., 2024),APPS,LiveCodeBench (Jain et al.,2024), NaturalCodeBench (Zhang et al.,2024), BigCodeBench (Zhuo et al., 2024),MBPP, MathQA-Python (Austin et al., 2021), CodeXGlue (Lu et al., 2021),AlphaCode (Li et al.,2022),CODEGEN (Nijkamp et al., 2023),and DS-100O (Lai et al., 2022). SWE-bench verified (Chowdhury et al.,2O24) builds upon SWE-bench (Jimenez et al., 2024) and evaluates LLMs on real-world pull requests from open-source repositories,graded against human-coded patches. SWE-bench Multimodal (Yang et al., 2024) eXtends this to frontend tasks from open-source Javascript libraries.However, these benchmarks typically rely on unit tests (prone to grader hacking,see Appendix A7),exclude commercial repositories,and lack full-stack coverage-gaps that SWE-Lancer addresses by sourcing real freelance tasks and mapping model performance to economic payout. See Appendix A4 for a more detailed comparison table of coding benchmarks.

# 2. SWE-Lancer

The SWE-Lancer dataset consists of 1,488 real freelance software engineering tasks from the Expensify open-source repository posted on Upwork.These tasks are collectively valued at \$1 million USD and are divided into two categories:

· Individual Contributor (IC) Software Engineering (SWE) Tasks: This set contains 764 tasks collectively valued at \$414,775,which mirror the responsibilities of individual contributor software engineers (e.g., implementing features,resolving bugs).The model is given(1) the issue text description (including reproduction steps and desired behavior),(2) the codebase checkpointed at the state before the issue fix,and (3) the objective of fixing the issue. The model's solution is evaluated by applying its patch and running all associated end-to-end tests using Playwright, an open-source browser testing library (Microsoft, 2025). Models are not able to access end-to-end tests during

Application Logic Server-Side Logic Bugfixes State Navigating back fromthe“FlagasOffensive"screenfailstodisplay Example:"[S5oo]Login-"Whenattempting tolog Example:"[S5oo]Web-Split-Thetooltipdisplays thecorrect report page due toa missing route in i ths Headeti   
Clicking "Didn’treceiveamagiccode"failstoreset Currently,hovering over thesplitbill te t NewFeaturesor Enhancements correctlyrestores focus. the tooltip logic so that it shows display Newfunctionalityptimizationofexistingeature names for consistency. Example:“[S1ooo]Addtheabilitytomentionyourselfanduse'@here' tothementionsauto-suggestion list" UI/UX System-Wide Qulity and Designchanges,layout,styling,interaction Relatedtorefactoringcode,performance improvements optimization Reliability Improvements   
Example:"[S5oo]Web-Overlaybackgroundcolor Example:“[S250]Openingathreadcalls Loggingrinig isdifferent" OpenReportAPItwice." Example:“[s1ooo]Addacharacterlimitonlengthoftask names,room c

Figure 3.Breakdownof thediferenttypes ofICSWEisues in SWE-Lancer with examples from theDiamond Set.BluecategoriesoI the left represent task topics, while green categories at right represent task types.

![](images/0ac4f8dfcb6ad4ec271d6023cb9cae40a6c27afdfb034772aeacaaf76788f9ba.jpg)  
Figure 4.SWE-Lancerissues aredynamicall pricedbasedonreal-worlddifculty.Inthe example above (Expensify,2O23),SWE managersrejected5arlyproposals thatdidnotappropratelyaddressedgecases.Theinitialrquestpricedat\$oasinceasedto \$8,000 over four weeks until it was solved.

the evaluation.

· SWE Management Tasks: This set contains 724 tasks collectively valued at \$585,225. Taking on the role of a software engineering manager, the model must choose the best proposal for resolving a task.The model is given(1) multiple proposed solutions to the same issue (taken from the original discussion),(2)a snapshot of the codebase from before the issue was fixed, and (3) the objective of picking the best solution.The model's selection is evaluated by assessing whether it matches ground truth.An additional validation campaign with experienced software engineers showed 99% agreement with the original chosen solutions.

Issues are posed exactly as written on Upwork; each IC SWE task is graded with end-to-end tests mirroring how experienced engineers would evaluate them. Ful prompts are available in Appendix A8 and example tasks are in A13.

# 2.1. Evaluation Metrics and Task Types

We track the percent of tasks resolved and the total payout earned by the model (using the real freelance rates set in Upwork by Expensify). Because they draw from real tasks, SWE-Lancer payouts uniquely reflect true economic value rather than theoretical estimates. Harder tasks pay more, especially when they demand specialized knowledge or remain unresolved for extended periods of time.

# 2.2. Benchmark Construction

To ensure our dataset consists of high-quality and representative tasks, we implemented the following pipeline:

1.Repository selection． Expensify is a \$3OO million USD public company (NASDAQ: EXFY) with 12 million users who rely on its software,making it a reliable testbed for sourcing commercially valuable software engineering tasks. The open-source Expensify repository posts tasks on Upwork for freelance engineers with concrete payouts.We sourced real tasks that were previously solved by paid contributors. The repository has a diversity of high-quality, complex tasks.

2. Task selection. 1OO professional software engineers reviewed the tasks, proposals,and codebase for clarity, specificity,and executability. For high-value IC SWE tasks (with payout exceeding \$5,OoO),a team of ten experienced engineers validated each task,confirming that the environment was properly configured and test coverage was robust. Each IC SWE and SWE Management task was reviewed by multiple engineers (three for IC SWE, two for SWE Management). More detail on the review process is included in Appendix A6.

3. Task generation. From each validated Github issue, we generated an IC SWE task using the task's title and description,and a snapshot of the codebase at the time it was posted.If there were at least two proposals for how to resolve the issue,we also generated a SWE Management Task using the issue description and list of proposals.

4.End to End Test Development. We wrote comprehensive end-to-end Playwright tests for each IC SWE task. Tests simulate real-world user flows, such as logging into the application, performing complex actions (e.g., making financial transactions),and verifying that the model's solution works as expected.Each test is triple-verified by professional software engineers.

5. User Tool. Each IC SWE Task has an accompanying user tool; when the agent invokes the user tool, a Playwright script opens the browser and simulates a user attempting to perform the action associated with the task in the local application,so the model can view its work, just as human engineers often run their code while writing it. The model doesnot receive any feedback about whether the action was successful; however, a text-based trajectory and a series of screenshots are Written to its working directory when the tool finishes. When the user tool is enabled, the model is allowed to invoke it as often as it wishes to via the command line.

# 3. Experiments and Results

# 3.1. Experiment Setup

For our experiments,agents run in a Docker container with the repository preconfigured and no Internet access, preventing them from retrieving external information. We further remove the GitHub remote and any future commits to avoid the possibility of models scraping code difs or pull request details.Each agent has a single attempt (pass@1),mirroring the constraints of typical online SWE freelancing platforms.Models operate with a basic scaffold that lets them browse the local codebase,modify files,and execute terminal commands.Full details of our execution environment and scaffolds can be found in Appendix A5.

For our baseline setup, we evaluate OpenAI's gpt-4o-2024- 08-06, OpenAI's ol (with "high" reasoning effort (OpenAI, 2025)),and Anthropic's Claude 3.5 Sonnet (claude-3-5- sonnet-20240620).Additional results on open-source models (Deepseek-R1, Llama 3.3) are in Appendix A15. All models have the user tool enabled.1

# 3.2. Main Results

As shown in Figure 5,all models earn well below the full \$1 million USD of possible payout on the full SWE-Lancer dataset.

![](images/d1bfbd8e758f8bf5457bab4e3ddd40caf8b2c4d643b08b0a0bcc10ce1ad07c33.jpg)  
Figure 5.Total payouts earned by each model on the full SWELancer dataset including both IC SWE and SWE Manager tasks.

To illustrate model performance across experiments,we present the pass @1 accuracies,corresponding “earnings" (i.e.,total payout),and earn rates (i.e.,payout received divided by total possible payout) for IC SWE and SWE Manager tasks, summarized in Table 1.² All models earn well below the full \$1 million USD of possible payout on the full SWE-Lancer dataset.

As shown in Figure 6,all models performed better on SWE Manager tasks than on IC SWE tasks,which remain largely unsaturated. For IC SWE tasks, pass @1 and earnings rates remain below 30%. On SWE Manager tasks, the best performing model- Claude 3.5 Sonnet - scores 45% on the

![](images/5c92a3ea55e7e2013842746cf702f06dd4344757aac0c4aae63bf1c0fe777738.jpg)  
Figure 6.Model pass @1 performance on IC SWE and SWE Manager in the Diamond set and the full set; Claude 3.5 Sonnet performs the best and earns \$2O8K in total on the Diamond set and over \$40OK on the Full set.

Diamond set. 3.5 Sonnet exhibits the strongest performance on both IC SWE and SWE Manager tasks, outperforming the next-best model (o1) by 9.7% on IC SWE (Diamond) tasks and 3.4% on SWE Manager (Diamond) tasks.

# 3.3.Increasing Number of Attempts

To evaluate how performance changes with more attempts, we evaluate GPT-4o and o1 using the pass @k metric (Chen et al.,2O21).We estimate the percentage of tasks which the agent solves correctly, given n total samples and c correct (n- C samples as: pass@k := EProblems|1 k .The main result for k ∈ [1,7] is shown in Figure 7. For all models, allowing more attempts leads to a consistent increase in pass rate. This curve is especially noticeable for ol,where allowing for 6 more atempts nearly triples the percentage of solved tasks. GPT-4o with pass@6 achieves the same score as o1 with pass @1 (16.5%).

# 3.4.Increasing Test Time Compute

We measure pass @1 as a function of inference-time compute. On IC SWE tasks within the Diamond set, experiments with ol and user tool enabled show that higher reasoning effort (test-time compute) improves pass @1 from 9.3% (Low reasoning effort) to 16.5% (High reasoning effort),with corresponding increases in earnings (which increased from \$16K to \$29K) and earn rates (increased from 6.8% to 12.1%). Figure 8 shows the breakout of pass @1 by task price range for different levels of test-time compute and indicates greater test-time-compute can improve performance on harder, more expensive problems in particular.

![](images/15f92eb6ae814fd63dcfaa38b4bc26f632ebb8e5cb48af617256f0a37d1a4abd.jpg)  
Figure 7.Pass @k performance on IC SWE tasks within the SWELancer Diamond set increases with the number of attempts allowed.

![](images/f3904391bc8df5083f7732e8201b25cad5d57907aecc9871219a17f977e956e6.jpg)  
Figure 8.ol pass @1 performance on Diamond IC SWE tasks by price range of task.Higher reasoning effort improves overall pass @1 and improves performance on harder,more expensive tasks.

![](images/955ec8ffbe43ea4af604b81d3d7a7afe41c86f6b821be422f58f3f449161a8d1.jpg)  
Figure 9.Pass @1 performance on Diamond IC SWE tasks slightly decreases when the model cannot use the user tool.

Table1.Fullpass@1AccuraciesandEarings forICSWEandSWEManagerEaringsareroundedtothenearestthousand.SWE-Lancer Diamondcontains237ICSWEtasks worth \$36300and265SWEManagertasks worth \$64500,totaling502tasksworth\$500,800. The fullsethas764ICSWEtasks worth\$414,775and74SWEManagertasks worth \$8525,totaling1,48 tasks worth\$,00. Inthetablebelow,overallmetricsforSWE-LancerDiamondandSWE-LncerFullsetsarehighlightedinblue,whilebaselinemtricsfor IC SWE (Diamond) and SWE Manager (Diamond) tasks are highlighted in green.   

<table><tr><td>Model</td><td>User Tool</td><td>Dataset</td><td>Reasoning Effort</td><td>pass@1</td><td>Dollars Earned /Total</td><td>Earn Rate</td></tr><tr><td>GPT-40</td><td>Yes</td><td>IC SWE (Diamond)</td><td>N/A</td><td>8.0%</td><td>$14k / $236k</td><td>6.0%</td></tr><tr><td>01</td><td>Yes</td><td>IC SWE (Diamond)</td><td>Low</td><td>9.3%</td><td>$16k / $236k</td><td>6.8%</td></tr><tr><td>01</td><td>Yes</td><td>IC SWE (Diamond)</td><td>Medium</td><td>15.6%</td><td>$24k / $236k</td><td>9.9%</td></tr><tr><td>01</td><td>Yes</td><td>IC SWE (Diamond)</td><td> High</td><td>16.5%</td><td>$29k / $236k</td><td>12.1%</td></tr><tr><td> 3.5 Sonnet</td><td>Yes</td><td>IC SWE (Diamond)</td><td>N/A</td><td>26.2%</td><td>$58k / $236k</td><td>24.5%</td></tr><tr><td>GPT-40</td><td>No</td><td>IC SWE (Diamond)</td><td>N/A</td><td>8.0%</td><td>$17k /$236k</td><td>7.2%</td></tr><tr><td>01</td><td>No</td><td>IC SWE (Diamond)</td><td>High</td><td>13.1%</td><td>$23k/$236k</td><td>9.7%</td></tr><tr><td>GPT-40</td><td>Yes</td><td>IC SWE (Full)</td><td>N/A</td><td>8.6%</td><td>$29k /$415k</td><td>6.9%</td></tr><tr><td>01</td><td>Yes</td><td>IC SWE (Full)</td><td>High</td><td>20.3%</td><td>$78k/$415k</td><td>18.9%</td></tr><tr><td>3.5 Sonnet</td><td>Yes</td><td>IC SWE (Full)</td><td>N/A</td><td>21.1%</td><td>$89k /$415k</td><td>21.5%</td></tr><tr><td> GPT-40</td><td>N/A</td><td>SWE Manager (Diamond)</td><td>N/A</td><td>37.0%</td><td>$125k /$265k</td><td>47.1%</td></tr><tr><td>01</td><td>N/A</td><td>SWE Manager (Diamond)</td><td>High</td><td>41.5%</td><td>$137k / $265k</td><td>51.8%</td></tr><tr><td>3.5 Sonnet</td><td>N/A</td><td>SWE Manager (Diamond)</td><td>N/A</td><td>44.9%</td><td>$150k / $265k</td><td>56.8%</td></tr><tr><td>GPT-40</td><td>N/A</td><td>SWE Manager (Full)</td><td>N/A</td><td>38.7%</td><td>$275k /$585k</td><td>47.0%</td></tr><tr><td>01</td><td>N/A</td><td>SWE Manager (Full)</td><td>High</td><td>46.3%</td><td>$302k /$585k</td><td>51.6%</td></tr><tr><td>3.5 Sonnet</td><td>N/A</td><td>SWE Manager (Full)</td><td>High</td><td>47.0%</td><td>$314k/$585k</td><td>53.7%</td></tr><tr><td> GPT-40</td><td>N/A</td><td> SWE-Lancer Diamond</td><td>N/A</td><td>23.3%</td><td> $139k / $501k</td><td> 27.7%</td></tr><tr><td>01</td><td>N/A</td><td> SWE-Lancer Diamond</td><td> High</td><td>29.7%</td><td> $166k / $501k</td><td> 33.1%</td></tr><tr><td> 3.5 Sonnet</td><td>N/A</td><td> SWE-Lancer Diamond</td><td>N/A</td><td> 36.1%</td><td>$208k / $501k</td><td> 41.5%</td></tr><tr><td> GPT-40</td><td>N/A</td><td>SWE-Lancer Full</td><td>N/A</td><td>23.3%</td><td> $304k / $1M</td><td></td></tr><tr><td>01</td><td>N/A</td><td> SWE-Lancer Full</td><td> High</td><td>32.9%</td><td>$380k / $1M</td><td>30.4% 38.0%</td></tr><tr><td> 3.5 Sonnet</td><td>N/A</td><td> SWE-Lancer Full</td><td>N/A</td><td>33.7%</td><td> $403k / $1M</td><td>40.3%</td></tr></table>

![](images/366bfa4d4e53c5ee00a91eac695dddfa8463592aff6097f871db02969ba0918b.jpg)  
\*5oli   
Figure 10.Atemptingtasks witholbefore passingfailedtass tofreeancers couldreducecosts by1/3onDiamond ICSWE tasks.

# 3.5.Removing User Tool

In IC SWE tasks,removing the user tool only minimally reduces pass@1 rates, as shown in Figure 9. However, we observe that stronger models make more effective use of the

user tool and therefore experience a greater performance drop under this ablation.

# 3.6. Price Analysis

How does the cost of running these models compare to the cost of freelancers? We performed 5 runs of GPT-4o and o1 on SWE-Lancer Diamond IC SWE tasks (237 tasks, worth \$236,300 total) using the OpenAI API and our open-source eval harness.Because each SWE-Lancer task has an actual cost paid out to the freelancer who solved it, we can compare API vs. freelancer costs in solving all tasks.We measured several key metrics (results below are the average over 5 runs):

1. Cost Savings (pass@1): GPT-4o fails 90% of IC SWE tasks on the first try,and ol fails 80%,but using models first and deferring to humans only on tasks the models fail would reduce total costs.We calculate cost savings as: Cost savings = 1- Modelcosts +Freeacer costs ontasks modelfilsCompared to Total freelancercost tosolveall tasks the freelancer cost of solving all 237 tasks (\$236,300 empirically), GPT-4o + freelancer fallback is 8.84% cheaper; o1 + freelancer fallback is 13.26% cheaper.

2. Cost Savings (pass@5): Giving models 5 attempts before freelancer fallback improves savings and pass rates: GPT-4o reaches 18.6% total cost savings; 01 yields 33.5% savings as shown in Figure 10.

3. Model Contribution Ratio = modelcosttoete: GPT-40 returns \$O.1O per \$1 spent in total model+freelancer costs;ol returns \$O.16 per \$1 spent. This ratio is low because models fail most tasks in SWE-Lancer Diamond and require freelancers to solve the rest, but should increase for new frontier models and may alreadybe high for easier tasks.

This preliminary analysis contains many limitations we hope future work can expand upon. First, it is based on cost to a customer (i.e., using API pricing). Our estimate of API costs in this analysis includes retries and excludes batch API pricing, which would both reduce model cost and increase the model contribution ratio. Second, all SWE-Lancer prices reflect historical bounties actually paid, which may differ from current rates. Third, there was nontrivial variance in cost across runs.The 95% confidence interval for GPT-4o was \$194.31 ± \$17.31,while o1's was \$1,623.76± \$56.68. Lastly,our pass@5 savings analysis assumes quick verification of correct solutions,but in practice this can take over 24 hours.Please see Appendix Section A14 for more detail.

# 3.7. Discussion

Results indicate that the real-world freelance work in our benchmark remains challenging for frontier language models. The best performing model, Claude 3.5 Sonnet, earns \$208,05O on the SWE-Lancer Diamond set and resolves 26.2% of IC SWE issues; however, the majority of its solutions are incorrect,and higher reliability is needed for trustworthy deployment.

The strongest models perform well across all task types. Tables 2 and 3 show pass @1 rates for IC SWE Diamond tasks across different task categories.Sonnet 3.5 performs best, followed by ol and then GPT-4o,and pass@1 on Manager tasks is often more than double pass rate on IC SWE tasks. Sonnet 3.5 outperforms ol by nearly 15% on UI/UX tasks in particular,and nearly 1O% on IC SWE tasks involving implementing new features or enhancements.

Effective tool use distinguishes top performers.We find that the strongest models make frequent use of the user tool, and are able to efficiently parse its outputs to reproduce, localize,and iteratively debug issues.The user tool often takes a nontrivial amount of time to run - a period of 90 to 120 seconds - during which weaker models such as GPT-4o are prone to abandoning the tool altogether. The best performing models reason about the delay (which is disclosed in their instructions),set appropriate timeouts,and review results when available.An example user tool trajectory is in Appendix A10.

Agents excel at localizing, but fail to root cause, resulting in partial or flawed solutions.Agents pinpoint the source of an issue remarkably quickly,using keyword searches across the whole repository to quickly locate the relevant file and functions - often far faster than a human would. However, they often exhibit a limited understanding of how the issue spans multiple components or files,and fail to address the root cause,leading to solutions that are incorrect or insuffciently comprehensive.We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit.We provide several qualitative summaries of trajectories in Appendix A9.

# 4. Limitations

Diversity of repositories and tasks.We sourced tasks exclusively from the Upwork freelancer platform and the Expensify repository. While we believe the Expensify repository is representative of full-stack engineering work, incorporating tasks from other freelance sites and open-source repositories could broaden the scope of this evaluation. In particular, infrastructure engineering tasks (e.g. refining Kubernetes cluster architectures and debugging pod failures, node crashes, or networking problems) are underrepresented in this iteration of SWE-Lancer because they are disproportionately underrepresented in Expensify's posted task set.

Scope.Freelance tasks tend to be more self-contained than full-time software engineering tasks (Gupta et al., 2020). SWE-Lancer would also be less representative of “zero to one” SWE work as all tasks build off of a well-established and maintained codebase. However, the entire front-end portion of the Expensify repository is maintained by freelance software engineers,which makes it closer to real-world engineering than other freelance postings.We also include SWE management tasks,which freelancers rarely handle. Still, while SWE-Lancer may be a good leading indicator of how automation could affect freelance software engineering, we remain cautious about extrapolating impact beyond this.

Table 2. Diamond Task Pass Rates by Task Type   

<table><tr><td rowspan="2">Task Type</td><td colspan="4">IC SWE</td><td colspan="4">SWE Manager</td></tr><tr><td>GPT-40</td><td>01</td><td>Sonnet 3.5</td><td>n</td><td>GPT-40</td><td>01</td><td>Sonnet 3.5</td><td>n</td></tr><tr><td>Application Logic (Client-Side)</td><td>8.0%</td><td>15.9%</td><td>23.9%</td><td>176</td><td>36.3%</td><td>42.3%</td><td>45.8%</td><td>201</td></tr><tr><td>UI/UX</td><td>2.4%</td><td>17.1%</td><td>31.7%</td><td>41</td><td>32.7%</td><td>32.7%</td><td>40.8%</td><td>49</td></tr><tr><td>Server-Side Logic</td><td>23.5%</td><td>23.5%</td><td>41.2%</td><td>17</td><td>53.8%</td><td>61.5%</td><td>38.5%</td><td>13</td></tr><tr><td>System-Wide Quality and Reliability Tasks</td><td>0.0%</td><td>0.0%</td><td>0.0%</td><td>3</td><td>100.0%</td><td>50.0%</td><td>100.0%</td><td>2</td></tr></table>

Modalities.SWE-Lancer tasks are text-only, though issue writers often include a screen recording. This evaluation does not test how much screenshots or videos can improve model performance.

Environments.Unlike real engineers, the models cannot ask clarifying or follow-up questions.

Contamination. Contamination is a common limitation of coding benchmarks that rely on open-source repositories. SWE-Lancer Diamond tasks originate from public GitHub issues between 2023 and 2024. Depending on model cutoff date,contamination in training data is possible.Moreover, because tasks are posted online,a model with browsing capabilities or an internet search tool can look up solutions. Therefore, when running this eval, it is important to either disable browsing or limit sites accessed to prevent directlookups,and to ideally also post-hoc filter for any instances of cheating.This will help ensure evaluation results measure real capability rather than contamination.Note that Table 4 in Appendix A2 shows no clear performance improvement for tasks predating the models’knowledge cutoffs, suggesting limited impact of contamination for those tasks.

# 5. Future Work

Economic analysis.Model performance on SWE-Lancer maps to real-world economic value; each task in the benchmark has an associated payout that was sent to a freelancer via Upwork. We hope this benchmark enables research into the broader societal impacts of autonomous agents, including impacts on labor markets,productivity,and the acceleration of AI R&D.

Multimodality. Our current evaluation harness does not support multimodal inputs; however, the majority of Expensify GitHub issues contain videos (and,occasionally, screenshots） of the expected behavior. Additionally, the user tool saves screenshots to the output directory upon completion,which current models are unable to view. Because there is not widespread support for such inputs,we prioritized making information about the prompt and environment available via text (for example,through text-based trajectories from the user tool).

# 6. Conclusion

We introduce SWE-Lancer, a benchmark for evaluating model performance on real-world, freelance software engineering work. This software engineering benchmark maps model performance to real monetary value,and improves realism of model benchmarks by evaluating more complex full-stack software engineering and management skills. We hope SWE-Lancer enables greater research into the economic impact of AI model development.

# Impact Statement

AI models with strong real-world software engineering capabilities could enhance productivity, expand access to highquality engineering capabilities,and reduce barriers to technological progress.However, they could also shift labor demand—especially in the short term for entry-level and freelance software engineers-and have broader long-term implications for the software industry. Improving AI software engineering is not without risk.Advanced systems could carry model autonomy risk in self-improvement and potential exfiltration,while automatically generated code may contain security flaws,disrupt existing features, or stray from industry best practices,a consideration that is important if the world increasingly relies on model-generated code. SWE-Lancer provides a concrete framework to start tying model capabilities to potential real-world software automation potential, therefore helping better measure its economic and social implications.By quantifying AI progress in software engineering,we aim to help inform the world about the potential economic impacts of AI model development, while underscoring the need for careful and responsible deployment. To further support responsible AI progress,we open-source a public eval set and report results on frontier AI models to help level-set the implications of AI progress. Future work should explore the societal and economic implications of AI-driven development, ensuring these systems are integrated safely and effectively.

# Acknowledgements

This evaluation would not have been possible without the help of those at OpenAI who work on foundational systems used by teams across the company. In particular, we would like to thank Evan Mays and Kevin Liu for underlying evaluation infrastructure development, Nat McAleese for coming up with the name SWE-Lancer and for valuable feedback and collaboration on the price analysis,Rachel Dias for human data vendor sourcing and setup, Jack Edwards for open-source codebase cleanup and price analysis support, Grace Kim for task validation support, Oliver Jaffe, Jack Edwards,Giulio Starace and James Aung for support with running open-source models,Patrick Chao for plotting library development, Olivia Watkins and Philip Guo for valuable feedback on the price analysis,Phoebe Thacker andMia Glaese for human data team leadership,Olivia Watkins,Patrick Chao,Miles Wang,Leyton Ho,Andy Applebaum,Leon Maksin, Joel Parish,Kevin Liu,Aleksander Madry,and Alex Beutel for feedback on earlier drafts of the paper, the many software engineer annotators for qualitychecking tasks, the OpenAI Preparedness, Safety Systems, and Research organizations for sponsoring this work, and the Upwork and Expensify developers and contributors for open-sourcing their original process.

# References

Anthropic. Responsible scaling policy. Technical report, Anthropic， October 2024. URL https://assets.anthropic.com/ m/24a47b00f10301cd/original/ Anthropic-Responsible-Scaling-Policy-2 pdf.

Austin, J., Odena,A., Nye,M.,Bosma,M.,Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q, and Sutton, C.Program synthesis with large language models,2021.URL https://arxiv.org/abs/2108. 07732.

Brar, H. K. and Kaur, P. J. Differentiating integration testing and unit testing. In 2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom), pp. 796-798,2015.

Chen,M.,Tworek,J., Jun,H., Yuan,Q., de Oliveira Pinto, H. P.,Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray,A., Puri,R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power,A., Kaiser,L., Bavarian, M., Winter, C., Tillet, P., Such, F.P., Cummings, D.，Plappert,M.,Chantzis,F.,Barnes,E.，HerbertVoss,A., Guss,W.H.，Nichol,A.,Paino,A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders,W., Hesse, C., Carr, A. N., Leike, J.，Achiam, J.,Misra,V.,Morikawa,E.,Radford,A., Knight, M., Brundage,M.,Murati,M.,Mayer,K.,Welinder,P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I.,and Zaremba, W. Evaluating large language models trained on code,2021. URL https://arxiv.org/abs/ 2107.03374.

Chowdhury, N., Aung,J., Shern, C. J., Jaffe, O., Sherburn,D., Starace, G., Mays, E., Dias, R., Aljubeh, M., Glaese, M., Jimenez, C.E., Yang, J.,Liu, K.,and Madry, A.Introducing swe-bench verified.arXiv preprint arXiv:2407.01489,2024.

DeepMind,G. Introducing the frontier safetyframework,2024. URL https: //deepmind.com/blog/article/ introducing-the-frontier-safety-framew

Expensify. Issue #14958: zip/postcode validation error message not displayed for entering‘,‘ on the home address screen. https://github.com/Expensify/ App/issues/14958,2023.

Expensify. Issue #25889: Dev: Share code avatar differs from profile avatar.https://github.com/ Expensify/App/issues/25889,2024a.

Expensify. Issue #41239:Add support for copy/pasting images on ios. https://github.com/Expensify/ App/issues/41239,2024b.

Gupta, V.,Fernandez-Crehuet, J.M.,and Hanne,T. Freelancers in the software development process: A sys  
4-tefatie mappngstudy.Processes8(1):1215,2020. doi:10.3390/pr8101215.URL https://www.mdpi. com/2227-9717/8/10/1215.

Hendrycks,D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo,E., Burns, C., Puranik, S., He,H., Song, D., and Steinhardt, J. Measuring coding challenge competence with apps,2021. URL https://arxiv.org/abs/ 2105.09938.

Jain,N., Han, K., Gu,A., Li, W.-D., Yan, F., Zhang, T., Wang， S.， Solar-Lezama, A.， Sen， K.,and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024.URL https://arxiv.org/abs/2403.07974.

Jimenez, C.E., Yang, J., Wettig,A., Yao, S., Pei, K.,Press, O.,and Narasimhan, K. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770.

Lai,Y., Li, C., Wang, Y., Zhang, T., Zhong,R., Zettlemoyer, L., tau Yih, S.W.,Fried, D., Wang, S., and Yu, T. Ds1000:A natural and reliable benchmark for data science code generation, 2022. URL https : //arxiv .org/ abs/2211.11501.

Lam,M. S., Teoh, J.,Landay,J. A., Heer, J., and Bernstein, M. S. Concept induction: Analyzing unstructured text with high-level concepts using lloom. In Proceedings of the CHl Conference on Human Factors in Computing Systems,CHI'24,pp.1-28.ACM,2024.doi: 10.1145/ 3613904.3642830. URL http://dx.doi .org/10. 1145/3613904.3642830.

Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J.,Leblond,R., Eccles, T.， Keeling，J.， Gimeno,F., Dal Lago, A., Hubert, T., Choy, P., de Masson d'Autume, C.,Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J., Gowal， S.， Cherepanov,A.，Molloy， J.， Mankowitz, :k.D.J.，Sutherland Robson，E.，Kohli,P.，de Freitas, N.,Kavukcuoglu, K.，and Vinyals, O. Competitionlevel code generation with alphacode. Science,378 (6624):1092-1097,December 2022. ISSN 1095-9203. doi: 10.1126/science.abq1158. URL http ://dx .doi . org/10.1126/science.abql158.

Liu, T., Xu, C.,and McAuley, J. Repobench: Benchmarking repository-level code auto-completion systems,2023. URLhttps://arxiv.org/abs/2306.03091.

Lu,S., Guo, D.,，Ren， S., Huang, J.,， Svyatkovskiy,A., Blanco,A., Clement, C., Drain,D., Jiang,D., Tang,D., Li, G., Zhou,L., Shou,L., Zhou,L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N.,Deng, S. K., Fu, S., and Liu, S. Codexglue: A machine learning benchmark dataset for code understanding and generation,2021. URL https://arxiv.org/abs/2102.04664.

Microsoft. Playwright: Fast and reliable end-to-end testing for modern web apps. https://playwright . dev/,2025.

Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T.Y.， Singh, S., Tang, X., von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models, 2024. URL https : //arxiv.org/ abs/2308.07124.

Nijkamp, E., Pang,B., Hayashi, H., Tu,L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. Codegen: An open large language model for code with multi-turn program synthesis,2023. URL https://arxiv.org/abs/2203. 13474.

OpenAI. Openaipreparednessframework, 2023. URL https://cdn.openai.com/ openai-preparedness-framework-beta. pdf.

OpenAI. Learning to reason with llms,2024a.

OpenAI. Openai o3 and o3-mini-12 days of openai: Day 12,2024b.

OpenAI. OpenAI Authentication API Documentation, 2025. URL https://platform.openai.com/ docs/api-reference/authentication.

Paul，R.， Tsai，W. T.，Chen，Y.， Fan， C.，Cao，Z., and Huang,H. End-to-end (e2e) testing and evaluation of high-assurance systems. In Pham,H. (ed.), Springer Handbook of Engineering Statistics, Springer Handbooks,pp.439-467.Springer, London,London, 2006.ISBN 978-1-85233-806-0．doi: 10.1007/ 978-1-84628-288-1_24. URL https://doi.org/ 10.1007/978-1-84628-288-1_24.

Quan, S., Yang, J., Yu,B., Zheng,B.,Liu, D., Yang,A., Ren, X., Gao, B.,Miao, Y., Feng, Y., Wang, Z., Yang, J.,Cui, Z., Fan, Y., Zhang, Y., Hui, B., Lin, J.,and Qwen Team, A.G. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257, 2025. URL ht tps : //arxiv.org/abs/2501.01257.

Si, C.， Yang,D.,and Hashimoto, T. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers,2024. URL https: //arxiv. org/abs/2409.04109.

Tian, M., Gao,L., Zhang, S.D., Chen, X.,Fan, C., Guo, X., Haas, R., Ji, P., Krongchon, K., Li, Y., Liu, S., Luo, D., Ma, Y., Tong, H., Trinh, K., Tian, C., Wang, Z., Wu, B., Xiong, Y., Yin, S., Zhu, M., Lieret, K., Lu, Y., Liu, G., Du, Y., Tao,T., Press, O., Callan, J.,Huerta,E.,and Peng, H. Scicode: A research coding benchmark curated by scientists,2024. URL https://arxiv.org/abs / 2407.13168.

Yang, J., Jimenez, C. E., Zhang,A.L., Lieret, K., Yang, J., Wu, X., Press, O., Muennighoff, N., Synnaeve, G., Narasimhan, K. R., Yang, D., Wang, S. I., and Press, O. Swe-bench multimodal: Do ai systems generalize to visual software domains?, 2024. URL https : / / arxiv.0rg/abs/2410.03859.

Zhang, S., Zhao, H., Liu, X., Zheng, Q., Qi, Z., Gu, X., Zhang, X., Dong,Y.,and Tang, J. Naturalcodebench: Examining coding performance mismatch on humaneval and natural user prompts,2024. URL https : //arxiv . org/abs/2405.04520.

Zhong,R., Zhang, P.,Li, S., Ahn, J., Klein, D., and Steinhardt, J. Goal driven discovery of distributional differences via language descriptions,2023. URL https : //arxiv.org/abs/2302.14233.

Zhuo, T. Y., Vu, M. C., Chim, J., Hu, H., Yu, W., Widyasari, R.,Yusuf, I. N.B., Zhan,H., He, J., Paul, I.,Brunner，S., Gong,C.,Hoang, T., Zebaze,A.R.,Hong, X., Li, W.-D., Kaddour, J., Xu, M., Zhang, Z.， Yadav, P., Jain, N., Gu, A., Cheng, Z., Liu, J., Liu, Q., Wang, Z., Lo, D., Hui, B., Muennighoff, N., Fried, D., Du, X., de Vries,H., and Werra, L. V. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions, 2024. URL https : //arxiv.org/abs/2406.15877.

# A. Appendix

# A.1. Pass@1 Rate Analysis by Category

Table 3 shows pass@1 rates for SWE-Lancer Diamond categorized by the tasks‘ nature of work.

Table 3.Diamond Task Pass Rates by Nature of Work   

<table><tr><td rowspan="2">Task Type</td><td colspan="4">IC SWE</td><td colspan="4"> SWE Manager</td></tr><tr><td>GPT-40</td><td>01</td><td>Sonnet 3.5</td><td>n</td><td>GPT-40</td><td>01</td><td>Sonnet 3.5</td><td>n</td></tr><tr><td>Bug Fixes</td><td>9.6%</td><td>19.2%</td><td>28.4%</td><td>208</td><td>34.7%</td><td>39.9%</td><td>44.0%</td><td>248</td></tr><tr><td>New Features or Enhancements</td><td>0.0%</td><td>4.8%</td><td>14.3%</td><td>21</td><td>69.2%</td><td>69.2%</td><td>61.5%</td><td>13</td></tr><tr><td>Maintenance, QA,Testing,or Reliability</td><td>0.0%</td><td>12.5%</td><td>0.0%</td><td>8</td><td>75.0%</td><td>50.0%</td><td>50.0%</td><td>4</td></tr></table>

To determine each task's category (as shown in Tables 2 and 3),we had ol review each entire Github issue thread, including alldescriptions andcomments,tree separate times for each ICSWE and SWE Management task.If these three categorizations were not unanimous, we manually assigned the final categories.

# A.2. Contamination Analysis

Table 4.Diamond Pass @1 Rate Analysis Based on Task Creation Date.   

<table><tr><td></td><td></td><td colspan="2">IC SWE</td><td colspan="2">Manager</td></tr><tr><td>Model</td><td>Cutoff Date</td><td>Pass @1 (Before)</td><td>Pass@1 (After)</td><td>Pass @1 (Before)</td><td>Pass @1 (After)</td></tr><tr><td>GPT-40</td><td>2023-10-01</td><td>0.038 (n=79)</td><td>0.101 (n=158)</td><td>0.373 (n=142)</td><td>0.244 (n=123)</td></tr><tr><td>01</td><td>2023-10-01</td><td>0.139 (n=79)</td><td>0.177 (n=158)</td><td>0.387 (n=142)</td><td>0.407 (n=123)</td></tr><tr><td>Claude 3.5 Sonnet</td><td>2024-04-01</td><td>0.178 (n=118)</td><td>0.345 (n=119)</td><td>0.452 (n=210)</td><td>0.436 (n=55)</td></tr></table>

# A.3.Further exploration of SWE Management Tasks

Within SWE Management tasks,the winning proposal is often selected for nuanced reasons,such asaddresing edge cases or reducing external dependencies.For example,one task (Expensify,2O24b)involves selecting a proposal for implementing image-pasting on iOS.The wining proposal handled multiple clipboard formats (e.g.,base64,blob),minimized permission prompts,and aligned with native iOS behavior by enabling direct pasting into the compose bar. Other proposals were deprioritized due to reliance on third-party libraries,complex modifications,or inconsistent cross-platform support.

Example: "[\$2ooo] Add support for copy/pasting images on iOS #41239 Description (paraphrased):Users want theability to copyan image to their clipboard and paste it directly into a ChatintheExpensifyiOSapp.Currently,users mustsavethe imageandmanuallyuploadit,which isinefficient. IC SWEs submit proposals original bounty: \$250 (April 30) ↓bounty increase \$50o (May 16) → \$1K (May 23) final bounty: \$2K (July 9) Proposal 1 by @ShridharGoel ...Proposal 3 by @suneox ...Proposal 4 by @s77rt Proposed Changes & Optional Images Proposed Changes & Optional Images Proposed Changes & Optional Images Add a new option "Paste image"in the+ Use case 1: Handling base64 image data iOS:RCTUITextView menu of chats,which willshow only when 1.Patch canPerformAction: method to ontesthas forcesthteoptioifthe 日 attachment modal with the image. string) ss 回 2.Patch paste:method to setInageToBePasted(data); j/Stg Andromponentviewviaonasteprop ） 1.Patch onTextContextMenuItem method Newoptioninattachmentpicker: t,1eg(file)); via onPaste prop @ Usecase 2: Handing imagescopiedas et,(fstel) App:Composer 1.Add onPaste prop and ens1cons. blobs (shortened to fit) call Clipboard.getImagePNG/Clipb oard.getImage }) Unaersost Thisproposaladdsa"Pasteimage"optiontothe Thisproposal detectsclipboardcontentas This new proposaluses patches to native ioschatmenu，whichetectscopiedcliord base64stringsorblobs,convertsthemintofile componentstocomprehensivelyadd imagepaste images,validatesormat,storestheUad objects,andenablesuserstopasteintuitively supportfortheComposeronbothioSand opensimagesintheattachmentmodalforsharing. throughlong-pressgesturesora"Pastelmage" Android. button. ← ← 2 SWE Managers provide feedback on proposals and ultimately accept or reject them SWE Manager Feedback: "Pasting the image SWE Manager Feedback (after some back and from the URLin the clipboard seemsa forth):"Ithink your method to detect workaround,because when we copied the onPasteFile and make the paste option SWE Manager Feedback:"Can you share a image,we click "copy image"not "copy image appearafter copyingan imagefrom the gallery test branch? Also are we able to use the same address",sowemayneedtohandlethmage isunreliable.If Clipboard.getImagePNG() approach in Android, Clipboard asfileObject or base64 string. works for iOS and Clipboard.getImage(）for has Clipboard.getImage() for Android" nde IC SWE: Yes,'ve updated the proposal. clicking on the input then click paste." gettheimagecontent.What doyouthink?" onlyworked for image URLs,which is Srsa Rejeesive.t X Prwasa Reecteiv.t handled imagescopiedfromthe native,handlingallcases.Ithandled SOu Prowosal Accepntid and uncommon,and failed tosupport raw gallerybutfailedtosupportweb galleryand web imagesas raw image imagedata. images. contet Figure11.Real-world ilustrationoftheiteration processforsubmitingand chosingsolutionstoGithub issues.This task was incorporated into our SWE-Lancer dataset.

# A.4. Coding and Software Engineering Evaluations Comparison Table

Table 5.ComparisonofSWE-LancertoexistingcodingandSWE-relatedbenchmarks:SWE-Lanceruniquely maps modelperformancet monetarymetricscorporatesfulacktassadmanagementtassndvauatessingexeutio-baseded-tdtestsatd by engieers. In contrast, other benchmarks rely on pre-existing tests within repositories or tests generated by LLMs.   

<table><tr><td rowspan=1 colspan=1>Evaluation</td><td rowspan=1 colspan=1>Language</td><td rowspan=1 colspan=1>Dataset Source</td><td rowspan=1 colspan=1>Data Type</td><td rowspan=1 colspan=1>CapabilitiesTested</td><td rowspan=1 colspan=1>MonetaryMetric?</td><td rowspan=1 colspan=1>EvaluationMethod</td><td rowspan=1 colspan=1>Source ofEvaluationMethod</td></tr><tr><td rowspan=1 colspan=1>SWE-Lancer</td><td rowspan=1 colspan=1> Javascript /Typescript</td><td rowspan=1 colspan=1>Open-source andcommercialExpensify repo ofissues posted onUpwork</td><td rowspan=1 colspan=1>Professionalsoftware engineeringproblem statements</td><td rowspan=1 colspan=1>Software engineering</td><td rowspan=1 colspan=1>?</td><td rowspan=1 colspan=1>End-to-endtest</td><td rowspan=1 colspan=1> Hand-written byprofessionalsoftware engineers</td></tr><tr><td rowspan=1 colspan=1>SWE-Bench Verified</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Open-sourceGitHub repos</td><td rowspan=1 colspan=1>GitHub issues fromall developerframeworks</td><td rowspan=1 colspan=1>Softwaremaintenance</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Sourced fromoriginal repository</td></tr><tr><td rowspan=1 colspan=1>SWE-BenchMultimodal</td><td rowspan=1 colspan=1>Javascript /Typescript/HTML/CSS</td><td rowspan=1 colspan=1>Open-sourceGitHub repos</td><td rowspan=1 colspan=1>GitHub issuesfocused ondeveloper tools andutilities</td><td rowspan=1 colspan=1>Softwaremaintenancetasks usingmultimodaldata</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Sourced fromoriginal repository</td></tr><tr><td rowspan=1 colspan=1>RepoCoder</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Open-source Pythonrepos</td><td rowspan=1 colspan=1>GitHub issues fromPython repositoriesthat meet certaincriteria (post Jan‘23,non-fork,100+stars,80%+ Python,unit tests)</td><td rowspan=1 colspan=1>Codecompletion</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test,similaritymetrics</td><td rowspan=1 colspan=1>Sourced fromoriginal repository</td></tr><tr><td rowspan=1 colspan=1>RepoBench</td><td rowspan=1 colspan=1>Python andJava</td><td rowspan=1 colspan=1>Github-CodeDataset and newlycrawled GitHub data(post-Feb 2023)</td><td rowspan=1 colspan=1>Single and multi-filecode snippets</td><td rowspan=1 colspan=1>Code retrievaland completion</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Similaritymetrics</td><td rowspan=1 colspan=1>N/A</td></tr><tr><td rowspan=1 colspan=1>LiveCodeBench</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>LeetCode, AtCoder,and CodeForcescontests</td><td rowspan=1 colspan=1>Coding problemsdesigned for use incompetitiveprogrammingcompetitions</td><td rowspan=1 colspan=1>Codegeneration</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Generated usingLLMs</td></tr><tr><td rowspan=1 colspan=1>NaturalCodeBench</td><td rowspan=1 colspan=1>Python andJava</td><td rowspan=1 colspan=1>User queries fromonline codingservices</td><td rowspan=1 colspan=1>Problem statementsderived fromreal-world userqueries</td><td rowspan=1 colspan=1>Codegeneration</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Automatedunit testing</td><td rowspan=1 colspan=1>Generated usingGPT-4 then refinedby humans</td></tr><tr><td rowspan=1 colspan=1>BigCodeBench</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Human expertcuration</td><td rowspan=1 colspan=1>Natural languagetasks composed ofmultiple substeps</td><td rowspan=1 colspan=1>Codecompletion</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Generated usingGPT-4 then refinedby humans</td></tr><tr><td rowspan=1 colspan=1>HumanEval</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Hand-written byhuman experts</td><td rowspan=1 colspan=1>Python functions</td><td rowspan=1 colspan=1>Codegeneration</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit tests</td><td rowspan=1 colspan=1>Hand-written byhuman experts</td></tr><tr><td rowspan=1 colspan=1>CodeXGLUE</td><td rowspan=1 colspan=1>Python /Java/JS/PHP/Ruby/Go /C/C#)</td><td rowspan=1 colspan=1>Open-sourceGitHub repos &amp;newly curated sets</td><td rowspan=1 colspan=1>14 task-specificcode/text datasets</td><td rowspan=1 colspan=1>Codeunderstanding&amp; generation</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test,similaritymetrics</td><td rowspan=1 colspan=1>Benchmark authors(Lu et al., 2021)</td></tr><tr><td rowspan=1 colspan=1>MBPP</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Crowd sourced,some are expertverified</td><td rowspan=1 colspan=1>Short entry-levelPython functions</td><td rowspan=1 colspan=1>Codecompletion</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Crowd sourced</td></tr><tr><td rowspan=1 colspan=1>MathQA-Python</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Converted fromMathQA</td><td rowspan=1 colspan=1>Solvingmathematical wordproblems usingPython</td><td rowspan=1 colspan=1>Pythonprogramsynthesis</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Correctness</td><td rowspan=1 colspan=1>Sourced fromMathQA</td></tr><tr><td rowspan=1 colspan=1>AlphaCode</td><td rowspan=1 colspan=1>C++/Python /Java</td><td rowspan=1 colspan=1>Scraped fromCodeforces</td><td rowspan=1 colspan=1>Competition codingproblems</td><td rowspan=1 colspan=1>Codegeneration</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Scraped fromCodeforces</td></tr><tr><td rowspan=1 colspan=1>MTPB</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Human expertwritten</td><td rowspan=1 colspan=1>Multi-turn codingproblems andprompts</td><td rowspan=1 colspan=1>Codecompletion</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Human expertwritten</td></tr><tr><td rowspan=1 colspan=1>DS-1000</td><td rowspan=1 colspan=1>Python</td><td rowspan=1 colspan=1>Collected fromStackOverflow</td><td rowspan=1 colspan=1>Data scienceproblems</td><td rowspan=1 colspan=1>Fixing code</td><td rowspan=1 colspan=1>×</td><td rowspan=1 colspan=1>Unit test</td><td rowspan=1 colspan=1>Collected fromStackOverflow</td></tr></table>

Comparing Learnings Against Prior Benchmarks: For SWE-Lancer's IC SWE tasks,one comparable benchmark is SWE-Bench Multimodal, where GPT-4o solved 12.2% of visual programming tasks when given the best scaffold and multimodal capabilities.This is comparable to GPT-4o scoring 8.0% onour IC SWE Diamond tasks when the model can ingest image screeshots via the user tool.On SWE-Bench Verified,the industry standard benchmark for software enginering, GPT-4o curently scores 38.8% using the best available scaffold,but only 8%on SWE-Lancer Diamond IC SWE tasks -likely because SWE-Bench Verified focuses on bugfixes,and SWE-Lancer includes open-ended problems (adding new features,reliability)andful-stack development.This highlights the need for more chalenging,unsaturated

SWE benchmarks.

# A.5.Additional Scaffold Details

Each agentis executed within a Microsoft Azure Standard-D2as-v4 virtual machine with 2 vCPUs and 8GB RAM. We execute agents within Docker containers using a pre-built SWE-Lancer image, which we open-source.The container has 64 GB of shared memory allocated and 192GB of maximum memory available.Allthe libraries the agent can acess (i.e, the ones relevant for the Expensify repository)are installed in the container.

The agent does not have multimodal image or video inputs.For each rollut,the maximum number of toolcalls is 100 and the maximum time allowed is 3 hours.All rollouts are conducted with temperature 1.0.

Except for our pass @k experiments, each is just a single rollout.

# A.6.Task Curation Criteria

![](images/d297944a6da0152d7306852061398dab432ecff665dcce20f7b9ae33c7c80956.jpg)  
Figure12.LifecycleofanUpworktask.First,projectmaintaierspostasoftwareengiengtasktoUpworkwithnasoiatedaout. Then,userssbitiepthroosalsfofixingtesse.Finallasedviewelects teestprooal-(usedtateWE Manager tasks). The wining proposal is implemented (used to create IC SWE tasks).

We manualy filtered tasks according to the folowing criteria.Each task in our final set was screened by at least three experienced human software engineers.

1. The issue is sourced from an open-source repository.   
2. The issue has an associated, concrete payout.   
3. The issue was resolved in the past.   
4.The issue is wel-specifed and clear-models mustbe able toactuallyunderstand the taskand implement afix, without   
being able to ask for clarification.   
5. The PR that resolved the issue actually fixes it-this fix is confirmed by executing locally.

There are additional criteria for SWE Management Tasks:

1. There were at least two well-defined proposals for the issue.   
2. The proposals considered contain sufficient information about the correct fix.

Further detail is provided below on how we assess specific criteria and labeled other atributes for each given task.

To confirm that the issue was well-specifed andclear,we asked software engineers torate each issue from O-3(below) and only accepted issues where the majority of human engineers rated them as 0.

Readthe issuedescriptionagainhere,and imagine that youare anexperiencedsoftware engineer with2-3yearsofrelevant experience and familiarity with the codebase.You have been instructed to createaPR that successullyresolves it. You have fullaccess tothecodebaseandcanuse thetitle,description,reproduction steps,andanyattached videos/images fromthe linked issue when forming your solution.

Please note,however,that you are not able to ask for clarification and would need to work exclusively from the above information.Please do notread the comments below the isse when making your decision—you must rely solely on the content of the issue itself and codebase/docs.

Please assign a rating from O to 3 using the rubric below.

O- The issue is well-specified,and it is clear what is required for a successful solution.   
1-Thereare some blanks tofllinabout theissue,butthere isasensible interpretationofwhatis requiredforasuccesul solution.   
2-The issue is vague,and there is room for ambiguity.It is unclear what a successful solution would look like.   
3-It is almost impossible to understand what you are being asked to do without further information.

Issue Well-Specified Score:[Oto 3]Please explainyourchoiceabove.Includereferences to specific filenames,function/- class names,or lines of code where relevant.[Text box]

o confirm thatthe tests were valid for allreasonable,correct solutions, we asked engineers toratethe tests according D a rubric and only accepted issues where the majority of human engineers rated them as 0.

Pleasere-review the pullrequest thataddressedthe issue.Does it contain valid, wel-scoped tests so thatallreasonable solutions pass?

Please assign a rating from O to 3 using the rubric below.

O- The tests perfectly cover all possible solutions.   
1- The tests cover the majority of correct solutions; however, some unusual solutions may be missed.   
2- The tests work, but some perfectly reasonable solutions may be missed by the tests.   
3-The tests are too narrow/broad or they look for something different than what the issue is about.

# A.7.SWE-Lancer Robustness to Cheating

Existing SWE benchmarks in the literature often rely on unit tests that are narrow and prone to cheating. An example of :heating by ol in SWE-Bench Verified is shown below:

An excerpt from the problem description is included below:

Queryset raises NotSupportedError when RHs has filterable=False attribute.

In response,ol adds an underscore before filterable so that the branch never executes:

2: ASSISTANT → ALL   
django/db/models/sql/query.py   
<<<<<< SEARCH

if not getattr(expression, 'filterable'， True):raise NotSupportedError(expression.__class__.__name__ +'_is_disallowed_in_the_filter_''clause.'）

if not getattr(expression, '_filterable'，True): raise NotSupportedError( expression._class_.__name_ +'_is_disallowed_in_the_filter_' 'clause.' ）   
< dEDT△E

Aslanguage models growmorecapable,itisincreasinglyimportantopublishchalenging,un-ackable software engineering benchmarks.SWE-Lancer's human-validated end-to-end tests emulate the actual workflow of a human engineer,rather thanrelying on a handful of selectivelychosen test cases. As aresult,SWE-Lancer is inherently more resistanttocheating and can more accurately reflect a model's capacity to produce genuine,economically valuable solutions for real-world engineering challenges.

# A.8. Prompting

Models are prompted with the folowing general templatefor IC SWE Tasks. We did not find variance in performance when including vs. excluding the prices from the promtp.

# IC SWE Task Instructions

"""You are an expert software engineer maintaining the Expensify repository. It has already been cloned, and is present in /app/expensify (your CwD). You have been tasked with fixing the following issue:

{issue title} {full HTML documentation of issue， including how to reproduce the bug, the actual vs. expected behavior， and which platforms are affected}

The reward for the correct fix is \${price}.   
You do not need to push your changes to the repository, we will deal with this for you.

Do not attempt to run the app: simply make your changes and then end your turn.

Do not end your turn until you have submitted your code changes.

---

You will be provided with a user tool, which you can call by running   
"bash -i -c 'user-tool'" in the command line, which has a user open the app with your local changes and perform the actions specified in the bug report. It could take up to a minute for the user to finish using the app.   
When you call the user tool， when the user finishes   
/app/expensifv/user tool/

will be populated with folders called output_browserl ... n for as many browsers as were used by the user during their test. Inside each folder, you will find a file called trace.trace, which contains the trajectory the user took. Here are a few things to note:

- You will need to parse this file programmatically in order to open it. - If a row has "type":"screencast-frame", then the "shal" property will correspond to a .jpeg file in the resources folder, which is a screenshot of the browser at that time. If you want to view the page as HTML， rather than a JPEG, you can look for rows that are called "type":"frame-snapshot"; you can print them out to see the HTML at this point in time. You should review the rows with type "type" set to "log"， "before"， or "after" as they are important and show what actions are being taken. : All of the rows are in order， so e.g. you can find an interesting log row and then look at the next row that is a frame-snapshot to see the HTML.

You should use this to help you iteratively debug and fix the issue.   
You can call it as often as you need to.

When you call the user tool, you wait 90,000 milliseconds (e.g. by setting the timeout argument to 90000)! "" "

# SWE Management Task Instructions

f\*"You are an expert software engineering manager working on the Expensify repository. You have tasked your team with addressing the following issue:

{issue title} {full HTML documentation of issue, including how to reproduce the bug， the actual vs. expected behavior, and which platforms are affected)

You will be paid {\`price'} if your team fixes the issue. Since you are a manager， your task is to select the best proposal regarding how to fix the issue from the below options:

For your convenience, the Expensify repository has already been cloned, and is present in /app/expensify (your CwD). You should not attempt to run the app or implement the entire fix, but you can review the codebase to understand the context of the issue.

When you are ready to submit your answer， please create a JSON file in   
/app/expensify/manager_decisions.json with the following format: selected_proposal_id: int,   
}   
You must write your output to this path exactly:   
'/app/expensify/manager_decisions.json'. You must not submit to anywhere   
in /mnt/data, just write to '/app/expensify/manager_decisions.json' exactly!   
" "

# A.9. Qualitative Analysis of Rollouts

Weconduct a qualitative analysis of task instance from SWE-Lancer,focusing on model-generated solutions and debuging strategies.The analysis is based onagent trajectories under the default configuration ofo1,4o,and Claude 3.5 Sonnet.

Issue Summary. The Expensify app fails to display a validation eror message when users enter a comma(,) in the zip/postcode field on the Home Addresscreen, alowing invalid input. This issue occurs across multiple platforms and may stemfrom missing input sanitization or incomplete validation logic.The bug was observed in staging and production, potentially introduced during the N7 Account Setings update (Issue #14958).

Note on end-to-end tests.The end-to-end tests we developed for each isue are robust and reflecthe key considerations fromthe original Github discussion. Whilethis issue initially concerned the commas in zip-code validation,much of the discusion in the Github thread focused onimproving validation overall,including country-specific regex dictionaries and handling edge cases where the user hasn't selected acountry or a country lacks apostal system.Thus,our end-to-end tests measure not just whether the model's code causes the validation error to appear when entering acomma,but also whether it can comprehensively handle the full range of global zip-code formats.

# o1 Interaction Summary

# 1. Locate the Target File

· The model begins by scanning the directory structure to find relevant files:

!ls -l /app/expensify

· It searches for zip-related validation logic: !grep -irn "zip" /app/expensify

· To find the isValidZipCode function, it narrows the search:

!grep -irn "ValidationUtils" /app/expensify/src I grep -i zip !grep -irn "isvalidzipCode" /app/expensify/src

· The model identifies the right file to modify and sees that we use a regex to validate zip code:

!grep -A 20 "function isValidzipCode" /app/expensify/src/libs/ValidationUtils.ts

· The model finds a reference to Addres sForm.t sx aims to understand how the zipPost Code field is being processed and how the zip code input values are sanitized before being validated or used

!grep -C 40 "zipPostCode?.trim" /app/expensify/src/ components/AddressForm.tsx

# 2. Search for the Issue

· The model looks for where ZIPCODE appears in the codebase. Through this, it finds the regex used to validate US zip codes.

!grep -irn "zIP_CODE:" /app/expensify/src · The model looks more to see how texttAddressForm is used in the overall codebase, and specifically how zipcode is used.

!grep -A 50 -B 50 "import AddressForm" /app/expensify/src/pages/ AddressPage.tsx

# 3. Diagnose the Validation Logic

· The model reasons through the root cause of why a validation error does not display when a user enters a comma in the“zip code” field in the “Home Address Screen”,and concludes that the likely issue is that the Input Wrapper for the zip code field bypasses the form validation logic by using onValueChange. The error is being set in the validator but is not displayed due to either: 1) missing triggers for validation during typing (onChange) or focus loss (onBlur), or 2) Mismatched field IDs causing the validation result to not propagate correctly.   
It hypothesizes that the fix involves removing the manual state handler for the zipcode field,so the "Home Address” form now relies solely on the form's built-in validation logic (via the validat or prop in the FormProvider),and entering a comma in the ZIP/postcode field will immediately show the “incorrect zip format” error. (a) Validation is not triggered dynamically (e.g., on typing or focus loss).   
(b) Mismatched field IDs (zipPost Code vs. addres sZipCode) may prevent the error from displaying.

4. Modify the Code

· The model makes a patch, removing the custom onValueChange so that the form's natural validator is invoked and the zip/postcode error now displays as expected when the user enters a comma.

diff --git a/src/components/AddressForm.tsx   
b/src/components/AddressForm.tsx   
index 71928c1..ce4f25c 100644   
--- a/src/components/AddressForm.tsx   
+++ b/src/components/AddressForm.tsx   
@@ -182,7 +182,6 @@ function AddressForm({ /> </view> <View style={styles.formSpaceVertical} /> {/\* Remove onValueChange so this field uses the   
form's standard validation \*/} <InputWrapper InputComponent={TextInput } inputID={INPUT_IDS.ZIP_POST_CODE}   
@@ -190,7 +189,6 @@ function AddressForm({ label={translate('common.zip')} onValueChange={setZipcode} defaultValue={zipcode} maxLength={countryRegexDetails? .maxLength} shouldSaveDraft={shouldSaveDraft}

# 5. Clean Up and Submit

· The model submits the patch, believing the issue is resolved.   
The model'sfxaddresses the basic issueof theerrornot showingup whenthe usertypesacomma,butit does not fully address other issues which were captured in the real PR.

Evaluation.ol successfully identifed the relevant files,including ValidationUtils.ts and AddressForm.tsx, andcorrectly recognized that byremovingonValueChange itcouldfallback to the form's normal validator, which would solve this particular issue.However,ol's solution was notfullycomprehensiveand did not pass our end-to-end test.It didn't address other issues which were all addressed in the pull request and Github discussion:

· Missing Validation Triggers: ol’s patch does not enable validation on key events like onBlur oronChange, which is necessary for providing immediate fedback to users.The real pullrequest added logic to validate dynamically when the user interacts with the feld.

· No Country-Specific Validation: o1's patch does not handle country-specific zipcode formats or provide tailored feedback.The pul request implemented dynamic validation based on country-specific regexes and included zipFormat hints to guide users.

· Does Not Address Non-Zip Countries: ol's patch overlooks cases where some countries do not require a zip code. The pullrequest included logic to handle these scenarios,such as disabling or hiding the zip feld when applicable.

·Error Message Improvements: ol’s patch does not enhance error messages for clarity or localization. The pull request ensured erors were dynamically adjusted and localized based on the user's country and language setings.

# 4o Interaction Summary

# 1. Locate the Target File

· The model scans the directory structure to find relevant files:

os.listdir("/app/expensify")

· It navigates to the source code:

src_path = "/app/expensify/src" os.listdir(src_path)

· After multiple iterations, it finds and opens Addres sPage .t sx:

address_page_path = os.path.join(pages_path， 'AddressPage.tsx') with open(address_page_path，'r') as f: address_page_content = f.read() print(address_page_content[:2000])

· The model searches for AddressForm, which likely contains input validation, directing it to Validat i onUt ils .t s, where the zip/postcode validation logic is located.

# 2. Search for the Issue

· The model searches for postcode-related validation within Validat ionUt ils .t s by writing a Python function.   
· Itfnds the isValidZipCode (zipCode:string) function and its associated regex for validating zip codes.   
· The model then examines CONST.t s to locate the regex: US:{ regex: /^[0-9]{5}(?:[- ][0-9]{4})?\$/ }

· Despite correctly identifying the regex that flags commas as invalid, the model notices that the validation error message is not shown, indicating the issue lies in how errors are handled within AddressForm .t sx. ·The model searches forthe error message related to zip/postcode validation and hypothesizes it is triggered by:

errors.zipPostCode = translate   
(' privatePersonalDetails.error.incorrectZipFormat', { zipFormat: countryZipFormat });

# 3. Modify the Code

· The model decides to modify the code to ensure that validation errors display for the zip code field. · Instead of fixing the validation logic, it incorrectly modifies the regex itself.   
· The model identifies and modifies Addres sForm .tsx to allow multiple zip code formats, but this change does not fix the validation issue:   
@@ -3615,7 +3615,7 @@ const CONST = { GA:{}， GB:{ regex: /^[A-Z]{1,2}[0-9R][0-9A-Z]?\s\*[0-9][A-Z-CIKMOV]{2}\$/   
+ regex: /^(,l[A-Z]{1,2}[0-9R][0-9A-Z]?   
\s\*（[0-9][ABD-HJLNP-UW-Z]{2}）?)\$/ samples: 'LA102UX，BL2F8FX，BD1S9LU，WR4G 6LH'， }， GD:{}，   
@@ -4134,7 +4134,7 @@ const CONST = { UG:{}， UM:{}， US:{ regex:/^[0-9]{5}(?:[- ][0-9]{4})?\$/   
+ regex: /^(,l[A-Za-z0-9]{5,10})\$/ samples:'12345，12345-1234，12345 1234', }，

UY:{，

# 4. Attempt to Use the User Tool

· The model attempts to test the fix using the user tool but struggles with execution. ·It does not verify if an error messge appears in the UI,failing to confirm whether the issue is actually resolved.

# 5. Clean Up and Submit

· The model submits the modified regex, believing the issue is resolved.   
· However,the fix does not surface an error message when a user enters a comma, meaning the original problem persists.

Evaluation. GPT-4ofollowed asimilarapproach too1,correctly identifying the necessary filesand analyzing theregex patern.However,italsooverlookedthe UImechansmsresponsiblefordisplaying validationerors,failing torecognizethat the isue stemmed fromhow validation logic was triggered. Instead of modifying the validation logic,GPT-4oatempted to adjusttheregex,whichdid notaddres thecore problem.The modelconducted minimal testinganddid notverify whether error messagesappearedasexpectedinthe UI.Ialsodidnotattemptafullfrmsubmissiontovalidate itsfix,imiting the effectiveness of its approach.

# Claude3.5 Sonnet Interaction Summary

# 1. Locate the Target File

· The model scans the Expensify repository:

ls -la /app/expensify

· It then searches for files related to addresses and validation, writing a function to do so:

# import os

def search_files(directory， keyword): matching_files = [] for root， dirs, files in os.walk(directory): for file in files: if keyword.lower() in file.lower(): matching_files.append(os.path.join(root， file)) return matching_files

address_files = search_files('/app/expensify/src/components'， 'address')   
print("Address-related_files_in_components_directory:")   
for file in address_files: print (file)   
print("\nSearching_for_validation-related_files:")   
validation_files = search_files('/app/expensify/src/components', 'validat')   
for file in validation_files: print (file)

# 2. Identify the Validation Logic

· The model identifies AddressForm.t sx as where validation logic is implemented. It reads in the content to find where zip code / postcode validation is implemented, and then uses regex to find the specific validator function.

import re

with open(' /app/expensify/src/components/AddressForm.tsx'，'r') as file: content = file.read()

# Locate the validator function   
validator_pattern = re.compile(r'const_validator_=_useCallback\((.\*?)\s\*=>\s \*{(.\*?)}\s\*,\s\*\[translate\]\);'，re.DOTALL)   
validator_match = validator_pattern.search(content)   
if validator_match: print("Validator_function_found:") print(validator_match.group(0))   
else: print ("validator_function_not_found")

# 3. Modify the Code and Submit

· The model modifies the validator function to enforce the no comma rule, generating the following dif. This modifies Addres sForm .t sx to ensure that entering a comma in the Zip /Postcode field triggers a validation error messge, by adding a new validation rule that specifically checks for a comma.

· Although this fixed the specified bug,the zip code validation logic is too restrictive,and doesn't support international zip codes which are checked for in our end-to-end tests.

diff --git a/src/components/AddressForm.tsx b/src/components/AddressForm.tsx   
index 4470481d2be..ecd380bf9ce 100644   
--- a/src/components/AddressForm.tsx   
+++ b/src/components/AddressForm.tsx   
@@ -118,7 +118,10 @@ function AddressForm({ ErrorUtils.addErrorMessage(errors，'firstName'，translate(' bankAccount.error.firstName')); if (countrySpecificZipRegex) {   
+ if (values.zipPostCode && values.zipPostCode.includes(',')) {   
+ errors.zipPostCode = translate('privatePersonalDetails.error .noCommaAllowed');   
+ } else if (countrySpecificZipRegex){   
+ if (!countrySpecificZipRegex.test (values.zipPostCode?.trim() .toUpperCase())）{ if (ValidationUtils.isRequiredFulfilled(values. zipPostCode?.trim())）{ errors.zipPostCode = translate(' privatePersonalDetails.error.incorrectZipFormat' '{zipFormat: countryZipFormat});   
@@ -126,8 +129,11 @@ function AddressForm({ errors.zipPostCode = translate('common.error. fieldRequired'); } }   
+ } else if (!CONST.GENERIC_ZIP_CODE_REGEX.test(values?. zipPostCode?.trim()?.toUpperCase() ?? '')）{   
+ errors.zipPostCode = translate('privatePersonalDetails.error .incorrectZipFormat');   
+ } return errors;

Evaluation.Claude3.5 Sonnet was extremelyquick to identifyvalidation logicas therootcauseofthe isse andfixed the specified bug,butit was notfullycomprehensive and did not passour end-to-end test.Despite fixing the logic so commas inthe zipcodefeldnowgetflagged,andidentifyingthatvalidation wasfailingforvalidinternationalzipcodes,Claudedid not address other issues which were addressed in the pul request:

· Missng Country-Specific Zip Code Validation: Claude's patch used a generic validation approach and did not incorporate country-specific ZIP code formats. The pull request introduced a regex-based validation system that dynamically applies the correct format based on the selected country.

·No Dynamic Format Guidance: Claude's patch did not provide users with real-time hints on the expected ZIP code format for their country. The pullrequest added a feature that dynamically updates the hint text to show the correct format when a country is selected

·Failure to Handle Countries Without ZIP Codes: Claude's solution assumed that all countries require a ZIP code, leading to false validation erors in places where postal codes are not used.The pull request correctly allows empty ZIP fields when a country does not have a postal system.

·Lack of Centralized Validation Logic: Claude's patch scatered ZIP code validation across multiple locations in thecodebase.Thepulrequestcentralized validation logic intoautility function (ValidationUtils),making it easierto maintain and extend.

·Inconsistent Error Messaging: Claude's patch did not enhance error messages, making them generic and unhelpful. The pullrquest ensured that error messages now specify the correct ZIP format when validation fails, guiding users to enter the correct value.

· Validation Not Integrated with User Actions: Claude's solution did not validate ZIP codes dynamically based on user input. The pullrequest implemented onBlur and onChange validation triggers, ensuring that errors appear in real-time as the user types.

# A.10. Model Usage of User Tool

Here we provide an example of ol using the user tool to help solve an issue in which an error page appears when a new user navigates toaconcierge link after finishing the onboarding flow,when instead theyshould be able to navigate to the concierge chat.

# Example of o1 using the user tool

1. Extracting Logs and Snapshots: The model parses trace .t race to extract logs and frame snapshots from the user tool, to understand key UI transitions and find error messages or timeouts related to the "join” button or concierge navigation.

trace_path = "/app/expensify/user_tool/output_browserl/trace.trace" logs =

with open(trace_path, "r"， encoding="utf-8") as f: for line in f: data = json.loads(line) if data.get("type") in ["log"， "before"， "after"]: logs.append(data)

for entry in logs: print (entry)

2. Searching for Mising Elements in Snapshots: The model then searches frame snapshot s for the "Join" button so it can see if the UI is rendering it correctly. If"join” was missing, it could mean arendering issue or a missing translation. If "join” was present but disabled,it suggested a logic issue in the button's properties.

trace_path = "/app/expensify/user_tool/output_browserl/trace.trace" frame_snapshots = []

with open(trace_path， "r",encoding="utf-8") as f: for line in f: data = json.loads(line)

if data.get("type") == "frame-snapshot": frame_snapshots.append(data) snapshots_near_call_15 = [] start_call_15 = 25250.356 end_call_15  = 55271.523

for snap in frame_snapshots: time = snap.get("timestamp") if time is not None and start_call_15 <= time <= end_call_15: snapshots_near_call_15.append(snap)

for i, snap in enumerate(snapshots_near_call_15): html = snap.get("snapshot"，{}).get("html"， "") if "Join" in html: print(f"Snapshot_{i}_has_'Join'_in_it.") if "join" in html: print(f"Snapshot_{i}_has_'join'_in_it.")

3. Checking for the Join Button's Disabling Logic: The model then searches user too traces for isDisab led conditions affecting the buton by running

subprocess.run(["grep", "-Rni", "Join"， "src/pages/signin"])

It finds the following logic, which supported its suspicions that the buton was incorectly disabled due to network status or account errors.

<Button isDisabled={network.isOffline Il !!account?.message} success large text={translate('welcomeSignUpForm.join')} isLoading={account?.isLoading} onPress={() => Session.signUpUser()} pressOnEnter style={[styles.mb2]} >

4. Fixing the "Join" Buton: The model removed isDisabled from the code to ensure that the Join button could always be clicked.

5. Fixing the Missng Translation for "Join": The model checked if welcomeSignUpForm. join was missing anbd added a transition. Without a defined translation,"Join” would not appear on the button.

diff --git a/src/languages/en.ts b/src/languages/en.ts  
index 0000000..1111111 100644  
--- a/src/languages/en.ts  
+++ b/src/languages/en.ts  
@@ -1,6 +1,9 @@  
export default{  
+ welcomeSignUpForm:{join:'Join',}，common:{cancel: 'Cancel',yes: 'Yes',

6. Fixing Concierge Navigation Isues: The model then examined ConciergePage.t sx and found the app tried to go back when there was no history. It added a check to prevent navigation errors. This helped resolve the root cause of the issue: it prevented the app from breaking when a new user without prior history wanted to navigate to the Concierge page.

diff --git a/src/pages/ConciergePage.tsx b/src/pages/ConciergePage.tsx   
index 1234567..abcdef0 100644   
--- a/src/pages/ConciergePage.tsx   
+++ b/src/pages/ConciergePage.tsx   
@@ -35,7 +35,9 @@ function ConciergePage({session}: ConciergePageProps){ if(!session Il !session.authToken){ Session.signOutAndRedirectToSignIn () ; return; }   
+ } else if (navigation.canGoBack()）{   
+ navigation.goBack(); // Only navigate back if history exists.   
+ }

7. Verifying the Fixes with the User Tool: After applying allfixes,the model reran the user tool to confirm it worked appropriately.

In summary,o1used theuser tooltoiterativelydebug theissueby extracting logs,analyzing UIsnapshots,andidentifying key failures.Itfirstranthe tool tocollecttrace.trace logs,fltering forlog,before,and after entries todetect errors and timeouts. It then inspected frame-snapshot data to verify if the“Join"button was rendered correctlyand whether it was disabled. Upon discovering isues with buttn accessibilityand missing translations,ol modifed the UIlogic and added missng translations,thenreranthe user tool tovalidatefixes.Finally,itused thetoltoconfirmthatnewusers could successfully sign up and access Concierge without encountering timeouts or navigation errors.

# A.11.Holdout Set

We maintaina \$499,2OO private holdout set to mitigate risks from contamination and model browsing. This internal-only setcontains \$178,475 worthof IC SWE tasks that were augmented by professonal software engineers: to construct this split, we held the task description and testcases forissues constant,and instructed software engineers familiar with the Expensifyrepository to reintroduce the bug or issue in a distinct but equivalent way.This approach mirors real-world software development cycles where recurring issues (suchas downtime or unexpected behavior)have diverse and persistent rootcauses.As withalltasks,tese isses were triple-verifedbyprofessional software engineersforclarity,faithfulness, and feasibility.

# A.12.Additional SWE-Lancer Dataset Details

Iere we describe more details around dataset composition for both the SWE-Lancer Diamond set and ful set.

Table 6. Number of tasks by task type within SWE-Lancer Diamond and SWE-Lancer Full datasets.   

<table><tr><td rowspan="3">Task Type</td><td colspan="4">Diamond</td><td colspan="4">Full</td></tr><tr><td colspan="2">IC SWE</td><td colspan="2"> SWE Manager</td><td colspan="2">IC SWE</td><td colspan="2"> SWE Manager</td></tr><tr><td>%</td><td>n</td><td>%</td><td>n</td><td>%</td><td>n</td><td>%</td><td>n</td></tr><tr><td>Application Logic (Client-Side)</td><td>74.26%</td><td>176</td><td>75.85%</td><td>201</td><td>75.92%</td><td>580</td><td>70.99%</td><td>514</td></tr><tr><td>UI/UX</td><td>17.30%</td><td>41</td><td>18.49%</td><td>49</td><td>16.23%</td><td>124</td><td>22.38%</td><td>162</td></tr><tr><td>Server-Side Logic</td><td>7.17%</td><td>17</td><td>4.91%</td><td>13</td><td>7.33%</td><td>56</td><td>5.39%</td><td>39</td></tr><tr><td>System-Wide Quality and Reliability Tasks</td><td>1.27%</td><td>3</td><td>0.75%</td><td>2</td><td>0.52%</td><td>4</td><td>0.83%</td><td>6</td></tr><tr><td>Infrastructure/Devops</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0.41%</td><td>3</td></tr></table>

Table 7. Number of tasks by task nature within SWE-Lancer Diamond and SWE-Lancer Full datasets.   

<table><tr><td rowspan="3">Nature of Work</td><td colspan="4">Diamond</td><td colspan="4">Full</td></tr><tr><td colspan="2">IC SWE</td><td colspan="2">SWE Manager</td><td colspan="2">IC SWE</td><td colspan="2">SWE Manager</td></tr><tr><td>%</td><td>n</td><td>%</td><td>n</td><td>%</td><td>n</td><td>%</td><td>n</td></tr><tr><td>Bug Fixes</td><td>87.76%</td><td>208</td><td>93.58%</td><td>248</td><td>87.04%</td><td>665</td><td>92.13%</td><td>667</td></tr><tr><td>New Features or Enhancements</td><td>8.86%</td><td>21</td><td>4.91%</td><td>13</td><td>8.90%</td><td>68</td><td>5.80%</td><td>42</td></tr><tr><td>Maintenance,QA,Testing,or Reliability improvements</td><td>3.38%</td><td>8</td><td>1.51%</td><td>4</td><td>4.06%</td><td>31</td><td>2.07%</td><td>15</td></tr></table>

Table 8. Resolution time and comments count within SWE-Lancer Diamond and SWE-Lancer Full datasets.   

<table><tr><td rowspan="2"></td><td colspan="3">Diamond</td><td colspan="3">Full</td></tr><tr><td>IC SWE</td><td> SWE Manager</td><td>Both</td><td>IC SWE</td><td> SWE Manager</td><td>Both</td></tr><tr><td>Metric Resolution Time</td><td>25.00</td><td>27.99</td><td>26.15</td><td>20.33</td><td>25.00</td><td>21.47</td></tr><tr><td>Comments</td><td>41.94</td><td>51.55</td><td>47.01</td><td>36.39</td><td>47.39</td><td>41.74</td></tr></table>

Table 9.Proportion of Tasks in each Price Range for SWE-Lancer Diamond and SWE-Lancer Full   

<table><tr><td>Price Range</td><td>Diamond</td><td>Full</td></tr><tr><td>Under $500</td><td>31.5%</td><td>44.2%</td></tr><tr><td>$500 - $1000</td><td>33.9%</td><td>28.1%</td></tr><tr><td>$1000 - $2000</td><td>25.3%</td><td>23.9%</td></tr><tr><td>$2000 and up</td><td>9.4%</td><td>3.8%</td></tr></table>

Table 10. IC SWE statistics for SWE-Lancer Diamond and SWE-Lancer Full datasets   

<table><tr><td rowspan="2">Metric</td><td colspan="2">Diamond</td><td colspan="2">Full</td></tr><tr><td>Mean</td><td>Max</td><td>Mean</td><td>Max</td></tr><tr><td>Lines of Code</td><td>69</td><td>1812</td><td>55</td><td>1812</td></tr><tr><td>Numberof Files</td><td>2</td><td>21</td><td>2</td><td>39</td></tr><tr><td>Issue Word Count</td><td>271</td><td>466</td><td>270</td><td>2842</td></tr></table>

# A.13.Example IC SWE and SWE Management Task Prompts

Below, we share an example, filled-in IC SWE task prompt, exactly as it is given to the model.

# ExampleICSWETask

You are an expert software engineer maintaining the Expensify repository. It has already been cloned, and is present in /app/expensify (your CwD). You have been tasked with fixing the following issue:

[HOLD for payment 2023-10-09] [\$500] A new tab opens when downloading offline

## Action Performed:   
1. From user A, Send an attachment to user B.   
2. Be offline for user B before opening the attachment 3. Click on the attachment sent from User A   
4. Click on the download button at the top   
## Expected Result:   
the inactive download button is shown.

## Actual Result: A new tab is opened

Can the user still use Expensify without this being fixed? Have you informed them of the workaround?

## Platforms:   
<!--1   
Check off any platforms that are affected by this issue   
11-V   
Which of our officially supported platforms is this issue occurring on?   
-[ ] Android / native [] Android / Chrome [] iOS / native [j iOS / Safari [x] MacOS / Chrome / Safari [] MacOs / Desktop

https://github.com/Expensify/App/assets/43996225/17738f72-39d8-401e-b1b5- f3cf7df871a3

https://github.com/Expensify/App/assets/43996225/f98e3eef-36f0-4ddc-a632-95 f03052651a

\*\*Expensify/Expensify Issue URL:\*\*   
\*\*Issue reported by:\*\* @daveSeife   
\*\*Slack conversation:\*\* https://expensify.slack.com/archives/C049HHMv9SM/ p1692409444867919

[View all open jobs on GitHub](https://github.com/Expensify/App/issues?q=is%3Aopen+ is%3Aissue+label%3A%22Help+Wanted%22)

<li>Automatic offers: </li>   
<ul>   
<li>fedirjh | Reviewer | 26848952</li>   
<li>daveSeife | Reporter I 26848953</li>   
</ul></li> </ul>   
</details>

The reward for the correct fix is \$500.0. You do not need to push your changes to the repository, we will deal with this for you.

)o not attempt to run the app: simply make your changes and then end your turn. Do not end your turn until you have submitted your code changes. You absolutely must not leave action items / TODOs.

Below, we share an example, filled-in SWE Manager task prompt, exactly as it is given to the model.

Example SWEManager Task   
You are an expert software engineering manager working on the Expensify repository. You have tasked your team with addressing the following issue:   
[Paid] [\$800] zip /postcode validation error message not displayed for entering \`, on the Home address screen   
## Action Performed:   
1. go to Settings > profile> personal details >Home address   
2. type the zip /postcode asei,   
## Expected Result:   
it should show validation error message   
## Actual Result:   
does not show validation message   
## Workaround:   
unknown   
## Platforms:   
<!--   
Check off any platforms that are affected by this issue i>   
Which of our officially supported platforms is this issue occurring on? ] Android / native [] Android / Chrome [] ioS / native [] iOs / Safari [ 1 MacOS / Chrome / Safari [] MacOs / Desktop   
\*\*Version Number:\*\* 1.2.67-7   
\*\*Reproducible in staging?:\*\* Y   
\*\*Reproducible in production?:\*\* new feature   
\*\*If this was caught during regression testing, add the test name, ID and link from TestRail:\*\*   
\*\*Email or phone of affected tester (no customers):\*\*   
\*\*Logs:\*\* https://stackoverflow.com/c/expensify/questions/4856   
\*\*Notes/Photos/Videos:\*\*   
https://user-images.githubusercontent.com/43996225/217662506-507818da-7e76-4a4a-b8b8 -5525fc66e651.mov   
https://user-images.githubusercontent.com/43996225/217662521-34c3d804-483d-4e42-a8fe -e87aaf5490fc.mp4   
\*\*Expensify/Expensify Issue URL:\*\*   
\*\*Issue reported by:\*\* @gadhiyamanan   
\*\*Slack conversation:\*\* https://expensify.slack.com/archives/C049HHMV9SM/ p1675838827973319   
[View all open jobs on GitHub](https://github.com/Expensify/App/issues?q=is%3Aopen+ is%3Aissue+label%3A%22Help+Wanted%22)   
<details><summary>Upwork Automation - Do Not Edit</summary> <ul> <li>Upwork Job URL: https://www.upwork.com/jobs/\~015b692899f4b1568b</li> <li>Upw0rk Job ID: 1623772333283323904</1i> <li>Last Price Increase: 2023-03-02</li> </ul>   
</details>

You will be paid \$8000.0 if your team fixes the issue. Since you are a manager, your task is to select the best proposal regaerding how to fix the issue from the below options:

Proposal: 0:

# Proposal

## Restating the problem Some unwanted characters are allowed in zip code

## Root Cause

'Regex ‘ is not being validated correctly for zip code

# Solution

Here are some conditions for the post code:

Every postal code system uses only A-Z and/or 0-9 and sometimes space/dash   
Not every country uses postal codes (ex. Ireland outside of Dublin)， but we'll ignore that here. The shortest postal code format is Sierra Leone with NN   
The longest is American Samoa with NNNNN-NNNNNN   
You should allow one space or dash.   
Should not begin or end with space or dash

[a-z0-9][a-z0-9\- ]{0,10}[a-z0-9]\$' This regex should cover all those conditions source: https://stackoverflow.com/a/19844362/15373641

So I'll just this validation to the \`Personal information Page' as well as the 'Home Address Screen\`. Not posting a code diff due to changed proposal guidelines

\*\*Note:\*\*

As mentioned by @Beamanator in https://expensify.slack.com/archives/C049HHMV9SM/ p1675845424815519?thread_ts=1675838827.973319&cid=C049HHMV9SM We also have to allow letters like in Canadian zip codes, therefore allowing the alphabets

Proposal: 1:

ORIGINAL PROP0SAL (comment #1424743785):

## Proposal

### Please re-state the problem that we are trying to solve in this issue. The validation on the 'AddressPage' component is insufficient， leading to many problems. We need to add proper validation for all the fields in the component.

### What is the root cause of that problem?

lhe root cause behind this issue is that none of the values on the home address are validated properly, the only thing checked is if the fields are empty or not in

<table><tr><td> SWE-Lancer</td></tr><tr><td>&#x27;AddressPage&#x27;.</td></tr><tr><td>https://github.com/Expensify/App/blob/b113107ba03dce15a4e87bad994829987f916dfa/src/ pages/settings/Profile/PersonalDetails/AddressPage.js#Ll15-L122</td></tr><tr><td>### What changes do you think we should make in order to solve the problem? The validation in `AddressPage&#x27; is outdated and does not validate all fields properly. The best case would be to check the validations performed in ` CompanyPage` and then use the `ValidationUtils` to ensure that not just the zip</td></tr><tr><td>code，but all fields are correctly validated. **EDIT:** Adding the required checks for each field:</td></tr><tr><td> For each required field, we can run it through ValidationUtils.isRequiredFulfilled</td></tr><tr><td>()‘first. **Zip code**: Zip codes across the world can vary differently, they need not be of 5 digits only. I suggest we just do a basic sanity check and not do anything else</td></tr><tr><td> **Solution 1**</td></tr><tr><td>Create a map of regexes with respective country specific zip code matching and then validate according to user input. **Solution 2 (Better IMO)**</td></tr><tr><td>It doesn&#x27;t really matter whatever the zip code the user enters. We can just perform a basic sanity check and then let the user enter whatever they want. The regex for the same is- ^[\w][\w\- ]{0,10}[\w]$`.</td></tr><tr><td>ADDITIONAL NOTE (comment #1424971869): &quot;My [proposal](https://github.com/Expensify/App/issues/14958#issuecomment</td></tr><tr><td>-1424743785) will resolve https://github.com/Expensify/App/issues/15003 as well ! &quot;</td></tr><tr><td>FURTHER DISCUSSION (comment #1427798502):</td></tr><tr><td>&quot;Thanks for the feedback and context! For the zip code in Personal Information page, since we want to support zip code from any region in the world, it isn&#x27;t really possible to create a single super regex to perform validation according to the country.</td></tr><tr><td>We are presented with two choices right now: **Solution 1** Create a map of regexes with respective country specific zip code matching and then</td></tr><tr><td>validate according to user input. **Solution 2 (Better IMO)**</td></tr><tr><td>It doesn&#x27;t really matter whatever the zip code the user enters. We can just perform a basic sanity check and then let the user enter whatever they want. The regex for the same is - ^[\w][\w\- ]{0,10}[\w]$.</td></tr><tr><td>As for the other fields, no need for the detailed validation and we can perform just the basic sanity check.&quot;</td></tr><tr><td></td></tr><tr><td>ADDITIONAL CLARIFICATION (comment #1431621152): &quot;My [proposal] (https://github.com/Expensify/App/issues/14958#issuecomment</td></tr></table>

SWE-Lancer ! " FURTHER UPDATES (selected excerpts from conversation by same author): (comment #1427798502 also mentioned): "Thanks for the feedback and context! For the zip code in Personal Information page, since we want to support zip code from any region in the world, it isn't really possible to create a single super regex to perform validation according to the" country. We can just do a basic sanity check, or we can optionally do a map of country code to zip code." (comment #1442138427) : "If we proceed with adding the map to \`CoNsT.js\`, we can store each country's name, iso code, and the regex. Then if the user picks a country that does not exist in the map, we fallback to a simpler approach. That might be the best scenario." (comment #1450806934): "Yes please let's keep it all in the E/App repo for now. If we proceed with adding this in E/A, we should create a new file for storing the regex map. Or we could place it in CONsT.js\`. Ha, sure. We could either (1) proceed with adding the map to \`CoNST.js\`， (2) create a new file for storing the regex map, or (3) create a new npm package for this." (comment #1459820688) : "While investigating more, I ran across a potential issue, can you please shed some light on this: Our most common use case involves the user searching for an address in the address line and choosing from the autocomplete. However, sometimes the zip code from that API is incomplete or doesn't match the stricter regex. In those cases, maybe we should help the user by letting them correct it . We'll show an error for them to fix it." (End of unified proposal from 'Prince-Mendiratta' .) Proposal: 2: ## Proposal ### Please re-state the problem that we are trying to solve in this issue. Validation errors are not shown in the address page with zip code ### What is the root cause of that problem? We don't validate zip codes. In fact_ In the particular page i don't see any validation except empty check validation. https://github.com/Expensify/App/blob/main/src/pages/settings/Profile/ PersonalDetails/AddressPage.js#L116 ### What changes do you think we should make in order to solve the problem? <!-- DO NOT POST CODE DIFFS --> We can add proper validation for all fields in the particular page including the

concerned zip code. We already have validations for these in other flows like company step. We can reuse the same functionality here as well.

For ex: We already have validation util for zipcode here - https://github.com/ Expensify/App/blob/main/src/libs/ValidationUtils.js#L167

Proposal: 3:

## Proposal

### Re-state the problem

zip/postcode validation error message not displayed for entering ， on the Homeaddress screen

### What is the root cause of that problem?

The root cause of this problem is the non-existent validation function for zip/ postcode field.

### What changes do you think we should make in order to solve the problem?   
<!-- DO NOT POST CODE DIFFS -->   
Solution is very simple!!! Before clicking save button, just validate the zip/ postcode field by using this function and show the validation error message if the user entered wrong.   
111   
/\*\*   
\* @param {String} postcode   
\* @returns {Boolean}   
\*/   
function isValidPostalCode(postcode){ return /^[0-9]{5}(?:-[0-9]{4})?\$/.test(postcode);   
}

### What alternative solutions did you explore? (Optional)

Optionally, we can use postal code API for postal code validation from https:// developer.bring.com/api/postal-code/

Proposal: 4:

## Proposal

### Please re-state the problem that we are trying to solve in this issue. Some unwanted characters are allowed in zip code

### What is the root cause of that problem? 'Regex \` is not being validated correctly for zip code ### What changes do you think we should make in order to solve the problem?

<!-- DO NOT POST CODE DIFFS -->

Regex' is not validated for zip code. I'd propose adding an existing ValidationUtils' function for zip code or a new function if it's not already there. Then hooking that to all forms that have zip code.

\*\*Note:\*\* We can also handle unexpected punctuation like ',' by either removing it or showing an error.

Proposal: 5:

ORIGINAL PROPOSAL (comment #1427806164) :

## Proposal

### Please re-state the problem that we are trying to solve in this issue. \`is allowed in zip / postcode input validation - space-only (' \`) is allowed in all fields but not actually saved

### What is the root cause of that problem?

There's no validation check for all fields on Home address screen, except \`required' validation.

https://github.com/Expensify/App/blob/39d9b660e387b28688a221b976f0fbf52873cc67/src/pages/settings/Profile/PersonalDetails/AddressPage.js#L98-L123

Then why are they saved and then reset to previous values when re-visit this page? ( main root cause of #15003)

passes current validation and \`updateHomeAddress' api is called.

lhe issue is that we pass trimmed values as api params so actually empty values are sent to server which causes api to fail.

### What changes do you think we should make in order to solve the problem?

\*\*For zipcode:\*\*

I checked context:

> be aware that the current validation on that page only allows numbers (and an optional -) but in the new Home Address form we have to also allow letters since some countries have letters in their zip codes

So I think just allowing letters in current validation of zip code which was used in bank account step should be simple solution.

https://github.com/Expensify/App/blob/39d9b660e387b28688a221b976f0fbf52873cc67/src/CONST.js#L780

New zipcode regex for Home address would be: /[a-zA-Z0-9]{5}(?:[- ][a-zA-Z0-9]{4})?/}

We can add new flag param called \`shouldAllowLetters' in \`isValidZipCode' function and use this in both Home address page and Personal information page.   
https://github.com/Expensify/App/blob/39d9b660e387b28688a221b976f0fbf52873cc67/src/ libs/ValidationUtils.js#Ll67-L169

\*\*For all fields:\*\*

Validate trimmed value instead of raw value by using \`isRequiredFullfilled\` function defined in 'ValidationUtils'

(optional) For other strict validations, we can use existing logic in \`CompanyStep

\*\*Addon: \*\*   
I noticed no-trim issue happens on \`CompanyStep' as well, which has the same root cause as #15003. i\` is correct in frontend validation but incorrect in backend validation because it's trimmed on api call. It would be good to fix them too in this issue.   
https://user-images.githubusercontent.com/108292595/219962759-8c140cf8-5888-4af9- b478-b3a16a2ee1b3.mp4   
UPDATED PROPOSAL (comment #1436038776):   
"## Proposal   
[Updated] (https://github.com/Expensify/App/issues/14958#issuecomment-1427806164) based on scope change https://github.com/Expensify/App/issues/14958#issuecomment -1431431974"   
(No major changes shown, just referencing the original proposal and stating it has been updated.)   
For your convenience, the Expensify repository has already been cloned, and is present in /app/expensify (your CWD). You should not attempt to run the app or implement the entire fix, but you can review the codebase to understand the context of the issue.   
When you are ready to submit your answer, please create a JSON file in /app/ expensify/manager_decisions.json with the following format:   
{ selected_proposal_id: int,   
}   
You must write your output to this path exactly: '/app/expensify/manager_decisions. json'. You must not submit to anywhere in /mnt/data, just write to '/app/ expensify/manager_decisions.json' exactly!

# A.14.Price Analysis - Additional Details

To estimate API costs,we created new OpenAI API keys and measured the amount spent after each run. We used gpt-4o-2024-08-06 and OpenAI's o1 with high reasoning effort.

We also calculated these additional metrics (results below are the average over 5 runs):

1.CostRatio,notconditioned onsucces: Ignoring success and simply calculating total model APIcost/total freelancer cost across all tasks: GPT-4o costs O.08% of freelancer cost and o1 costs 0.7%.   
2.Model Earnings oncorrectly solved tasks /Model API Cost over alltasks: On solved tasks: GPT-4o earns \$109 for every \$1 of API cost, and ol earns \$20 for every \$1 of API cost.

# A.15.Evaluation of Open-Source Models

In addition to the main models evaluated in Section 3,we alsoran preliminary evaluations on two open-source models: Deepseek-Rland Meta Llama 3.37OB Instruct; however,performance here maybe underelicited due to som limitations described below.

Table11.Preliminarypass@1AcuraciesandEarningsforICSWEandSWE Manageronopen-source models.Earningsareroundedto thenearestthousand.SWE-LancerDiamondcontains237ICSWEtasks worth\$236,30and265SWEManagertasks worth\$264,500, totaling 502 tasks worth\$500,800.Thefullsethas764ICSWEtasksworth \$414,775and724SWEManagertasksworth \$585225, totaling 1,488 tasks worth \$1,000.000.   

<table><tr><td>Model</td><td>User Tool</td><td>Dataset</td><td>pass@1</td><td>Dollars Earned /Total</td><td>Earn Rate</td></tr><tr><td>Deepseek-R1</td><td>Yes</td><td>IC SWE (Diamond)</td><td>3.8%</td><td>$4k / $236k</td><td>1.6%</td></tr><tr><td>Llama 3.3 70B</td><td>Yes</td><td> IC SWE (Diamond)</td><td>4.6%</td><td>$5k / $236k</td><td>1.9%</td></tr><tr><td>Deepseek-R1</td><td>Yes</td><td>IC SWE (Full)</td><td>5.2%</td><td>$17k / $415k</td><td>4.1%</td></tr><tr><td>Llama 3.3 70B</td><td>Yes</td><td>IC SWE (Full)</td><td>5.9%</td><td>$17k / $415k</td><td>4.0%</td></tr><tr><td>Deepseek-R1</td><td>N/A</td><td> SWE Manager (Diamond)</td><td>40.4%</td><td>$121k / $265k</td><td>45.6%</td></tr><tr><td>Llama 3.3 70B</td><td>N/A</td><td> SWE Manager (Diamond)</td><td>40.4%</td><td>$120k /$265k</td><td>45.2%</td></tr><tr><td>Deepseek-R1</td><td>N/A</td><td>SWE Manager (Full)</td><td>46.7%</td><td>$276k / $585k</td><td>47.2%</td></tr><tr><td>Llama 3.3 70B</td><td>N/A</td><td> SWE Manager (Full)</td><td>44.1%</td><td>$260k / $585k</td><td>44.5%</td></tr><tr><td>Deepseek-R1</td><td>N/A</td><td>SWE-Lancer Diamond</td><td>23.1%</td><td>$124k / $501k</td><td>24.8%</td></tr><tr><td>Llama 3.3 70B</td><td>N/A</td><td>SWE-Lancer Diamond</td><td>23.5%</td><td>$124k /$501k</td><td>24.8%</td></tr><tr><td>Deepseek-R1</td><td>N/A</td><td>SWE-Lancer Full</td><td>25.4%</td><td>$293k / $1M</td><td>29.3%</td></tr><tr><td>Llama 3.3 70B</td><td>N/A</td><td>SWE-Lancer Full</td><td>24.5%</td><td>$277k /$1M</td><td>27.7%</td></tr></table>

The preliminary results in Table 11 reflect one seed on our default scafold.We suspect a more customized scaffold could result in significantly higher performance on IC SWE tasks (which are more dependent on scaffolding) from these open-source models,especially Deepseek-R1. We used the same scafold used to run the OpenAIand Anthropic models; customizing this scaffld to better serve open-source models would likely elicit bettr performance.Below,we summarize the common failure modes we observed from Deepseek-R1 and Llama 3.3 70B under the current scaffold setup:

1.Poorinstruction following led to models struggling to navigate the codebase and localizethe isse.Our scaffold provided guidance for how the modelcould familiarize itself with the repository,essentially instructing the model to begin by running ls with a Python block, waiting for areply,and always proceeding step by step. While models like Claude 3.5 Sonnet, GPT-4o,andol followed these instructions tounderstand the codebase structure,Deepseek-Rl in particular would not follow these instructions and would rarely inspect the directory tree,instead either halucinating the repository structure or running a superficial context-blind search, with something like the below:

import os

target_file = "UserProfilePage.js" found = None

for root，_dirs， files in os.walk("/workspace/project") if target_file in files: found = os.path.join(root，target_file) break

print("Found:"， found if found else "Not_found")

The above code from Rl incorrectly assumes the repo lives under /workspace/pro ject,relies on an exact name match(which would miss if the file was actually called UserProfilePage.t sx or userProfilePage . js, and returns a binary outcome (found vs. not found) instead of listing related files.

2.Despite instructions to proceed step by step,models would try to solve the whole task at once. Our scaffold instructed models to nottry to solve the problem allatonce,butboth Deepseek-R1 and Llama 3.3 would try to write a full patch right away instead of fully understanding how the problem arises. Claude, GPT-4o,and ol all frequently grepped the codebase with various keywords and read the contents of key fles to understand the isse-we did not observe this with the open-source models.

3.Models would give up early.Particularly with Llama 3.3 7OB, we saw that it would frequently end the task before making significant progress orfnishing it, despite our eval harnesscontaining prompts to not give upuntil the patch is completed.

4.Models would repeatedly retry the same failed strategies.Especialy with Deepseek-R1,the model would often try similar code blocks at each turn, despite seeing output that those attempts were unsuccessful.

5.Opensource models were substantiallylesslikely touse the user-tool.If theydid invoke it, they frequentlyfailed to correctly parse its output when it was invoked.