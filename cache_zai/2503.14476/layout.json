{"code":0,"doc_size":826801,"doc_type":"pdf","dst_path":"oss://glm-data-ocr-data/services/maas/pa/d9bf8aa3-274b-4e31-a2e4-7425f3cc2ce8.tar","markdown":"## DAPO: An Open-Source LLM Reinforcement Learning System at Scale\n\n1ByteDance Seed2Institute for AI Industry Research (AIR), Tsinghua University 3The University of Hong Kong 4SIA-Lab of Tsinghua AIR and ByteDance Seed\n\nFull author list in Contributions\n\n## Abstract\n\nInference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework a, along with a carefully curated and processed dataset.These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.\n\nDate: March 17, 2025\n\nCorrespondence: zhouhao@air.tsinghua.edu.cn, wangmingxuan.89@bytedance.com\n\nProject Page: https://dapo-sia.github.io/\n\n![f234c98bed8363e60dfb60ebc163f714](imgs/f234c98bed8363e60dfb60ebc163f714.jpg)\n\nFigure 1 AIME 2024 scores of DAPO on the Qwen2.5-32B base model, outperforming the previous SoTA DeepSeekR1-Zero-Qwen-32B using $50\\%$ training steps. The x-axis represents the gradient update steps.\n\n\n\n## 1Introduction\n\nTest-time scaling such as OpenAI’s o1 [1] and DeepSeek’s R1 [2] brings a profound paradigm shift to Large Language Models (LLMs) [3–7]. Test-time scaling enables longer Chain-of-Thought thinking and induces sophisticated reasoning behaviors, which makes the models superior in competitive math and coding tasks like AIME and Codeforces.\n\nThe central technique driving the revolution is large-scale Reinforcement Learning (RL), which elicits complex reasoning behaviors such as self-verifcation and iterative refnement. However, the actual algorithm andii key recipe for scalable RL training remains a myth, hidden from technical reports of existing reasoning models [1, 2, 8–11]. In this paper, we reveal signifcant obstacles in large-scale RL training and open-source ai scalable RL system with fully open-sourced algorithm, training code and dataset that provides democratized solutions with industry-level RL results.\n\nWe experiment over Qwen2.5-32B [12] as the pretrained model for RL. In our initial GRPO run, we achieved only 30 points on AIME — a performance signifcantly below DeepSeek’s RL (47 points). A thorough analysisi reveals that the naive GRPO baseline sufers from several key issues such as entropy collapse, reward noise,f and training instability. The broader community has encountered similar challenges in reproducing DeepSeek’s results [13–19] suggesting that critical training details may have been omitted in the R1 paper that are required to develop an industry-level, large-scale, and reproducible RL system.\n\nTo close this gap, we release an open-source state-of-the-art system for large-scale LLM RL, which achieves 50 points on AIME 2024 based on Qwen2.5-32B model, outperforming previous state-of-the-art results achieved by DeepSeek-R1-Zero-Qwen-32B [2] (47 points) using $50\\%$ training steps (Figure 1). We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and introduce 4 key techniques to make RL shine in the long-CoT RL scenario. Details are presented in Section 3.\n\n1.Clip-Higher, which promotes the diversity of the system and avoids entropy collapse;\n\n2.Dynamic Sampling, which improves training efciency and stability;fi\n\n3.Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios;\n\n4.Overlong Reward Shaping, which reduces reward noise and stabilizes training.\n\nOur implementation is based on verl [20]. By fully releasing our state-of-the-art RL system including training code and data, we aim to reveal valuable insights to large-scale LLM RL that beneft the larger community.i\n\n## 2Preliminary\n\n## 2.1Proximal Policy Optimization (PPO)\n\nPPO [21] introduces a clipped surrogate objective for policy optimization. By constraining the policy updates within a proximal region of the previous policy using clip, PPO stabilizes training and improves sample efciency. Specifcally, PPO updates the policy by maximizing the following objective:fii\n\n$$\\mathcal{J}_{\\text{PPO}}(\\theta)=\\mathbb{E}_{(q,a)\\sim\\mathcal{D},o_{\\leq t} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid q)}\\Bigg[\\min\\Bigg(\\frac{\\pi_{\\theta}(o_{t}\\mid q,o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_{t}\\mid q,o_{<t})} \\hat{A}_{t},\\ \\text{clip}\\Bigg(\\frac{\\pi_{\\theta}(o_{t}\\mid q,o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_{t}\\mid q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\Bigg)\\hat{A}_{t}\\Bigg)\\Bigg]$$\n\n(1)\n\nwhere $(q,a)$  is a question-answer pair from the data distribution $\\mathcal{D}, \\varepsilon$ is the clipping range of importance sampling ratio, and $\\hat{A}_{t}$ is an estimator of the advantage at time step $t$  Given the value function $V$ and the reward function $R, \\hat{A}_{t}$ is computed using the Generalized Advantage Estimation (GAE) [22]:\n\n$$\\hat{A}_{t}^{\\text{GAE}(\\gamma,\\lambda)}=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^{l}\\delta_{t+l},$$\n\n(2)\n\nwhere\n\n$$\\delta_{l}=R_{l}+\\gamma V(s_{l+1})-V(s_{l}),\\quad0\\leq\\gamma,\\lambda\\leq1.$$\n\n(3)\n\n\n\n![69bfebaf05ee75a3456d5dd867d2cccd](imgs/69bfebaf05ee75a3456d5dd867d2cccd.jpg)\n\nFigure 2 The accuracy on the AIME test set and the entropy of the actor model’s generated probabilities during the RL training process, both before and after applying Clip-Higher strategy.\n\n## 2.2Group Relative Policy Optimization (GRPO)\n\nCompared to PPO, GRPO eliminates the value function and estimates the advantage in a group-relative manner. For a specifc question-answer pairi $(q,a),$  the behavior policy $\\pi_{\\theta_{\\text{old}}}$  samples a group of $G$  individual responses $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}.$  Then, the advantage of the $i$ th response is calculated by normalizing the group-level rewards $\\{R_{i} \\}_{i=1}^{G} :$\n\n$$\\hat{A}_{i,t}=\\frac{r_{i}-\\mathrm{mean}(\\{R_{i}\\}_{i=1}^{G})}{\\mathrm{std}(\\{R_{i}\\}_{i=1}^{G})}.$$\n\n(4)\n\nSimilar to PPO, GRPO adopts a clipped objective, together with a directly imposed KL penalty term:\n\n$$\\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n\n$$\\Bigg[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\Bigg( \\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\Big)\\hat{A}_{i,t}\\Big)-\\beta D_{\\text{KL}}(\\pi_{\\theta}||\\pi_{\\text{ref}})\\Bigg)\\Bigg].$$\n\n(5)\n\nwhere\n\n$$r_{i , t}(\\theta)=\\frac{\\pi_{\\theta}(o_{i , t}\\mid q,o_{i ,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i , t}\\mid q,o_{i ,<t})}.$$\n\n(6)\n\nIt is also worth noting that GRPO computes the objective at the sample-level. To be exact, GRPO frsti calculates the mean loss within each generated sequence, before averaging the loss of diferent samples. As wef will be discussing in Section 3.3, such diference may have an impact on the performance of the algorithm.f\n\n## 2.3Removing KL Divergence\n\nThe KL penalty term is used to regulate the divergence between the online policy and the frozen reference policy. In the RLHF scenario [23], the goal of RL is to align the model behavior without diverging too far from the initial model. However, during training the long-CoT reasoning model, the model distribution can diverge signifcantly from the initial model, thus this restriction is not necessary. Therefore, we will excludei the KL term from our proposed algorithm.\n\n\n\n## 2.4Rule-based Reward Modeling\n\nThe use of reward model usually sufers from the reward hacking problem [24–29]. Instead, we directly usef the fnal accuracy of a verifable task as the outcome reward, computed using the following rule:ii\n\n$$R(\\hat{y},y)=\\begin{cases}1,&\\texttt{is\\_equivalent}(\\hat{y},y)\\\\ -1,&\\text{otherwise}\\end{cases}$$\n\n(7)\n\nwhere $y$ is the ground-truth answer and $\\widehat{y}$  is the predicted answer. This is proved to be an efective approachf to activating the base model’s reasoning capability, as shown in multiple domains such as automated theorem proving [30–33], computer programming [34–37], and mathematics competition [2].\n\n## 3DAPO\n\nWe propose the Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm. DAPO samples a group of outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ for each question $4$  paired with the answer a, and optimizes the policy via the following objective:\n\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n\n$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$\n\n(8)\n\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G,$$\n\nwhere\n\n$$r_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(o_{i,t}\\mid q,o_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t}\\mid q,o_{i,<t})},\\quad\\hat{A}_{i,t}=\\frac{R_{i}-\\mathrm{mean}(\\{R_{i}\\}_{i=1}^{G})}{\\mathrm{std}(\\{R_{i}\\}_{i=1}^{G})}.$$\n\n(9)\n\nThe full algorithm can be found in Algorithm 1. In this section, we will introduce the key techniques associated with DAPO.\n\n## 3.1Raise the Ceiling: Clip-Higher\n\nIn our initial experiments using naive PPO [21] or GRPO [38], we observed the entropy collapse phenomenon:the entropy of the policy decreases quickly as training progresses (Figure 2b). The sampled responses of certain groups tend to be nearly identical. This indicates limited exploration and early deterministic policy,which can hinder the scaling process.\n\nWe propose the Clip-Higher strategy to address this issue. Clipping over the importance sampling ratio is introduced in Clipped Proximal Policy Optimization (PPO-Clip) [21] to restrict the trust region and enhance the stability of RL. We identify that the upper clip can restrict the exploration of the policy, where making an ‘exploitation’ token more probable is much easier yet the probability of an unlikely ‘exploration’ token is too tightly bounded to be uplifted.\n\nConcretely, when $\\varepsilon=0.2$ (the default value of most algorithms) and $\\hat{A}_{i , t}>0$ (the system tries to increase the probability), consider two actions with probabilities $\\pi_{\\theta_{\\text{old}}}(o_{i} | q)=0.01$  and 0.9. The upper bounds of the increased probabilities $\\pi_{\\boldsymbol{\\theta}}\\big(o_{\\boldsymbol{i}}\\ \\big|\\ q\\big)$  are 0.012 and 1.08, respectively $(\\pi_{\\theta_{\\text{old}}}\\cdot(1+\\epsilon)).$  This implies that‘exploitation’ tokens with a higher probability $\\left(\\text{\\textit{e.g.}, 0.9}\\right)$  are not constrained to get even extremely larger probabilities like 0.999. Conversely, for low-probability ‘exploration’ tokens, achieving a non-trivial increase in probability is considerably more challenging. Empirically, we also observe that the mean probability of up-clipped tokens is low: $\\pi_{\\theta}(o_{i}\\mid q)<0.2$ (Figure 3a). This fnding supports our intuition that the upperi clipping threshold indeed restricts the probability increase of low-probability ‘exploration’ tokens, thereby potentially constraining the exploration of the system.\n\n\n\n![7258ec59c0a6e5cc8bbf95ee06d45020](imgs/7258ec59c0a6e5cc8bbf95ee06d45020.jpg)\n\nFigure 3 The mean up-clipped probability as well as the ratio of prompts with accuracy=1.\n\nAdhering to the Clip-Higher strategy, we decouple the lower and higher clipping range as $\\varepsilon_{\\text{low}}$  and $\\varepsilon_{\\text{high}},$  as highlighted in Equation 10:\n\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n\n$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$\n\n(10)\n\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$\n\nWe increase the value of $\\varepsilon_{\\mathbf{high}}$  to leave more room for the increase of low-probability tokens. As shown in Figure 2, this adjustment efectively enhances the policy’s entropy and facilitates the generation of moref diverse samples. We keep $\\varepsilon_{\\text{low}}$ as it is, because increasing it will suppress the probability of these tokens to 0,resulting in the collapse of the sampling space.\n\n## 3.2The More the Merrier: Dynamic Sampling\n\nExisting RL algorithm sufers from the gradient-decreasing problem when some prompts have accuracy equalf to 1. For example for GRPO, if all outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ of a particular prompt are correct and receive the same reward, the resulting advantage for this group is zero. A zero advantage results in zero policy gradients,shrinking the magnitude and increasing the noise sensitivity of the batch gradient, thereby degrading sample efciency. Empirically, the number of samples with accuracy equal to 1 continues to increase, as shown infi Figure 3b. This means that the efective number of prompts in each batch keeps decreasing, which can leadf to larger variance in gradient and dampens the gradient signals for model training.\n\nTo this end, we propose to over-sample and filter out prompts with the accuracy equal to 1 and 0 as illustrated in Equation 11, leaving all prompts in the batch with efective gradients and keeping a consistent number off prompts. The sampling cost for each batch is dynamic. Before training, we keep sampling until the batch is fully flled with samples whose accuracy is neither 0 nor 1.i\n\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n\n$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$\n\n(11)\n\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$\n\n\n\n![17fe2f3cbdcf1d4e2666692383dc8fc1](imgs/17fe2f3cbdcf1d4e2666692383dc8fc1.jpg)\n\n(a) Entropy of actor model’s generation probabilities.(b) Average length of actor model-generated responses\n\nFigure 4 The entropy of the probability distribution of the actor model, as well as the changes in response length.\n\nNote that this strategy does not necessarily impede training efciency, because the generation time is typicallyfi dominated by the generation of long-tail samples if the RL system is synchronized and the generation stage is not pipelined. Besides, we fnd that with dynamic sampling the experiment achieves the same performancei faster as shown in Figure 6.\n\n## 3.3Rebalancing Act: Token-Level Policy Gradient Loss\n\nThe original GRPO algorithm employs a sample-level loss calculation, which involves frst averaging the lossesi by token within each sample and then aggregating the losses across samples. In this approach, each sample is assigned an equal weight in the fnal loss computation. However, we fnd that this method of loss reductionii introduces several challenges in the context of long-CoT RL scenarios.\n\nSince all samples are assigned the same weight in the loss calculation, tokens within longer responses (which contain more tokens) may have a disproportionately lower contribution to the overall loss, which can lead to two adverse efects. First, for high-quality long samples, this efect can impede the model’s ability to learnff reasoning-relevant patterns within them. Second, we observe that excessively long samples often exhibit low-quality patterns such as gibberish and repetitive words. Thus, sample-level loss calculation, due to its inability to efectively penalize those undesirable patterns in long samples, leads to an unhealthy increase inf entropy and response length, as shown in Figure 4a and Figure 4b.\n\nWe introduce a Token-level Policy Gradient Loss in the long-CoT RL scenario to address the above limitations:\n\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n\n$$\\Bigg[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}{\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)}\\Bigg],$$\n\n(12)\n\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$\n\nIn this setting, longer sequences can have more infuence on the overall gradient update compared to shorterl sequences. Moreover, from the perspective of individual tokens, if a particular generation pattern can lead to an increase or decrease in reward, it will be equally prompted or suppressed, regardless of the length of the response in which it appears.\n\n## 3.4Hide and Seek: Overlong Reward Shaping\n\nIn RL training, we typically set a maximum length for generation, with overlong samples truncated accordingly.We fnd that improper reward shaping for truncated samples can introduce reward noise and signifcantlyii disrupt the training process.\n\n\n\n![34dc88665925095061ed6585e4b68349](imgs/34dc88665925095061ed6585e4b68349.jpg)\n\nFigure 5 The accuracy of the actor model on AIME and the entropy of its generation probabilities, both before and after applying Overlong Reward Shaping strategy.\n\nBy default, we assign a punitive reward to truncated samples. This approach may introduce noise into the training process, as a sound reasoning process can be penalized solely due to its excessive length. Such penalties can potentially confuse the model regarding the validity of its reasoning process.\n\nTo investigate the impact of this reward noise, we frst apply an Overlong Filtering strategy which masksi the loss of truncated samples. We fnd that this approach signifcantly stabilizes training and enhancesii performance, as demonstrated in Figure 5.\n\n## Algorithm 1DAPO: Decoupled Clip and Dynamic sAmpling Policy Optimization\n\nitial policy model $\\pi_{\\theta};$  reawrd model $R$  task prompts D; hyperparameters εlow, εhigh\n\n1: for step $=1\\text{,}...,\\text{M}$ do\n\n2:Sample a batch $\\mathcal{D}_{b}$  from $\\mathcal{D}$\n\n3:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$\n\n4:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)$  for each question $q\\in\\mathcal{D}_{b}$\n\n5:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$  by running $R$\n\n6:Filter out $o_{i}$  and add the remaining to the dynamic sampling bufer (Dynamic Sampling Equation (11))f 7:if bufer sizef $\\boldsymbol{n_{b}<N:}$\n\n8:continue\n\n9：For each $O_{2}$ in the bufer, computef $\\hat{A}_{i,t}$ for the $t$ th token of $o_{i}$  (Equation (9))\n\n10:for iteration $=1, ..., \\mu$ do\n\n11:Update the policy model $\\pi_{\\theta}$  by maximizing the DAPO objective (Equation (8))\n\nOutput $\\pi_{\\theta}$\n\nFurthermore, we propose Soft Overlong Punishment (Equation 13), a length-aware penalty mechanism designed to shape the reward for truncated samples. Specifcally, when the response length exceeds the predefnedii maximum value, we defne a punishment interval. Within this interval, the longer the response, the greater thei punishment it receives. This penalty is added to the original rule-based correctness reward, thereby signaling to the model to avoid excessively long responses.\n\n$$R_{\\text{length}}(y)=\\begin{cases}0,&|y|\\leq L_{\\text{max}}-L_{\\text{cache}}\\\\ \\frac{(L_{\\text{max}}-L_{\\text{cache}})-|y|}{L_{\\text{cache}}},&L_{\\text{max}}-L_{\\text{cache}}<|y|\\leq L_{\\text{max}}\\\\ -1,&L_{\\text{max}}<|y|\\end{cases}$$\n\n(13)\n\n\n\n![bdf737fe4b1cdce5dd33c6102f5e8376](imgs/bdf737fe4b1cdce5dd33c6102f5e8376.jpg)\n\nFigure 6 The training progress before and after applying dynamic sampling on a baseline setting.\n\n## 3.5Dataset Transformation\n\nOur dataset is sourced from the web and ofcial competition homepages through a combination of webfi scraping and manual annotation. The answers of math dataset typically come in a variety of formats, such as expression, formula and number, which makes it challenging to design comprehensive rules to parse them.To provide accurate reward signals using rules and minimize errors introduced by formula parsers, inspired by AIME, we select and transform the answer√s into integers, which are easy to parse. For example, if the original answer is expressed in the form of $\\tfrac{a+\\sqrt{b}}{c},$  we instruct the LLM to modify the question so that the expected answer becomes $a+b+c.$  After selection and transformation, we obtained the DAPO-Math-17K dataset, which consists of 17K prompts, each paired with an integer as the answer.\n\n## 4Experiments\n\n## 4.1Training Details\n\nIn this work, we focus specifcally on mathematical tasks to evaluate our algorithm, which can be readilyi transferred to other tasks. We adopt the verl framework [20] for training. We use naive GRPO [38] as our baseline algorithm and estimate advantages using group reward normalization.\n\nFor hyper-parameters, we utilize the AdamW [39] optimizer with a constant learning rate of $1\\times10^{-6},$ incorporating a linear warm-up over 20 rollout steps. For rollout, the prompt batch size is 512 and we sample 16 responses for each prompt. For training, the mini-batch size is set to 512, i.e., 16 gradient updates for each rollout step. For Overlong Reward Shaping, we set the expected maximum length as 16,384 tokens and allocate additional 4,096 tokens as the soft punish cache. Therefore, the maximum number of tokens for generation is set to 20,480 tokens. As for the Clip-Higher mechanism, we set the clipping parameter $\\varepsilon_{\\text{low}}$ to 0.2 and $\\varepsilon_{\\text{high}}$  to 0.28, which efectively balance the trade-of between exploration and exploitation. Forff evaluation on AIME, we repeat the evaluation set for 32 times and report avg@32 for results stability. The inference hyperparameters of evaluation are set to temperature 1.0 and topp 0.7.\n\n## 4.2Main Results\n\nExperiments on AIME 2024 demonstrate that DAPO has successfully trained the Qwen-32B Base model into a powerful reasoning model, achieving performance superior to DeepSeek’s experiments on Qwen2.5-32B using the R1 approach. In Figure 1, we observe a substantial improvement of performance on AIME 2024,with accuracy increasing from near $0\\%$ to $50\\%.$  Notably, this improvement is achieved with only $50\\%$ of the training steps required by DeepSeek-R1-Zero-Qwen-32B.\n\nWe analyze the contributions of each training technique in our methodology, as detailed in Table 1. The observed improvements demonstrate the efectiveness of these techniques in RL training, each contributingf\n\n\n\nTable 1 Main results of progressive techniques applied to DAPO\n\n\n\n<html><body><table><tr><td>Model</td><td>AIME24avg@32</td></tr><tr><td>DeepSeek-R1-Zero-Qwen-32B</td><td>47</td></tr><tr><td>Naive GRPO</td><td>30</td></tr><tr><td>+ Overlong Filtering</td><td>36</td></tr><tr><td>+ Clip-Higher</td><td>38</td></tr><tr><td>+ Soft Overlong Punishment</td><td>41</td></tr><tr><td>+ Token-level Loss</td><td>42</td></tr><tr><td>+ Dynamic Sampling (DAPO)</td><td>50</td></tr></table></body></html>\n\nseveral accuracy points in AIME 2024. Notably, given the vanilla GRPO setting, only $30\\%$ accuracy can be reached by training from a Qwen2.5-32B base model.\n\nFor token-level loss, although it brings less performance improvement, we fnd it enhances training stabilityi and makes the length increase more healthily.\n\nWhen applying Dynamic Sampling, although more data needs to be sampled due to the fltering out ofi zero-gradient data, the overall training time is not signifcantly afected. As shown in Figure 6, although theif number of sampling instances increases, the model’s convergence time is even reduced, due to fewer training steps required.\n\n## 4.3Training Dynamics\n\nReinforcement learning on large language models is not only a cutting-edge research direction but also an intrinsically complex systems engineering challenge, characterized by the interdependence of its various subsystems. Modifcations to any single subsystem can propagate through the system, leading to unforeseeni consequences due to the intricate interplay among these components. Even seemingly minor changes in initial conditions, such as variations in data and hyperparameters, can amplify through iterative reinforcement learning processes, yielding substantial deviations in outcomes. This complexity often confronts researchers with a dilemma: even after meticulous analysis and well-founded expectations that a modifcation will enhancei specifc aspects of the training process, the actual results frequently diverge from the anticipated trajectory.i Therefore, monitoring of key intermediate results during experimentation is essential for swiftly identifying the sources of discrepancies and, ultimately, for refning the system.i\n\nThe Length of Generated Responses is a metric closely related to training stability and performance,as shown in Figure 7a. The increase in length provides the model with a larger space for exploration,allowing more complex reasoning behaviors to be sampled and gradually reinforced through training.However, it is important to note that length does not always maintain a continuous upward trend during training. In some considerable periods, it can exhibit a trend of stagnation or even decline, which has also been demonstrated in [2]. We typically use length in conjunction with validation accuracy as indicators to assess whether an experiment is deteriorating.\n\nThe Dynamics of Reward during training has always been one of the crucial monitoring indicators in reinforcement learning, as shown in Figure 7b. In the majority of our experiments, the trend of reward increase is relatively stable and does not fuctuate or decline signifcantly due to adjustments inli experimental settings. This indicates that, given a reliable reward signal, language models can robustly ft the distribution of training set. However, we fnd that the fnal reward on the training set ofteniii exhibits little correlation with the accuracy on the validation set, which indicates overftting to thei training set.\n\nThe Entropy of the Actor Model and Generation Probability are related to the model’s exploration capability and are key metrics that we closely monitor in our experiments. Intuitively, the model’s entropy needs to be maintained within an appropriate range. An excessively low entropy indicates that the probability distribution is overly sharp, leading to a loss of exploration capability. Conversely, an\n\n\n\n![5bdab7ae42ec5ebfca5430d963d8be7e](imgs/5bdab7ae42ec5ebfca5430d963d8be7e.jpg)\n\nFigure 7 The metric curves of response length, reward score, generation entropy, and the mean probability of DAPO,which show the dynamics of RL training and serve as essential monitoring indicators to identify potential issues.\n\nexcessively high entropy is often associated with issues of over-exploration such as gibberish and repetitive generation. For the generation probability, the situation is exactly the opposite. As demonstrated in Section 3.1, by applying the Clip-Higher strategy, we efectively addressed the issue of entropy collapse.f In subsequent experiments, we fnd that maintaining a slow upward trend in entropy is conducive to thei improvement of model performance, shown in Figure 7c and Figure 7d.\n\n## 4.4Case Study\n\nDuring the RL training process, we observe an interesting phenomenon: the reasoning patterns of the actor model evolve dynamically over time. Specifcally, the algorithm not only reinforces existing reasoning patternsi that facilitate correct problem-solving but also gradually gives rise to entirely new modes of reasoning that were initially absent. This fnding reveals the adaptability and exploration capability of RL algorithms andi ofers new insights into the learning mechanisms of the model.f\n\nFor example, in the early stages of model training, there was virtually no occurrence of checking and refectingl on previous reasoning steps. However, as training progresses, the model exhibits distinct behaviors of refectionl and backtracking, as shown in Table 2. This observation sheds light on further exploration into interpreting the emergence of reasoning abilities during RL, which we leave for future research.\n\n## 5Conclusion\n\nIn this paper, we release a fully open-sourced system for large-scale LLM RL, including algorithm, code infrastructure, and dataset. The system achieves state-of-the-art large-scale LLM RL performance (AIME 50 using Qwen-32B pretrained model).We propose the Decoupled Clip and Dynamic sAmpling Policy\n\n\n\n## Question:\n\nGiven that the base $ABC$ of the tetrahedron $\\boldsymbol{S-ABC}$ is an equilateral triangle, the projection $H$  of point $A$  on the face $\\mathit{SBC}$ is the orthocenter of $\\triangle SBC,$  the dihedral angle $H-AB-C$ is $30^{\\circ}$ and $SA=2,$  fnd the volume of the tetrahedron. The answer is in the formi $\\frac{k}{m}.$  so give the value of $k+m.$\n\n## Response:\n\nLet us consider the given problem: We need to determine ...\n\nNext, let $S$ have coordinates $(x_{0},y_{0},z_{0}).$ Since $\\it SA\\ =\\ 2,$ the distance formula gives us $\\sqrt{(x_{0})^{2}+(y_{0})^{2}+(z_{0})^{2}}=2,$ so we can write $x_{0}^{2}+y_{0}^{2}+z_{0}^{2}=4.$ Now, remember that $H$ is the orthogonal projection of\n\n...\n\nHowever, wait a moment, let’s rethink about the dihedral angle involving planes in a more thoughtful geometric way. Consider the plane $\\alpha_{1}=ABC,$  which lies entirely on the xy coordinate plane (as $z=0\\text{)}$  Let the plane $\\alpha_{2}=SBC.$  The point $A$  projected perpendicularly to plane $\\alpha_{2}$  lands on H.  The line $l=AB\\ \\ldots$\n\nTable 2 Emergence of Refective Behavior in Reinforcement Learningl\n\nOptimization (DAPO) algorithm, and introduce 4 key techniques to make RL powerfully efective and efcientffi in the long-CoT RL scenario. Additionally, by open-sourcing the training code and dataset, we provide the broader research community and society with practical access to a scalable reinforcement learning solution,enabling all to beneft from these advancements.i\n\n\n\n## Contributions\n\n## Project Lead\n\nQiying $\\mathrm{Yu}^{1,2,4}$\n\n## Algorithm\n\nQiying $\\mathrm{Yu}^{1,2,4},$  Zheng Zhang1, Ruofei Zhu1, Yufeng Yuan1, Xiaochen $\\mathrm{Zuo}^{1},$  Yu $\\mathrm{Yue}^{1}$\n\n## Infrastructure∗\n\nWeinan Dai1,2,4, Tiantian Fan1, Gaohong Liu1, Juncai $\\mathrm{Liu}^{1},$  Lingjun Liu1, Xin Liu1, Haibin Lin1, Zhiqi Lin1,Bole Ma1, Guangming Sheng1,3, Yuxuan Tong1,2,4, Qiying $\\mathrm{Yu}^{1,2,4},$  Chi Zhang1, Mofan Zhang1, Ru Zhang1, Wang Zhang1, Hang Zhu1, Jinhua $\\mathrm{Zhu}^{1}$\n\n∗Last-Name in Alphabetical Order\n\n## Dataset\n\nJiaze Chen1, Jiangjie Chen1,4, Chengyi Wang1, Hongli $\\mathrm{Yu}^{1,2,4},$  Yuxuan Song1,2,4, Xiangpeng Wei1, Qiying $\\mathrm{Yu}^{1,2,4}$\n\n## Supervision\n\nHao Zhou2,4, Jingjing $\\text{Liu}^{\\text{\\tiny{2}},\\text{\\tiny{4}}},$ Wei-Ying Ma2,4, Ya-Qin Zhang2,4, Lin $\\text{Yan}^{1,4},$  Mu Qiao1,4, Yonghui Wu1,Mingxuan Wang1,4\n\n## Affiliation\n\n1ByteDance Seed\n\n2Institute for AI Industry Research (AIR), Tsinghua University\n\n3The University of Hong Kong\n\n4SIA-Lab of Tsinghua AIR and ByteDance Seed\n\n## Acknowledgments\n\nWe thank Zhengyin Du, Shengding Hu, Kai Shen, Tianyang Zhan, Zhen Xiao, Renjie Zheng, Li Han, Kaihua Jiang as well as other colleagues at ByteDance for their support for the DAPO project.\n\n\n\n## References\n\n[1] OpenAI. Learning to reason with llms, 2024.\n\n[2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n\n[3] OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\n[4] Anthropic. Claude 3.5 sonnet, 2024.\n\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models are few-shot learners.Advances in neural information processing systems, 33:1877–1901, 2020.\n\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023.\n\n[7] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.\n\n[8] XAI. Grok 3 beta — the age of reasoning agents, 2024.\n\n[9] Google DeepMind. Gemini 2.0 fash thinking, 2024.l\n\n[10] Qwen. Qwq-32b: Embracing the power of reinforcement learning, 2024.\n\n[11] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025.\n\n[12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.\n\n[13] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, et al. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548, 2025.\n\n[14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasonerzero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025.\n\n[15] Jian Hu. Reinforce++: A simple and efcient approach for aligning large language models. arXiv preprintfi arXiv:2501.03262, 2025.\n\n[16] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu,Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025.\n\n[17] Jung Hyun Lee, June Yong Yang, Byeongho Heo, Dongyoon Han, and Kang Min Yoo.Token-supervised value models for enhancing mathematical reasoning capabilities of large language models.arXiv preprint arXiv:2407.12863, 2024.\n\n[18] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville,and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refned credit assignment. arXivi preprint arXiv:2410.01679, 2024.\n\n[19] Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. What’s behind ppo’s collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025.\n\n[20] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridfow: A fexible and efcient rlhf framework. arXiv preprint arXiv:2409.19256, 2024.llfi\n\n[21] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n\n\n[22] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation, 2018.\n\n[23] Long Ouyang, Jefrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhinif Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,editors, Advances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran Associates,Inc., 2022.\n\n[24] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety, 2016.\n\n[25] Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg. Reinforcement learning with a corrupted reward channel, 2017.\n\n[26] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specifcation gaming: the fip side of ai ingenuity, 2020.il\n\n[27] Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward tampering problems and solutions in reinforcement learning: A causal infuence diagram perspective, 2021.l\n\nao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 202\n\n[29] Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024.\n\n[30] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020.\n\n[31] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476–482, 2024.\n\nTrinh and Thang Luong. Alphageometry: An olympiad-level ai system for geometry, 2024.\n\n[33] AlphaProof and AlphaGeometry Teams. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024.\n\n[34] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314–21328, 2022.\n\n[35] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.Refexion: Language agents with verbal reinforcement learning, 2023.l\n\n[36] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug,2023.\n\n[37] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025.\n\n[38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\n\n\n\n## Appendix\n\n## ADataset Transformation\n\nHere is an example of data transformation, where we demonstrate the original problem, the transformed problem, and the model outputs during the transformation process (the original is in markdown text, which has been rendered for readability).\n\nTo ensure the model can complete this task with high accuracy, our goal is to have it perform comprehensive reasoning while avoiding hallucinations. We achieve this by providing a clear thinking framework that encourages thorough reasoning. Specifcally, we employ Chain of Thought (CoT) reasoning along with fouri well-defned steps: extract the answer format, rewrite the problem statement, solve the modifed problem andii provide an integer as the fnal answer. For each step, we provide few-shot solutions or detailed guidelines toi guide the model’s reasoning.\n\nThrough this approach, we have observed that in most cases, the LLM can generate reformulations with both format and quality that are satisfactory.\n\n## Original Problem\n\nLet x and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible value of x. Answer: $11-2\\sqrt{6}$\n\n## Transformed Problem\n\nLet x and be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible $y$ value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$ and $n$  are integers. Please fndi the value of $\\boldsymbol{k+m+n.}$  Answer: 19\n\n## Model’s output (Markdown Text)Transformation Thought Process\n\nThe original answer to the problem is an algebraic expression $11-2\\sqrt{6}.$  We need to transform this into an integer form. First, note that the answer consists of an integer part and a square root term.According to the given rule, we replace the integer and square root components with undetermined coefcients. The original problem usesfi $x$  and as variab√les, so we choose other letters as undetermined $y$  coefcients. We express the answer in the formfi $\\boldsymbol{k-m\\sqrt{n}}$ and require the student to fndi $k+m+n$\n\nAnswer Format\n\nk −mn\n\n## Transformed Problem\n\nLet $x$  and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$  and $n$  are integers. Please fndi the value of $\\boldsymbol{k+m+n.}$\n\nAnswer Calculation for the Transformed Problem\n\nThe original answer is: $11-2\\sqrt{6}.$\n\nThe required format is: $\\boldsymbol{k-m\\sqrt{n}}.$\n\nMatching the coefcients:fi $k=11, m=2, n=6.$\n\nThe fnal result is:i $k+m+n=11+2+6=19.$\n\n## Final Answer\n\n\n\n## BSupplementary Case\n\n## Question:\n\nAmong the 900 residents of Aimeville, there are 195 who own a diamond ring, 367 who own a set of golf clubs, and 562 who own a garden spade. In addition, each of the 900 residents owns a bag of candy hearts. There are 437 residents who own exactly two of these things, and 234 residents who own exactly three of these things. Find the number of residents of Aimeville who own all four of these things.\n\n## Response:\n\nStep by Step Solution:\n\n1.Let’s defne the following variables:i\n\nLet $A$  be the set of residents who own a diamond ring.\n\nLet $B$  be the set of residents who own a set of golf clubs.\n\nLet $C$ be the set of residents who own a garden spade.\n\nLet $D$  be the set of residents who own a bag of candy hearts. Since all 900 residents own a bag of candy hearts, $|D|=900.$\n\nLet $a_{i}$ denote the number of residents who own exactly $i$  out of the 4 items.\n\nLet’s use the principle of inclusion-exclusion for the union of 4 sets $A,B,C,D.$\n\n...\n\n$$a_{4}=\\tfrac{-219}{-4}=\\tfrac{219}{4}=54.75.$$\n\nSince $a_{4},$  the number of people owning all four items, must be a whole number, our current approach needs to be reconsidered in terms of the combinatorial counting of people owning diferentf combinations of the items.\n\nInstead of directly using the inclusion-exclusion principle, we can use a diferent approach involvingf the number of people owning combinations of the items $A,B,C,$ and D.\n\nLet $x_{i}$ be the number of people owning exactly $i$  item among $A,B,C$ (excluding the item $D$  which everyone owns).\n\nThen the total number of people owning exactly $k$  items among the $A,B,C,$ and $D$  can be broken down:\n\nTable 3 A case of refective behavior emergence.l\n\n","msg":"","ocr_all":false,"page_count":16,"pages":[{"abandon_blocks":[{"bbox":{"x0":132,"x1":1086,"y0":95,"y1":155},"conf":0.8401,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":135,"x1":449,"y0":104,"y1":144},"font_size":0.0,"text":""},{"bbox":{"x0":815,"x1":910,"y0":102,"y1":147},"font_size":0.0,"text":""},{"bbox":{"x0":893,"x1":1080,"y0":99,"y1":137},"font_size":0.0,"text":""}],"source":"layout det","text":""},{"bbox":{"x0":20,"x1":75,"y0":399,"y1":1132},"conf":0.4788,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":32,"x1":67,"y0":779,"y1":1120},"font_size":0.0,"text":"3 2v67441.a052:viXr"},{"bbox":{"x0":38,"x1":67,"y0":645,"y1":766},"font_size":0.0,"text":"[cs.LG]"},{"bbox":{"x0":26,"x1":77,"y0":402,"y1":645},"font_size":0.0,"text":"  20 May 2025"},{"bbox":{"x0":21,"x1":75,"y0":767,"y1":777},"font_size":20.0,"text":" "}],"source":"layout det","text":""},{"bbox":{"x0":598,"x1":623,"y0":1481,"y1":1509},"conf":0.3516,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":607,"x1":615,"y0":1488,"y1":1498},"font_size":0.0,"text":"1"}],"source":"layout det","text":""},{"bbox":{"x0":207,"x1":515,"y0":935,"y1":961},"conf":0.2971,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":211,"x1":511,"y0":936,"y1":957},"font_size":0.0,"text":"ahttps://github.com/volcengine/verl"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":160,"x1":1063,"y0":198,"y1":290},"conf":0.9433,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":166,"x1":1054,"y0":201,"y1":243},"font_size":0.0,"text":"DAPO: An Open-Source LLM Reinforcement Learning"},{"bbox":{"x0":472,"x1":752,"y0":246,"y1":280},"font_size":0.0,"text":"System at Scale"}],"source":"layout det","text":"DAPO: An Open-Source LLM Reinforcement Learning System at Scale"},{"bbox":{"x0":211,"x1":1011,"y0":343,"y1":427},"conf":0.8109,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":219,"x1":1003,"y0":345,"y1":375},"font_size":0.0,"text":"1ByteDance Seed2Institute for AI Industry Research (AIR), Tsinghua University"},{"bbox":{"x0":460,"x1":762,"y0":366,"y1":401},"font_size":0.0,"text":"3The University of Hong Kong"},{"bbox":{"x0":377,"x1":845,"y0":388,"y1":424},"font_size":0.0,"text":"4SIA-Lab of Tsinghua AIR and ByteDance Seed"}],"source":"layout det","text":"1ByteDance Seed2Institute for AI Industry Research (AIR), Tsinghua University 3The University of Hong Kong 4SIA-Lab of Tsinghua AIR and ByteDance Seed"},{"bbox":{"x0":465,"x1":759,"y0":436,"y1":465},"conf":0.9012,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":469,"x1":755,"y0":436,"y1":462},"font_size":0.0,"text":"Full author list in Contributions"}],"source":"layout det","text":"Full author list in Contributions"},{"bbox":{"x0":553,"x1":671,"y0":523,"y1":555},"conf":0.8963,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":557,"x1":669,"y0":525,"y1":553},"font_size":0.0,"text":"Abstract"}],"source":"layout det","text":"Abstract"},{"bbox":{"x0":182,"x1":1043,"y0":567,"y1":839},"conf":0.9755,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":1038,"y0":566,"y1":596},"font_size":0.0,"text":"Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning"},{"bbox":{"x0":186,"x1":1036,"y0":594,"y1":617},"font_size":0.0,"text":"as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art"},{"bbox":{"x0":186,"x1":1036,"y0":619,"y1":640},"font_size":0.0,"text":"reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus"},{"bbox":{"x0":186,"x1":1036,"y0":642,"y1":665},"font_size":0.0,"text":"the community still struggles to reproduce their RL training results. We propose the Decoupled"},{"bbox":{"x0":186,"x1":1036,"y0":667,"y1":690},"font_size":0.0,"text":"Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a"},{"bbox":{"x0":186,"x1":1034,"y0":690,"y1":713},"font_size":0.0,"text":"state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B"},{"bbox":{"x0":186,"x1":1036,"y0":714,"y1":738},"font_size":0.0,"text":"base model. Unlike previous works that withhold training details, we introduce four key techniques"},{"bbox":{"x0":183,"x1":1038,"y0":734,"y1":764},"font_size":0.0,"text":"of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training"},{"bbox":{"x0":186,"x1":1038,"y0":761,"y1":784},"font_size":0.0,"text":"code, which is built on the verl framework a, along with a carefully curated and processed dataset."},{"bbox":{"x0":183,"x1":1038,"y0":780,"y1":810},"font_size":0.0,"text":"These components of our open-source system enhance reproducibility and support future research"},{"bbox":{"x0":186,"x1":394,"y0":810,"y1":832},"font_size":0.0,"text":"in large-scale LLM RL."}],"source":"layout det","text":"Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework a, along with a carefully curated and processed dataset.These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL."},{"bbox":{"x0":184,"x1":371,"y0":850,"y1":875},"conf":0.8552,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":368,"y0":850,"y1":873},"font_size":0.0,"text":"Date: March 17, 2025"}],"source":"layout det","text":"Date: March 17, 2025"},{"bbox":{"x0":183,"x1":877,"y0":877,"y1":900},"conf":0.5458,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":873,"y0":874,"y1":898},"font_size":0.0,"text":"Correspondence: zhouhao@air.tsinghua.edu.cn, wangmingxuan.89@bytedance.com"}],"source":"layout det","text":"Correspondence: zhouhao@air.tsinghua.edu.cn, wangmingxuan.89@bytedance.com"},{"bbox":{"x0":182,"x1":567,"y0":901,"y1":926},"conf":0.578,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":560,"y0":899,"y1":922},"font_size":0.0,"text":"Project Page: https://dapo-sia.github.io/"}],"source":"layout det","text":"Project Page: https://dapo-sia.github.io/"},{"bbox":{"x0":176,"x1":1036,"y0":1026,"y1":1373},"conf":0.9653,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![f234c98bed8363e60dfb60ebc163f714](imgs/f234c98bed8363e60dfb60ebc163f714.jpg)"},{"bbox":{"x0":133,"x1":1092,"y0":1386,"y1":1442},"conf":0.9096,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1389,"y1":1412},"font_size":0.0,"text":"Figure 1 AIME 2024 scores of DAPO on the Qwen2.5-32B base model, outperforming the previous SoTA DeepSeek-"},{"bbox":{"x0":138,"x1":903,"y0":1411,"y1":1436},"font_size":0.0,"text":"R1-Zero-Qwen-32B using $50\\%$ training steps. The x-axis represents the gradient update steps."}],"source":"layout det","text":"Figure 1 AIME 2024 scores of DAPO on the Qwen2.5-32B base model, outperforming the previous SoTA DeepSeekR1-Zero-Qwen-32B using $50\\%$ training steps. The x-axis represents the gradient update steps."}],"formula_dets":[{"bbox":{"x0":348,"x1":385,"y0":1413,"y1":1434},"conf":0.76,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":182,"x1":1043,"y0":567,"y1":839},"conf":0.9755,"label":"Text","label_id":1},{"bbox":{"x0":176,"x1":1036,"y0":1026,"y1":1373},"conf":0.9653,"label":"Figure","label_id":3},{"bbox":{"x0":160,"x1":1063,"y0":198,"y1":290},"conf":0.9433,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":1092,"y0":1386,"y1":1442},"conf":0.9096,"label":"Figure caption","label_id":4},{"bbox":{"x0":465,"x1":759,"y0":436,"y1":465},"conf":0.9012,"label":"Text","label_id":1},{"bbox":{"x0":553,"x1":671,"y0":523,"y1":555},"conf":0.8963,"label":"Title","label_id":0},{"bbox":{"x0":184,"x1":371,"y0":850,"y1":875},"conf":0.8552,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1086,"y0":95,"y1":155},"conf":0.8401,"label":"Abandon","label_id":2},{"bbox":{"x0":211,"x1":1011,"y0":343,"y1":427},"conf":0.8109,"label":"Text","label_id":1},{"bbox":{"x0":600,"x1":621,"y0":1482,"y1":1508},"conf":0.5824,"label":"Abandon","label_id":2},{"bbox":{"x0":182,"x1":567,"y0":901,"y1":926},"conf":0.578,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":515,"y0":935,"y1":961},"conf":0.5638,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":877,"y0":877,"y1":900},"conf":0.5458,"label":"Text","label_id":1},{"bbox":{"x0":20,"x1":75,"y0":399,"y1":1132},"conf":0.4788,"label":"Abandon","label_id":2},{"bbox":{"x0":598,"x1":623,"y0":1481,"y1":1509},"conf":0.3516,"label":"Abandon","label_id":2},{"bbox":{"x0":207,"x1":515,"y0":935,"y1":961},"conf":0.2971,"label":"Abandon","label_id":2},{"bbox":{"x0":375,"x1":850,"y0":397,"y1":426},"conf":0.2055,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[607,1488],[615,1488],[615,1498],[607,1498]],"score":0.8645},{"poly":[[138,1411],[903,1412],[903,1436],[138,1434]],"score":0.7574},{"poly":[[140,1389],[1084,1389],[1084,1412],[140,1412]],"score":0.773},{"poly":[[569,1345],[679,1345],[679,1368],[569,1368]],"score":0.8274},{"poly":[[988,1328],[1038,1328],[1038,1351],[988,1351]],"score":0.9197},{"poly":[[837,1330],[876,1330],[876,1350],[837,1350]],"score":0.9021},{"poly":[[682,1330],[720,1330],[720,1350],[682,1350]],"score":0.8574},{"poly":[[526,1330],[564,1330],[564,1350],[526,1350]],"score":0.8236},{"poly":[[369,1330],[409,1330],[409,1350],[369,1350]],"score":0.9328},{"poly":[[230,1333],[239,1333],[239,1348],[230,1348]],"score":0.7897},{"poly":[[213,1312],[226,1312],[226,1330],[213,1330]],"score":0.7328},{"poly":[[890,1297],[1011,1297],[1011,1320],[890,1320]],"score":0.8211},{"poly":[[855,1299],[888,1299],[888,1317],[855,1317]],"score":0.7778},{"poly":[[891,1275],[1009,1275],[1009,1299],[891,1299]],"score":0.7877},{"poly":[[206,1279],[223,1279],[223,1295],[206,1295]],"score":0.8855},{"poly":[[893,1257],[1003,1257],[1003,1275],[893,1275]],"score":0.8891},{"poly":[[201,1236],[225,1236],[225,1264],[201,1264]],"score":0.6718},{"poly":[[201,1206],[225,1206],[225,1229],[201,1229]],"score":0.7833},{"poly":[[808,1160],[1006,1160],[1006,1176],[808,1176]],"score":0.788},{"poly":[[536,1138],[560,1138],[560,1152],[536,1152]],"score":0.6784},{"poly":[[196,1139],[219,1130],[227,1153],[205,1162]],"score":0.7583},{"poly":[[589,1097],[653,1101],[650,1139],[586,1134]],"score":0.7217},{"poly":[[562,1106],[580,1106],[580,1120],[562,1120]],"score":0.6442},{"poly":[[201,1066],[228,1066],[228,1087],[201,1087]],"score":0.7568},{"poly":[[201,1031],[231,1031],[231,1053],[201,1053]],"score":0.8294},{"poly":[[211,936],[511,936],[511,957],[211,957]],"score":0.8248},{"poly":[[186,899],[560,899],[560,922],[186,922]],"score":0.737},{"poly":[[186,874],[873,874],[873,898],[186,898]],"score":0.7248},{"poly":[[186,850],[368,850],[368,873],[186,873]],"score":0.7786},{"poly":[[186,810],[394,810],[394,832],[186,832]],"score":0.9529},{"poly":[[183,780],[1038,782],[1038,810],[183,808]],"score":0.6803},{"poly":[[32,779],[67,779],[67,1120],[32,1120]],"score":0.7546},{"poly":[[186,761],[1038,761],[1038,784],[186,784]],"score":0.7507},{"poly":[[183,734],[1038,736],[1038,764],[183,762]],"score":0.6695},{"poly":[[186,714],[1036,714],[1036,738],[186,738]],"score":0.7628},{"poly":[[186,690],[1034,690],[1034,713],[186,713]],"score":0.7365},{"poly":[[186,667],[1036,667],[1036,690],[186,690]],"score":0.7443},{"poly":[[38,645],[67,645],[67,766],[38,766]],"score":0.6655},{"poly":[[186,642],[1036,642],[1036,665],[186,665]],"score":0.759},{"poly":[[186,619],[1036,619],[1036,640],[186,640]],"score":0.9298},{"poly":[[186,594],[1036,594],[1036,617],[186,617]],"score":0.7463},{"poly":[[185,566],[1038,568],[1038,596],[185,594]],"score":0.7065},{"poly":[[557,525],[669,525],[669,553],[557,553]],"score":0.9016},{"poly":[[469,436],[755,439],[755,462],[469,459]],"score":0.7878},{"poly":[[26,403],[71,402],[77,644],[32,645]],"score":0.7325},{"poly":[[378,388],[845,391],[845,424],[377,421]],"score":0.6804},{"poly":[[461,366],[762,371],[761,401],[460,396]],"score":0.6335},{"poly":[[220,345],[1003,347],[1003,375],[219,373]],"score":0.8184},{"poly":[[472,246],[752,246],[752,280],[472,280]],"score":0.8385},{"poly":[[166,201],[1054,203],[1054,243],[166,241]],"score":0.8516},{"poly":[[135,104],[449,104],[449,144],[135,144]],"score":0.8246},{"poly":[[815,102],[910,102],[910,147],[815,147]],"score":0.8254},{"poly":[[893,103],[1079,99],[1080,133],[894,137]],"score":0.708}],"page_no":0,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":599,"x1":623,"y0":1527,"y1":1553},"conf":0.3078,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1530,"y1":1551},"font_size":0.0,"text":"2"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":326,"y0":196,"y1":226},"conf":0.8716,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":319,"y0":200,"y1":223},"font_size":0.0,"text":"1Introduction"}],"source":"layout det","text":"1Introduction"},{"bbox":{"x0":134,"x1":1091,"y0":242,"y1":343},"conf":0.9546,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":244,"y1":271},"font_size":0.0,"text":"Test-time scaling such as OpenAI’s o1 [1] and DeepSeek’s R1 [2] brings a profound paradigm shift to Large"},{"bbox":{"x0":140,"x1":1083,"y0":271,"y1":294},"font_size":0.0,"text":"Language Models (LLMs) [3–7]. Test-time scaling enables longer Chain-of-Thought thinking and induces"},{"bbox":{"x0":138,"x1":1083,"y0":295,"y1":317},"font_size":0.0,"text":"sophisticated reasoning behaviors, which makes the models superior in competitive math and coding tasks"},{"bbox":{"x0":138,"x1":378,"y0":317,"y1":342},"font_size":0.0,"text":"like AIME and Codeforces."}],"source":"layout det","text":"Test-time scaling such as OpenAI’s o1 [1] and DeepSeek’s R1 [2] brings a profound paradigm shift to Large Language Models (LLMs) [3–7]. Test-time scaling enables longer Chain-of-Thought thinking and induces sophisticated reasoning behaviors, which makes the models superior in competitive math and coding tasks like AIME and Codeforces."},{"bbox":{"x0":135,"x1":1089,"y0":350,"y1":499},"conf":0.9716,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":355,"y1":376},"font_size":0.0,"text":"The central technique driving the revolution is large-scale Reinforcement Learning (RL), which elicits complex"},{"bbox":{"x0":138,"x1":1083,"y0":379,"y1":401},"font_size":0.0,"text":"reasoning behaviors such as self-verifcation and iterative refnement. However, the actual algorithm andii"},{"bbox":{"x0":136,"x1":1084,"y0":401,"y1":426},"font_size":0.0,"text":"key recipe for scalable RL training remains a myth, hidden from technical reports of existing reasoning"},{"bbox":{"x0":138,"x1":1084,"y0":426,"y1":449},"font_size":0.0,"text":"models [1, 2, 8–11]. In this paper, we reveal signifcant obstacles in large-scale RL training and open-source ai"},{"bbox":{"x0":138,"x1":1083,"y0":450,"y1":472},"font_size":0.0,"text":"scalable RL system with fully open-sourced algorithm, training code and dataset that provides democratized"},{"bbox":{"x0":138,"x1":492,"y0":474,"y1":495},"font_size":0.0,"text":"solutions with industry-level RL results."}],"source":"layout det","text":"The central technique driving the revolution is large-scale Reinforcement Learning (RL), which elicits complex reasoning behaviors such as self-verifcation and iterative refnement. However, the actual algorithm andii key recipe for scalable RL training remains a myth, hidden from technical reports of existing reasoning models [1, 2, 8–11]. In this paper, we reveal signifcant obstacles in large-scale RL training and open-source ai scalable RL system with fully open-sourced algorithm, training code and dataset that provides democratized solutions with industry-level RL results."},{"bbox":{"x0":135,"x1":1090,"y0":505,"y1":656},"conf":0.9723,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1083,"y0":510,"y1":531},"font_size":0.0,"text":"We experiment over Qwen2.5-32B [12] as the pretrained model for RL. In our initial GRPO run, we achieved"},{"bbox":{"x0":140,"x1":1083,"y0":534,"y1":556},"font_size":0.0,"text":"only 30 points on AIME — a performance signifcantly below DeepSeek’s RL (47 points). A thorough analysisi"},{"bbox":{"x0":136,"x1":1084,"y0":556,"y1":581},"font_size":0.0,"text":"reveals that the naive GRPO baseline sufers from several key issues such as entropy collapse, reward noise,f"},{"bbox":{"x0":138,"x1":1084,"y0":582,"y1":604},"font_size":0.0,"text":"and training instability. The broader community has encountered similar challenges in reproducing DeepSeek’s"},{"bbox":{"x0":138,"x1":1084,"y0":606,"y1":629},"font_size":0.0,"text":"results [13–19] suggesting that critical training details may have been omitted in the R1 paper that are"},{"bbox":{"x0":138,"x1":827,"y0":630,"y1":652},"font_size":0.0,"text":"required to develop an industry-level, large-scale, and reproducible RL system."}],"source":"layout det","text":"We experiment over Qwen2.5-32B [12] as the pretrained model for RL. In our initial GRPO run, we achieved only 30 points on AIME — a performance signifcantly below DeepSeek’s RL (47 points). A thorough analysisi reveals that the naive GRPO baseline sufers from several key issues such as entropy collapse, reward noise,f and training instability. The broader community has encountered similar challenges in reproducing DeepSeek’s results [13–19] suggesting that critical training details may have been omitted in the R1 paper that are required to develop an industry-level, large-scale, and reproducible RL system."},{"bbox":{"x0":135,"x1":1089,"y0":661,"y1":787},"conf":0.9706,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":665,"y1":688},"font_size":0.0,"text":"To close this gap, we release an open-source state-of-the-art system for large-scale LLM RL, which achieves 50"},{"bbox":{"x0":140,"x1":1084,"y0":690,"y1":713},"font_size":0.0,"text":"points on AIME 2024 based on Qwen2.5-32B model, outperforming previous state-of-the-art results achieved"},{"bbox":{"x0":138,"x1":1084,"y0":711,"y1":736},"font_size":0.0,"text":"by DeepSeek-R1-Zero-Qwen-32B [2] (47 points) using $50\\%$ training steps (Figure 1). We propose the Decoupled"},{"bbox":{"x0":140,"x1":1084,"y0":738,"y1":759},"font_size":0.0,"text":"Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and introduce 4 key techniques to make"},{"bbox":{"x0":138,"x1":788,"y0":759,"y1":784},"font_size":0.0,"text":"RL shine in the long-CoT RL scenario. Details are presented in Section 3."}],"source":"layout det","text":"To close this gap, we release an open-source state-of-the-art system for large-scale LLM RL, which achieves 50 points on AIME 2024 based on Qwen2.5-32B model, outperforming previous state-of-the-art results achieved by DeepSeek-R1-Zero-Qwen-32B [2] (47 points) using $50\\%$ training steps (Figure 1). We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and introduce 4 key techniques to make RL shine in the long-CoT RL scenario. Details are presented in Section 3."},{"bbox":{"x0":159,"x1":937,"y0":793,"y1":823},"conf":0.8889,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":161,"x1":931,"y0":795,"y1":820},"font_size":0.0,"text":"1. Clip-Higher, which promotes the diversity of the system and avoids entropy collapse;"}],"source":"layout det","text":"1.Clip-Higher, which promotes the diversity of the system and avoids entropy collapse;"},{"bbox":{"x0":160,"x1":791,"y0":829,"y1":858},"conf":0.8626,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":163,"x1":785,"y0":833,"y1":855},"font_size":0.0,"text":"2. Dynamic Sampling, which improves training efciency and stability;fi"}],"source":"layout det","text":"2.Dynamic Sampling, which improves training efciency and stability;fi"},{"bbox":{"x0":160,"x1":874,"y0":865,"y1":895},"conf":0.8873,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":161,"x1":866,"y0":866,"y1":893},"font_size":0.0,"text":"3. Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios;"}],"source":"layout det","text":"3.Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios;"},{"bbox":{"x0":160,"x1":880,"y0":902,"y1":931},"conf":0.881,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":161,"x1":873,"y0":903,"y1":929},"font_size":0.0,"text":"4. Overlong Reward Shaping, which reduces reward noise and stabilizes training."}],"source":"layout det","text":"4.Overlong Reward Shaping, which reduces reward noise and stabilizes training."},{"bbox":{"x0":134,"x1":1088,"y0":936,"y1":989},"conf":0.9251,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1083,"y0":936,"y1":964},"font_size":0.0,"text":"Our implementation is based on verl [20]. By fully releasing our state-of-the-art RL system including training"},{"bbox":{"x0":140,"x1":1073,"y0":965,"y1":987},"font_size":0.0,"text":"code and data, we aim to reveal valuable insights to large-scale LLM RL that beneft the larger community.i"}],"source":"layout det","text":"Our implementation is based on verl [20]. By fully releasing our state-of-the-art RL system including training code and data, we aim to reveal valuable insights to large-scale LLM RL that beneft the larger community.i"},{"bbox":{"x0":135,"x1":321,"y0":1011,"y1":1044},"conf":0.8942,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":134,"x1":316,"y0":1010,"y1":1043},"font_size":0.0,"text":"2Preliminary"}],"source":"layout det","text":"2Preliminary"},{"bbox":{"x0":135,"x1":575,"y0":1056,"y1":1088},"conf":0.9043,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":570,"y0":1059,"y1":1082},"font_size":0.0,"text":"2.1Proximal Policy Optimization (PPO)"}],"source":"layout det","text":"2.1Proximal Policy Optimization (PPO)"},{"bbox":{"x0":135,"x1":1090,"y0":1095,"y1":1174},"conf":0.9599,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1099,"y1":1122},"font_size":0.0,"text":"PPO [21] introduces a clipped surrogate objective for policy optimization. By constraining the policy updates"},{"bbox":{"x0":140,"x1":1084,"y0":1124,"y1":1145},"font_size":0.0,"text":"within a proximal region of the previous policy using clip, PPO stabilizes training and improves sample"},{"bbox":{"x0":138,"x1":898,"y0":1147,"y1":1170},"font_size":0.0,"text":"efciency. Specifcally, PPO updates the policy by maximizing the following objective:fii"}],"source":"layout det","text":"PPO [21] introduces a clipped surrogate objective for policy optimization. By constraining the policy updates within a proximal region of the previous policy using clip, PPO stabilizes training and improves sample efciency. Specifcally, PPO updates the policy by maximizing the following objective:fii"},{"bbox":{"x0":158,"x1":1042,"y0":1183,"y1":1253},"conf":0.8866,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\text{PPO}}(\\theta)=\\mathbb{E}_{(q,a)\\sim\\mathcal{D},o_{\\leq t} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid q)}\\Bigg[\\min\\Bigg(\\frac{\\pi_{\\theta}(o_{t}\\mid q,o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_{t}\\mid q,o_{<t})} \\hat{A}_{t},\\ \\text{clip}\\Bigg(\\frac{\\pi_{\\theta}(o_{t}\\mid q,o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_{t}\\mid q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\Bigg)\\hat{A}_{t}\\Bigg)\\Bigg]$$"},{"bbox":{"x0":1055,"x1":1088,"y0":1204,"y1":1233},"conf":0.788,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1051,"x1":1086,"y0":1203,"y1":1233},"font_size":0.0,"text":"(1)"}],"source":"layout det","text":"(1)"},{"bbox":{"x0":136,"x1":1088,"y0":1264,"y1":1344},"conf":0.9399,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1086,"y0":1266,"y1":1293},"font_size":0.0,"text":"where $(q,a)$  is a question-answer pair from the data distribution $\\mathcal{D}, \\varepsilon$ is the clipping range of importance"},{"bbox":{"x0":138,"x1":1084,"y0":1292,"y1":1315},"font_size":0.0,"text":"sampling ratio, and $\\hat{A}_{t}$ is an estimator of the advantage at time step $t$  Given the value function $V$ and the"},{"bbox":{"x0":138,"x1":953,"y0":1315,"y1":1340},"font_size":0.0,"text":"reward function $R, \\hat{A}_{t}$ is computed using the Generalized Advantage Estimation (GAE) [22]:"}],"source":"layout det","text":"where $(q,a)$  is a question-answer pair from the data distribution $\\mathcal{D}, \\varepsilon$ is the clipping range of importance sampling ratio, and $\\hat{A}_{t}$ is an estimator of the advantage at time step $t$  Given the value function $V$ and the reward function $R, \\hat{A}_{t}$ is computed using the Generalized Advantage Estimation (GAE) [22]:"},{"bbox":{"x0":487,"x1":716,"y0":1357,"y1":1421},"conf":0.9325,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\hat{A}_{t}^{\\text{GAE}(\\gamma,\\lambda)}=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^{l}\\delta_{t+l},$$"},{"bbox":{"x0":1055,"x1":1089,"y0":1374,"y1":1404},"conf":0.8335,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1056,"x1":1088,"y0":1374,"y1":1404},"font_size":0.0,"text":"(2)"}],"source":"layout det","text":"(2)"},{"bbox":{"x0":134,"x1":198,"y0":1435,"y1":1461},"conf":0.8175,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":196,"y0":1437,"y1":1460},"font_size":0.0,"text":"where"}],"source":"layout det","text":"where"},{"bbox":{"x0":422,"x1":800,"y0":1460,"y1":1486},"conf":0.8714,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\delta_{l}=R_{l}+\\gamma V(s_{l+1})-V(s_{l}),\\quad0\\leq\\gamma,\\lambda\\leq1.$$"},{"bbox":{"x0":1054,"x1":1089,"y0":1458,"y1":1489},"conf":0.8386,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1056,"x1":1088,"y0":1459,"y1":1488},"font_size":0.0,"text":"(3)"}],"source":"layout det","text":"(3)"}],"formula_dets":[{"bbox":{"x0":487,"x1":716,"y0":1357,"y1":1421},"conf":0.9325,"label":"print_isolated","label_id":1},{"bbox":{"x0":199,"x1":244,"y0":1271,"y1":1293},"conf":0.8994,"label":"print_embedding","label_id":0},{"bbox":{"x0":158,"x1":1042,"y0":1183,"y1":1253},"conf":0.8866,"label":"print_isolated","label_id":1},{"bbox":{"x0":422,"x1":800,"y0":1460,"y1":1486},"conf":0.8714,"label":"print_isolated","label_id":1},{"bbox":{"x0":317,"x1":340,"y0":1292,"y1":1314},"conf":0.8507,"label":"print_embedding","label_id":0},{"bbox":{"x0":990,"x1":1008,"y0":1296,"y1":1312},"conf":0.8495,"label":"print_embedding","label_id":0},{"bbox":{"x0":596,"x1":635,"y0":712,"y1":733},"conf":0.8416,"label":"print_embedding","label_id":0},{"bbox":{"x0":721,"x1":762,"y0":1266,"y1":1290},"conf":0.8302,"label":"print_embedding","label_id":0},{"bbox":{"x0":284,"x1":335,"y0":1315,"y1":1339},"conf":0.6842,"label":"print_embedding","label_id":0},{"bbox":{"x0":746,"x1":758,"y0":1297,"y1":1312},"conf":0.5761,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":135,"x1":1090,"y0":505,"y1":656},"conf":0.9723,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1089,"y0":350,"y1":499},"conf":0.9716,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1089,"y0":661,"y1":787},"conf":0.9706,"label":"Text","label_id":1},{"bbox":{"x0":149,"x1":1049,"y0":1181,"y1":1255},"conf":0.9639,"label":"Equation","label_id":8},{"bbox":{"x0":135,"x1":1090,"y0":1095,"y1":1174},"conf":0.9599,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1091,"y0":242,"y1":343},"conf":0.9546,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1088,"y0":1264,"y1":1344},"conf":0.9399,"label":"Text","label_id":1},{"bbox":{"x0":484,"x1":720,"y0":1357,"y1":1423},"conf":0.9386,"label":"Equation","label_id":8},{"bbox":{"x0":134,"x1":1088,"y0":936,"y1":989},"conf":0.9251,"label":"Text","label_id":1},{"bbox":{"x0":418,"x1":806,"y0":1457,"y1":1490},"conf":0.9123,"label":"Equation","label_id":8},{"bbox":{"x0":135,"x1":575,"y0":1056,"y1":1088},"conf":0.9043,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":321,"y0":1011,"y1":1044},"conf":0.8942,"label":"Title","label_id":0},{"bbox":{"x0":159,"x1":937,"y0":793,"y1":823},"conf":0.8889,"label":"Text","label_id":1},{"bbox":{"x0":160,"x1":874,"y0":865,"y1":895},"conf":0.8873,"label":"Text","label_id":1},{"bbox":{"x0":160,"x1":880,"y0":902,"y1":931},"conf":0.881,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":326,"y0":196,"y1":226},"conf":0.8716,"label":"Title","label_id":0},{"bbox":{"x0":160,"x1":791,"y0":829,"y1":858},"conf":0.8626,"label":"Text","label_id":1},{"bbox":{"x0":1054,"x1":1089,"y0":1458,"y1":1489},"conf":0.8386,"label":"Equation caption","label_id":9},{"bbox":{"x0":1055,"x1":1089,"y0":1374,"y1":1404},"conf":0.8335,"label":"Equation caption","label_id":9},{"bbox":{"x0":134,"x1":198,"y0":1435,"y1":1461},"conf":0.8175,"label":"Text","label_id":1},{"bbox":{"x0":1055,"x1":1088,"y0":1204,"y1":1233},"conf":0.788,"label":"Equation caption","label_id":9},{"bbox":{"x0":599,"x1":623,"y0":1527,"y1":1553},"conf":0.3078,"label":"Abandon","label_id":2},{"bbox":{"x0":602,"x1":621,"y0":1528,"y1":1551},"conf":0.2271,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1530],[619,1530],[619,1551],[604,1551]],"score":0.7978},{"poly":[[421,1462],[802,1462],[802,1485],[421,1485]],"score":0.8179},{"poly":[[1056,1459],[1088,1459],[1088,1488],[1056,1488]],"score":0.8596},{"poly":[[136,1437],[196,1437],[196,1460],[136,1460]],"score":0.9086},{"poly":[[602,1396],[635,1403],[630,1425],[597,1418]],"score":0.7316},{"poly":[[1056,1374],[1088,1374],[1088,1404],[1056,1404]],"score":0.8505},{"poly":[[595,1371],[720,1371],[720,1407],[595,1407]],"score":0.6596},{"poly":[[582,1376],[597,1376],[597,1393],[582,1393]],"score":0.6153},{"poly":[[604,1358],[629,1358],[629,1374],[604,1374]],"score":0.6851},{"poly":[[138,1317],[953,1317],[953,1340],[138,1340]],"score":0.726},{"poly":[[138,1294],[1084,1294],[1084,1315],[138,1315]],"score":0.8352},{"poly":[[138,1267],[1086,1269],[1086,1292],[138,1290]],"score":0.7319},{"poly":[[975,1239],[985,1239],[985,1247],[975,1247]],"score":0.7903},{"poly":[[499,1223],[645,1223],[645,1246],[499,1246]],"score":0.8188},{"poly":[[720,1221],[873,1217],[873,1246],[721,1249]],"score":0.8027},{"poly":[[154,1201],[441,1211],[440,1239],[153,1229]],"score":0.7469},{"poly":[[868,1204],[983,1204],[983,1233],[868,1233]],"score":0.6984},{"poly":[[445,1207],[489,1202],[491,1227],[448,1232]],"score":0.7695},{"poly":[[1051,1203],[1086,1203],[1086,1233],[1051,1233]],"score":0.8213},{"poly":[[635,1199],[719,1205],[717,1235],[633,1229]],"score":0.7289},{"poly":[[988,1201],[1023,1201],[1023,1233],[988,1233]],"score":0.8006},{"poly":[[735,1195],[861,1195],[861,1218],[735,1218]],"score":0.9132},{"poly":[[511,1191],[645,1195],[645,1220],[510,1216]],"score":0.7521},{"poly":[[138,1147],[898,1147],[898,1170],[138,1170]],"score":0.7208},{"poly":[[140,1124],[1084,1124],[1084,1145],[140,1145]],"score":0.8384},{"poly":[[140,1099],[1084,1099],[1084,1122],[140,1122]],"score":0.7317},{"poly":[[138,1059],[570,1059],[570,1082],[138,1082]],"score":0.7985},{"poly":[[135,1010],[316,1015],[315,1043],[134,1038]],"score":0.7443},{"poly":[[140,965],[1073,965],[1073,987],[140,987]],"score":0.8344},{"poly":[[138,936],[1083,941],[1083,964],[138,959]],"score":0.7415},{"poly":[[161,903],[873,906],[873,929],[161,926]],"score":0.7125},{"poly":[[161,866],[866,870],[866,893],[161,889]],"score":0.7161},{"poly":[[163,833],[785,833],[785,855],[163,855]],"score":0.8377},{"poly":[[161,795],[931,797],[931,820],[161,818]],"score":0.7709},{"poly":[[138,759],[788,761],[788,784],[138,782]],"score":0.7701},{"poly":[[140,738],[1084,738],[1084,759],[140,759]],"score":0.8862},{"poly":[[138,711],[1084,713],[1084,736],[138,734]],"score":0.7637},{"poly":[[140,690],[1084,690],[1084,713],[140,713]],"score":0.6859},{"poly":[[140,665],[1084,665],[1084,688],[140,688]],"score":0.6946},{"poly":[[138,630],[827,630],[827,652],[138,652]],"score":0.818},{"poly":[[138,606],[1084,606],[1084,629],[138,629]],"score":0.7115},{"poly":[[138,582],[1084,582],[1084,604],[138,604]],"score":0.7824},{"poly":[[136,556],[1084,558],[1084,581],[136,579]],"score":0.7566},{"poly":[[140,535],[1083,535],[1083,556],[140,556]],"score":0.827},{"poly":[[140,510],[1083,510],[1083,531],[140,531]],"score":0.861},{"poly":[[138,474],[492,474],[492,495],[138,495]],"score":0.856},{"poly":[[138,450],[1083,450],[1083,472],[138,472]],"score":0.8101},{"poly":[[138,426],[1084,426],[1084,449],[138,449]],"score":0.7033},{"poly":[[136,401],[1084,403],[1084,426],[136,424]],"score":0.7457},{"poly":[[138,380],[1083,380],[1083,401],[138,401]],"score":0.8313},{"poly":[[140,355],[1084,355],[1084,376],[140,376]],"score":0.8735},{"poly":[[138,317],[378,318],[377,342],[138,340]],"score":0.7222},{"poly":[[138,295],[1083,295],[1083,317],[138,317]],"score":0.8711},{"poly":[[140,271],[1083,271],[1083,294],[140,294]],"score":0.7485},{"poly":[[138,244],[1084,248],[1084,271],[138,267]],"score":0.7431},{"poly":[[140,205],[175,205],[175,218],[140,218]],"score":0.721},{"poly":[[170,200],[319,200],[319,223],[170,223]],"score":0.8269}],"page_no":1,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":598,"x1":624,"y0":1526,"y1":1554},"conf":0.4767,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":602,"x1":622,"y0":1528,"y1":1551},"font_size":0.0,"text":"3"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":144,"x1":1080,"y0":200,"y1":544},"conf":0.9797,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![69bfebaf05ee75a3456d5dd867d2cccd](imgs/69bfebaf05ee75a3456d5dd867d2cccd.jpg)"},{"bbox":{"x0":133,"x1":1091,"y0":555,"y1":610},"conf":0.9318,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":559,"y1":581},"font_size":10.0,"text":"Figure 2 The accuracy on the AIME test set and the entropy of the actor model’s generated probabilities during the"},{"bbox":{"x0":138,"x1":738,"y0":579,"y1":606},"font_size":10.0,"text":"RL training process, both before and after applying Clip-Higher strategy."}],"source":"layout det","text":"Figure 2 The accuracy on the AIME test set and the entropy of the actor model’s generated probabilities during the RL training process, both before and after applying Clip-Higher strategy."},{"bbox":{"x0":134,"x1":662,"y0":640,"y1":675},"conf":0.8791,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":654,"y0":645,"y1":668},"font_size":10.0,"text":"2.2Group Relative Policy Optimization (GRPO)"}],"source":"layout det","text":"2.2Group Relative Policy Optimization (GRPO)"},{"bbox":{"x0":135,"x1":1091,"y0":681,"y1":783},"conf":0.9563,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":681,"y1":706},"font_size":0.0,"text":"Compared to PPO, GRPO eliminates the value function and estimates the advantage in a group-relative"},{"bbox":{"x0":138,"x1":1083,"y0":708,"y1":733},"font_size":0.0,"text":"manner. For a specifc question-answer pairi $(q,a),$  the behavior policy $\\pi_{\\theta_{\\text{old}}}$  samples a group of $G$  individual"},{"bbox":{"x0":141,"x1":1082,"y0":731,"y1":757},"font_size":10.0,"text":"responses $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}.$  Then, the advantage of the $i$ th response is calculated by normalizing the group-level"},{"bbox":{"x0":141,"x1":290,"y0":757,"y1":781},"font_size":10.0,"text":"rewards $\\{R_{i} \\}_{i=1}^{G} :$"}],"source":"layout det","text":"Compared to PPO, GRPO eliminates the value function and estimates the advantage in a group-relative manner. For a specifc question-answer pairi $(q,a),$  the behavior policy $\\pi_{\\theta_{\\text{old}}}$  samples a group of $G$  individual responses $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}.$  Then, the advantage of the $i$ th response is calculated by normalizing the group-level rewards $\\{R_{i} \\}_{i=1}^{G} :$"},{"bbox":{"x0":490,"x1":732,"y0":777,"y1":833},"conf":0.9462,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\hat{A}_{i,t}=\\frac{r_{i}-\\mathrm{mean}(\\{R_{i}\\}_{i=1}^{G})}{\\mathrm{std}(\\{R_{i}\\}_{i=1}^{G})}.$$"},{"bbox":{"x0":1053,"x1":1089,"y0":789,"y1":822},"conf":0.8275,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1054,"x1":1089,"y0":789,"y1":820},"font_size":10.0,"text":"(4)"}],"source":"layout det","text":"(4)"},{"bbox":{"x0":133,"x1":1044,"y0":847,"y1":881},"conf":0.9106,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1041,"y0":851,"y1":874},"font_size":10.0,"text":"Similar to PPO, GRPO adopts a clipped objective, together with a directly imposed KL penalty term:"}],"source":"layout det","text":"Similar to PPO, GRPO adopts a clipped objective, together with a directly imposed KL penalty term:"},{"bbox":{"x0":194,"x1":533,"y0":893,"y1":926},"conf":0.6994,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$"},{"bbox":{"x0":273,"x1":1025,"y0":929,"y1":1001},"conf":0.8774,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\Bigg[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\Bigg( \\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\Big)\\hat{A}_{i,t}\\Big)-\\beta D_{\\text{KL}}(\\pi_{\\theta}||\\pi_{\\text{ref}})\\Bigg)\\Bigg].$$"},{"bbox":{"x0":1054,"x1":1089,"y0":929,"y1":961},"conf":0.8236,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1054,"x1":1086,"y0":931,"y1":962},"font_size":10.0,"text":"(5)"}],"source":"layout det","text":"(5)"},{"bbox":{"x0":133,"x1":198,"y0":1010,"y1":1037},"conf":0.8337,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":196,"y0":1011,"y1":1036},"font_size":10.0,"text":"where"}],"source":"layout det","text":"where"},{"bbox":{"x0":486,"x1":735,"y0":1029,"y1":1084},"conf":0.9469,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$r_{i , t}(\\theta)=\\frac{\\pi_{\\theta}(o_{i , t}\\mid q,o_{i ,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i , t}\\mid q,o_{i ,<t})}.$$"},{"bbox":{"x0":1054,"x1":1089,"y0":1041,"y1":1072},"conf":0.8436,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1054,"x1":1089,"y0":1040,"y1":1071},"font_size":10.0,"text":"(6)"}],"source":"layout det","text":"(6)"},{"bbox":{"x0":134,"x1":1091,"y0":1098,"y1":1179},"conf":0.9517,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":1101,"y1":1125},"font_size":10.0,"text":"It is also worth noting that GRPO computes the objective at the sample-level. To be exact, GRPO frsti"},{"bbox":{"x0":138,"x1":1084,"y0":1127,"y1":1148},"font_size":10.0,"text":"calculates the mean loss within each generated sequence, before averaging the loss of diferent samples. As wef"},{"bbox":{"x0":138,"x1":1069,"y0":1152,"y1":1173},"font_size":10.0,"text":"will be discussing in Section 3.3, such diference may have an impact on the performance of the algorithm.f"}],"source":"layout det","text":"It is also worth noting that GRPO computes the objective at the sample-level. To be exact, GRPO frsti calculates the mean loss within each generated sequence, before averaging the loss of diferent samples. As wef will be discussing in Section 3.3, such diference may have an impact on the performance of the algorithm.f"},{"bbox":{"x0":134,"x1":469,"y0":1192,"y1":1226},"conf":0.8993,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":464,"y0":1196,"y1":1219},"font_size":10.0,"text":"2.3Removing KL Divergence"}],"source":"layout det","text":"2.3Removing KL Divergence"},{"bbox":{"x0":134,"x1":1091,"y0":1231,"y1":1360},"conf":0.9624,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1084,"y0":1233,"y1":1257},"font_size":0.0,"text":"The KL penalty term is used to regulate the divergence between the online policy and the frozen reference"},{"bbox":{"x0":138,"x1":1084,"y0":1259,"y1":1280},"font_size":0.0,"text":"policy. In the RLHF scenario [23], the goal of RL is to align the model behavior without diverging too far"},{"bbox":{"x0":138,"x1":1084,"y0":1282,"y1":1305},"font_size":0.0,"text":"from the initial model. However, during training the long-CoT reasoning model, the model distribution can"},{"bbox":{"x0":138,"x1":1084,"y0":1307,"y1":1328},"font_size":0.0,"text":"diverge signifcantly from the initial model, thus this restriction is not necessary. Therefore, we will excludei"},{"bbox":{"x0":138,"x1":519,"y0":1332,"y1":1353},"font_size":0.0,"text":"the KL term from our proposed algorithm."}],"source":"layout det","text":"The KL penalty term is used to regulate the divergence between the online policy and the frozen reference policy. In the RLHF scenario [23], the goal of RL is to align the model behavior without diverging too far from the initial model. However, during training the long-CoT reasoning model, the model distribution can diverge signifcantly from the initial model, thus this restriction is not necessary. Therefore, we will excludei the KL term from our proposed algorithm."}],"formula_dets":[{"bbox":{"x0":486,"x1":735,"y0":1029,"y1":1084},"conf":0.9469,"label":"print_isolated","label_id":1},{"bbox":{"x0":490,"x1":732,"y0":777,"y1":833},"conf":0.9462,"label":"print_isolated","label_id":1},{"bbox":{"x0":526,"x1":576,"y0":710,"y1":733},"conf":0.8902,"label":"print_embedding","label_id":0},{"bbox":{"x0":215,"x1":290,"y0":758,"y1":781},"conf":0.8778,"label":"print_embedding","label_id":0},{"bbox":{"x0":231,"x1":301,"y0":731,"y1":757},"conf":0.8776,"label":"print_embedding","label_id":0},{"bbox":{"x0":273,"x1":1025,"y0":929,"y1":1001},"conf":0.8774,"label":"print_isolated","label_id":1},{"bbox":{"x0":756,"x1":795,"y0":716,"y1":733},"conf":0.8464,"label":"print_embedding","label_id":0},{"bbox":{"x0":972,"x1":988,"y0":712,"y1":728},"conf":0.8407,"label":"print_embedding","label_id":0},{"bbox":{"x0":564,"x1":574,"y0":737,"y1":752},"conf":0.7495,"label":"print_embedding","label_id":0},{"bbox":{"x0":194,"x1":533,"y0":893,"y1":926},"conf":0.6994,"label":"print_isolated","label_id":1}],"height":792,"layout_dets":[{"bbox":{"x0":144,"x1":1080,"y0":200,"y1":544},"conf":0.9797,"label":"Figure","label_id":3},{"bbox":{"x0":134,"x1":1091,"y0":1231,"y1":1360},"conf":0.9624,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1091,"y0":681,"y1":783},"conf":0.9563,"label":"Text","label_id":1},{"bbox":{"x0":190,"x1":1033,"y0":890,"y1":1003},"conf":0.956,"label":"Equation","label_id":8},{"bbox":{"x0":134,"x1":1091,"y0":1098,"y1":1179},"conf":0.9517,"label":"Text","label_id":1},{"bbox":{"x0":486,"x1":737,"y0":778,"y1":837},"conf":0.9326,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1091,"y0":555,"y1":610},"conf":0.9318,"label":"Figure caption","label_id":4},{"bbox":{"x0":482,"x1":741,"y0":1025,"y1":1089},"conf":0.9229,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1044,"y0":847,"y1":881},"conf":0.9106,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":469,"y0":1192,"y1":1226},"conf":0.8993,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":662,"y0":640,"y1":675},"conf":0.8791,"label":"Title","label_id":0},{"bbox":{"x0":1054,"x1":1089,"y0":1041,"y1":1072},"conf":0.8436,"label":"Equation caption","label_id":9},{"bbox":{"x0":133,"x1":198,"y0":1010,"y1":1037},"conf":0.8337,"label":"Text","label_id":1},{"bbox":{"x0":1053,"x1":1089,"y0":789,"y1":822},"conf":0.8275,"label":"Equation caption","label_id":9},{"bbox":{"x0":1054,"x1":1089,"y0":929,"y1":961},"conf":0.8236,"label":"Equation caption","label_id":9},{"bbox":{"x0":598,"x1":624,"y0":1526,"y1":1554},"conf":0.4767,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[602,1528],[622,1528],[622,1551],[602,1551]],"score":0.8795},{"poly":[[138,1332],[519,1332],[519,1353],[138,1353]],"score":0.8924},{"poly":[[138,1307],[1084,1307],[1084,1328],[138,1328]],"score":0.8438},{"poly":[[138,1282],[1084,1282],[1084,1305],[138,1305]],"score":0.7341},{"poly":[[138,1259],[1084,1259],[1084,1280],[138,1280]],"score":0.8819},{"poly":[[136,1233],[1084,1234],[1084,1257],[136,1256]],"score":0.7469},{"poly":[[138,1196],[464,1196],[464,1219],[138,1219]],"score":0.7828},{"poly":[[138,1152],[1069,1152],[1069,1173],[138,1173]],"score":0.8533},{"poly":[[138,1127],[1084,1127],[1084,1148],[138,1148]],"score":0.8431},{"poly":[[138,1101],[1084,1102],[1084,1125],[138,1124]],"score":0.7615},{"poly":[[565,1058],[728,1058],[728,1081],[565,1081]],"score":0.8618},{"poly":[[480,1042],[564,1037],[566,1067],[482,1071]],"score":0.773},{"poly":[[1054,1040],[1089,1040],[1089,1071],[1054,1071]],"score":0.8027},{"poly":[[574,1028],[720,1028],[720,1056],[574,1056]],"score":0.86},{"poly":[[136,1011],[196,1011],[196,1036],[136,1036]],"score":0.8328},{"poly":[[386,980],[414,980],[414,993],[386,993]],"score":0.7983},{"poly":[[318,977],[349,977],[349,995],[318,995]],"score":0.7949},{"poly":[[348,962],[382,962],[382,992],[348,992]],"score":0.6581},{"poly":[[293,960],[316,960],[316,987],[293,987]],"score":0.8156},{"poly":[[422,937],[1031,937],[1031,987],[422,987]],"score":0.7606},{"poly":[[353,937],[376,937],[376,960],[353,960]],"score":0.7345},{"poly":[[294,937],[316,937],[316,960],[294,960]],"score":0.7382},{"poly":[[1054,931],[1086,931],[1086,962],[1054,962]],"score":0.79},{"poly":[[384,931],[416,931],[416,949],[384,949]],"score":0.8365},{"poly":[[319,931],[346,931],[346,954],[319,954]],"score":0.6917},{"poly":[[192,891],[524,899],[523,928],[191,919]],"score":0.7524},{"poly":[[140,851],[1041,851],[1041,874],[140,874]],"score":0.7964},{"poly":[[575,805],[695,805],[695,835],[575,835]],"score":0.7979},{"poly":[[1054,789],[1089,789],[1089,820],[1054,820]],"score":0.8696},{"poly":[[138,708],[1083,708],[1083,731],[138,731]],"score":0.7477},{"poly":[[138,681],[1084,683],[1084,706],[138,705]],"score":0.7696},{"poly":[[140,645],[654,645],[654,668],[140,668]],"score":0.8466},{"poly":[[138,579],[738,582],[738,606],[138,602]],"score":0.7242},{"poly":[[140,559],[1084,559],[1084,581],[140,581]],"score":0.834},{"poly":[[733,516],[966,516],[966,540],[733,540]],"score":0.7605},{"poly":[[264,516],[476,516],[476,538],[264,538]],"score":0.8987},{"poly":[[852,471],[896,476],[893,502],[849,497]],"score":0.8061},{"poly":[[374,471],[418,476],[415,501],[372,496]],"score":0.8054},{"poly":[[1036,460],[1071,460],[1071,478],[1036,478]],"score":0.9314},{"poly":[[975,460],[1013,460],[1013,478],[975,478]],"score":0.9213},{"poly":[[915,460],[951,460],[951,478],[915,478]],"score":0.938},{"poly":[[855,460],[891,460],[891,478],[855,478]],"score":0.9126},{"poly":[[795,460],[832,460],[832,478],[795,478]],"score":0.9154},{"poly":[[557,460],[592,460],[592,478],[557,478]],"score":0.8841},{"poly":[[497,460],[534,460],[534,478],[497,478]],"score":0.9225},{"poly":[[437,460],[472,460],[472,478],[437,478]],"score":0.9218},{"poly":[[376,460],[412,460],[412,478],[376,478]],"score":0.8988},{"poly":[[314,460],[351,460],[351,478],[314,478]],"score":0.8955},{"poly":[[256,460],[288,460],[288,478],[256,478]],"score":0.9135},{"poly":[[203,462],[218,462],[218,477],[203,477]],"score":0.7322},{"poly":[[737,459],[767,459],[767,478],[737,478]],"score":0.8726},{"poly":[[692,455],[705,469],[692,482],[679,469]],"score":0.7378},{"poly":[[168,442],[201,442],[201,462],[168,462]],"score":0.7961},{"poly":[[647,439],[675,439],[675,459],[647,459]],"score":0.8536},{"poly":[[168,414],[200,414],[200,434],[168,434]],"score":0.8643},{"poly":[[649,408],[674,408],[674,427],[649,427]],"score":0.7649},{"poly":[[170,384],[201,384],[201,406],[170,406]],"score":0.7401},{"poly":[[650,380],[675,380],[675,396],[650,396]],"score":0.814},{"poly":[[168,355],[200,355],[200,376],[168,376]],"score":0.7779},{"poly":[[647,346],[674,346],[674,366],[647,366]],"score":0.8538},{"poly":[[166,327],[198,327],[198,348],[166,348]],"score":0.751},{"poly":[[649,315],[674,315],[674,335],[649,335]],"score":0.6649},{"poly":[[958,298],[1051,286],[1056,320],[963,333]],"score":0.6783},{"poly":[[649,285],[674,285],[674,305],[649,305]],"score":0.7742},{"poly":[[164,275],[198,270],[200,290],[167,294]],"score":0.752},{"poly":[[168,244],[200,244],[200,264],[168,264]],"score":0.8077},{"poly":[[976,231],[1061,231],[1061,249],[976,249]],"score":0.7908},{"poly":[[238,229],[324,229],[324,248],[238,248]],"score":0.6956},{"poly":[[649,224],[674,224],[674,244],[649,244]],"score":0.8361},{"poly":[[973,216],[1059,216],[1059,233],[973,233]],"score":0.9489},{"poly":[[238,216],[321,216],[321,233],[238,233]],"score":0.9566},{"poly":[[168,214],[201,214],[201,234],[168,234]],"score":0.8442}],"page_no":2,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":599,"x1":624,"y0":1526,"y1":1553},"conf":0.3955,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":605,"x1":619,"y0":1531,"y1":1549},"font_size":0.0,"text":"4"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":524,"y0":196,"y1":230},"conf":0.8408,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":521,"y0":196,"y1":228},"font_size":0.0,"text":"2.4Rule-based Reward Modeling"}],"source":"layout det","text":"2.4Rule-based Reward Modeling"},{"bbox":{"x0":134,"x1":1089,"y0":236,"y1":292},"conf":0.9201,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":239,"y1":262},"font_size":0.0,"text":"The use of reward model usually sufers from the reward hacking problem [24–29]. Instead, we directly usef"},{"bbox":{"x0":140,"x1":980,"y0":264,"y1":285},"font_size":0.0,"text":"the fnal accuracy of a verifable task as the outcome reward, computed using the following rule:ii"}],"source":"layout det","text":"The use of reward model usually sufers from the reward hacking problem [24–29]. Instead, we directly usef the fnal accuracy of a verifable task as the outcome reward, computed using the following rule:ii"},{"bbox":{"x0":441,"x1":779,"y0":302,"y1":369},"conf":0.9404,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$R(\\hat{y},y)=\\begin{cases}1,&\\texttt{is\\_equivalent}(\\hat{y},y)\\\\ -1,&\\text{otherwise}\\end{cases}$$"},{"bbox":{"x0":1054,"x1":1089,"y0":321,"y1":353},"conf":0.8414,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1054,"x1":1088,"y0":322,"y1":351},"font_size":0.0,"text":"(7)"}],"source":"layout det","text":"(7)"},{"bbox":{"x0":133,"x1":1091,"y0":380,"y1":462},"conf":0.9458,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1083,"y0":384,"y1":408},"font_size":0.0,"text":"where $y$ is the ground-truth answer and $\\widehat{y}$  is the predicted answer. This is proved to be an efective approachf"},{"bbox":{"x0":140,"x1":1083,"y0":409,"y1":431},"font_size":0.0,"text":"to activating the base model’s reasoning capability, as shown in multiple domains such as automated theorem"},{"bbox":{"x0":138,"x1":863,"y0":432,"y1":455},"font_size":0.0,"text":"proving [30–33], computer programming [34–37], and mathematics competition [2]."}],"source":"layout det","text":"where $y$ is the ground-truth answer and $\\widehat{y}$  is the predicted answer. This is proved to be an efective approachf to activating the base model’s reasoning capability, as shown in multiple domains such as automated theorem proving [30–33], computer programming [34–37], and mathematics competition [2]."},{"bbox":{"x0":134,"x1":254,"y0":480,"y1":510},"conf":0.8658,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":249,"y0":483,"y1":507},"font_size":0.0,"text":"3DAPO"}],"source":"layout det","text":"3DAPO"},{"bbox":{"x0":133,"x1":1092,"y0":525,"y1":606},"conf":0.9443,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":528,"y1":553},"font_size":0.0,"text":"We propose the Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm. DAPO samples"},{"bbox":{"x0":135,"x1":1086,"y0":551,"y1":581},"font_size":0.0,"text":"a group of outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ for each question $4$  paired with the answer a, and optimizes the policy via the"},{"bbox":{"x0":138,"x1":311,"y0":576,"y1":601},"font_size":0.0,"text":"following objective:"}],"source":"layout det","text":"We propose the Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm. DAPO samples a group of outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ for each question $4$  paired with the answer a, and optimizes the policy via the following objective:"},{"bbox":{"x0":229,"x1":583,"y0":618,"y1":651},"conf":0.7138,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$"},{"bbox":{"x0":351,"x1":993,"y0":654,"y1":724},"conf":0.8748,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$"},{"bbox":{"x0":1053,"x1":1090,"y0":677,"y1":709},"conf":0.8368,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1056,"x1":1088,"y0":676,"y1":706},"font_size":0.0,"text":"(8)"}],"source":"layout det","text":"(8)"},{"bbox":{"x0":307,"x1":702,"y0":726,"y1":769},"conf":0.7774,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G,$$"},{"bbox":{"x0":133,"x1":199,"y0":781,"y1":808},"conf":0.8279,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":195,"y0":784,"y1":807},"font_size":0.0,"text":"where"}],"source":"layout det","text":"where"},{"bbox":{"x0":353,"x1":868,"y0":799,"y1":855},"conf":0.9377,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$r_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(o_{i,t}\\mid q,o_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t}\\mid q,o_{i,<t})},\\quad\\hat{A}_{i,t}=\\frac{R_{i}-\\mathrm{mean}(\\{R_{i}\\}_{i=1}^{G})}{\\mathrm{std}(\\{R_{i}\\}_{i=1}^{G})}.$$"},{"bbox":{"x0":1054,"x1":1089,"y0":812,"y1":843},"conf":0.8332,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1056,"x1":1088,"y0":813,"y1":843},"font_size":0.0,"text":"(9)"}],"source":"layout det","text":"(9)"},{"bbox":{"x0":132,"x1":1090,"y0":860,"y1":914},"conf":0.9371,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1083,"y0":863,"y1":884},"font_size":0.0,"text":"The full algorithm can be found in Algorithm 1. In this section, we will introduce the key techniques associated"},{"bbox":{"x0":141,"x1":249,"y0":888,"y1":906},"font_size":0.0,"text":"with DAPO."}],"source":"layout det","text":"The full algorithm can be found in Algorithm 1. In this section, we will introduce the key techniques associated with DAPO."},{"bbox":{"x0":133,"x1":515,"y0":928,"y1":963},"conf":0.922,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":509,"y0":932,"y1":955},"font_size":0.0,"text":"3.1Raise the Ceiling: Clip-Higher"}],"source":"layout det","text":"3.1Raise the Ceiling: Clip-Higher"},{"bbox":{"x0":133,"x1":1093,"y0":967,"y1":1070},"conf":0.9476,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":972,"y1":993},"font_size":0.0,"text":"In our initial experiments using naive PPO [21] or GRPO [38], we observed the entropy collapse phenomenon:"},{"bbox":{"x0":136,"x1":1084,"y0":993,"y1":1018},"font_size":0.0,"text":"the entropy of the policy decreases quickly as training progresses (Figure 2b). The sampled responses of"},{"bbox":{"x0":135,"x1":1086,"y0":1015,"y1":1044},"font_size":0.0,"text":"certain groups tend to be nearly identical. This indicates limited exploration and early deterministic policy,"},{"bbox":{"x0":136,"x1":467,"y0":1041,"y1":1066},"font_size":0.0,"text":"which can hinder the scaling process."}],"source":"layout det","text":"In our initial experiments using naive PPO [21] or GRPO [38], we observed the entropy collapse phenomenon:the entropy of the policy decreases quickly as training progresses (Figure 2b). The sampled responses of certain groups tend to be nearly identical. This indicates limited exploration and early deterministic policy,which can hinder the scaling process."},{"bbox":{"x0":133,"x1":1092,"y0":1074,"y1":1202},"conf":0.9642,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1079,"y1":1101},"font_size":0.0,"text":"We propose the Clip-Higher strategy to address this issue. Clipping over the importance sampling ratio is"},{"bbox":{"x0":138,"x1":1084,"y0":1104,"y1":1125},"font_size":0.0,"text":"introduced in Clipped Proximal Policy Optimization (PPO-Clip) [21] to restrict the trust region and enhance"},{"bbox":{"x0":136,"x1":1084,"y0":1125,"y1":1150},"font_size":0.0,"text":"the stability of RL. We identify that the upper clip can restrict the exploration of the policy, where making"},{"bbox":{"x0":138,"x1":1084,"y0":1150,"y1":1172},"font_size":0.0,"text":"an ‘exploitation’ token more probable is much easier yet the probability of an unlikely ‘exploration’ token is"},{"bbox":{"x0":140,"x1":447,"y0":1175,"y1":1196},"font_size":0.0,"text":"too tightly bounded to be uplifted."}],"source":"layout det","text":"We propose the Clip-Higher strategy to address this issue. Clipping over the importance sampling ratio is introduced in Clipped Proximal Policy Optimization (PPO-Clip) [21] to restrict the trust region and enhance the stability of RL. We identify that the upper clip can restrict the exploration of the policy, where making an ‘exploitation’ token more probable is much easier yet the probability of an unlikely ‘exploration’ token is too tightly bounded to be uplifted."},{"bbox":{"x0":132,"x1":1092,"y0":1205,"y1":1430},"conf":0.9763,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1208,"y1":1234},"font_size":0.0,"text":"Concretely, when $\\varepsilon=0.2$ (the default value of most algorithms) and $\\hat{A}_{i , t}>0$ (the system tries to increase"},{"bbox":{"x0":138,"x1":1083,"y0":1234,"y1":1259},"font_size":0.0,"text":"the probability), consider two actions with probabilities $\\pi_{\\theta_{\\text{old}}}(o_{i} | q)=0.01$  and 0.9. The upper bounds"},{"bbox":{"x0":138,"x1":1084,"y0":1259,"y1":1283},"font_size":0.0,"text":"of the increased probabilities $\\pi_{\\boldsymbol{\\theta}}\\big(o_{\\boldsymbol{i}}\\ \\big|\\ q\\big)$  are 0.012 and 1.08, respectively $(\\pi_{\\theta_{\\text{old}}}\\cdot(1+\\epsilon)).$  This implies that"},{"bbox":{"x0":138,"x1":1084,"y0":1282,"y1":1307},"font_size":0.0,"text":"‘exploitation’ tokens with a higher probability $\\left(\\text{\\textit{e.g.}, 0.9}\\right)$  are not constrained to get even extremely larger"},{"bbox":{"x0":138,"x1":1084,"y0":1305,"y1":1328},"font_size":0.0,"text":"probabilities like 0.999. Conversely, for low-probability ‘exploration’ tokens, achieving a non-trivial increase"},{"bbox":{"x0":138,"x1":1084,"y0":1330,"y1":1353},"font_size":0.0,"text":"in probability is considerably more challenging. Empirically, we also observe that the mean probability of"},{"bbox":{"x0":140,"x1":1084,"y0":1353,"y1":1378},"font_size":0.0,"text":"up-clipped tokens is low: $\\pi_{\\theta}(o_{i}\\mid q)<0.2$ (Figure 3a). This fnding supports our intuition that the upperi"},{"bbox":{"x0":136,"x1":1083,"y0":1376,"y1":1401},"font_size":0.0,"text":"clipping threshold indeed restricts the probability increase of low-probability ‘exploration’ tokens, thereby"},{"bbox":{"x0":138,"x1":615,"y0":1401,"y1":1426},"font_size":0.0,"text":"potentially constraining the exploration of the system."}],"source":"layout det","text":"Concretely, when $\\varepsilon=0.2$ (the default value of most algorithms) and $\\hat{A}_{i , t}>0$ (the system tries to increase the probability), consider two actions with probabilities $\\pi_{\\theta_{\\text{old}}}(o_{i} | q)=0.01$  and 0.9. The upper bounds of the increased probabilities $\\pi_{\\boldsymbol{\\theta}}\\big(o_{\\boldsymbol{i}}\\ \\big|\\ q\\big)$  are 0.012 and 1.08, respectively $(\\pi_{\\theta_{\\text{old}}}\\cdot(1+\\epsilon)).$  This implies that‘exploitation’ tokens with a higher probability $\\left(\\text{\\textit{e.g.}, 0.9}\\right)$  are not constrained to get even extremely larger probabilities like 0.999. Conversely, for low-probability ‘exploration’ tokens, achieving a non-trivial increase in probability is considerably more challenging. Empirically, we also observe that the mean probability of up-clipped tokens is low: $\\pi_{\\theta}(o_{i}\\mid q)<0.2$ (Figure 3a). This fnding supports our intuition that the upperi clipping threshold indeed restricts the probability increase of low-probability ‘exploration’ tokens, thereby potentially constraining the exploration of the system."}],"formula_dets":[{"bbox":{"x0":441,"x1":779,"y0":302,"y1":369},"conf":0.9404,"label":"print_isolated","label_id":1},{"bbox":{"x0":353,"x1":868,"y0":799,"y1":855},"conf":0.9377,"label":"print_isolated","label_id":1},{"bbox":{"x0":314,"x1":377,"y0":555,"y1":579},"conf":0.8998,"label":"print_embedding","label_id":0},{"bbox":{"x0":370,"x1":508,"y0":1353,"y1":1378},"conf":0.8909,"label":"print_embedding","label_id":0},{"bbox":{"x0":406,"x1":487,"y0":1259,"y1":1282},"conf":0.8908,"label":"print_embedding","label_id":0},{"bbox":{"x0":784,"x1":917,"y0":1259,"y1":1283},"conf":0.888,"label":"print_embedding","label_id":0},{"bbox":{"x0":651,"x1":822,"y0":1236,"y1":1259},"conf":0.8868,"label":"print_embedding","label_id":0},{"bbox":{"x0":753,"x1":825,"y0":1208,"y1":1234},"conf":0.8867,"label":"print_embedding","label_id":0},{"bbox":{"x0":351,"x1":993,"y0":654,"y1":724},"conf":0.8748,"label":"print_isolated","label_id":1},{"bbox":{"x0":557,"x1":643,"y0":1283,"y1":1307},"conf":0.8672,"label":"print_embedding","label_id":0},{"bbox":{"x0":299,"x1":364,"y0":1210,"y1":1231},"conf":0.8603,"label":"print_embedding","label_id":0},{"bbox":{"x0":488,"x1":500,"y0":388,"y1":408},"conf":0.8189,"label":"print_embedding","label_id":0},{"bbox":{"x0":307,"x1":702,"y0":726,"y1":769},"conf":0.7774,"label":"print_isolated","label_id":1},{"bbox":{"x0":229,"x1":583,"y0":618,"y1":651},"conf":0.7138,"label":"print_isolated","label_id":1},{"bbox":{"x0":541,"x1":552,"y0":564,"y1":577},"conf":0.7102,"label":"print_embedding","label_id":0},{"bbox":{"x0":196,"x1":208,"y0":393,"y1":407},"conf":0.6749,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":132,"x1":1092,"y0":1205,"y1":1430},"conf":0.9763,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1092,"y0":1074,"y1":1202},"conf":0.9642,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1093,"y0":967,"y1":1070},"conf":0.9476,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1091,"y0":380,"y1":462},"conf":0.9458,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1092,"y0":525,"y1":606},"conf":0.9443,"label":"Text","label_id":1},{"bbox":{"x0":348,"x1":873,"y0":793,"y1":859},"conf":0.939,"label":"Equation","label_id":8},{"bbox":{"x0":132,"x1":1090,"y0":860,"y1":914},"conf":0.9371,"label":"Text","label_id":1},{"bbox":{"x0":437,"x1":784,"y0":301,"y1":372},"conf":0.9229,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":515,"y0":928,"y1":963},"conf":0.922,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":1089,"y0":236,"y1":292},"conf":0.9201,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":998,"y0":614,"y1":775},"conf":0.8678,"label":"Equation","label_id":8},{"bbox":{"x0":134,"x1":254,"y0":480,"y1":510},"conf":0.8658,"label":"Title","label_id":0},{"bbox":{"x0":1054,"x1":1089,"y0":321,"y1":353},"conf":0.8414,"label":"Equation caption","label_id":9},{"bbox":{"x0":134,"x1":524,"y0":196,"y1":230},"conf":0.8408,"label":"Title","label_id":0},{"bbox":{"x0":1053,"x1":1090,"y0":677,"y1":709},"conf":0.8368,"label":"Equation caption","label_id":9},{"bbox":{"x0":1054,"x1":1089,"y0":812,"y1":843},"conf":0.8332,"label":"Equation caption","label_id":9},{"bbox":{"x0":224,"x1":997,"y0":614,"y1":745},"conf":0.8308,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":199,"y0":781,"y1":808},"conf":0.8279,"label":"Text","label_id":1},{"bbox":{"x0":599,"x1":624,"y0":1526,"y1":1553},"conf":0.3955,"label":"Abandon","label_id":2},{"bbox":{"x0":304,"x1":703,"y0":723,"y1":772},"conf":0.309,"label":"Equation","label_id":8}],"ocr_all":false,"ocr_dets":[{"poly":[[605,1531],[619,1531],[619,1549],[605,1549]],"score":0.9205},{"poly":[[138,1403],[615,1401],[615,1424],[138,1426]],"score":0.7101},{"poly":[[136,1376],[1083,1378],[1083,1401],[136,1399]],"score":0.7358},{"poly":[[140,1355],[1084,1355],[1084,1376],[140,1376]],"score":0.8357},{"poly":[[138,1330],[1084,1330],[1084,1353],[138,1353]],"score":0.7128},{"poly":[[138,1305],[1084,1305],[1084,1328],[138,1328]],"score":0.7196},{"poly":[[138,1282],[1084,1282],[1084,1305],[138,1305]],"score":0.7144},{"poly":[[138,1259],[1084,1259],[1084,1282],[138,1282]],"score":0.7303},{"poly":[[138,1234],[1083,1234],[1083,1257],[138,1257]],"score":0.7171},{"poly":[[140,1209],[1084,1209],[1084,1233],[140,1233]],"score":0.7101},{"poly":[[140,1175],[447,1175],[447,1196],[140,1196]],"score":0.8346},{"poly":[[138,1150],[1084,1150],[1084,1172],[138,1172]],"score":0.8232},{"poly":[[136,1125],[1084,1127],[1084,1150],[136,1148]],"score":0.7531},{"poly":[[138,1104],[1084,1104],[1084,1125],[138,1125]],"score":0.8295},{"poly":[[140,1079],[1084,1079],[1084,1101],[140,1101]],"score":0.852},{"poly":[[136,1041],[467,1043],[467,1066],[136,1064]],"score":0.7126},{"poly":[[135,1015],[1086,1016],[1086,1044],[135,1043]],"score":0.6498},{"poly":[[136,995],[1084,993],[1084,1016],[136,1018]],"score":0.7527},{"poly":[[138,972],[1084,972],[1084,993],[138,993]],"score":0.8421},{"poly":[[138,932],[509,932],[509,955],[138,955]],"score":0.7674},{"poly":[[141,888],[249,888],[249,906],[141,906]],"score":0.913},{"poly":[[140,863],[1083,863],[1083,884],[140,884]],"score":0.8798},{"poly":[[429,828],[600,825],[601,853],[429,857]],"score":0.7308},{"poly":[[710,818],[832,824],[830,860],[708,854]],"score":0.6588},{"poly":[[351,815],[431,815],[431,840],[351,840]],"score":0.8018},{"poly":[[1056,813],[1088,813],[1088,843],[1056,843]],"score":0.8913},{"poly":[[618,807],[681,814],[678,846],[614,839]],"score":0.7134},{"poly":[[439,804],[585,804],[585,825],[439,825]],"score":0.9828},{"poly":[[677,797],[866,797],[866,830],[677,830]],"score":0.7083},{"poly":[[138,784],[195,784],[195,807],[138,807]],"score":0.8662},{"poly":[[306,729],[700,729],[700,762],[306,762]],"score":0.757},{"poly":[[1056,676],[1088,676],[1088,706],[1056,706]],"score":0.8238},{"poly":[[452,673],[526,673],[526,703],[452,703]],"score":0.6609},{"poly":[[506,660],[990,665],[989,715],[505,709]],"score":0.6285},{"poly":[[404,665],[419,665],[419,683],[404,683]],"score":0.9077},{"poly":[[457,653],[519,653],[519,678],[457,678]],"score":0.8509},{"poly":[[228,617],[571,624],[570,652],[228,645]],"score":0.725},{"poly":[[138,576],[311,578],[311,601],[138,599]],"score":0.8387},{"poly":[[135,553],[1086,551],[1086,579],[135,581]],"score":0.6768},{"poly":[[138,528],[1084,530],[1084,553],[138,551]],"score":0.7645},{"poly":[[171,483],[249,483],[249,507],[171,507]],"score":0.9196},{"poly":[[138,485],[166,485],[166,505],[138,505]],"score":0.7866},{"poly":[[138,432],[863,432],[863,455],[138,455]],"score":0.7883},{"poly":[[140,409],[1083,409],[1083,431],[140,431]],"score":0.8312},{"poly":[[140,384],[1083,384],[1083,406],[140,406]],"score":0.8795},{"poly":[[587,342],[679,342],[679,360],[587,360]],"score":0.944},{"poly":[[544,338],[597,338],[597,361],[544,361]],"score":0.8389},{"poly":[[439,322],[529,322],[529,350],[439,350]],"score":0.798},{"poly":[[1054,322],[1088,322],[1088,351],[1054,351]],"score":0.8768},{"poly":[[592,308],[778,310],[778,335],[592,333]],"score":0.7349},{"poly":[[545,294],[573,321],[551,342],[524,315]],"score":0.6823},{"poly":[[140,264],[980,264],[980,285],[140,285]],"score":0.8486},{"poly":[[140,239],[1084,239],[1084,262],[140,262]],"score":0.7345},{"poly":[[137,196],[521,200],[520,228],[136,224]],"score":0.7529}],"page_no":3,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":124,"x1":1065,"y0":1524,"y1":1583},"conf":0.2021,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1530,"y1":1549},"font_size":0.0,"text":"5"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":143,"x1":1079,"y0":200,"y1":546},"conf":0.9793,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![7258ec59c0a6e5cc8bbf95ee06d45020](imgs/7258ec59c0a6e5cc8bbf95ee06d45020.jpg)"},{"bbox":{"x0":228,"x1":993,"y0":556,"y1":588},"conf":0.9087,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":236,"x1":985,"y0":559,"y1":581},"font_size":0.0,"text":"Figure 3 The mean up-clipped probability as well as the ratio of prompts with accuracy=1."}],"source":"layout det","text":"Figure 3 The mean up-clipped probability as well as the ratio of prompts with accuracy=1."},{"bbox":{"x0":132,"x1":1088,"y0":619,"y1":676},"conf":0.9496,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1086,"y0":620,"y1":652},"font_size":0.0,"text":"Adhering to the Clip-Higher strategy, we decouple the lower and higher clipping range as $\\varepsilon_{\\text{low}}$  and $\\varepsilon_{\\text{high}},$  as"},{"bbox":{"x0":138,"x1":384,"y0":647,"y1":672},"font_size":0.0,"text":"highlighted in Equation 10:"}],"source":"layout det","text":"Adhering to the Clip-Higher strategy, we decouple the lower and higher clipping range as $\\varepsilon_{\\text{low}}$  and $\\varepsilon_{\\text{high}},$  as highlighted in Equation 10:"},{"bbox":{"x0":229,"x1":583,"y0":690,"y1":723},"conf":0.7235,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$"},{"bbox":{"x0":351,"x1":992,"y0":726,"y1":794},"conf":0.8688,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$"},{"bbox":{"x0":1043,"x1":1090,"y0":748,"y1":781},"conf":0.8196,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1043,"x1":1088,"y0":746,"y1":779},"font_size":0.0,"text":"(10)"}],"source":"layout det","text":"(10)"},{"bbox":{"x0":305,"x1":701,"y0":797,"y1":841},"conf":0.8196,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$"},{"bbox":{"x0":131,"x1":1092,"y0":849,"y1":955},"conf":0.9546,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":851,"y1":879},"font_size":0.0,"text":"We increase the value of $\\varepsilon_{\\mathbf{high}}$  to leave more room for the increase of low-probability tokens. As shown in"},{"bbox":{"x0":140,"x1":1083,"y0":876,"y1":899},"font_size":0.0,"text":"Figure 2, this adjustment efectively enhances the policy’s entropy and facilitates the generation of moref"},{"bbox":{"x0":138,"x1":1086,"y0":903,"y1":926},"font_size":0.0,"text":"diverse samples. We keep $\\varepsilon_{\\text{low}}$ as it is, because increasing it will suppress the probability of these tokens to 0,"},{"bbox":{"x0":136,"x1":549,"y0":924,"y1":949},"font_size":0.0,"text":"resulting in the collapse of the sampling space."}],"source":"layout det","text":"We increase the value of $\\varepsilon_{\\mathbf{high}}$  to leave more room for the increase of low-probability tokens. As shown in Figure 2, this adjustment efectively enhances the policy’s entropy and facilitates the generation of moref diverse samples. We keep $\\varepsilon_{\\text{low}}$ as it is, because increasing it will suppress the probability of these tokens to 0,resulting in the collapse of the sampling space."},{"bbox":{"x0":134,"x1":644,"y0":967,"y1":1001},"conf":0.9063,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":135,"x1":640,"y0":967,"y1":998},"font_size":0.0,"text":"3.2The More the Merrier: Dynamic Sampling"}],"source":"layout det","text":"3.2The More the Merrier: Dynamic Sampling"},{"bbox":{"x0":133,"x1":1092,"y0":1006,"y1":1183},"conf":0.9761,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1083,"y0":1008,"y1":1032},"font_size":0.0,"text":"Existing RL algorithm sufers from the gradient-decreasing problem when some prompts have accuracy equalf"},{"bbox":{"x0":136,"x1":1086,"y0":1031,"y1":1059},"font_size":0.0,"text":"to 1. For example for GRPO, if all outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ of a particular prompt are correct and receive the same"},{"bbox":{"x0":136,"x1":1084,"y0":1056,"y1":1081},"font_size":0.0,"text":"reward, the resulting advantage for this group is zero. A zero advantage results in zero policy gradients,"},{"bbox":{"x0":136,"x1":1084,"y0":1082,"y1":1104},"font_size":0.0,"text":"shrinking the magnitude and increasing the noise sensitivity of the batch gradient, thereby degrading sample"},{"bbox":{"x0":136,"x1":1084,"y0":1106,"y1":1129},"font_size":0.0,"text":"efciency. Empirically, the number of samples with accuracy equal to 1 continues to increase, as shown infi"},{"bbox":{"x0":138,"x1":1084,"y0":1127,"y1":1152},"font_size":0.0,"text":"Figure 3b. This means that the efective number of prompts in each batch keeps decreasing, which can leadf"},{"bbox":{"x0":136,"x1":865,"y0":1152,"y1":1178},"font_size":0.0,"text":"to larger variance in gradient and dampens the gradient signals for model training."}],"source":"layout det","text":"Existing RL algorithm sufers from the gradient-decreasing problem when some prompts have accuracy equalf to 1. For example for GRPO, if all outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ of a particular prompt are correct and receive the same reward, the resulting advantage for this group is zero. A zero advantage results in zero policy gradients,shrinking the magnitude and increasing the noise sensitivity of the batch gradient, thereby degrading sample efciency. Empirically, the number of samples with accuracy equal to 1 continues to increase, as shown infi Figure 3b. This means that the efective number of prompts in each batch keeps decreasing, which can leadf to larger variance in gradient and dampens the gradient signals for model training."},{"bbox":{"x0":133,"x1":1091,"y0":1187,"y1":1290},"conf":0.959,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1084,"y0":1186,"y1":1211},"font_size":0.0,"text":"To this end, we propose to over-sample and filter out prompts with the accuracy equal to 1 and 0 as illustrated"},{"bbox":{"x0":138,"x1":1084,"y0":1213,"y1":1235},"font_size":0.0,"text":"in Equation 11, leaving all prompts in the batch with efective gradients and keeping a consistent number off"},{"bbox":{"x0":136,"x1":1084,"y0":1236,"y1":1261},"font_size":0.0,"text":"prompts. The sampling cost for each batch is dynamic. Before training, we keep sampling until the batch is"},{"bbox":{"x0":138,"x1":654,"y0":1261,"y1":1283},"font_size":0.0,"text":"fully flled with samples whose accuracy is neither 0 nor 1.i"}],"source":"layout det","text":"To this end, we propose to over-sample and filter out prompts with the accuracy equal to 1 and 0 as illustrated in Equation 11, leaving all prompts in the batch with efective gradients and keeping a consistent number off prompts. The sampling cost for each batch is dynamic. Before training, we keep sampling until the batch is fully flled with samples whose accuracy is neither 0 nor 1.i"},{"bbox":{"x0":229,"x1":583,"y0":1314,"y1":1347},"conf":0.6829,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$"},{"bbox":{"x0":350,"x1":992,"y0":1350,"y1":1420},"conf":0.8537,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$"},{"bbox":{"x0":1043,"x1":1090,"y0":1372,"y1":1405},"conf":0.837,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1044,"x1":1084,"y0":1373,"y1":1401},"font_size":0.0,"text":"(11)"}],"source":"layout det","text":"(11)"},{"bbox":{"x0":306,"x1":704,"y0":1422,"y1":1466},"conf":0.8135,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$"}],"formula_dets":[{"bbox":{"x0":526,"x1":589,"y0":1035,"y1":1059},"conf":0.9046,"label":"print_embedding","label_id":0},{"bbox":{"x0":351,"x1":992,"y0":726,"y1":794},"conf":0.8688,"label":"print_isolated","label_id":1},{"bbox":{"x0":363,"x1":404,"y0":860,"y1":879},"conf":0.8582,"label":"print_embedding","label_id":0},{"bbox":{"x0":350,"x1":992,"y0":1350,"y1":1420},"conf":0.8537,"label":"print_isolated","label_id":1},{"bbox":{"x0":365,"x1":400,"y0":910,"y1":925},"conf":0.8245,"label":"print_embedding","label_id":0},{"bbox":{"x0":930,"x1":965,"y0":633,"y1":648},"conf":0.8242,"label":"print_embedding","label_id":0},{"bbox":{"x0":305,"x1":701,"y0":797,"y1":841},"conf":0.8196,"label":"print_isolated","label_id":1},{"bbox":{"x0":306,"x1":704,"y0":1422,"y1":1466},"conf":0.8135,"label":"print_isolated","label_id":1},{"bbox":{"x0":1009,"x1":1056,"y0":630,"y1":650},"conf":0.7663,"label":"print_embedding","label_id":0},{"bbox":{"x0":229,"x1":583,"y0":690,"y1":723},"conf":0.7235,"label":"print_isolated","label_id":1},{"bbox":{"x0":229,"x1":583,"y0":1314,"y1":1347},"conf":0.6829,"label":"print_isolated","label_id":1}],"height":792,"layout_dets":[{"bbox":{"x0":143,"x1":1079,"y0":200,"y1":546},"conf":0.9793,"label":"Figure","label_id":3},{"bbox":{"x0":133,"x1":1092,"y0":1006,"y1":1183},"conf":0.9761,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":998,"y0":1309,"y1":1468},"conf":0.964,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1091,"y0":1187,"y1":1290},"conf":0.959,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":997,"y0":686,"y1":845},"conf":0.9549,"label":"Equation","label_id":8},{"bbox":{"x0":131,"x1":1092,"y0":849,"y1":955},"conf":0.9546,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1088,"y0":619,"y1":676},"conf":0.9496,"label":"Text","label_id":1},{"bbox":{"x0":228,"x1":993,"y0":556,"y1":588},"conf":0.9087,"label":"Figure caption","label_id":4},{"bbox":{"x0":134,"x1":644,"y0":967,"y1":1001},"conf":0.9063,"label":"Title","label_id":0},{"bbox":{"x0":1043,"x1":1090,"y0":1372,"y1":1405},"conf":0.837,"label":"Equation caption","label_id":9},{"bbox":{"x0":1043,"x1":1090,"y0":748,"y1":781},"conf":0.8196,"label":"Equation caption","label_id":9},{"bbox":{"x0":598,"x1":624,"y0":1526,"y1":1554},"conf":0.2703,"label":"Abandon","label_id":2},{"bbox":{"x0":303,"x1":701,"y0":795,"y1":843},"conf":0.2422,"label":"Equation","label_id":8},{"bbox":{"x0":124,"x1":1065,"y0":1524,"y1":1583},"conf":0.2021,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1530],[619,1530],[619,1549],[604,1549]],"score":0.8639},{"poly":[[306,1422],[697,1422],[697,1457],[306,1457]],"score":0.71},{"poly":[[1044,1373],[1084,1373],[1084,1401],[1044,1401]],"score":0.9169},{"poly":[[506,1358],[996,1358],[996,1409],[506,1409]],"score":0.6664},{"poly":[[401,1358],[421,1358],[421,1381],[401,1381]],"score":0.7216},{"poly":[[457,1348],[519,1348],[519,1371],[457,1371]],"score":0.9349},{"poly":[[228,1312],[572,1320],[572,1348],[228,1340]],"score":0.7254},{"poly":[[138,1261],[654,1261],[654,1282],[138,1282]],"score":0.8395},{"poly":[[136,1238],[1084,1236],[1084,1259],[136,1261]],"score":0.7618},{"poly":[[138,1213],[1084,1213],[1084,1234],[138,1234]],"score":0.85},{"poly":[[136,1186],[1084,1188],[1084,1211],[136,1209]],"score":0.7072},{"poly":[[136,1152],[865,1155],[865,1178],[136,1175]],"score":0.7078},{"poly":[[138,1127],[1084,1129],[1084,1152],[138,1150]],"score":0.7744},{"poly":[[136,1106],[1084,1106],[1084,1129],[136,1129]],"score":0.7119},{"poly":[[136,1082],[1084,1082],[1084,1104],[136,1104]],"score":0.7836},{"poly":[[136,1056],[1084,1058],[1084,1081],[136,1079]],"score":0.7193},{"poly":[[136,1031],[1086,1031],[1086,1059],[136,1059]],"score":0.6371},{"poly":[[140,1008],[1083,1008],[1083,1031],[140,1031]],"score":0.7333},{"poly":[[135,967],[640,970],[640,998],[135,995]],"score":0.7273},{"poly":[[136,924],[549,926],[549,949],[136,947]],"score":0.7559},{"poly":[[138,903],[1086,903],[1086,926],[138,926]],"score":0.7709},{"poly":[[140,876],[1083,876],[1083,899],[140,899]],"score":0.7317},{"poly":[[138,851],[1084,853],[1084,876],[138,874]],"score":0.7644},{"poly":[[306,799],[698,799],[698,832],[306,832]],"score":0.7818},{"poly":[[454,774],[516,774],[516,792],[454,792]],"score":0.6903},{"poly":[[363,754],[462,754],[462,797],[363,797]],"score":0.6298},{"poly":[[1043,746],[1088,746],[1088,779],[1043,779]],"score":0.938},{"poly":[[452,742],[526,742],[526,774],[452,774]],"score":0.7231},{"poly":[[507,733],[994,733],[994,784],[507,784]],"score":0.6169},{"poly":[[404,736],[421,736],[421,754],[404,754]],"score":0.8155},{"poly":[[457,724],[519,724],[519,747],[457,747]],"score":0.8192},{"poly":[[227,686],[572,695],[572,723],[226,714]],"score":0.7004},{"poly":[[138,648],[384,647],[384,670],[138,672]],"score":0.7576},{"poly":[[136,620],[1086,624],[1086,652],[136,648]],"score":0.7109},{"poly":[[236,559],[985,559],[985,581],[236,581]],"score":0.883},{"poly":[[632,516],[1068,516],[1068,540],[632,540]],"score":0.7338},{"poly":[[235,515],[506,516],[505,540],[234,538]],"score":0.8143},{"poly":[[848,471],[895,476],[893,501],[846,496]],"score":0.7984},{"poly":[[374,471],[420,476],[417,501],[372,496]],"score":0.8256},{"poly":[[993,460],[1028,460],[1028,478],[993,478]],"score":0.9594},{"poly":[[915,460],[950,460],[950,478],[915,478]],"score":0.955},{"poly":[[755,460],[792,460],[792,478],[755,478]],"score":0.9314},{"poly":[[685,462],[700,462],[700,477],[685,477]],"score":0.7819},{"poly":[[557,460],[592,460],[592,478],[557,478]],"score":0.8873},{"poly":[[497,460],[534,460],[534,478],[497,478]],"score":0.9471},{"poly":[[437,460],[474,460],[474,478],[437,478]],"score":0.9183},{"poly":[[378,460],[414,460],[414,478],[378,478]],"score":0.8927},{"poly":[[319,460],[354,460],[354,478],[319,478]],"score":0.9041},{"poly":[[261,460],[291,460],[291,478],[261,478]],"score":0.9095},{"poly":[[210,462],[225,462],[225,477],[210,477]],"score":0.9039},{"poly":[[835,459],[870,459],[870,478],[835,478]],"score":0.7899},{"poly":[[166,411],[200,411],[200,431],[166,431]],"score":0.7081},{"poly":[[168,371],[198,371],[198,391],[168,391]],"score":0.829},{"poly":[[647,366],[674,366],[674,386],[647,386]],"score":0.7551},{"poly":[[728,353],[740,353],[740,365],[728,365]],"score":0.8016},{"poly":[[170,333],[200,333],[200,353],[170,353]],"score":0.7842},{"poly":[[649,333],[674,333],[674,350],[649,350]],"score":0.8213},{"poly":[[649,299],[674,299],[674,315],[649,315]],"score":0.6828},{"poly":[[170,294],[198,294],[198,314],[170,314]],"score":0.8642},{"poly":[[168,256],[200,256],[200,274],[168,274]],"score":0.8315},{"poly":[[647,226],[675,226],[675,246],[647,246]],"score":0.7406},{"poly":[[170,214],[200,214],[200,234],[170,234]],"score":0.7747}],"page_no":4,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":599,"x1":624,"y0":1526,"y1":1554},"conf":0.3551,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":602,"x1":622,"y0":1528,"y1":1551},"font_size":0.0,"text":"6"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":143,"x1":1079,"y0":199,"y1":506},"conf":0.9644,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![17fe2f3cbdcf1d4e2666692383dc8fc1](imgs/17fe2f3cbdcf1d4e2666692383dc8fc1.jpg)"},{"bbox":{"x0":144,"x1":1082,"y0":514,"y1":547},"conf":0.8134,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":150,"x1":1076,"y0":515,"y1":540},"font_size":0.0,"text":"(a) Entropy of actor model’s generation probabilities.(b) Average length of actor model-generated responses"}],"source":"layout det","text":"(a) Entropy of actor model’s generation probabilities.(b) Average length of actor model-generated responses"},{"bbox":{"x0":141,"x1":1083,"y0":555,"y1":588},"conf":0.9235,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":145,"x1":1076,"y0":559,"y1":581},"font_size":0.0,"text":"Figure 4 The entropy of the probability distribution of the actor model, as well as the changes in response length."}],"source":"layout det","text":"Figure 4 The entropy of the probability distribution of the actor model, as well as the changes in response length."},{"bbox":{"x0":133,"x1":1090,"y0":618,"y1":724},"conf":0.9679,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":624,"y1":645},"font_size":0.0,"text":"Note that this strategy does not necessarily impede training efciency, because the generation time is typicallyfi"},{"bbox":{"x0":138,"x1":1084,"y0":645,"y1":670},"font_size":0.0,"text":"dominated by the generation of long-tail samples if the RL system is synchronized and the generation stage is"},{"bbox":{"x0":138,"x1":1084,"y0":672,"y1":693},"font_size":0.0,"text":"not pipelined. Besides, we fnd that with dynamic sampling the experiment achieves the same performancei"},{"bbox":{"x0":138,"x1":384,"y0":695,"y1":718},"font_size":0.0,"text":"faster as shown in Figure 6."}],"source":"layout det","text":"Note that this strategy does not necessarily impede training efciency, because the generation time is typicallyfi dominated by the generation of long-tail samples if the RL system is synchronized and the generation stage is not pipelined. Besides, we fnd that with dynamic sampling the experiment achieves the same performancei faster as shown in Figure 6."},{"bbox":{"x0":133,"x1":755,"y0":736,"y1":770},"conf":0.9002,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":750,"y0":739,"y1":762},"font_size":0.0,"text":"3.3Rebalancing Act: Token-Level Policy Gradient Loss"}],"source":"layout det","text":"3.3Rebalancing Act: Token-Level Policy Gradient Loss"},{"bbox":{"x0":133,"x1":1091,"y0":775,"y1":877},"conf":0.959,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":779,"y1":802},"font_size":0.0,"text":"The original GRPO algorithm employs a sample-level loss calculation, which involves frst averaging the lossesi"},{"bbox":{"x0":140,"x1":1084,"y0":804,"y1":825},"font_size":0.0,"text":"by token within each sample and then aggregating the losses across samples. In this approach, each sample is"},{"bbox":{"x0":138,"x1":1084,"y0":827,"y1":848},"font_size":0.0,"text":"assigned an equal weight in the fnal loss computation. However, we fnd that this method of loss reductionii"},{"bbox":{"x0":138,"x1":755,"y0":850,"y1":871},"font_size":0.0,"text":"introduces several challenges in the context of long-CoT RL scenarios."}],"source":"layout det","text":"The original GRPO algorithm employs a sample-level loss calculation, which involves frst averaging the lossesi by token within each sample and then aggregating the losses across samples. In this approach, each sample is assigned an equal weight in the fnal loss computation. However, we fnd that this method of loss reductionii introduces several challenges in the context of long-CoT RL scenarios."},{"bbox":{"x0":133,"x1":1091,"y0":881,"y1":1059},"conf":0.9744,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":883,"y1":908},"font_size":0.0,"text":"Since all samples are assigned the same weight in the loss calculation, tokens within longer responses (which"},{"bbox":{"x0":138,"x1":1084,"y0":911,"y1":932},"font_size":0.0,"text":"contain more tokens) may have a disproportionately lower contribution to the overall loss, which can lead to"},{"bbox":{"x0":138,"x1":1084,"y0":934,"y1":957},"font_size":0.0,"text":"two adverse efects. First, for high-quality long samples, this efect can impede the model’s ability to learnff"},{"bbox":{"x0":138,"x1":1084,"y0":959,"y1":980},"font_size":0.0,"text":"reasoning-relevant patterns within them. Second, we observe that excessively long samples often exhibit"},{"bbox":{"x0":138,"x1":1084,"y0":982,"y1":1003},"font_size":0.0,"text":"low-quality patterns such as gibberish and repetitive words. Thus, sample-level loss calculation, due to its"},{"bbox":{"x0":138,"x1":1084,"y0":1006,"y1":1028},"font_size":0.0,"text":"inability to efectively penalize those undesirable patterns in long samples, leads to an unhealthy increase inf"},{"bbox":{"x0":138,"x1":725,"y0":1030,"y1":1053},"font_size":0.0,"text":"entropy and response length, as shown in Figure 4a and Figure 4b."}],"source":"layout det","text":"Since all samples are assigned the same weight in the loss calculation, tokens within longer responses (which contain more tokens) may have a disproportionately lower contribution to the overall loss, which can lead to two adverse efects. First, for high-quality long samples, this efect can impede the model’s ability to learnff reasoning-relevant patterns within them. Second, we observe that excessively long samples often exhibit low-quality patterns such as gibberish and repetitive words. Thus, sample-level loss calculation, due to its inability to efectively penalize those undesirable patterns in long samples, leads to an unhealthy increase inf entropy and response length, as shown in Figure 4a and Figure 4b."},{"bbox":{"x0":132,"x1":1094,"y0":1061,"y1":1096},"conf":0.8063,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":1063,"y1":1087},"font_size":0.0,"text":"We introduce a Token-level Policy Gradient Loss in the long-CoT RL scenario to address the above limitations:"}],"source":"layout det","text":"We introduce a Token-level Policy Gradient Loss in the long-CoT RL scenario to address the above limitations:"},{"bbox":{"x0":229,"x1":581,"y0":1104,"y1":1136},"conf":0.7135,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$"},{"bbox":{"x0":349,"x1":994,"y0":1139,"y1":1210},"conf":0.8672,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\Bigg[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}{\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)}\\Bigg],$$"},{"bbox":{"x0":1043,"x1":1089,"y0":1162,"y1":1194},"conf":0.8447,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1044,"x1":1086,"y0":1162,"y1":1190},"font_size":0.0,"text":"(12)"}],"source":"layout det","text":"(12)"},{"bbox":{"x0":306,"x1":700,"y0":1210,"y1":1255},"conf":0.7838,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$"},{"bbox":{"x0":132,"x1":1091,"y0":1261,"y1":1366},"conf":0.9426,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1083,"y0":1264,"y1":1287},"font_size":0.0,"text":"In this setting, longer sequences can have more infuence on the overall gradient update compared to shorterl"},{"bbox":{"x0":138,"x1":1084,"y0":1289,"y1":1312},"font_size":0.0,"text":"sequences. Moreover, from the perspective of individual tokens, if a particular generation pattern can lead to"},{"bbox":{"x0":138,"x1":1084,"y0":1313,"y1":1335},"font_size":0.0,"text":"an increase or decrease in reward, it will be equally prompted or suppressed, regardless of the length of the"},{"bbox":{"x0":138,"x1":396,"y0":1338,"y1":1360},"font_size":0.0,"text":"response in which it appears."}],"source":"layout det","text":"In this setting, longer sequences can have more infuence on the overall gradient update compared to shorterl sequences. Moreover, from the perspective of individual tokens, if a particular generation pattern can lead to an increase or decrease in reward, it will be equally prompted or suppressed, regardless of the length of the response in which it appears."},{"bbox":{"x0":134,"x1":658,"y0":1378,"y1":1413},"conf":0.8998,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":652,"y0":1379,"y1":1408},"font_size":0.0,"text":"3.4Hide and Seek: Overlong Reward Shaping"}],"source":"layout det","text":"3.4Hide and Seek: Overlong Reward Shaping"},{"bbox":{"x0":133,"x1":1093,"y0":1417,"y1":1499},"conf":0.9295,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1086,"y0":1417,"y1":1444},"font_size":0.0,"text":"In RL training, we typically set a maximum length for generation, with overlong samples truncated accordingly."},{"bbox":{"x0":138,"x1":1083,"y0":1442,"y1":1469},"font_size":0.0,"text":"We fnd that improper reward shaping for truncated samples can introduce reward noise and signifcantlyii"},{"bbox":{"x0":138,"x1":389,"y0":1467,"y1":1492},"font_size":0.0,"text":"disrupt the training process."}],"source":"layout det","text":"In RL training, we typically set a maximum length for generation, with overlong samples truncated accordingly.We fnd that improper reward shaping for truncated samples can introduce reward noise and signifcantlyii disrupt the training process."}],"formula_dets":[{"bbox":{"x0":349,"x1":994,"y0":1139,"y1":1210},"conf":0.8672,"label":"print_isolated","label_id":1},{"bbox":{"x0":306,"x1":700,"y0":1210,"y1":1255},"conf":0.7838,"label":"print_isolated","label_id":1},{"bbox":{"x0":229,"x1":581,"y0":1104,"y1":1136},"conf":0.7135,"label":"print_isolated","label_id":1}],"height":792,"layout_dets":[{"bbox":{"x0":133,"x1":1091,"y0":881,"y1":1059},"conf":0.9744,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1090,"y0":618,"y1":724},"conf":0.9679,"label":"Text","label_id":1},{"bbox":{"x0":143,"x1":1079,"y0":199,"y1":506},"conf":0.9644,"label":"Figure","label_id":3},{"bbox":{"x0":133,"x1":1091,"y0":775,"y1":877},"conf":0.959,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1091,"y0":1261,"y1":1366},"conf":0.9426,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":999,"y0":1102,"y1":1256},"conf":0.9408,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1093,"y0":1417,"y1":1499},"conf":0.9295,"label":"Text","label_id":1},{"bbox":{"x0":141,"x1":1083,"y0":555,"y1":588},"conf":0.9235,"label":"Figure caption","label_id":4},{"bbox":{"x0":133,"x1":755,"y0":736,"y1":770},"conf":0.9002,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":658,"y0":1378,"y1":1413},"conf":0.8998,"label":"Title","label_id":0},{"bbox":{"x0":1043,"x1":1089,"y0":1162,"y1":1194},"conf":0.8447,"label":"Equation caption","label_id":9},{"bbox":{"x0":144,"x1":1082,"y0":514,"y1":547},"conf":0.8134,"label":"Figure caption","label_id":4},{"bbox":{"x0":132,"x1":1094,"y0":1061,"y1":1096},"conf":0.8063,"label":"Text","label_id":1},{"bbox":{"x0":599,"x1":624,"y0":1526,"y1":1554},"conf":0.3551,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[602,1528],[622,1528],[622,1551],[602,1551]],"score":0.7781},{"poly":[[138,1467],[389,1469],[389,1492],[138,1490]],"score":0.7715},{"poly":[[138,1442],[1083,1445],[1083,1469],[138,1465]],"score":0.6912},{"poly":[[138,1417],[1086,1421],[1086,1444],[138,1440]],"score":0.6968},{"poly":[[138,1379],[652,1383],[652,1408],[138,1404]],"score":0.7014},{"poly":[[138,1338],[396,1338],[396,1360],[138,1360]],"score":0.8521},{"poly":[[138,1313],[1084,1313],[1084,1335],[138,1335]],"score":0.8619},{"poly":[[138,1289],[1084,1289],[1084,1312],[138,1312]],"score":0.7269},{"poly":[[140,1264],[1083,1264],[1083,1287],[140,1287]],"score":0.7523},{"poly":[[303,1213],[697,1213],[697,1246],[303,1246]],"score":0.7212},{"poly":[[456,1193],[481,1193],[481,1204],[456,1204]],"score":0.6222},{"poly":[[1044,1162],[1086,1162],[1086,1190],[1044,1190]],"score":0.9004},{"poly":[[504,1147],[998,1147],[998,1198],[504,1198]],"score":0.6522},{"poly":[[402,1152],[417,1152],[417,1170],[402,1170]],"score":0.8489},{"poly":[[456,1138],[516,1138],[516,1162],[456,1162]],"score":0.8258},{"poly":[[227,1100],[571,1109],[570,1137],[226,1128]],"score":0.6905},{"poly":[[138,1063],[1084,1064],[1084,1087],[138,1086]],"score":0.7507},{"poly":[[138,1030],[725,1030],[725,1053],[138,1053]],"score":0.7185},{"poly":[[138,1006],[1084,1006],[1084,1028],[138,1028]],"score":0.8515},{"poly":[[138,982],[1084,982],[1084,1003],[138,1003]],"score":0.8271},{"poly":[[138,959],[1084,959],[1084,980],[138,980]],"score":0.8325},{"poly":[[138,934],[1084,934],[1084,957],[138,957]],"score":0.7099},{"poly":[[138,911],[1084,911],[1084,932],[138,932]],"score":0.7961},{"poly":[[138,883],[1084,884],[1084,908],[138,906]],"score":0.7558},{"poly":[[138,850],[755,850],[755,871],[138,871]],"score":0.8533},{"poly":[[138,827],[1084,827],[1084,848],[138,848]],"score":0.8202},{"poly":[[140,804],[1084,804],[1084,825],[140,825]],"score":0.8505},{"poly":[[138,779],[1084,779],[1084,802],[138,802]],"score":0.72},{"poly":[[138,739],[750,739],[750,762],[138,762]],"score":0.8161},{"poly":[[138,695],[384,695],[384,718],[138,718]],"score":0.7269},{"poly":[[138,672],[1084,672],[1084,693],[138,693]],"score":0.853},{"poly":[[138,645],[1084,647],[1084,670],[138,668]],"score":0.7317},{"poly":[[140,624],[1084,624],[1084,645],[140,645]],"score":0.8489},{"poly":[[145,559],[1076,559],[1076,581],[145,581]],"score":0.8416},{"poly":[[622,515],[1076,516],[1076,540],[622,538]],"score":0.7646},{"poly":[[150,515],[617,516],[617,540],[150,538]],"score":0.7539},{"poly":[[855,471],[900,476],[898,501],[852,496]],"score":0.7682},{"poly":[[369,471],[417,476],[414,501],[367,496]],"score":0.7858},{"poly":[[994,460],[1029,460],[1029,478],[994,478]],"score":0.9592},{"poly":[[916,460],[953,460],[953,478],[916,478]],"score":0.913},{"poly":[[762,460],[798,460],[798,478],[762,478]],"score":0.9034},{"poly":[[695,462],[712,462],[712,477],[695,477]],"score":0.8437},{"poly":[[514,460],[549,460],[549,478],[514,478]],"score":0.9728},{"poly":[[434,460],[471,460],[471,478],[434,478]],"score":0.9108},{"poly":[[356,460],[389,460],[389,478],[356,478]],"score":0.9553},{"poly":[[276,460],[309,460],[309,478],[276,478]],"score":0.9717},{"poly":[[210,464],[218,464],[218,475],[210,475]],"score":0.7574},{"poly":[[840,459],[875,459],[875,478],[840,478]],"score":0.7938},{"poly":[[170,431],[195,431],[195,447],[170,447]],"score":0.7491},{"poly":[[650,414],[682,414],[682,434],[650,434]],"score":0.7967},{"poly":[[512,395],[571,391],[573,409],[513,413]],"score":0.7633},{"poly":[[166,391],[195,391],[195,411],[166,411]],"score":0.649},{"poly":[[630,375],[649,375],[649,424],[630,424]],"score":0.8147},{"poly":[[649,361],[682,361],[682,380],[649,380]],"score":0.8881},{"poly":[[170,353],[193,353],[193,373],[170,373]],"score":0.8021},{"poly":[[374,317],[395,324],[383,357],[362,350]],"score":0.6725},{"poly":[[166,318],[191,312],[196,334],[171,339]],"score":0.6306},{"poly":[[647,309],[682,309],[682,328],[647,328]],"score":0.7121},{"poly":[[833,287],[882,332],[866,350],[816,305]],"score":0.634},{"poly":[[629,297],[652,297],[652,383],[629,383]],"score":0.6847},{"poly":[[168,280],[193,280],[193,300],[168,300]],"score":0.7574},{"poly":[[146,256],[171,256],[173,411],[148,411]],"score":0.601},{"poly":[[170,243],[195,243],[195,262],[170,262]],"score":0.7497},{"poly":[[400,231],[429,235],[418,308],[390,304]],"score":0.6684},{"poly":[[953,233],[1061,233],[1061,249],[953,249]],"score":0.9076},{"poly":[[474,233],[582,233],[582,249],[474,249]],"score":0.9821},{"poly":[[951,216],[1058,216],[1058,233],[951,233]],"score":0.955},{"poly":[[472,216],[579,216],[579,233],[472,233]],"score":0.937},{"poly":[[647,206],[684,206],[684,224],[647,224]],"score":0.8711},{"poly":[[170,208],[195,208],[195,224],[170,224]],"score":0.806}],"page_no":5,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":128,"x1":1089,"y0":1524,"y1":1583},"conf":0.2307,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1528,"y1":1548},"font_size":0.0,"text":"7"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":143,"x1":1082,"y0":200,"y1":545},"conf":0.978,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![34dc88665925095061ed6585e4b68349](imgs/34dc88665925095061ed6585e4b68349.jpg)"},{"bbox":{"x0":132,"x1":1091,"y0":555,"y1":610},"conf":0.9406,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":559,"y1":581},"font_size":0.0,"text":"Figure 5 The accuracy of the actor model on AIME and the entropy of its generation probabilities, both before and"},{"bbox":{"x0":136,"x1":565,"y0":581,"y1":606},"font_size":0.0,"text":"after applying Overlong Reward Shaping strategy."}],"source":"layout det","text":"Figure 5 The accuracy of the actor model on AIME and the entropy of its generation probabilities, both before and after applying Overlong Reward Shaping strategy."},{"bbox":{"x0":133,"x1":1092,"y0":642,"y1":725},"conf":0.9496,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":647,"y1":668},"font_size":0.0,"text":"By default, we assign a punitive reward to truncated samples. This approach may introduce noise into the"},{"bbox":{"x0":138,"x1":1084,"y0":672,"y1":693},"font_size":0.0,"text":"training process, as a sound reasoning process can be penalized solely due to its excessive length. Such"},{"bbox":{"x0":136,"x1":930,"y0":693,"y1":719},"font_size":0.0,"text":"penalties can potentially confuse the model regarding the validity of its reasoning process."}],"source":"layout det","text":"By default, we assign a punitive reward to truncated samples. This approach may introduce noise into the training process, as a sound reasoning process can be penalized solely due to its excessive length. Such penalties can potentially confuse the model regarding the validity of its reasoning process."},{"bbox":{"x0":132,"x1":1092,"y0":736,"y1":818},"conf":0.9496,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1084,"y0":738,"y1":762},"font_size":0.0,"text":"To investigate the impact of this reward noise, we frst apply an Overlong Filtering strategy which masksi"},{"bbox":{"x0":138,"x1":1084,"y0":766,"y1":787},"font_size":0.0,"text":"the loss of truncated samples. We fnd that this approach signifcantly stabilizes training and enhancesii"},{"bbox":{"x0":136,"x1":514,"y0":787,"y1":812},"font_size":0.0,"text":"performance, as demonstrated in Figure 5."}],"source":"layout det","text":"To investigate the impact of this reward noise, we frst apply an Overlong Filtering strategy which masksi the loss of truncated samples. We fnd that this approach signifcantly stabilizes training and enhancesii performance, as demonstrated in Figure 5."},{"bbox":{"x0":134,"x1":845,"y0":836,"y1":868},"conf":0.831,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":840,"y0":840,"y1":861},"font_size":0.0,"text":"Algorithm 1DAPO: Decoupled Clip and Dynamic sAmpling Policy Optimization"}],"source":"layout det","text":"Algorithm 1DAPO: Decoupled Clip and Dynamic sAmpling Policy Optimization"},{"bbox":{"x0":205,"x1":943,"y0":871,"y1":900},"conf":0.7038,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":940,"y0":868,"y1":903},"font_size":0.0,"text":"itial policy model $\\pi_{\\theta};$  reawrd model $R$  task prompts D; hyperparameters εlow, εhigh"}],"source":"layout det","text":"itial policy model $\\pi_{\\theta};$  reawrd model $R$  task prompts D; hyperparameters εlow, εhigh"},{"bbox":{"x0":137,"x1":359,"y0":899,"y1":922},"conf":0.8405,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":143,"x1":356,"y0":898,"y1":921},"font_size":0.0,"text":"1: for step $=1\\text{,}...,\\text{M}$ do"}],"source":"layout det","text":"1: for step $=1\\text{,}...,\\text{M}$ do"},{"bbox":{"x0":139,"x1":438,"y0":923,"y1":945},"conf":0.8923,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":143,"x1":430,"y0":916,"y1":947},"font_size":0.0,"text":"2:Sample a batch $\\mathcal{D}_{b}$  from $\\mathcal{D}$"}],"source":"layout det","text":"2:Sample a batch $\\mathcal{D}_{b}$  from $\\mathcal{D}$"},{"bbox":{"x0":140,"x1":549,"y0":946,"y1":969},"conf":0.902,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":143,"x1":543,"y0":940,"y1":972},"font_size":0.0,"text":"3:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$"}],"source":"layout det","text":"3:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$"},{"bbox":{"x0":140,"x1":754,"y0":970,"y1":994},"conf":0.8978,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":746,"y0":970,"y1":995},"font_size":0.0,"text":"4:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)$  for each question $q\\in\\mathcal{D}_{b}$"}],"source":"layout det","text":"4:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)$  for each question $q\\in\\mathcal{D}_{b}$"},{"bbox":{"x0":141,"x1":788,"y0":995,"y1":1018},"conf":0.8731,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":785,"y0":992,"y1":1020},"font_size":0.0,"text":"5:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$  by running $R$"}],"source":"layout det","text":"5:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$  by running $R$"},{"bbox":{"x0":137,"x1":1090,"y0":1020,"y1":1068},"conf":0.5862,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":1081,"y0":1015,"y1":1041},"font_size":0.0,"text":"6:Filter out $o_{i}$  and add the remaining to the dynamic sampling bufer (Dynamic Sampling Equation (11))f"},{"bbox":{"x0":143,"x1":379,"y0":1040,"y1":1064},"font_size":0.0,"text":"7:if bufer sizef $\\boldsymbol{n_{b}<N:}$"}],"source":"layout det","text":"6:Filter out $o_{i}$  and add the remaining to the dynamic sampling bufer (Dynamic Sampling Equation (11))f 7:if bufer sizef $\\boldsymbol{n_{b}<N:}$"},{"bbox":{"x0":139,"x1":305,"y0":1066,"y1":1088},"conf":0.6811,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":303,"y0":1064,"y1":1087},"font_size":0.0,"text":"8:continue"}],"source":"layout det","text":"8:continue"},{"bbox":{"x0":138,"x1":882,"y0":1088,"y1":1117},"conf":0.6008,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":137,"x1":871,"y0":1086,"y1":1118},"font_size":0.0,"text":"9：For each $O_{2}$ in the bufer, computef $\\hat{A}_{i,t}$ for the $t$ th token of $o_{i}$  (Equation (9))"}],"source":"layout det","text":"9：For each $O_{2}$ in the bufer, computef $\\hat{A}_{i,t}$ for the $t$ th token of $o_{i}$  (Equation (9))"},{"bbox":{"x0":138,"x1":433,"y0":1116,"y1":1138},"conf":0.7986,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":432,"y0":1114,"y1":1139},"font_size":0.0,"text":"10:for iteration $=1, ..., \\mu$ do"}],"source":"layout det","text":"10:for iteration $=1, ..., \\mu$ do"},{"bbox":{"x0":138,"x1":933,"y0":1139,"y1":1165},"conf":0.6662,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":923,"y0":1135,"y1":1162},"font_size":0.0,"text":"11:Update the policy model $\\pi_{\\theta}$  by maximizing the DAPO objective (Equation (8))"}],"source":"layout det","text":"11:Update the policy model $\\pi_{\\theta}$  by maximizing the DAPO objective (Equation (8))"},{"bbox":{"x0":135,"x1":234,"y0":1164,"y1":1189},"conf":0.5175,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":229,"y0":1160,"y1":1188},"font_size":0.0,"text":"Output $\\pi_{\\theta}$"}],"source":"layout det","text":"Output $\\pi_{\\theta}$"},{"bbox":{"x0":133,"x1":1092,"y0":1237,"y1":1367},"conf":0.9577,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1241,"y1":1262},"font_size":0.0,"text":"Furthermore, we propose Soft Overlong Punishment (Equation 13), a length-aware penalty mechanism designed"},{"bbox":{"x0":138,"x1":1084,"y0":1266,"y1":1287},"font_size":0.0,"text":"to shape the reward for truncated samples. Specifcally, when the response length exceeds the predefnedii"},{"bbox":{"x0":138,"x1":1086,"y0":1289,"y1":1312},"font_size":0.0,"text":"maximum value, we defne a punishment interval. Within this interval, the longer the response, the greater thei"},{"bbox":{"x0":136,"x1":1084,"y0":1312,"y1":1337},"font_size":0.0,"text":"punishment it receives. This penalty is added to the original rule-based correctness reward, thereby signaling"},{"bbox":{"x0":138,"x1":567,"y0":1338,"y1":1360},"font_size":0.0,"text":"to the model to avoid excessively long responses."}],"source":"layout det","text":"Furthermore, we propose Soft Overlong Punishment (Equation 13), a length-aware penalty mechanism designed to shape the reward for truncated samples. Specifcally, when the response length exceeds the predefnedii maximum value, we defne a punishment interval. Within this interval, the longer the response, the greater thei punishment it receives. This penalty is added to the original rule-based correctness reward, thereby signaling to the model to avoid excessively long responses."},{"bbox":{"x0":341,"x1":881,"y0":1390,"y1":1482},"conf":0.9278,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$R_{\\text{length}}(y)=\\begin{cases}0,&|y|\\leq L_{\\text{max}}-L_{\\text{cache}}\\\\ \\frac{(L_{\\text{max}}-L_{\\text{cache}})-|y|}{L_{\\text{cache}}},&L_{\\text{max}}-L_{\\text{cache}}<|y|\\leq L_{\\text{max}}\\\\ -1,&L_{\\text{max}}<|y|\\end{cases}$$"},{"bbox":{"x0":1042,"x1":1090,"y0":1419,"y1":1452},"conf":0.8459,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1043,"x1":1089,"y0":1417,"y1":1450},"font_size":0.0,"text":"(13)"}],"source":"layout det","text":"(13)"}],"formula_dets":[{"bbox":{"x0":341,"x1":881,"y0":1390,"y1":1482},"conf":0.9278,"label":"print_isolated","label_id":1},{"bbox":{"x0":356,"x1":418,"y0":996,"y1":1019},"conf":0.9104,"label":"print_embedding","label_id":0},{"bbox":{"x0":363,"x1":527,"y0":971,"y1":995},"conf":0.8841,"label":"print_embedding","label_id":0},{"bbox":{"x0":499,"x1":533,"y0":1091,"y1":1117},"conf":0.8793,"label":"print_embedding","label_id":0},{"bbox":{"x0":452,"x1":543,"y0":952,"y1":970},"conf":0.8646,"label":"print_embedding","label_id":0},{"bbox":{"x0":336,"x1":360,"y0":927,"y1":944},"conf":0.8574,"label":"print_embedding","label_id":0},{"bbox":{"x0":243,"x1":324,"y0":900,"y1":921},"conf":0.8566,"label":"print_embedding","label_id":0},{"bbox":{"x0":308,"x1":379,"y0":1045,"y1":1064},"conf":0.8477,"label":"print_embedding","label_id":0},{"bbox":{"x0":688,"x1":746,"y0":974,"y1":993},"conf":0.8399,"label":"print_embedding","label_id":0},{"bbox":{"x0":267,"x1":283,"y0":974,"y1":990},"conf":0.7998,"label":"print_embedding","label_id":0},{"bbox":{"x0":311,"x1":400,"y0":1116,"y1":1139},"conf":0.7709,"label":"print_embedding","label_id":0},{"bbox":{"x0":413,"x1":430,"y0":927,"y1":941},"conf":0.7543,"label":"print_embedding","label_id":0},{"bbox":{"x0":767,"x1":785,"y0":998,"y1":1014},"conf":0.7327,"label":"print_embedding","label_id":0},{"bbox":{"x0":283,"x1":300,"y0":1028,"y1":1040},"conf":0.7325,"label":"print_embedding","label_id":0},{"bbox":{"x0":276,"x1":293,"y0":1101,"y1":1114},"conf":0.7262,"label":"print_embedding","label_id":0},{"bbox":{"x0":208,"x1":229,"y0":1172,"y1":1185},"conf":0.7016,"label":"print_embedding","label_id":0},{"bbox":{"x0":642,"x1":659,"y0":1003,"y1":1016},"conf":0.7015,"label":"print_embedding","label_id":0},{"bbox":{"x0":524,"x1":545,"y0":878,"y1":895},"conf":0.7009,"label":"print_embedding","label_id":0},{"bbox":{"x0":451,"x1":471,"y0":1148,"y1":1161},"conf":0.6816,"label":"print_embedding","label_id":0},{"bbox":{"x0":604,"x1":614,"y0":1097,"y1":1111},"conf":0.6796,"label":"print_embedding","label_id":0},{"bbox":{"x0":721,"x1":738,"y0":1101,"y1":1114},"conf":0.671,"label":"print_embedding","label_id":0},{"bbox":{"x0":367,"x1":393,"y0":883,"y1":897},"conf":0.6393,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":143,"x1":1082,"y0":200,"y1":545},"conf":0.978,"label":"Figure","label_id":3},{"bbox":{"x0":133,"x1":1092,"y0":1237,"y1":1367},"conf":0.9577,"label":"Text","label_id":1},{"bbox":{"x0":334,"x1":886,"y0":1386,"y1":1485},"conf":0.9553,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1092,"y0":642,"y1":725},"conf":0.9496,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1092,"y0":736,"y1":818},"conf":0.9496,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1091,"y0":555,"y1":610},"conf":0.9406,"label":"Figure caption","label_id":4},{"bbox":{"x0":140,"x1":549,"y0":946,"y1":969},"conf":0.902,"label":"Text","label_id":1},{"bbox":{"x0":140,"x1":754,"y0":970,"y1":994},"conf":0.8978,"label":"Text","label_id":1},{"bbox":{"x0":139,"x1":438,"y0":923,"y1":945},"conf":0.8923,"label":"Text","label_id":1},{"bbox":{"x0":141,"x1":788,"y0":995,"y1":1018},"conf":0.8731,"label":"Text","label_id":1},{"bbox":{"x0":1042,"x1":1090,"y0":1419,"y1":1452},"conf":0.8459,"label":"Equation caption","label_id":9},{"bbox":{"x0":137,"x1":359,"y0":899,"y1":922},"conf":0.8405,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":845,"y0":836,"y1":868},"conf":0.831,"label":"Title","label_id":0},{"bbox":{"x0":138,"x1":433,"y0":1116,"y1":1138},"conf":0.7986,"label":"Text","label_id":1},{"bbox":{"x0":140,"x1":385,"y0":1042,"y1":1065},"conf":0.7963,"label":"Text","label_id":1},{"bbox":{"x0":205,"x1":943,"y0":871,"y1":900},"conf":0.7038,"label":"Text","label_id":1},{"bbox":{"x0":139,"x1":305,"y0":1066,"y1":1088},"conf":0.6811,"label":"Text","label_id":1},{"bbox":{"x0":138,"x1":933,"y0":1139,"y1":1165},"conf":0.6662,"label":"Text","label_id":1},{"bbox":{"x0":138,"x1":882,"y0":1088,"y1":1117},"conf":0.6008,"label":"Text","label_id":1},{"bbox":{"x0":137,"x1":1090,"y0":1020,"y1":1068},"conf":0.5862,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":234,"y0":1164,"y1":1189},"conf":0.5175,"label":"Text","label_id":1},{"bbox":{"x0":180,"x1":921,"y0":1140,"y1":1164},"conf":0.4922,"label":"Text","label_id":1},{"bbox":{"x0":598,"x1":624,"y0":1525,"y1":1554},"conf":0.2674,"label":"Abandon","label_id":2},{"bbox":{"x0":128,"x1":1089,"y0":1524,"y1":1583},"conf":0.2307,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1528],[619,1528],[619,1548],[604,1548]],"score":0.8331},{"poly":[[634,1448],[734,1452],[733,1482],[633,1478]],"score":0.7909},{"poly":[[472,1452],[507,1452],[507,1478],[472,1478]],"score":0.7466},{"poly":[[518,1429],[570,1438],[566,1461],[514,1453]],"score":0.7659},{"poly":[[631,1417],[883,1421],[883,1454],[630,1450]],"score":0.6885},{"poly":[[337,1415],[456,1419],[455,1454],[336,1450]],"score":0.7482},{"poly":[[1043,1417],[1089,1417],[1089,1450],[1043,1450]],"score":0.8888},{"poly":[[516,1412],[617,1416],[616,1441],[515,1437]],"score":0.6675},{"poly":[[469,1414],[517,1414],[517,1439],[469,1439]],"score":0.616},{"poly":[[639,1396],[808,1396],[808,1419],[639,1419]],"score":0.8899},{"poly":[[461,1391],[492,1391],[492,1419],[461,1419]],"score":0.7294},{"poly":[[138,1338],[567,1338],[567,1360],[138,1360]],"score":0.8284},{"poly":[[136,1312],[1084,1313],[1084,1337],[136,1335]],"score":0.7127},{"poly":[[138,1289],[1086,1289],[1086,1312],[138,1312]],"score":0.7363},{"poly":[[138,1266],[1084,1266],[1084,1287],[138,1287]],"score":0.8579},{"poly":[[140,1241],[1084,1241],[1084,1262],[140,1262]],"score":0.8477},{"poly":[[137,1160],[235,1164],[234,1188],[136,1184]],"score":0.7665},{"poly":[[226,1138],[923,1138],[923,1162],[226,1162]],"score":0.7952},{"poly":[[141,1135],[178,1135],[178,1162],[141,1162]],"score":0.8502},{"poly":[[196,1115],[432,1115],[432,1138],[196,1138]],"score":0.7448},{"poly":[[141,1114],[180,1114],[180,1138],[141,1138]],"score":0.8839},{"poly":[[191,1089],[871,1091],[871,1115],[191,1114]],"score":0.757},{"poly":[[141,1086],[173,1091],[168,1118],[137,1113]],"score":0.6644},{"poly":[[221,1068],[303,1068],[303,1086],[221,1086]],"score":0.856},{"poly":[[141,1064],[168,1064],[168,1087],[141,1087]],"score":0.8559},{"poly":[[193,1041],[386,1041],[386,1064],[193,1064]],"score":0.7602},{"poly":[[143,1040],[168,1040],[168,1064],[143,1064]],"score":0.8011},{"poly":[[191,1015],[1081,1018],[1081,1041],[191,1038]],"score":0.757},{"poly":[[141,1016],[168,1016],[168,1040],[141,1040]],"score":0.7515},{"poly":[[191,992],[788,992],[788,1020],[191,1020]],"score":0.6883},{"poly":[[141,993],[168,993],[168,1015],[141,1015]],"score":0.8406},{"poly":[[193,970],[748,970],[748,993],[193,993]],"score":0.7896},{"poly":[[141,970],[170,970],[170,992],[141,992]],"score":0.7927},{"poly":[[190,940],[551,944],[550,972],[189,968]],"score":0.6614},{"poly":[[143,945],[171,945],[171,967],[143,967]],"score":0.8144},{"poly":[[189,919],[435,916],[436,944],[190,947]],"score":0.6885},{"poly":[[143,921],[168,921],[168,944],[143,944]],"score":0.8058},{"poly":[[143,898],[356,898],[356,921],[143,921]],"score":0.765},{"poly":[[136,868],[940,873],[939,903],[136,898]],"score":0.6083},{"poly":[[140,840],[840,840],[840,861],[140,861]],"score":0.9154},{"poly":[[136,789],[514,787],[514,810],[136,812]],"score":0.7596},{"poly":[[138,766],[1084,766],[1084,787],[138,787]],"score":0.8261},{"poly":[[136,738],[1084,739],[1084,762],[136,761]],"score":0.7466},{"poly":[[136,696],[930,693],[930,716],[136,719]],"score":0.7062},{"poly":[[138,672],[1084,672],[1084,693],[138,693]],"score":0.8642},{"poly":[[140,647],[1084,647],[1084,668],[140,668]],"score":0.8512},{"poly":[[136,581],[565,582],[565,606],[136,604]],"score":0.7604},{"poly":[[140,559],[1084,559],[1084,581],[140,581]],"score":0.8406},{"poly":[[735,516],[966,516],[966,540],[735,540]],"score":0.7635},{"poly":[[256,516],[484,516],[484,538],[256,538]],"score":0.877},{"poly":[[847,471],[893,477],[889,502],[844,496]],"score":0.8049},{"poly":[[374,471],[422,476],[419,501],[372,496]],"score":0.7346},{"poly":[[1013,460],[1048,460],[1048,478],[1013,478]],"score":0.9561},{"poly":[[945,460],[980,460],[980,478],[945,478]],"score":0.9521},{"poly":[[873,460],[908,460],[908,478],[873,478]],"score":0.9142},{"poly":[[803,460],[840,460],[840,478],[803,478]],"score":0.868},{"poly":[[735,460],[772,460],[772,478],[735,478]],"score":0.9462},{"poly":[[675,462],[690,462],[690,477],[675,477]],"score":0.8548},{"poly":[[537,460],[572,460],[572,478],[537,478]],"score":0.9152},{"poly":[[469,460],[504,460],[504,478],[469,478]],"score":0.9456},{"poly":[[401,460],[436,460],[436,478],[401,478]],"score":0.9714},{"poly":[[333,460],[368,460],[368,478],[333,478]],"score":0.9599},{"poly":[[266,460],[301,460],[301,478],[266,478]],"score":0.9476},{"poly":[[211,464],[220,464],[220,475],[211,475]],"score":0.9178},{"poly":[[649,439],[664,439],[664,455],[649,455]],"score":0.7868},{"poly":[[168,416],[200,416],[200,436],[168,436]],"score":0.858},{"poly":[[170,383],[200,383],[200,403],[170,403]],"score":0.8706},{"poly":[[991,363],[1053,363],[1053,422],[991,422]],"score":0.6824},{"poly":[[168,350],[198,350],[198,370],[168,370]],"score":0.7637},{"poly":[[649,346],[662,346],[662,361],[649,361]],"score":0.8062},{"poly":[[168,317],[200,317],[200,337],[168,337]],"score":0.7022},{"poly":[[296,312],[313,312],[313,325],[296,325]],"score":0.6192},{"poly":[[650,302],[660,302],[660,317],[650,317]],"score":0.8144},{"poly":[[516,282],[569,282],[569,314],[516,314]],"score":0.7588},{"poly":[[168,282],[200,282],[200,302],[168,302]],"score":0.7608},{"poly":[[349,279],[386,279],[386,307],[349,307]],"score":0.648},{"poly":[[323,280],[341,280],[341,292],[323,292]],"score":0.7299},{"poly":[[146,276],[175,276],[175,393],[146,393]],"score":0.7069},{"poly":[[361,251],[412,251],[412,284],[361,284]],"score":0.7538},{"poly":[[168,251],[201,251],[201,269],[168,269]],"score":0.8785},{"poly":[[700,234],[818,234],[818,251],[700,251]],"score":0.7764},{"poly":[[464,233],[579,233],[579,249],[464,249]],"score":0.9661},{"poly":[[700,219],[822,219],[822,238],[700,238]],"score":0.8129},{"poly":[[168,218],[201,218],[201,236],[168,236]],"score":0.8768},{"poly":[[461,216],[584,216],[584,233],[461,233]],"score":0.9611},{"poly":[[649,208],[662,208],[662,223],[649,223]],"score":0.7265}],"page_no":6,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":113,"x1":1083,"y0":1525,"y1":1583},"conf":0.207,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":602,"x1":622,"y0":1528,"y1":1551},"font_size":0.0,"text":"8"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":308,"x1":886,"y0":227,"y1":536},"conf":0.9711,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![bdf737fe4b1cdce5dd33c6102f5e8376](imgs/bdf737fe4b1cdce5dd33c6102f5e8376.jpg)"},{"bbox":{"x0":208,"x1":1014,"y0":544,"y1":575},"conf":0.931,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":546,"y1":571},"font_size":10.0,"text":"Figure 6 The training progress before and after applying dynamic sampling on a baseline setting."}],"source":"layout det","text":"Figure 6 The training progress before and after applying dynamic sampling on a baseline setting."},{"bbox":{"x0":135,"x1":464,"y0":608,"y1":638},"conf":0.899,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":459,"y0":612,"y1":634},"font_size":10.0,"text":"3.5Dataset Transformation"}],"source":"layout det","text":"3.5Dataset Transformation"},{"bbox":{"x0":134,"x1":1092,"y0":647,"y1":849},"conf":0.977,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":650,"y1":672},"font_size":0.0,"text":"Our dataset is sourced from the web and ofcial competition homepages through a combination of webfi"},{"bbox":{"x0":138,"x1":1084,"y0":675,"y1":696},"font_size":0.0,"text":"scraping and manual annotation. The answers of math dataset typically come in a variety of formats, such as"},{"bbox":{"x0":136,"x1":1086,"y0":696,"y1":721},"font_size":0.0,"text":"expression, formula and number, which makes it challenging to design comprehensive rules to parse them."},{"bbox":{"x0":138,"x1":1084,"y0":721,"y1":742},"font_size":0.0,"text":"To provide accurate reward signals using rules and minimize errors introduced by formula parsers, inspired"},{"bbox":{"x0":138,"x1":1084,"y0":746,"y1":767},"font_size":0.0,"text":"by AIME, we select and transform the answer√s into integers, which are easy to parse. For example, if the"},{"bbox":{"x0":141,"x1":1082,"y0":768,"y1":798},"font_size":10.0,"text":"original answer is expressed in the form of $\\tfrac{a+\\sqrt{b}}{c},$  we instruct the LLM to modify the question so that the"},{"bbox":{"x0":138,"x1":1084,"y0":797,"y1":818},"font_size":10.0,"text":"expected answer becomes $a+b+c.$  After selection and transformation, we obtained the DAPO-Math-17K"},{"bbox":{"x0":140,"x1":863,"y0":820,"y1":842},"font_size":0.0,"text":"dataset, which consists of 17K prompts, each paired with an integer as the answer."}],"source":"layout det","text":"Our dataset is sourced from the web and ofcial competition homepages through a combination of webfi scraping and manual annotation. The answers of math dataset typically come in a variety of formats, such as expression, formula and number, which makes it challenging to design comprehensive rules to parse them.To provide accurate reward signals using rules and minimize errors introduced by formula parsers, inspired by AIME, we select and transform the answer√s into integers, which are easy to parse. For example, if the original answer is expressed in the form of $\\tfrac{a+\\sqrt{b}}{c},$  we instruct the LLM to modify the question so that the expected answer becomes $a+b+c.$  After selection and transformation, we obtained the DAPO-Math-17K dataset, which consists of 17K prompts, each paired with an integer as the answer."},{"bbox":{"x0":135,"x1":335,"y0":867,"y1":901},"conf":0.8817,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":135,"x1":331,"y0":866,"y1":896},"font_size":10.0,"text":"4Experiments"}],"source":"layout det","text":"4Experiments"},{"bbox":{"x0":135,"x1":370,"y0":912,"y1":945},"conf":0.9017,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":364,"y0":914,"y1":937},"font_size":10.0,"text":"4.1Training Details"}],"source":"layout det","text":"4.1Training Details"},{"bbox":{"x0":134,"x1":1090,"y0":951,"y1":1029},"conf":0.9506,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1083,"y0":954,"y1":975},"font_size":10.0,"text":"In this work, we focus specifcally on mathematical tasks to evaluate our algorithm, which can be readilyi"},{"bbox":{"x0":138,"x1":1084,"y0":978,"y1":1000},"font_size":10.0,"text":"transferred to other tasks. We adopt the verl framework [20] for training. We use naive GRPO [38] as our"},{"bbox":{"x0":135,"x1":830,"y0":998,"y1":1028},"font_size":10.0,"text":"baseline algorithm and estimate advantages using group reward normalization."}],"source":"layout det","text":"In this work, we focus specifcally on mathematical tasks to evaluate our algorithm, which can be readilyi transferred to other tasks. We adopt the verl framework [20] for training. We use naive GRPO [38] as our baseline algorithm and estimate advantages using group reward normalization."},{"bbox":{"x0":134,"x1":1092,"y0":1034,"y1":1258},"conf":0.9773,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1084,"y0":1031,"y1":1063},"font_size":0.0,"text":"For hyper-parameters, we utilize the AdamW [39] optimizer with a constant learning rate of $1\\times10^{-6},$"},{"bbox":{"x0":138,"x1":1084,"y0":1061,"y1":1082},"font_size":0.0,"text":"incorporating a linear warm-up over 20 rollout steps. For rollout, the prompt batch size is 512 and we sample"},{"bbox":{"x0":138,"x1":1084,"y0":1086,"y1":1107},"font_size":0.0,"text":"16 responses for each prompt. For training, the mini-batch size is set to 512, i.e., 16 gradient updates for"},{"bbox":{"x0":138,"x1":1084,"y0":1110,"y1":1132},"font_size":0.0,"text":"each rollout step. For Overlong Reward Shaping, we set the expected maximum length as 16,384 tokens"},{"bbox":{"x0":138,"x1":1084,"y0":1134,"y1":1155},"font_size":0.0,"text":"and allocate additional 4,096 tokens as the soft punish cache. Therefore, the maximum number of tokens"},{"bbox":{"x0":135,"x1":1081,"y0":1153,"y1":1183},"font_size":0.0,"text":"for generation is set to 20,480 tokens. As for the Clip-Higher mechanism, we set the clipping parameter $\\varepsilon_{\\text{low}}$"},{"bbox":{"x0":138,"x1":1084,"y0":1181,"y1":1205},"font_size":0.0,"text":"to 0.2 and $\\varepsilon_{\\text{high}}$  to 0.28, which efectively balance the trade-of between exploration and exploitation. Forff"},{"bbox":{"x0":138,"x1":1084,"y0":1204,"y1":1228},"font_size":0.0,"text":"evaluation on AIME, we repeat the evaluation set for 32 times and report avg@32 for results stability. The"},{"bbox":{"x0":138,"x1":846,"y0":1229,"y1":1251},"font_size":0.0,"text":"inference hyperparameters of evaluation are set to temperature 1.0 and topp 0.7."}],"source":"layout det","text":"For hyper-parameters, we utilize the AdamW [39] optimizer with a constant learning rate of $1\\times10^{-6},$ incorporating a linear warm-up over 20 rollout steps. For rollout, the prompt batch size is 512 and we sample 16 responses for each prompt. For training, the mini-batch size is set to 512, i.e., 16 gradient updates for each rollout step. For Overlong Reward Shaping, we set the expected maximum length as 16,384 tokens and allocate additional 4,096 tokens as the soft punish cache. Therefore, the maximum number of tokens for generation is set to 20,480 tokens. As for the Clip-Higher mechanism, we set the clipping parameter $\\varepsilon_{\\text{low}}$ to 0.2 and $\\varepsilon_{\\text{high}}$  to 0.28, which efectively balance the trade-of between exploration and exploitation. Forff evaluation on AIME, we repeat the evaluation set for 32 times and report avg@32 for results stability. The inference hyperparameters of evaluation are set to temperature 1.0 and topp 0.7."},{"bbox":{"x0":135,"x1":337,"y0":1272,"y1":1302},"conf":0.8875,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":333,"y0":1275,"y1":1297},"font_size":10.0,"text":"4.2Main Results"}],"source":"layout det","text":"4.2Main Results"},{"bbox":{"x0":134,"x1":1092,"y0":1310,"y1":1436},"conf":0.948,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1313,"y1":1335},"font_size":10.0,"text":"Experiments on AIME 2024 demonstrate that DAPO has successfully trained the Qwen-32B Base model into"},{"bbox":{"x0":135,"x1":1083,"y0":1335,"y1":1360},"font_size":10.0,"text":"a powerful reasoning model, achieving performance superior to DeepSeek’s experiments on Qwen2.5-32B"},{"bbox":{"x0":138,"x1":1084,"y0":1361,"y1":1383},"font_size":10.0,"text":"using the R1 approach. In Figure 1, we observe a substantial improvement of performance on AIME 2024,"},{"bbox":{"x0":138,"x1":1084,"y0":1384,"y1":1407},"font_size":10.0,"text":"with accuracy increasing from near $0\\%$ to $50\\%.$  Notably, this improvement is achieved with only $50\\%$ of the"},{"bbox":{"x0":138,"x1":634,"y0":1407,"y1":1432},"font_size":10.0,"text":"training steps required by DeepSeek-R1-Zero-Qwen-32B."}],"source":"layout det","text":"Experiments on AIME 2024 demonstrate that DAPO has successfully trained the Qwen-32B Base model into a powerful reasoning model, achieving performance superior to DeepSeek’s experiments on Qwen2.5-32B using the R1 approach. In Figure 1, we observe a substantial improvement of performance on AIME 2024,with accuracy increasing from near $0\\%$ to $50\\%.$  Notably, this improvement is achieved with only $50\\%$ of the training steps required by DeepSeek-R1-Zero-Qwen-32B."},{"bbox":{"x0":132,"x1":1092,"y0":1441,"y1":1498},"conf":0.9214,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1086,"y0":1444,"y1":1467},"font_size":10.0,"text":"We analyze the contributions of each training technique in our methodology, as detailed in Table 1. The"},{"bbox":{"x0":136,"x1":1084,"y0":1467,"y1":1493},"font_size":10.0,"text":"observed improvements demonstrate the efectiveness of these techniques in RL training, each contributingf"}],"source":"layout det","text":"We analyze the contributions of each training technique in our methodology, as detailed in Table 1. The observed improvements demonstrate the efectiveness of these techniques in RL training, each contributingf"}],"formula_dets":[{"bbox":{"x0":524,"x1":576,"y0":768,"y1":798},"conf":0.8945,"label":"print_embedding","label_id":0},{"bbox":{"x0":373,"x1":459,"y0":798,"y1":818},"conf":0.8713,"label":"print_embedding","label_id":0},{"bbox":{"x0":997,"x1":1084,"y0":1035,"y1":1060},"conf":0.8554,"label":"print_embedding","label_id":0},{"bbox":{"x0":450,"x1":480,"y0":1385,"y1":1406},"conf":0.85,"label":"print_embedding","label_id":0},{"bbox":{"x0":986,"x1":1026,"y0":1384,"y1":1406},"conf":0.8459,"label":"print_embedding","label_id":0},{"bbox":{"x0":508,"x1":552,"y0":1384,"y1":1406},"conf":0.843,"label":"print_embedding","label_id":0},{"bbox":{"x0":238,"x1":279,"y0":1187,"y1":1205},"conf":0.7998,"label":"print_embedding","label_id":0},{"bbox":{"x0":1047,"x1":1081,"y0":1165,"y1":1180},"conf":0.6587,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":134,"x1":1092,"y0":1034,"y1":1258},"conf":0.9773,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1092,"y0":647,"y1":849},"conf":0.977,"label":"Text","label_id":1},{"bbox":{"x0":308,"x1":886,"y0":227,"y1":536},"conf":0.9711,"label":"Figure","label_id":3},{"bbox":{"x0":134,"x1":1090,"y0":951,"y1":1029},"conf":0.9506,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1092,"y0":1310,"y1":1436},"conf":0.948,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1014,"y0":544,"y1":575},"conf":0.931,"label":"Figure caption","label_id":4},{"bbox":{"x0":132,"x1":1092,"y0":1441,"y1":1498},"conf":0.9214,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":370,"y0":912,"y1":945},"conf":0.9017,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":464,"y0":608,"y1":638},"conf":0.899,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":337,"y0":1272,"y1":1302},"conf":0.8875,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":335,"y0":867,"y1":901},"conf":0.8817,"label":"Title","label_id":0},{"bbox":{"x0":599,"x1":624,"y0":1526,"y1":1554},"conf":0.3618,"label":"Abandon","label_id":2},{"bbox":{"x0":113,"x1":1083,"y0":1525,"y1":1583},"conf":0.207,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[602,1528],[622,1528],[622,1551],[602,1551]],"score":0.7778},{"poly":[[136,1467],[1084,1470],[1084,1493],[136,1490]],"score":0.7176},{"poly":[[140,1444],[1086,1444],[1086,1467],[140,1467]],"score":0.7138},{"poly":[[138,1409],[634,1407],[634,1431],[138,1432]],"score":0.7591},{"poly":[[138,1384],[1084,1384],[1084,1407],[138,1407]],"score":0.7151},{"poly":[[138,1361],[1084,1361],[1084,1383],[138,1383]],"score":0.8245},{"poly":[[135,1337],[1083,1335],[1083,1358],[135,1360]],"score":0.765},{"poly":[[140,1313],[1084,1313],[1084,1335],[140,1335]],"score":0.8352},{"poly":[[140,1275],[333,1275],[333,1297],[140,1297]],"score":0.8922},{"poly":[[138,1229],[846,1229],[846,1251],[138,1251]],"score":0.7996},{"poly":[[138,1204],[1084,1204],[1084,1228],[138,1228]],"score":0.7214},{"poly":[[138,1181],[1084,1181],[1084,1204],[138,1204]],"score":0.7003},{"poly":[[135,1153],[1086,1155],[1086,1183],[135,1181]],"score":0.6539},{"poly":[[138,1134],[1084,1134],[1084,1155],[138,1155]],"score":0.8347},{"poly":[[138,1110],[1084,1110],[1084,1132],[138,1132]],"score":0.829},{"poly":[[138,1086],[1084,1086],[1084,1107],[138,1107]],"score":0.8096},{"poly":[[138,1061],[1084,1061],[1084,1082],[138,1082]],"score":0.8178},{"poly":[[135,1035],[1084,1031],[1084,1059],[135,1063]],"score":0.6715},{"poly":[[135,998],[830,1000],[830,1028],[135,1026]],"score":0.6619},{"poly":[[138,978],[1084,978],[1084,1000],[138,1000]],"score":0.8392},{"poly":[[138,954],[1083,954],[1083,975],[138,975]],"score":0.8439},{"poly":[[140,914],[364,914],[364,937],[140,937]],"score":0.8601},{"poly":[[135,866],[331,868],[331,896],[135,894]],"score":0.8256},{"poly":[[140,820],[863,820],[863,842],[140,842]],"score":0.8319},{"poly":[[138,797],[1084,797],[1084,818],[138,818]],"score":0.8059},{"poly":[[138,746],[1084,746],[1084,767],[138,767]],"score":0.8122},{"poly":[[138,721],[1084,721],[1084,742],[138,742]],"score":0.8275},{"poly":[[136,696],[1086,698],[1086,721],[136,719]],"score":0.7251},{"poly":[[138,675],[1084,675],[1084,696],[138,696]],"score":0.8251},{"poly":[[140,650],[1084,650],[1084,672],[140,672]],"score":0.8683},{"poly":[[138,612],[459,612],[459,634],[138,634]],"score":0.9205},{"poly":[[211,546],[1008,548],[1008,571],[211,569]],"score":0.7647},{"poly":[[595,508],[643,512],[640,537],[593,532]],"score":0.8135},{"poly":[[677,495],[713,495],[713,515],[677,515]],"score":0.842},{"poly":[[469,495],[506,495],[506,515],[469,515]],"score":0.9145},{"poly":[[376,497],[391,497],[391,512],[376,512]],"score":0.888},{"poly":[[778,492],[818,492],[818,516],[778,516]],"score":0.7993},{"poly":[[572,494],[608,489],[611,511],[574,515]],"score":0.7818},{"poly":[[334,472],[364,472],[364,492],[334,492]],"score":0.8229},{"poly":[[727,465],[868,467],[868,485],[727,483]],"score":0.7657},{"poly":[[727,449],[860,450],[860,469],[727,467]],"score":0.8075},{"poly":[[334,419],[361,419],[361,439],[334,439]],"score":0.7931},{"poly":[[334,365],[359,365],[359,386],[334,386]],"score":0.7711},{"poly":[[334,314],[361,314],[361,333],[334,333]],"score":0.7341},{"poly":[[314,305],[338,305],[338,426],[314,426]],"score":0.8085},{"poly":[[758,259],[852,277],[847,305],[753,287]],"score":0.6699},{"poly":[[729,265],[765,270],[761,295],[725,289]],"score":0.6703},{"poly":[[707,259],[738,265],[734,283],[704,277]],"score":0.7579},{"poly":[[334,261],[363,261],[363,280],[334,280]],"score":0.675},{"poly":[[482,257],[526,257],[526,282],[482,282]],"score":0.6586}],"page_no":7,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":600,"x1":623,"y0":1527,"y1":1552},"conf":0.3715,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":620,"y0":1531,"y1":1549},"font_size":0.0,"text":"9"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":344,"x1":879,"y0":196,"y1":224},"conf":0.889,"font_size":0.0,"label":"Table caption","label_id":6,"lines":[{"bbox":{"x0":349,"x1":873,"y0":200,"y1":221},"font_size":0.0,"text":"Table 1 Main results of progressive techniques applied to DAPO"}],"source":"layout det","text":"Table 1 Main results of progressive techniques applied to DAPO"},{"bbox":{"x0":388,"x1":827,"y0":236,"y1":471},"conf":0.9806,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":403,"x1":463,"y0":246,"y1":270},"font_size":0.0,"text":"Model"},{"bbox":{"x0":688,"x1":814,"y0":243,"y1":275},"font_size":0.0,"text":"AIME24avg@32"},{"bbox":{"x0":404,"x1":671,"y0":280,"y1":303},"font_size":0.0,"text":"DeepSeek-R1-Zero-Qwen-32B"},{"bbox":{"x0":737,"x1":766,"y0":279,"y1":306},"font_size":0.0,"text":"47"},{"bbox":{"x0":405,"x1":524,"y0":315,"y1":337},"font_size":0.0,"text":"Naive GRPO"},{"bbox":{"x0":737,"x1":766,"y0":313,"y1":339},"font_size":0.0,"text":"30"},{"bbox":{"x0":403,"x1":594,"y0":336,"y1":365},"font_size":0.0,"text":"+ Overlong Filtering"},{"bbox":{"x0":736,"x1":766,"y0":337,"y1":363},"font_size":0.0,"text":"36"},{"bbox":{"x0":404,"x1":533,"y0":361,"y1":388},"font_size":0.0,"text":"+ Clip-Higher"},{"bbox":{"x0":736,"x1":766,"y0":362,"y1":387},"font_size":0.0,"text":"38"},{"bbox":{"x0":405,"x1":663,"y0":386,"y1":410},"font_size":0.0,"text":"+ Soft Overlong Punishment"},{"bbox":{"x0":738,"x1":764,"y0":387,"y1":409},"font_size":0.0,"text":"41"},{"bbox":{"x0":406,"x1":574,"y0":411,"y1":433},"font_size":0.0,"text":"+ Token-level Loss"},{"bbox":{"x0":738,"x1":765,"y0":410,"y1":433},"font_size":0.0,"text":"42"},{"bbox":{"x0":406,"x1":666,"y0":434,"y1":459},"font_size":0.0,"text":"+ Dynamic Sampling (DAPO)"},{"bbox":{"x0":736,"x1":766,"y0":434,"y1":459},"font_size":0.0,"text":"50"}],"source":"layout det","text":"<html><body><table><tr><td>Model</td><td>AIME24avg@32</td></tr><tr><td>DeepSeek-R1-Zero-Qwen-32B</td><td>47</td></tr><tr><td>Naive GRPO</td><td>30</td></tr><tr><td>+ Overlong Filtering</td><td>36</td></tr><tr><td>+ Clip-Higher</td><td>38</td></tr><tr><td>+ Soft Overlong Punishment</td><td>41</td></tr><tr><td>+ Token-level Loss</td><td>42</td></tr><tr><td>+ Dynamic Sampling (DAPO)</td><td>50</td></tr></table></body></html>"},{"bbox":{"x0":133,"x1":1089,"y0":504,"y1":558},"conf":0.9136,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":507,"y1":531},"font_size":0.0,"text":"several accuracy points in AIME 2024. Notably, given the vanilla GRPO setting, only $30\\%$ accuracy can be"},{"bbox":{"x0":136,"x1":605,"y0":530,"y1":554},"font_size":0.0,"text":"reached by training from a Qwen2.5-32B base model."}],"source":"layout det","text":"several accuracy points in AIME 2024. Notably, given the vanilla GRPO setting, only $30\\%$ accuracy can be reached by training from a Qwen2.5-32B base model."},{"bbox":{"x0":134,"x1":1091,"y0":564,"y1":618},"conf":0.9284,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1083,"y0":566,"y1":591},"font_size":0.0,"text":"For token-level loss, although it brings less performance improvement, we fnd it enhances training stabilityi"},{"bbox":{"x0":138,"x1":540,"y0":592,"y1":614},"font_size":0.0,"text":"and makes the length increase more healthily."}],"source":"layout det","text":"For token-level loss, although it brings less performance improvement, we fnd it enhances training stabilityi and makes the length increase more healthily."},{"bbox":{"x0":135,"x1":1090,"y0":624,"y1":726},"conf":0.9559,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":627,"y1":650},"font_size":0.0,"text":"When applying Dynamic Sampling, although more data needs to be sampled due to the fltering out ofi"},{"bbox":{"x0":138,"x1":1084,"y0":650,"y1":675},"font_size":0.0,"text":"zero-gradient data, the overall training time is not signifcantly afected. As shown in Figure 6, although theif"},{"bbox":{"x0":138,"x1":1083,"y0":673,"y1":700},"font_size":0.0,"text":"number of sampling instances increases, the model’s convergence time is even reduced, due to fewer training"},{"bbox":{"x0":138,"x1":271,"y0":701,"y1":723},"font_size":0.0,"text":"steps required."}],"source":"layout det","text":"When applying Dynamic Sampling, although more data needs to be sampled due to the fltering out ofi zero-gradient data, the overall training time is not signifcantly afected. As shown in Figure 6, although theif number of sampling instances increases, the model’s convergence time is even reduced, due to fewer training steps required."},{"bbox":{"x0":135,"x1":403,"y0":742,"y1":775},"conf":0.8989,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":399,"y0":746,"y1":769},"font_size":0.0,"text":"4.3Training Dynamics"}],"source":"layout det","text":"4.3Training Dynamics"},{"bbox":{"x0":135,"x1":1090,"y0":780,"y1":1024},"conf":0.9783,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":784,"y1":805},"font_size":0.0,"text":"Reinforcement learning on large language models is not only a cutting-edge research direction but also"},{"bbox":{"x0":138,"x1":1084,"y0":808,"y1":830},"font_size":0.0,"text":"an intrinsically complex systems engineering challenge, characterized by the interdependence of its various"},{"bbox":{"x0":138,"x1":1081,"y0":833,"y1":855},"font_size":0.0,"text":"subsystems. Modifcations to any single subsystem can propagate through the system, leading to unforeseeni"},{"bbox":{"x0":140,"x1":1083,"y0":856,"y1":878},"font_size":0.0,"text":"consequences due to the intricate interplay among these components. Even seemingly minor changes in initial"},{"bbox":{"x0":140,"x1":1083,"y0":881,"y1":903},"font_size":0.0,"text":"conditions, such as variations in data and hyperparameters, can amplify through iterative reinforcement"},{"bbox":{"x0":140,"x1":1084,"y0":904,"y1":927},"font_size":0.0,"text":"learning processes, yielding substantial deviations in outcomes. This complexity often confronts researchers"},{"bbox":{"x0":141,"x1":1083,"y0":928,"y1":948},"font_size":0.0,"text":"with a dilemma: even after meticulous analysis and well-founded expectations that a modifcation will enhancei"},{"bbox":{"x0":136,"x1":1086,"y0":950,"y1":975},"font_size":0.0,"text":"specifc aspects of the training process, the actual results frequently diverge from the anticipated trajectory.i"},{"bbox":{"x0":138,"x1":1084,"y0":973,"y1":998},"font_size":0.0,"text":"Therefore, monitoring of key intermediate results during experimentation is essential for swiftly identifying"},{"bbox":{"x0":140,"x1":733,"y0":1000,"y1":1023},"font_size":0.0,"text":"the sources of discrepancies and, ultimately, for refning the system.i"}],"source":"layout det","text":"Reinforcement learning on large language models is not only a cutting-edge research direction but also an intrinsically complex systems engineering challenge, characterized by the interdependence of its various subsystems. Modifcations to any single subsystem can propagate through the system, leading to unforeseeni consequences due to the intricate interplay among these components. Even seemingly minor changes in initial conditions, such as variations in data and hyperparameters, can amplify through iterative reinforcement learning processes, yielding substantial deviations in outcomes. This complexity often confronts researchers with a dilemma: even after meticulous analysis and well-founded expectations that a modifcation will enhancei specifc aspects of the training process, the actual results frequently diverge from the anticipated trajectory.i Therefore, monitoring of key intermediate results during experimentation is essential for swiftly identifying the sources of discrepancies and, ultimately, for refning the system.i"},{"bbox":{"x0":183,"x1":1091,"y0":1030,"y1":1206},"conf":0.9737,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":171,"x1":1084,"y0":1035,"y1":1056},"font_size":0.0,"text":"The Length of Generated Responses is a metric closely related to training stability and performance,"},{"bbox":{"x0":188,"x1":1084,"y0":1058,"y1":1079},"font_size":0.0,"text":"as shown in Figure 7a. The increase in length provides the model with a larger space for exploration,"},{"bbox":{"x0":190,"x1":1086,"y0":1084,"y1":1106},"font_size":0.0,"text":"allowing more complex reasoning behaviors to be sampled and gradually reinforced through training."},{"bbox":{"x0":188,"x1":1084,"y0":1105,"y1":1130},"font_size":0.0,"text":"However, it is important to note that length does not always maintain a continuous upward trend during"},{"bbox":{"x0":188,"x1":1083,"y0":1130,"y1":1153},"font_size":0.0,"text":"training. In some considerable periods, it can exhibit a trend of stagnation or even decline, which"},{"bbox":{"x0":188,"x1":1084,"y0":1152,"y1":1178},"font_size":0.0,"text":"has also been demonstrated in [2]. We typically use length in conjunction with validation accuracy as"},{"bbox":{"x0":186,"x1":708,"y0":1178,"y1":1203},"font_size":0.0,"text":"indicators to assess whether an experiment is deteriorating."}],"source":"layout det","text":"The Length of Generated Responses is a metric closely related to training stability and performance,as shown in Figure 7a. The increase in length provides the model with a larger space for exploration,allowing more complex reasoning behaviors to be sampled and gradually reinforced through training.However, it is important to note that length does not always maintain a continuous upward trend during training. In some considerable periods, it can exhibit a trend of stagnation or even decline, which has also been demonstrated in [2]. We typically use length in conjunction with validation accuracy as indicators to assess whether an experiment is deteriorating."},{"bbox":{"x0":183,"x1":1091,"y0":1210,"y1":1384},"conf":0.9757,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1084,"y0":1216,"y1":1238},"font_size":0.0,"text":"The Dynamics of Reward during training has always been one of the crucial monitoring indicators"},{"bbox":{"x0":188,"x1":1084,"y0":1239,"y1":1261},"font_size":0.0,"text":"in reinforcement learning, as shown in Figure 7b. In the majority of our experiments, the trend of"},{"bbox":{"x0":188,"x1":1084,"y0":1263,"y1":1285},"font_size":0.0,"text":"reward increase is relatively stable and does not fuctuate or decline signifcantly due to adjustments inli"},{"bbox":{"x0":188,"x1":1081,"y0":1287,"y1":1310},"font_size":0.0,"text":"experimental settings. This indicates that, given a reliable reward signal, language models can robustly"},{"bbox":{"x0":190,"x1":1083,"y0":1310,"y1":1333},"font_size":0.0,"text":"ft the distribution of training set. However, we fnd that the fnal reward on the training set ofteniii"},{"bbox":{"x0":190,"x1":1083,"y0":1335,"y1":1356},"font_size":0.0,"text":"exhibits little correlation with the accuracy on the validation set, which indicates overftting to thei"},{"bbox":{"x0":191,"x1":299,"y0":1361,"y1":1379},"font_size":0.0,"text":"training set."}],"source":"layout det","text":"The Dynamics of Reward during training has always been one of the crucial monitoring indicators in reinforcement learning, as shown in Figure 7b. In the majority of our experiments, the trend of reward increase is relatively stable and does not fuctuate or decline signifcantly due to adjustments inli experimental settings. This indicates that, given a reliable reward signal, language models can robustly ft the distribution of training set. However, we fnd that the fnal reward on the training set ofteniii exhibits little correlation with the accuracy on the validation set, which indicates overftting to thei training set."},{"bbox":{"x0":183,"x1":1090,"y0":1390,"y1":1493},"conf":0.9555,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1084,"y0":1394,"y1":1416},"font_size":0.0,"text":"The Entropy of the Actor Model and Generation Probability are related to the model’s exploration"},{"bbox":{"x0":188,"x1":1084,"y0":1417,"y1":1440},"font_size":0.0,"text":"capability and are key metrics that we closely monitor in our experiments. Intuitively, the model’s"},{"bbox":{"x0":190,"x1":1084,"y0":1442,"y1":1465},"font_size":0.0,"text":"entropy needs to be maintained within an appropriate range. An excessively low entropy indicates that"},{"bbox":{"x0":190,"x1":1084,"y0":1467,"y1":1488},"font_size":0.0,"text":"the probability distribution is overly sharp, leading to a loss of exploration capability. Conversely, an"}],"source":"layout det","text":"The Entropy of the Actor Model and Generation Probability are related to the model’s exploration capability and are key metrics that we closely monitor in our experiments. Intuitively, the model’s entropy needs to be maintained within an appropriate range. An excessively low entropy indicates that the probability distribution is overly sharp, leading to a loss of exploration capability. Conversely, an"}],"formula_dets":[{"bbox":{"x0":897,"x1":936,"y0":507,"y1":529},"conf":0.8171,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":388,"x1":827,"y0":236,"y1":471},"conf":0.9806,"label":"Table","label_id":5},{"bbox":{"x0":135,"x1":1090,"y0":780,"y1":1024},"conf":0.9783,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":1091,"y0":1210,"y1":1384},"conf":0.9757,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":1091,"y0":1030,"y1":1206},"conf":0.9737,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1090,"y0":624,"y1":726},"conf":0.9559,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":1090,"y0":1390,"y1":1493},"conf":0.9555,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1091,"y0":564,"y1":618},"conf":0.9284,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1089,"y0":504,"y1":558},"conf":0.9136,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":403,"y0":742,"y1":775},"conf":0.8989,"label":"Title","label_id":0},{"bbox":{"x0":344,"x1":879,"y0":196,"y1":224},"conf":0.889,"label":"Table caption","label_id":6},{"bbox":{"x0":600,"x1":623,"y0":1527,"y1":1552},"conf":0.3715,"label":"Abandon","label_id":2},{"bbox":{"x0":602,"x1":621,"y0":1529,"y1":1551},"conf":0.3422,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1531],[620,1531],[620,1549],[604,1549]],"score":0.7934},{"poly":[[190,1467],[1084,1467],[1084,1488],[190,1488]],"score":0.8178},{"poly":[[190,1442],[1084,1442],[1084,1465],[190,1465]],"score":0.6801},{"poly":[[188,1419],[1084,1417],[1084,1439],[188,1440]],"score":0.8411},{"poly":[[170,1394],[1084,1394],[1084,1416],[170,1416]],"score":0.7957},{"poly":[[191,1361],[299,1361],[299,1379],[191,1379]],"score":0.8348},{"poly":[[190,1335],[1083,1335],[1083,1356],[190,1356]],"score":0.7705},{"poly":[[190,1310],[1083,1310],[1083,1333],[190,1333]],"score":0.6489},{"poly":[[188,1287],[1081,1287],[1081,1310],[188,1310]],"score":0.65},{"poly":[[188,1264],[1084,1264],[1084,1285],[188,1285]],"score":0.7812},{"poly":[[188,1239],[1084,1239],[1084,1261],[188,1261]],"score":0.7746},{"poly":[[175,1216],[1084,1216],[1084,1238],[175,1238]],"score":0.7768},{"poly":[[186,1178],[708,1180],[708,1203],[186,1201]],"score":0.7324},{"poly":[[188,1152],[1084,1155],[1084,1178],[188,1175]],"score":0.7289},{"poly":[[188,1132],[1083,1130],[1083,1152],[188,1153]],"score":0.8269},{"poly":[[188,1105],[1084,1107],[1084,1130],[188,1129]],"score":0.6963},{"poly":[[190,1084],[1086,1084],[1086,1106],[190,1106]],"score":0.7799},{"poly":[[188,1058],[1084,1058],[1084,1079],[188,1079]],"score":0.7542},{"poly":[[171,1035],[1084,1035],[1084,1056],[171,1056]],"score":0.8199},{"poly":[[140,1002],[733,1002],[733,1023],[140,1023]],"score":0.8138},{"poly":[[138,973],[1084,975],[1084,998],[138,997]],"score":0.7448},{"poly":[[136,952],[1086,950],[1086,973],[136,975]],"score":0.6716},{"poly":[[141,931],[1083,931],[1083,947],[141,947]],"score":0.9835},{"poly":[[140,904],[1084,904],[1084,927],[140,927]],"score":0.6587},{"poly":[[140,881],[1083,881],[1083,903],[140,903]],"score":0.8051},{"poly":[[140,856],[1083,856],[1083,878],[140,878]],"score":0.7977},{"poly":[[138,833],[1081,833],[1081,855],[138,855]],"score":0.7949},{"poly":[[138,808],[1084,808],[1084,830],[138,830]],"score":0.8141},{"poly":[[140,784],[1084,784],[1084,805],[140,805]],"score":0.8292},{"poly":[[140,746],[399,746],[399,769],[140,769]],"score":0.7963},{"poly":[[138,701],[271,701],[271,723],[138,723]],"score":0.8208},{"poly":[[138,673],[1083,677],[1083,700],[138,696]],"score":0.6982},{"poly":[[138,652],[1084,650],[1084,673],[138,675]],"score":0.7169},{"poly":[[140,627],[1084,627],[1084,650],[140,650]],"score":0.711},{"poly":[[138,592],[540,592],[540,614],[138,614]],"score":0.8316},{"poly":[[138,566],[1083,568],[1083,591],[138,589]],"score":0.7763},{"poly":[[136,531],[605,530],[605,553],[136,554]],"score":0.7503},{"poly":[[138,508],[1084,508],[1084,531],[138,531]],"score":0.7072},{"poly":[[406,437],[667,437],[667,459],[406,459]],"score":0.8726},{"poly":[[737,437],[765,437],[765,459],[737,459]],"score":0.8769},{"poly":[[406,412],[574,412],[574,434],[406,434]],"score":0.8998},{"poly":[[738,412],[765,412],[765,434],[738,434]],"score":0.9361},{"poly":[[404,386],[660,388],[660,411],[404,409]],"score":0.7917},{"poly":[[738,388],[763,388],[763,411],[738,411]],"score":0.7868},{"poly":[[406,365],[534,365],[534,388],[406,388]],"score":0.7615},{"poly":[[737,365],[765,365],[765,386],[737,386]],"score":0.8852},{"poly":[[404,338],[592,342],[592,365],[404,361]],"score":0.7479},{"poly":[[737,340],[765,340],[765,361],[737,361]],"score":0.9334},{"poly":[[737,317],[765,317],[765,338],[737,338]],"score":0.8859},{"poly":[[402,317],[525,313],[526,336],[403,340]],"score":0.7863},{"poly":[[404,284],[670,284],[670,305],[404,305]],"score":0.8425},{"poly":[[738,282],[765,282],[765,305],[738,305]],"score":0.8789},{"poly":[[690,244],[814,253],[812,276],[688,267]],"score":0.7021},{"poly":[[404,251],[462,251],[462,269],[404,269]],"score":0.9633},{"poly":[[349,200],[873,200],[873,221],[349,221]],"score":0.8234}],"page_no":8,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":594,"x1":629,"y0":1525,"y1":1554},"conf":0.6157,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":595,"x1":629,"y0":1526,"y1":1553},"font_size":0.0,"text":"10"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":139,"x1":1085,"y0":199,"y1":839},"conf":0.9849,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![5bdab7ae42ec5ebfca5430d963d8be7e](imgs/5bdab7ae42ec5ebfca5430d963d8be7e.jpg)"},{"bbox":{"x0":131,"x1":1093,"y0":848,"y1":904},"conf":0.9424,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":851,"y1":874},"font_size":0.0,"text":"Figure 7 The metric curves of response length, reward score, generation entropy, and the mean probability of DAPO,"},{"bbox":{"x0":138,"x1":1051,"y0":874,"y1":896},"font_size":0.0,"text":"which show the dynamics of RL training and serve as essential monitoring indicators to identify potential issues."}],"source":"layout det","text":"Figure 7 The metric curves of response length, reward score, generation entropy, and the mean probability of DAPO,which show the dynamics of RL training and serve as essential monitoring indicators to identify potential issues."},{"bbox":{"x0":182,"x1":1092,"y0":934,"y1":1066},"conf":0.9606,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":1084,"y0":940,"y1":962},"font_size":0.0,"text":"excessively high entropy is often associated with issues of over-exploration such as gibberish and repetitive"},{"bbox":{"x0":188,"x1":1084,"y0":964,"y1":987},"font_size":0.0,"text":"generation. For the generation probability, the situation is exactly the opposite. As demonstrated in"},{"bbox":{"x0":188,"x1":1086,"y0":987,"y1":1009},"font_size":0.0,"text":"Section 3.1, by applying the Clip-Higher strategy, we efectively addressed the issue of entropy collapse.f"},{"bbox":{"x0":188,"x1":1086,"y0":1011,"y1":1035},"font_size":0.0,"text":"In subsequent experiments, we fnd that maintaining a slow upward trend in entropy is conducive to thei"},{"bbox":{"x0":186,"x1":813,"y0":1036,"y1":1058},"font_size":0.0,"text":"improvement of model performance, shown in Figure 7c and Figure 7d."}],"source":"layout det","text":"excessively high entropy is often associated with issues of over-exploration such as gibberish and repetitive generation. For the generation probability, the situation is exactly the opposite. As demonstrated in Section 3.1, by applying the Clip-Higher strategy, we efectively addressed the issue of entropy collapse.f In subsequent experiments, we fnd that maintaining a slow upward trend in entropy is conducive to thei improvement of model performance, shown in Figure 7c and Figure 7d."},{"bbox":{"x0":133,"x1":329,"y0":1077,"y1":1112},"conf":0.8875,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":325,"y0":1076,"y1":1107},"font_size":0.0,"text":"4.4Case Study"}],"source":"layout det","text":"4.4Case Study"},{"bbox":{"x0":132,"x1":1092,"y0":1117,"y1":1244},"conf":0.9642,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1084,"y0":1117,"y1":1142},"font_size":0.0,"text":"During the RL training process, we observe an interesting phenomenon: the reasoning patterns of the actor"},{"bbox":{"x0":136,"x1":1084,"y0":1142,"y1":1167},"font_size":0.0,"text":"model evolve dynamically over time. Specifcally, the algorithm not only reinforces existing reasoning patternsi"},{"bbox":{"x0":138,"x1":1084,"y0":1168,"y1":1190},"font_size":0.0,"text":"that facilitate correct problem-solving but also gradually gives rise to entirely new modes of reasoning that"},{"bbox":{"x0":138,"x1":1084,"y0":1193,"y1":1214},"font_size":0.0,"text":"were initially absent. This fnding reveals the adaptability and exploration capability of RL algorithms andi"},{"bbox":{"x0":138,"x1":684,"y0":1216,"y1":1238},"font_size":0.0,"text":"ofers new insights into the learning mechanisms of the model.f"}],"source":"layout det","text":"During the RL training process, we observe an interesting phenomenon: the reasoning patterns of the actor model evolve dynamically over time. Specifcally, the algorithm not only reinforces existing reasoning patternsi that facilitate correct problem-solving but also gradually gives rise to entirely new modes of reasoning that were initially absent. This fnding reveals the adaptability and exploration capability of RL algorithms andi ofers new insights into the learning mechanisms of the model.f"},{"bbox":{"x0":133,"x1":1092,"y0":1248,"y1":1353},"conf":0.9572,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1086,"y0":1247,"y1":1277},"font_size":0.0,"text":"For example, in the early stages of model training, there was virtually no occurrence of checking and refectingl"},{"bbox":{"x0":135,"x1":1084,"y0":1274,"y1":1299},"font_size":0.0,"text":"on previous reasoning steps. However, as training progresses, the model exhibits distinct behaviors of refectionl"},{"bbox":{"x0":135,"x1":1086,"y0":1294,"y1":1325},"font_size":0.0,"text":"and backtracking, as shown in Table 2. This observation sheds light on further exploration into interpreting"},{"bbox":{"x0":138,"x1":861,"y0":1323,"y1":1345},"font_size":0.0,"text":"the emergence of reasoning abilities during RL, which we leave for future research."}],"source":"layout det","text":"For example, in the early stages of model training, there was virtually no occurrence of checking and refectingl on previous reasoning steps. However, as training progresses, the model exhibits distinct behaviors of refectionl and backtracking, as shown in Table 2. This observation sheds light on further exploration into interpreting the emergence of reasoning abilities during RL, which we leave for future research."},{"bbox":{"x0":133,"x1":315,"y0":1369,"y1":1403},"conf":0.8816,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":135,"x1":309,"y0":1371,"y1":1399},"font_size":0.0,"text":"5Conclusion"}],"source":"layout det","text":"5Conclusion"},{"bbox":{"x0":133,"x1":1092,"y0":1415,"y1":1499},"conf":0.9364,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":1419,"y1":1442},"font_size":0.0,"text":"In this paper, we release a fully open-sourced system for large-scale LLM RL, including algorithm, code"},{"bbox":{"x0":138,"x1":1084,"y0":1442,"y1":1465},"font_size":0.0,"text":"infrastructure, and dataset. The system achieves state-of-the-art large-scale LLM RL performance (AIME"},{"bbox":{"x0":136,"x1":1084,"y0":1465,"y1":1490},"font_size":0.0,"text":"50 using Qwen-32B pretrained model).We propose the Decoupled Clip and Dynamic sAmpling Policy"}],"source":"layout det","text":"In this paper, we release a fully open-sourced system for large-scale LLM RL, including algorithm, code infrastructure, and dataset. The system achieves state-of-the-art large-scale LLM RL performance (AIME 50 using Qwen-32B pretrained model).We propose the Decoupled Clip and Dynamic sAmpling Policy"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":139,"x1":1085,"y0":199,"y1":839},"conf":0.9849,"label":"Figure","label_id":3},{"bbox":{"x0":132,"x1":1092,"y0":1117,"y1":1244},"conf":0.9642,"label":"Text","label_id":1},{"bbox":{"x0":182,"x1":1092,"y0":934,"y1":1066},"conf":0.9606,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1092,"y0":1248,"y1":1353},"conf":0.9572,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1093,"y0":848,"y1":904},"conf":0.9424,"label":"Figure caption","label_id":4},{"bbox":{"x0":133,"x1":1092,"y0":1415,"y1":1499},"conf":0.9364,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":329,"y0":1077,"y1":1112},"conf":0.8875,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":315,"y0":1369,"y1":1403},"conf":0.8816,"label":"Title","label_id":0},{"bbox":{"x0":594,"x1":629,"y0":1525,"y1":1554},"conf":0.6157,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[595,1526],[629,1526],[629,1553],[595,1553]],"score":0.8551},{"poly":[[136,1465],[1084,1467],[1084,1490],[136,1488]],"score":0.7845},{"poly":[[138,1442],[1084,1442],[1084,1465],[138,1465]],"score":0.7449},{"poly":[[138,1419],[1084,1419],[1084,1442],[138,1442]],"score":0.7372},{"poly":[[135,1371],[309,1371],[309,1399],[135,1399]],"score":0.7603},{"poly":[[138,1323],[861,1323],[861,1345],[138,1345]],"score":0.8848},{"poly":[[135,1294],[1086,1297],[1086,1325],[135,1322]],"score":0.6502},{"poly":[[135,1275],[1084,1274],[1084,1297],[135,1299]],"score":0.785},{"poly":[[135,1247],[1086,1249],[1086,1277],[135,1275]],"score":0.6663},{"poly":[[138,1216],[684,1216],[684,1238],[138,1238]],"score":0.8312},{"poly":[[138,1193],[1084,1193],[1084,1214],[138,1214]],"score":0.8369},{"poly":[[138,1168],[1084,1168],[1084,1190],[138,1190]],"score":0.8128},{"poly":[[136,1142],[1084,1143],[1084,1167],[136,1165]],"score":0.743},{"poly":[[136,1117],[1084,1119],[1084,1142],[136,1140]],"score":0.7807},{"poly":[[185,1076],[325,1079],[324,1107],[184,1104]],"score":0.7616},{"poly":[[138,1081],[198,1081],[198,1104],[138,1104]],"score":0.871},{"poly":[[186,1036],[813,1036],[813,1058],[186,1058]],"score":0.8502},{"poly":[[188,1011],[1086,1011],[1086,1035],[188,1035]],"score":0.733},{"poly":[[188,987],[1086,987],[1086,1008],[188,1008]],"score":0.804},{"poly":[[188,964],[1084,964],[1084,987],[188,987]],"score":0.6984},{"poly":[[188,940],[1084,940],[1084,962],[188,962]],"score":0.8416},{"poly":[[138,874],[1051,874],[1051,896],[138,896]],"score":0.8382},{"poly":[[138,851],[1084,851],[1084,874],[138,874]],"score":0.7243},{"poly":[[777,808],[960,810],[959,833],[777,832]],"score":0.8062},{"poly":[[251,808],[451,810],[451,833],[251,832]],"score":0.7716},{"poly":[[837,767],[950,771],[949,794],[836,790]],"score":0.8444},{"poly":[[320,767],[433,771],[432,794],[319,790]],"score":0.8358},{"poly":[[1021,756],[1054,756],[1054,774],[1021,774]],"score":0.9409},{"poly":[[961,756],[994,756],[994,774],[961,774]],"score":0.8856},{"poly":[[900,756],[935,756],[935,774],[900,774]],"score":0.8692},{"poly":[[838,756],[873,756],[873,774],[838,774]],"score":0.863},{"poly":[[777,756],[812,756],[812,774],[777,774]],"score":0.9033},{"poly":[[502,756],[537,756],[537,774],[502,774]],"score":0.8935},{"poly":[[321,756],[356,756],[356,774],[321,774]],"score":0.8119},{"poly":[[258,756],[293,756],[293,774],[258,774]],"score":0.9169},{"poly":[[732,754],[743,765],[733,774],[723,763]],"score":0.7141},{"poly":[[442,754],[479,754],[479,774],[442,774]],"score":0.7482},{"poly":[[382,754],[416,754],[416,774],[382,774]],"score":0.7601},{"poly":[[211,757],[221,757],[221,772],[211,772]],"score":0.84},{"poly":[[165,744],[196,744],[196,764],[165,764]],"score":0.8292},{"poly":[[684,734],[717,734],[717,752],[684,752]],"score":0.9095},{"poly":[[165,718],[196,718],[196,739],[165,739]],"score":0.7993},{"poly":[[684,700],[713,700],[713,718],[684,718]],"score":0.9147},{"poly":[[166,691],[198,691],[198,713],[166,713]],"score":0.7521},{"poly":[[228,681],[263,681],[263,705],[228,705]],"score":0.6825},{"poly":[[166,668],[198,668],[198,686],[166,686]],"score":0.8768},{"poly":[[680,665],[713,665],[713,683],[680,683]],"score":0.856},{"poly":[[414,637],[477,637],[477,662],[414,662]],"score":0.6212},{"poly":[[682,632],[713,632],[713,650],[682,650]],"score":0.8728},{"poly":[[684,597],[713,597],[713,615],[684,615]],"score":0.8646},{"poly":[[165,589],[198,589],[198,609],[165,609]],"score":0.7805},{"poly":[[665,576],[688,576],[688,706],[665,706]],"score":0.6424},{"poly":[[684,563],[715,563],[715,581],[684,581]],"score":0.8907},{"poly":[[165,563],[196,563],[196,584],[165,584]],"score":0.837},{"poly":[[851,544],[866,544],[866,558],[851,558]],"score":0.6015},{"poly":[[165,540],[196,540],[196,559],[165,559]],"score":0.8404},{"poly":[[682,526],[717,526],[717,549],[682,549]],"score":0.7045},{"poly":[[793,492],[945,492],[945,513],[793,513]],"score":0.8675},{"poly":[[243,492],[459,492],[459,515],[243,515]],"score":0.7489},{"poly":[[837,449],[952,452],[951,475],[836,472]],"score":0.7825},{"poly":[[321,449],[434,452],[434,475],[321,472]],"score":0.7758},{"poly":[[1021,437],[1054,437],[1054,455],[1021,455]],"score":0.9554},{"poly":[[963,437],[994,437],[994,455],[963,455]],"score":0.9227},{"poly":[[900,437],[935,437],[935,455],[900,455]],"score":0.8281},{"poly":[[840,437],[871,437],[871,455],[840,455]],"score":0.9368},{"poly":[[778,437],[812,437],[812,455],[778,455]],"score":0.9422},{"poly":[[730,441],[738,441],[738,452],[730,452]],"score":0.7563},{"poly":[[504,437],[537,437],[537,455],[504,455]],"score":0.9172},{"poly":[[444,437],[477,437],[477,455],[444,455]],"score":0.8966},{"poly":[[384,437],[416,437],[416,455],[384,455]],"score":0.8188},{"poly":[[323,437],[356,437],[356,455],[323,455]],"score":0.8787},{"poly":[[263,437],[294,437],[294,455],[263,455]],"score":0.9494},{"poly":[[215,441],[223,441],[223,450],[215,450]],"score":0.6335},{"poly":[[687,408],[717,408],[717,427],[687,427]],"score":0.8021},{"poly":[[687,376],[717,376],[717,394],[687,394]],"score":0.8432},{"poly":[[688,345],[717,345],[717,363],[688,363]],"score":0.8342},{"poly":[[167,341],[199,346],[196,364],[164,359]],"score":0.7609},{"poly":[[690,312],[715,312],[715,332],[690,332]],"score":0.8069},{"poly":[[166,295],[198,295],[198,314],[166,314]],"score":0.8708},{"poly":[[690,279],[717,279],[717,300],[690,300]],"score":0.7542},{"poly":[[667,269],[685,269],[685,375],[667,375]],"score":0.8547},{"poly":[[692,248],[715,248],[715,267],[692,267]],"score":0.8683},{"poly":[[798,248],[834,244],[837,265],[800,270]],"score":0.6517},{"poly":[[168,244],[196,244],[196,264],[168,264]],"score":0.7741},{"poly":[[693,218],[715,218],[715,233],[693,233]],"score":0.8865}],"page_no":9,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1527,"y1":1553},"conf":0.6219,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":595,"x1":627,"y0":1526,"y1":1554},"font_size":0.0,"text":"11"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":181,"x1":277,"y0":205,"y1":228},"conf":0.7905,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":186,"x1":274,"y0":205,"y1":228},"font_size":0.0,"text":"Question:"}],"source":"layout det","text":"Question:"},{"bbox":{"x0":181,"x1":1046,"y0":229,"y1":327},"conf":0.953,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1034,"y0":229,"y1":252},"font_size":0.0,"text":"Given that the base $ABC$ of the tetrahedron $\\boldsymbol{S-ABC}$ is an equilateral triangle, the projection $H$"},{"bbox":{"x0":185,"x1":1032,"y0":251,"y1":275},"font_size":0.0,"text":"of point $A$  on the face $\\mathit{SBC}$ is the orthocenter of $\\triangle SBC,$  the dihedral angle $H-AB-C$ is $30^{\\circ}$"},{"bbox":{"x0":183,"x1":1039,"y0":276,"y1":305},"font_size":0.0,"text":"and $SA=2,$  fnd the volume of the tetrahedron. The answer is in the formi $\\frac{k}{m}.$  so give the value of"},{"bbox":{"x0":187,"x1":245,"y0":306,"y1":323},"font_size":0.0,"text":"$k+m.$"}],"source":"layout det","text":"Given that the base $ABC$ of the tetrahedron $\\boldsymbol{S-ABC}$ is an equilateral triangle, the projection $H$  of point $A$  on the face $\\mathit{SBC}$ is the orthocenter of $\\triangle SBC,$  the dihedral angle $H-AB-C$ is $30^{\\circ}$ and $SA=2,$  fnd the volume of the tetrahedron. The answer is in the formi $\\frac{k}{m}.$  so give the value of $k+m.$"},{"bbox":{"x0":181,"x1":283,"y0":336,"y1":360},"conf":0.5738,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":185,"x1":279,"y0":335,"y1":358},"font_size":0.0,"text":"Response:"}],"source":"layout det","text":"Response:"},{"bbox":{"x0":180,"x1":719,"y0":360,"y1":386},"conf":0.8239,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":713,"y0":360,"y1":383},"font_size":0.0,"text":"Let us consider the given problem: We need to determine ..."}],"source":"layout det","text":"Let us consider the given problem: We need to determine ..."},{"bbox":{"x0":179,"x1":1041,"y0":403,"y1":484},"conf":0.9434,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":1039,"y0":406,"y1":434},"font_size":0.0,"text":"Next, let $S$ have coordinates $(x_{0},y_{0},z_{0}).$ Since $\\it SA\\ =\\ 2,$ the distance formula gives us"},{"bbox":{"x0":188,"x1":1039,"y0":429,"y1":457},"font_size":0.0,"text":"$\\sqrt{(x_{0})^{2}+(y_{0})^{2}+(z_{0})^{2}}=2,$ so we can write $x_{0}^{2}+y_{0}^{2}+z_{0}^{2}=4.$ Now, remember that $H$ is"},{"bbox":{"x0":186,"x1":437,"y0":454,"y1":475},"font_size":0.0,"text":"the orthogonal projection of"}],"source":"layout det","text":"Next, let $S$ have coordinates $(x_{0},y_{0},z_{0}).$ Since $\\it SA\\ =\\ 2,$ the distance formula gives us $\\sqrt{(x_{0})^{2}+(y_{0})^{2}+(z_{0})^{2}}=2,$ so we can write $x_{0}^{2}+y_{0}^{2}+z_{0}^{2}=4.$ Now, remember that $H$ is the orthogonal projection of"},{"bbox":{"x0":182,"x1":210,"y0":394,"y1":405},"conf":0.6407,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":205,"y0":385,"y1":405},"font_size":0.0,"text":"..."}],"source":"layout det","text":"..."},{"bbox":{"x0":180,"x1":1043,"y0":498,"y1":601},"conf":0.9508,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1036,"y0":503,"y1":525},"font_size":0.0,"text":"However, wait a moment, let’s rethink about the dihedral angle involving planes in a more thoughtful"},{"bbox":{"x0":181,"x1":1039,"y0":523,"y1":553},"font_size":0.0,"text":"geometric way. Consider the plane $\\alpha_{1}=ABC,$  which lies entirely on the xy coordinate plane (as"},{"bbox":{"x0":187,"x1":1038,"y0":551,"y1":574},"font_size":0.0,"text":"$z=0\\text{)}$  Let the plane $\\alpha_{2}=SBC.$  The point $A$  projected perpendicularly to plane $\\alpha_{2}$  lands on H."},{"bbox":{"x0":186,"x1":355,"y0":573,"y1":596},"font_size":0.0,"text":"The line $l=AB\\ \\ldots$"}],"source":"layout det","text":"However, wait a moment, let’s rethink about the dihedral angle involving planes in a more thoughtful geometric way. Consider the plane $\\alpha_{1}=ABC,$  which lies entirely on the xy coordinate plane (as $z=0\\text{)}$  Let the plane $\\alpha_{2}=SBC.$  The point $A$  projected perpendicularly to plane $\\alpha_{2}$  lands on H.  The line $l=AB\\ \\ldots$"},{"bbox":{"x0":322,"x1":903,"y0":642,"y1":674},"conf":0.5752,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":324,"x1":898,"y0":642,"y1":672},"font_size":0.0,"text":"Table 2 Emergence of Refective Behavior in Reinforcement Learningl"}],"source":"layout det","text":"Table 2 Emergence of Refective Behavior in Reinforcement Learningl"},{"bbox":{"x0":132,"x1":1093,"y0":708,"y1":811},"conf":0.951,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":713,"y1":734},"font_size":0.0,"text":"Optimization (DAPO) algorithm, and introduce 4 key techniques to make RL powerfully efective and efcientffi"},{"bbox":{"x0":138,"x1":1084,"y0":736,"y1":759},"font_size":0.0,"text":"in the long-CoT RL scenario. Additionally, by open-sourcing the training code and dataset, we provide the"},{"bbox":{"x0":138,"x1":1086,"y0":759,"y1":784},"font_size":0.0,"text":"broader research community and society with practical access to a scalable reinforcement learning solution,"},{"bbox":{"x0":136,"x1":562,"y0":784,"y1":806},"font_size":0.0,"text":"enabling all to beneft from these advancements.i"}],"source":"layout det","text":"Optimization (DAPO) algorithm, and introduce 4 key techniques to make RL powerfully efective and efcientffi in the long-CoT RL scenario. Additionally, by open-sourcing the training code and dataset, we provide the broader research community and society with practical access to a scalable reinforcement learning solution,enabling all to beneft from these advancements.i"}],"formula_dets":[{"bbox":{"x0":605,"x1":775,"y0":432,"y1":456},"conf":0.8888,"label":"print_embedding","label_id":0},{"bbox":{"x0":474,"x1":573,"y0":408,"y1":433},"conf":0.8801,"label":"print_embedding","label_id":0},{"bbox":{"x0":578,"x1":664,"y0":231,"y1":250},"conf":0.8732,"label":"print_embedding","label_id":0},{"bbox":{"x0":496,"x1":597,"y0":528,"y1":549},"conf":0.8705,"label":"print_embedding","label_id":0},{"bbox":{"x0":378,"x1":478,"y0":553,"y1":574},"conf":0.8681,"label":"print_embedding","label_id":0},{"bbox":{"x0":861,"x1":976,"y0":256,"y1":274},"conf":0.8675,"label":"print_embedding","label_id":0},{"bbox":{"x0":188,"x1":441,"y0":431,"y1":456},"conf":0.8665,"label":"print_embedding","label_id":0},{"bbox":{"x0":622,"x1":691,"y0":255,"y1":275},"conf":0.8659,"label":"print_embedding","label_id":0},{"bbox":{"x0":386,"x1":432,"y0":257,"y1":274},"conf":0.8641,"label":"print_embedding","label_id":0},{"bbox":{"x0":225,"x1":294,"y0":281,"y1":299},"conf":0.8427,"label":"print_embedding","label_id":0},{"bbox":{"x0":652,"x1":744,"y0":409,"y1":429},"conf":0.8408,"label":"print_embedding","label_id":0},{"bbox":{"x0":991,"x1":1011,"y0":436,"y1":452},"conf":0.8384,"label":"print_embedding","label_id":0},{"bbox":{"x0":363,"x1":412,"y0":231,"y1":250},"conf":0.8328,"label":"print_embedding","label_id":0},{"bbox":{"x0":578,"x1":594,"y0":556,"y1":571},"conf":0.8147,"label":"print_embedding","label_id":0},{"bbox":{"x0":262,"x1":278,"y0":258,"y1":273},"conf":0.8086,"label":"print_embedding","label_id":0},{"bbox":{"x0":1015,"x1":1034,"y0":234,"y1":249},"conf":0.7853,"label":"print_embedding","label_id":0},{"bbox":{"x0":283,"x1":298,"y0":412,"y1":427},"conf":0.7622,"label":"print_embedding","label_id":0},{"bbox":{"x0":838,"x1":862,"y0":277,"y1":305},"conf":0.7259,"label":"print_embedding","label_id":0},{"bbox":{"x0":187,"x1":244,"y0":553,"y1":573},"conf":0.7254,"label":"print_embedding","label_id":0},{"bbox":{"x0":1001,"x1":1032,"y0":257,"y1":274},"conf":0.7179,"label":"print_embedding","label_id":0},{"bbox":{"x0":906,"x1":928,"y0":561,"y1":574},"conf":0.6954,"label":"print_embedding","label_id":0},{"bbox":{"x0":187,"x1":245,"y0":306,"y1":323},"conf":0.6097,"label":"print_embedding","label_id":0},{"bbox":{"x0":264,"x1":355,"y0":577,"y1":596},"conf":0.5573,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":181,"x1":1046,"y0":229,"y1":327},"conf":0.953,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1093,"y0":708,"y1":811},"conf":0.951,"label":"Text","label_id":1},{"bbox":{"x0":180,"x1":1043,"y0":498,"y1":601},"conf":0.9508,"label":"Text","label_id":1},{"bbox":{"x0":179,"x1":1041,"y0":403,"y1":484},"conf":0.9434,"label":"Text","label_id":1},{"bbox":{"x0":180,"x1":719,"y0":360,"y1":386},"conf":0.8239,"label":"Text","label_id":1},{"bbox":{"x0":181,"x1":277,"y0":205,"y1":228},"conf":0.7905,"label":"Title","label_id":0},{"bbox":{"x0":182,"x1":210,"y0":394,"y1":405},"conf":0.6407,"label":"Text","label_id":1},{"bbox":{"x0":595,"x1":627,"y0":1527,"y1":1553},"conf":0.6219,"label":"Abandon","label_id":2},{"bbox":{"x0":322,"x1":903,"y0":642,"y1":674},"conf":0.5752,"label":"Figure caption","label_id":4},{"bbox":{"x0":181,"x1":283,"y0":336,"y1":360},"conf":0.5738,"label":"Title","label_id":0},{"bbox":{"x0":181,"x1":283,"y0":336,"y1":360},"conf":0.3162,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[595,1526],[627,1526],[627,1554],[595,1554]],"score":0.8091},{"poly":[[136,784],[562,784],[562,805],[136,805]],"score":0.9195},{"poly":[[138,759],[1086,761],[1086,784],[138,782]],"score":0.8182},{"poly":[[138,736],[1084,736],[1084,759],[138,759]],"score":0.7922},{"poly":[[140,713],[1084,713],[1084,734],[140,734]],"score":0.9165},{"poly":[[324,642],[898,644],[898,672],[324,670]],"score":0.6778},{"poly":[[186,573],[353,573],[353,596],[186,596]],"score":0.7744},{"poly":[[186,551],[1038,551],[1038,574],[186,574]],"score":0.7938},{"poly":[[181,525],[1039,523],[1039,551],[181,553]],"score":0.7005},{"poly":[[186,503],[1036,503],[1036,525],[186,525]],"score":0.942},{"poly":[[186,454],[437,454],[437,475],[186,475]],"score":0.8056},{"poly":[[186,429],[1039,429],[1039,457],[186,457]],"score":0.7167},{"poly":[[185,406],[1039,406],[1039,434],[185,434]],"score":0.7334},{"poly":[[186,360],[713,360],[713,383],[186,383]],"score":0.809},{"poly":[[185,335],[279,335],[279,358],[185,358]],"score":0.8518},{"poly":[[184,298],[252,302],[250,329],[182,325]],"score":0.7756},{"poly":[[183,276],[1039,276],[1039,304],[183,304]],"score":0.6574},{"poly":[[185,251],[1034,251],[1034,274],[185,274]],"score":0.7181},{"poly":[[186,229],[1036,229],[1036,252],[186,252]],"score":0.7916},{"poly":[[186,205],[274,205],[274,228],[186,228]],"score":0.8459}],"page_no":10,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":628,"y0":1526,"y1":1553},"conf":0.7194,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":597,"x1":629,"y0":1526,"y1":1554},"font_size":0.0,"text":"12"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":135,"x1":310,"y0":194,"y1":228},"conf":0.863,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":141,"x1":304,"y0":200,"y1":223},"font_size":0.0,"text":"Contributions"}],"source":"layout det","text":"Contributions"},{"bbox":{"x0":135,"x1":261,"y0":243,"y1":273},"conf":0.8431,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":256,"y0":242,"y1":271},"font_size":0.0,"text":"Project Lead"}],"source":"layout det","text":"Project Lead"},{"bbox":{"x0":135,"x1":271,"y0":276,"y1":309},"conf":0.8313,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":134,"x1":265,"y0":272,"y1":309},"font_size":0.0,"text":"Qiying $\\mathrm{Yu}^{1,2,4}$"}],"source":"layout det","text":"Qiying $\\mathrm{Yu}^{1,2,4}$"},{"bbox":{"x0":135,"x1":233,"y0":315,"y1":345},"conf":0.7042,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":232,"y0":313,"y1":345},"font_size":0.0,"text":"Algorithm"}],"source":"layout det","text":"Algorithm"},{"bbox":{"x0":134,"x1":883,"y0":348,"y1":382},"conf":0.878,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":877,"y0":345,"y1":381},"font_size":0.0,"text":"Qiying $\\mathrm{Yu}^{1,2,4},$  Zheng Zhang1, Ruofei Zhu1, Yufeng Yuan1, Xiaochen $\\mathrm{Zuo}^{1},$  Yu $\\mathrm{Yue}^{1}$"}],"source":"layout det","text":"Qiying $\\mathrm{Yu}^{1,2,4},$  Zheng Zhang1, Ruofei Zhu1, Yufeng Yuan1, Xiaochen $\\mathrm{Zuo}^{1},$  Yu $\\mathrm{Yue}^{1}$"},{"bbox":{"x0":135,"x1":274,"y0":387,"y1":414},"conf":0.4042,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":273,"y0":389,"y1":411},"font_size":0.0,"text":"Infrastructure∗"}],"source":"layout det","text":"Infrastructure∗"},{"bbox":{"x0":132,"x1":1093,"y0":419,"y1":500},"conf":0.9436,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1086,"y0":419,"y1":449},"font_size":0.0,"text":"Weinan Dai1,2,4, Tiantian Fan1, Gaohong Liu1, Juncai $\\mathrm{Liu}^{1},$  Lingjun Liu1, Xin Liu1, Haibin Lin1, Zhiqi Lin1,"},{"bbox":{"x0":135,"x1":1086,"y0":442,"y1":475},"font_size":0.0,"text":"Bole Ma1, Guangming Sheng1,3, Yuxuan Tong1,2,4, Qiying $\\mathrm{Yu}^{1,2,4},$  Chi Zhang1, Mofan Zhang1, Ru Zhang1,"},{"bbox":{"x0":138,"x1":487,"y0":472,"y1":495},"font_size":0.0,"text":"Wang Zhang1, Hang Zhu1, Jinhua $\\mathrm{Zhu}^{1}$"}],"source":"layout det","text":"Weinan Dai1,2,4, Tiantian Fan1, Gaohong Liu1, Juncai $\\mathrm{Liu}^{1},$  Lingjun Liu1, Xin Liu1, Haibin Lin1, Zhiqi Lin1,Bole Ma1, Guangming Sheng1,3, Yuxuan Tong1,2,4, Qiying $\\mathrm{Yu}^{1,2,4},$  Chi Zhang1, Mofan Zhang1, Ru Zhang1, Wang Zhang1, Hang Zhu1, Jinhua $\\mathrm{Zhu}^{1}$"},{"bbox":{"x0":134,"x1":451,"y0":504,"y1":536},"conf":0.9048,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":446,"y0":507,"y1":531},"font_size":0.0,"text":"∗Last-Name in Alphabetical Order"}],"source":"layout det","text":"∗Last-Name in Alphabetical Order"},{"bbox":{"x0":134,"x1":218,"y0":543,"y1":570},"conf":0.6343,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":216,"y0":544,"y1":568},"font_size":0.0,"text":"Dataset"}],"source":"layout det","text":"Dataset"},{"bbox":{"x0":134,"x1":1090,"y0":576,"y1":631},"conf":0.9322,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":131,"x1":1088,"y0":573,"y1":609},"font_size":0.0,"text":"Jiaze Chen1, Jiangjie Chen1,4, Chengyi Wang1, Hongli $\\mathrm{Yu}^{1,2,4},$  Yuxuan Song1,2,4, Xiangpeng Wei1, Qiying"},{"bbox":{"x0":139,"x1":200,"y0":604,"y1":625},"font_size":0.0,"text":"$\\mathrm{Yu}^{1,2,4}$"}],"source":"layout det","text":"Jiaze Chen1, Jiangjie Chen1,4, Chengyi Wang1, Hongli $\\mathrm{Yu}^{1,2,4},$  Yuxuan Song1,2,4, Xiangpeng Wei1, Qiying $\\mathrm{Yu}^{1,2,4}$"},{"bbox":{"x0":135,"x1":249,"y0":638,"y1":668},"conf":0.8591,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":248,"y0":637,"y1":665},"font_size":0.0,"text":"Supervision"}],"source":"layout det","text":"Supervision"},{"bbox":{"x0":134,"x1":1092,"y0":670,"y1":728},"conf":0.9311,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":133,"x1":1088,"y0":668,"y1":703},"font_size":0.0,"text":"Hao Zhou2,4, Jingjing $\\text{Liu}^{\\text{\\tiny{2}},\\text{\\tiny{4}}},$ Wei-Ying Ma2,4, Ya-Qin Zhang2,4, Lin $\\text{Yan}^{1,4},$  Mu Qiao1,4, Yonghui Wu1,"},{"bbox":{"x0":135,"x1":313,"y0":694,"y1":726},"font_size":0.0,"text":"Mingxuan Wang1,4"}],"source":"layout det","text":"Hao Zhou2,4, Jingjing $\\text{Liu}^{\\text{\\tiny{2}},\\text{\\tiny{4}}},$ Wei-Ying Ma2,4, Ya-Qin Zhang2,4, Lin $\\text{Yan}^{1,4},$  Mu Qiao1,4, Yonghui Wu1,Mingxuan Wang1,4"},{"bbox":{"x0":135,"x1":234,"y0":734,"y1":762},"conf":0.6926,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":134,"x1":231,"y0":732,"y1":761},"font_size":0.0,"text":"Affiliation"}],"source":"layout det","text":"Affiliation"},{"bbox":{"x0":135,"x1":298,"y0":767,"y1":799},"conf":0.9065,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":294,"y0":771,"y1":794},"font_size":0.0,"text":"1ByteDance Seed"}],"source":"layout det","text":"1ByteDance Seed"},{"bbox":{"x0":134,"x1":701,"y0":804,"y1":836},"conf":0.9219,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":697,"y0":802,"y1":833},"font_size":0.0,"text":"2Institute for AI Industry Research (AIR), Tsinghua University"}],"source":"layout det","text":"2Institute for AI Industry Research (AIR), Tsinghua University"},{"bbox":{"x0":135,"x1":415,"y0":839,"y1":871},"conf":0.9159,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":134,"x1":413,"y0":836,"y1":871},"font_size":0.0,"text":"3The University of Hong Kong"}],"source":"layout det","text":"3The University of Hong Kong"},{"bbox":{"x0":135,"x1":569,"y0":875,"y1":907},"conf":0.9031,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":565,"y0":874,"y1":904},"font_size":0.0,"text":"4SIA-Lab of Tsinghua AIR and ByteDance Seed"}],"source":"layout det","text":"4SIA-Lab of Tsinghua AIR and ByteDance Seed"},{"bbox":{"x0":134,"x1":370,"y0":927,"y1":963},"conf":0.8867,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":364,"y0":929,"y1":955},"font_size":0.0,"text":"Acknowledgments"}],"source":"layout det","text":"Acknowledgments"},{"bbox":{"x0":133,"x1":1089,"y0":971,"y1":1029},"conf":0.9253,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":975,"y1":998},"font_size":0.0,"text":"We thank Zhengyin Du, Shengding Hu, Kai Shen, Tianyang Zhan, Zhen Xiao, Renjie Zheng, Li Han, Kaihua"},{"bbox":{"x0":138,"x1":895,"y0":1000,"y1":1023},"font_size":0.0,"text":"Jiang as well as other colleagues at ByteDance for their support for the DAPO project."}],"source":"layout det","text":"We thank Zhengyin Du, Shengding Hu, Kai Shen, Tianyang Zhan, Zhen Xiao, Renjie Zheng, Li Han, Kaihua Jiang as well as other colleagues at ByteDance for their support for the DAPO project."}],"formula_dets":[{"bbox":{"x0":659,"x1":724,"y0":448,"y1":470},"conf":0.8124,"label":"print_embedding","label_id":0},{"bbox":{"x0":204,"x1":270,"y0":351,"y1":375},"conf":0.7971,"label":"print_embedding","label_id":0},{"bbox":{"x0":629,"x1":695,"y0":579,"y1":603},"conf":0.7508,"label":"print_embedding","label_id":0},{"bbox":{"x0":205,"x1":265,"y0":281,"y1":302},"conf":0.7174,"label":"print_embedding","label_id":0},{"bbox":{"x0":139,"x1":200,"y0":604,"y1":625},"conf":0.6723,"label":"print_embedding","label_id":0},{"bbox":{"x0":768,"x1":833,"y0":672,"y1":699},"conf":0.655,"label":"print_embedding","label_id":0},{"bbox":{"x0":833,"x1":877,"y0":353,"y1":374},"conf":0.6004,"label":"print_embedding","label_id":0},{"bbox":{"x0":344,"x1":404,"y0":674,"y1":698},"conf":0.5925,"label":"print_embedding","label_id":0},{"bbox":{"x0":442,"x1":487,"y0":473,"y1":493},"conf":0.5507,"label":"print_embedding","label_id":0},{"bbox":{"x0":748,"x1":797,"y0":351,"y1":376},"conf":0.5162,"label":"print_embedding","label_id":0},{"bbox":{"x0":614,"x1":659,"y0":424,"y1":446},"conf":0.5121,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":132,"x1":1093,"y0":419,"y1":500},"conf":0.9436,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1090,"y0":576,"y1":631},"conf":0.9322,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1092,"y0":670,"y1":728},"conf":0.9311,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1089,"y0":971,"y1":1029},"conf":0.9253,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":701,"y0":804,"y1":836},"conf":0.9219,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":415,"y0":839,"y1":871},"conf":0.9159,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":298,"y0":767,"y1":799},"conf":0.9065,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":451,"y0":504,"y1":536},"conf":0.9048,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":569,"y0":875,"y1":907},"conf":0.9031,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":370,"y0":927,"y1":963},"conf":0.8867,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":883,"y0":348,"y1":382},"conf":0.878,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":310,"y0":194,"y1":228},"conf":0.863,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":249,"y0":638,"y1":668},"conf":0.8591,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":261,"y0":243,"y1":273},"conf":0.8431,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":271,"y0":276,"y1":309},"conf":0.8313,"label":"Text","label_id":1},{"bbox":{"x0":595,"x1":628,"y0":1526,"y1":1553},"conf":0.7194,"label":"Abandon","label_id":2},{"bbox":{"x0":135,"x1":233,"y0":315,"y1":345},"conf":0.7042,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":234,"y0":734,"y1":762},"conf":0.6926,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":218,"y0":543,"y1":570},"conf":0.6343,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":274,"y0":387,"y1":414},"conf":0.4907,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":274,"y0":387,"y1":414},"conf":0.4042,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":218,"y0":543,"y1":570},"conf":0.2531,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[597,1526],[629,1526],[629,1554],[597,1554]],"score":0.8705},{"poly":[[138,1000],[895,1000],[895,1023],[138,1023]],"score":0.8028},{"poly":[[140,975],[1084,975],[1084,998],[140,998]],"score":0.7734},{"poly":[[140,929],[364,931],[364,955],[140,954]],"score":0.7791},{"poly":[[136,874],[565,876],[565,904],[136,903]],"score":0.7463},{"poly":[[135,836],[413,843],[412,871],[134,864]],"score":0.7086},{"poly":[[135,802],[697,805],[697,833],[135,830]],"score":0.7246},{"poly":[[140,771],[294,771],[294,794],[140,794]],"score":0.8983},{"poly":[[135,732],[231,736],[230,761],[134,757]],"score":0.8202},{"poly":[[135,698],[312,694],[313,723],[135,726]],"score":0.7593},{"poly":[[133,668],[1088,670],[1088,703],[133,701]],"score":0.7056},{"poly":[[136,640],[247,637],[248,661],[137,665]],"score":0.7755},{"poly":[[132,601],[205,597],[207,627],[134,631]],"score":0.743},{"poly":[[131,573],[1088,576],[1088,609],[131,606]],"score":0.7011},{"poly":[[136,544],[216,544],[216,568],[136,568]],"score":0.825},{"poly":[[138,507],[446,508],[446,531],[138,530]],"score":0.7982},{"poly":[[138,472],[486,472],[486,495],[138,495]],"score":0.8387},{"poly":[[135,442],[1086,442],[1086,475],[135,475]],"score":0.6536},{"poly":[[135,419],[1086,421],[1086,449],[135,447]],"score":0.7103},{"poly":[[138,389],[273,389],[273,411],[138,411]],"score":0.9748},{"poly":[[135,348],[878,345],[878,378],[135,381]],"score":0.6968},{"poly":[[136,317],[230,313],[232,341],[137,345]],"score":0.8705},{"poly":[[134,279],[268,272],[270,302],[136,309]],"score":0.7841},{"poly":[[136,246],[256,242],[256,267],[137,271]],"score":0.8076},{"poly":[[141,200],[304,200],[304,223],[141,223]],"score":0.8884}],"page_no":11,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1528,"y1":1552},"conf":0.6944,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1530,"y1":1551},"font_size":0.0,"text":"13"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":136,"x1":280,"y0":196,"y1":226},"conf":0.8806,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":276,"y0":201,"y1":223},"font_size":0.0,"text":"References"}],"source":"layout det","text":"References"},{"bbox":{"x0":145,"x1":547,"y0":243,"y1":270},"conf":0.8968,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":148,"x1":542,"y0":244,"y1":269},"font_size":0.0,"text":"[1] OpenAI. Learning to reason with llms, 2024."}],"source":"layout det","text":"[1] OpenAI. Learning to reason with llms, 2024."},{"bbox":{"x0":144,"x1":1089,"y0":277,"y1":347},"conf":0.9419,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":150,"x1":1084,"y0":280,"y1":302},"font_size":0.0,"text":"[2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi"},{"bbox":{"x0":176,"x1":1083,"y0":300,"y1":325},"font_size":0.0,"text":"Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv"},{"bbox":{"x0":176,"x1":444,"y0":322,"y1":347},"font_size":0.0,"text":"preprint arXiv:2501.12948, 2025."}],"source":"layout det","text":"[2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025."},{"bbox":{"x0":144,"x1":772,"y0":355,"y1":382},"conf":0.8868,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":148,"x1":767,"y0":355,"y1":381},"font_size":0.0,"text":"[3] OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023."}],"source":"layout det","text":"[3] OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023."},{"bbox":{"x0":145,"x1":477,"y0":388,"y1":414},"conf":0.8796,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":150,"x1":474,"y0":391,"y1":412},"font_size":0.0,"text":"[4] Anthropic. Claude 3.5 sonnet, 2024."}],"source":"layout det","text":"[4] Anthropic. Claude 3.5 sonnet, 2024."},{"bbox":{"x0":144,"x1":1090,"y0":422,"y1":494},"conf":0.9519,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":146,"x1":1084,"y0":422,"y1":447},"font_size":0.0,"text":"[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind"},{"bbox":{"x0":178,"x1":1086,"y0":447,"y1":469},"font_size":0.0,"text":"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models are few-shot learners."},{"bbox":{"x0":180,"x1":762,"y0":469,"y1":492},"font_size":0.0,"text":"Advances in neural information processing systems, 33:1877–1901, 2020."}],"source":"layout det","text":"[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models are few-shot learners.Advances in neural information processing systems, 33:1877–1901, 2020."},{"bbox":{"x0":143,"x1":1090,"y0":499,"y1":572},"conf":0.9409,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":150,"x1":1083,"y0":502,"y1":523},"font_size":0.0,"text":"[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul"},{"bbox":{"x0":176,"x1":1083,"y0":523,"y1":548},"font_size":0.0,"text":"Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with"},{"bbox":{"x0":178,"x1":755,"y0":548,"y1":569},"font_size":0.0,"text":"pathways. Journal of Machine Learning Research, 24(240):1–113, 2023."}],"source":"layout det","text":"[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023."},{"bbox":{"x0":145,"x1":1089,"y0":578,"y1":628},"conf":0.9297,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":148,"x1":1086,"y0":579,"y1":604},"font_size":0.0,"text":"[7] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,"},{"bbox":{"x0":176,"x1":1029,"y0":601,"y1":625},"font_size":0.0,"text":"Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024."}],"source":"layout det","text":"[7] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024."},{"bbox":{"x0":144,"x1":630,"y0":634,"y1":660},"conf":0.909,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":148,"x1":627,"y0":637,"y1":658},"font_size":0.0,"text":"[8] XAI. Grok 3 beta — the age of reasoning agents, 2024."}],"source":"layout det","text":"[8] XAI. Grok 3 beta — the age of reasoning agents, 2024."},{"bbox":{"x0":141,"x1":602,"y0":667,"y1":693},"conf":0.9033,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":150,"x1":599,"y0":670,"y1":691},"font_size":0.0,"text":"[9] Google DeepMind. Gemini 2.0 fash thinking, 2024.l"}],"source":"layout det","text":"[9] Google DeepMind. Gemini 2.0 fash thinking, 2024.l"},{"bbox":{"x0":139,"x1":762,"y0":701,"y1":728},"conf":0.9054,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":758,"y0":705,"y1":726},"font_size":0.0,"text":"[10] Qwen. Qwq-32b: Embracing the power of reinforcement learning, 2024."}],"source":"layout det","text":"[10] Qwen. Qwq-32b: Embracing the power of reinforcement learning, 2024."},{"bbox":{"x0":135,"x1":1089,"y0":735,"y1":807},"conf":0.9423,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":738,"y1":759},"font_size":0.0,"text":"[11] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,"},{"bbox":{"x0":176,"x1":1084,"y0":757,"y1":784},"font_size":0.0,"text":"Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint"},{"bbox":{"x0":180,"x1":373,"y0":780,"y1":804},"font_size":0.0,"text":"arXiv:2501.12599, 2025."}],"source":"layout det","text":"[11] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025."},{"bbox":{"x0":135,"x1":1090,"y0":813,"y1":863},"conf":0.9194,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":813,"y1":838},"font_size":0.0,"text":"[12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei"},{"bbox":{"x0":178,"x1":936,"y0":838,"y1":860},"font_size":0.0,"text":"Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024."}],"source":"layout det","text":"[12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024."},{"bbox":{"x0":135,"x1":1090,"y0":869,"y1":940},"conf":0.9491,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":871,"y1":894},"font_size":0.0,"text":"[13] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng"},{"bbox":{"x0":178,"x1":1083,"y0":894,"y1":916},"font_size":0.0,"text":"Liu, Xu Miao, Yang Lu, et al. An empirical study on eliciting and improving r1-like reasoning models. arXiv"},{"bbox":{"x0":176,"x1":446,"y0":912,"y1":939},"font_size":0.0,"text":"preprint arXiv:2503.04548, 2025."}],"source":"layout det","text":"[13] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, et al. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548, 2025."},{"bbox":{"x0":135,"x1":1091,"y0":945,"y1":1017},"conf":0.9439,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":949,"y1":970},"font_size":0.0,"text":"[14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasoner-"},{"bbox":{"x0":178,"x1":1083,"y0":972,"y1":993},"font_size":0.0,"text":"zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/"},{"bbox":{"x0":180,"x1":587,"y0":993,"y1":1015},"font_size":0.0,"text":"Open-Reasoner-Zero/Open-Reasoner-Zero, 2025."}],"source":"layout det","text":"[14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasonerzero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025."},{"bbox":{"x0":136,"x1":1090,"y0":1023,"y1":1074},"conf":0.9363,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":1025,"y1":1049},"font_size":0.0,"text":"[15] Jian Hu. Reinforce++: A simple and efcient approach for aligning large language models. arXiv preprintfi"},{"bbox":{"x0":178,"x1":374,"y0":1049,"y1":1071},"font_size":0.0,"text":"arXiv:2501.03262, 2025."}],"source":"layout det","text":"[15] Jian Hu. Reinforce++: A simple and efcient approach for aligning large language models. arXiv preprintfi arXiv:2501.03262, 2025."},{"bbox":{"x0":135,"x1":1091,"y0":1079,"y1":1129},"conf":0.9269,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1086,"y0":1082,"y1":1104},"font_size":0.0,"text":"[16] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu,"},{"bbox":{"x0":176,"x1":1043,"y0":1102,"y1":1127},"font_size":0.0,"text":"Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025."}],"source":"layout det","text":"[16] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu,Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025."},{"bbox":{"x0":134,"x1":1091,"y0":1135,"y1":1206},"conf":0.952,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1084,"y0":1137,"y1":1162},"font_size":0.0,"text":"[17] Jung Hyun Lee, June Yong Yang, Byeongho Heo, Dongyoon Han, and Kang Min Yoo.Token-supervised"},{"bbox":{"x0":178,"x1":1083,"y0":1162,"y1":1183},"font_size":0.0,"text":"value models for enhancing mathematical reasoning capabilities of large language models.arXiv preprint"},{"bbox":{"x0":178,"x1":373,"y0":1180,"y1":1201},"font_size":0.0,"text":"arXiv:2407.12863, 2024."}],"source":"layout det","text":"[17] Jung Hyun Lee, June Yong Yang, Byeongho Heo, Dongyoon Han, and Kang Min Yoo.Token-supervised value models for enhancing mathematical reasoning capabilities of large language models.arXiv preprint arXiv:2407.12863, 2024."},{"bbox":{"x0":136,"x1":1091,"y0":1212,"y1":1285},"conf":0.9496,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1216,"y1":1238},"font_size":0.0,"text":"[18] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville,"},{"bbox":{"x0":176,"x1":1084,"y0":1236,"y1":1261},"font_size":0.0,"text":"and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refned credit assignment. arXivi"},{"bbox":{"x0":178,"x1":444,"y0":1257,"y1":1284},"font_size":0.0,"text":"preprint arXiv:2410.01679, 2024."}],"source":"layout det","text":"[18] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville,and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refned credit assignment. arXivi preprint arXiv:2410.01679, 2024."},{"bbox":{"x0":135,"x1":1091,"y0":1291,"y1":1341},"conf":0.9267,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1294,"y1":1315},"font_size":0.0,"text":"[19] Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. What’s behind ppo’s collapse in long-cot? value"},{"bbox":{"x0":178,"x1":742,"y0":1313,"y1":1338},"font_size":0.0,"text":"optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025."}],"source":"layout det","text":"[19] Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. What’s behind ppo’s collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025."},{"bbox":{"x0":136,"x1":1090,"y0":1347,"y1":1396},"conf":0.9191,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":1084,"y0":1350,"y1":1371},"font_size":0.0,"text":"[20] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and"},{"bbox":{"x0":178,"x1":1013,"y0":1371,"y1":1394},"font_size":0.0,"text":"Chuan Wu. Hybridfow: A fexible and efcient rlhf framework. arXiv preprint arXiv:2409.19256, 2024.llfi"}],"source":"layout det","text":"[20] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridfow: A fexible and efcient rlhf framework. arXiv preprint arXiv:2409.19256, 2024.llfi"},{"bbox":{"x0":136,"x1":1089,"y0":1403,"y1":1452},"conf":0.9247,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1406,"y1":1429},"font_size":0.0,"text":"[21] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization"},{"bbox":{"x0":176,"x1":592,"y0":1426,"y1":1450},"font_size":0.0,"text":"algorithms. arXiv preprint arXiv:1707.06347, 2017."}],"source":"layout det","text":"[21] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":134,"x1":1091,"y0":1135,"y1":1206},"conf":0.952,"label":"Text","label_id":1},{"bbox":{"x0":144,"x1":1090,"y0":422,"y1":494},"conf":0.9519,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1091,"y0":1212,"y1":1285},"conf":0.9496,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1090,"y0":869,"y1":940},"conf":0.9491,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1091,"y0":945,"y1":1017},"conf":0.9439,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1089,"y0":735,"y1":807},"conf":0.9423,"label":"Text","label_id":1},{"bbox":{"x0":144,"x1":1089,"y0":277,"y1":347},"conf":0.9419,"label":"Text","label_id":1},{"bbox":{"x0":143,"x1":1090,"y0":499,"y1":572},"conf":0.9409,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1090,"y0":1023,"y1":1074},"conf":0.9363,"label":"Text","label_id":1},{"bbox":{"x0":145,"x1":1089,"y0":578,"y1":628},"conf":0.9297,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1091,"y0":1079,"y1":1129},"conf":0.9269,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1091,"y0":1291,"y1":1341},"conf":0.9267,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1089,"y0":1403,"y1":1452},"conf":0.9247,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1090,"y0":813,"y1":863},"conf":0.9194,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1090,"y0":1347,"y1":1396},"conf":0.9191,"label":"Text","label_id":1},{"bbox":{"x0":144,"x1":630,"y0":634,"y1":660},"conf":0.909,"label":"Text","label_id":1},{"bbox":{"x0":139,"x1":762,"y0":701,"y1":728},"conf":0.9054,"label":"Text","label_id":1},{"bbox":{"x0":141,"x1":602,"y0":667,"y1":693},"conf":0.9033,"label":"Text","label_id":1},{"bbox":{"x0":145,"x1":547,"y0":243,"y1":270},"conf":0.8968,"label":"Text","label_id":1},{"bbox":{"x0":144,"x1":772,"y0":355,"y1":382},"conf":0.8868,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":280,"y0":196,"y1":226},"conf":0.8806,"label":"Title","label_id":0},{"bbox":{"x0":145,"x1":477,"y0":388,"y1":414},"conf":0.8796,"label":"Text","label_id":1},{"bbox":{"x0":596,"x1":627,"y0":1528,"y1":1552},"conf":0.6944,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1530],[625,1530],[625,1551],[599,1551]],"score":0.9388},{"poly":[[176,1427],[592,1426],[592,1449],[176,1450]],"score":0.7},{"poly":[[140,1406],[1084,1406],[1084,1429],[140,1429]],"score":0.6945},{"poly":[[178,1371],[1013,1371],[1013,1394],[178,1394]],"score":0.6602},{"poly":[[141,1350],[1084,1350],[1084,1371],[141,1371]],"score":0.7778},{"poly":[[178,1315],[742,1313],[742,1336],[178,1338]],"score":0.7277},{"poly":[[140,1294],[1084,1294],[1084,1315],[140,1315]],"score":0.8155},{"poly":[[178,1261],[444,1257],[444,1280],[178,1284]],"score":0.7278},{"poly":[[176,1236],[1084,1238],[1084,1261],[176,1259]],"score":0.7335},{"poly":[[140,1216],[1084,1216],[1084,1238],[140,1238]],"score":0.7717},{"poly":[[178,1180],[373,1180],[373,1201],[178,1201]],"score":0.7473},{"poly":[[178,1162],[1083,1162],[1083,1183],[178,1183]],"score":0.8161},{"poly":[[138,1139],[1084,1137],[1084,1160],[138,1162]],"score":0.7023},{"poly":[[176,1102],[1043,1104],[1043,1127],[176,1125]],"score":0.7574},{"poly":[[140,1082],[1086,1082],[1086,1104],[140,1104]],"score":0.7845},{"poly":[[178,1049],[374,1049],[374,1071],[178,1071]],"score":0.8379},{"poly":[[138,1025],[1084,1026],[1084,1049],[138,1048]],"score":0.7631},{"poly":[[180,993],[587,993],[587,1015],[180,1015]],"score":0.7531},{"poly":[[178,972],[1083,972],[1083,993],[178,993]],"score":0.804},{"poly":[[140,949],[1084,949],[1084,970],[140,970]],"score":0.8074},{"poly":[[176,916],[446,912],[446,935],[176,939]],"score":0.7283},{"poly":[[178,894],[1083,894],[1083,916],[178,916]],"score":0.7866},{"poly":[[140,871],[1084,871],[1084,894],[140,894]],"score":0.6682},{"poly":[[178,838],[936,838],[936,860],[178,860]],"score":0.8138},{"poly":[[138,815],[1084,813],[1084,837],[138,838]],"score":0.7572},{"poly":[[180,782],[372,780],[373,802],[180,804]],"score":0.8301},{"poly":[[176,757],[1084,761],[1084,784],[176,780]],"score":0.6765},{"poly":[[140,738],[1084,738],[1084,759],[140,759]],"score":0.7943},{"poly":[[141,705],[758,705],[758,726],[141,726]],"score":0.8407},{"poly":[[150,670],[599,670],[599,691],[150,691]],"score":0.7906},{"poly":[[148,637],[627,637],[627,658],[148,658]],"score":0.7706},{"poly":[[176,601],[1029,602],[1029,625],[176,624]],"score":0.7477},{"poly":[[148,579],[1086,581],[1086,604],[148,602]],"score":0.6646},{"poly":[[178,548],[755,548],[755,569],[178,569]],"score":0.8231},{"poly":[[176,523],[1083,525],[1083,548],[176,546]],"score":0.6876},{"poly":[[150,502],[1083,502],[1083,523],[150,523]],"score":0.7909},{"poly":[[180,469],[762,469],[762,492],[180,492]],"score":0.7004},{"poly":[[178,447],[1086,447],[1086,469],[178,469]],"score":0.8061},{"poly":[[146,424],[1084,422],[1084,445],[146,447]],"score":0.6809},{"poly":[[150,391],[474,391],[474,412],[150,412]],"score":0.8766},{"poly":[[148,358],[767,355],[767,378],[148,381]],"score":0.6972},{"poly":[[176,323],[444,322],[444,345],[176,347]],"score":0.7136},{"poly":[[176,300],[1083,302],[1083,325],[176,323]],"score":0.7282},{"poly":[[150,280],[1084,280],[1084,302],[150,302]],"score":0.8303},{"poly":[[148,246],[542,244],[542,267],[148,269]],"score":0.7654},{"poly":[[140,201],[276,201],[276,223],[140,223]],"score":0.8567}],"page_no":12,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":597,"x1":626,"y0":1528,"y1":1551},"conf":0.7177,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1530,"y1":1553},"font_size":0.0,"text":"14"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":137,"x1":1089,"y0":201,"y1":250},"conf":0.9214,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1083,"y0":203,"y1":226},"font_size":0.0,"text":"[22] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous"},{"bbox":{"x0":178,"x1":610,"y0":224,"y1":246},"font_size":0.0,"text":"control using generalized advantage estimation, 2018."}],"source":"layout det","text":"[22] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation, 2018."},{"bbox":{"x0":138,"x1":1089,"y0":257,"y1":393},"conf":0.9707,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":257,"y1":282},"font_size":0.0,"text":"[23] Long Ouyang, Jefrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhinif"},{"bbox":{"x0":176,"x1":1084,"y0":280,"y1":305},"font_size":0.0,"text":"Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,"},{"bbox":{"x0":176,"x1":1083,"y0":302,"y1":327},"font_size":0.0,"text":"Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to"},{"bbox":{"x0":176,"x1":1086,"y0":323,"y1":348},"font_size":0.0,"text":"follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,"},{"bbox":{"x0":178,"x1":1084,"y0":348,"y1":371},"font_size":0.0,"text":"editors, Advances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran Associates,"},{"bbox":{"x0":176,"x1":264,"y0":368,"y1":391},"font_size":0.0,"text":"Inc., 2022."}],"source":"layout det","text":"[23] Long Ouyang, Jefrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhinif Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,editors, Advances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran Associates,Inc., 2022."},{"bbox":{"x0":136,"x1":1090,"y0":401,"y1":450},"conf":0.9208,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":404,"y1":426},"font_size":0.0,"text":"[24] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems"},{"bbox":{"x0":178,"x1":323,"y0":426,"y1":447},"font_size":0.0,"text":"in ai safety, 2016."}],"source":"layout det","text":"[24] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety, 2016."},{"bbox":{"x0":136,"x1":1090,"y0":456,"y1":504},"conf":0.9108,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":1084,"y0":459,"y1":480},"font_size":0.0,"text":"[25] Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg. Reinforcement learning with a"},{"bbox":{"x0":178,"x1":441,"y0":482,"y1":503},"font_size":0.0,"text":"corrupted reward channel, 2017."}],"source":"layout det","text":"[25] Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg. Reinforcement learning with a corrupted reward channel, 2017."},{"bbox":{"x0":135,"x1":1088,"y0":512,"y1":561},"conf":0.9306,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":515,"y1":538},"font_size":0.0,"text":"[26] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac"},{"bbox":{"x0":176,"x1":928,"y0":535,"y1":559},"font_size":0.0,"text":"Kenton, Jan Leike, and Shane Legg. Specifcation gaming: the fip side of ai ingenuity, 2020.il"}],"source":"layout det","text":"[26] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specifcation gaming: the fip side of ai ingenuity, 2020.il"},{"bbox":{"x0":135,"x1":1088,"y0":567,"y1":617},"conf":0.6407,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1083,"y0":569,"y1":594},"font_size":0.0,"text":"[27] Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward tampering problems and solutions"},{"bbox":{"x0":176,"x1":763,"y0":594,"y1":615},"font_size":0.0,"text":"in reinforcement learning: A causal infuence diagram perspective, 2021.l"}],"source":"layout det","text":"[27] Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward tampering problems and solutions in reinforcement learning: A causal infuence diagram perspective, 2021.l"},{"bbox":{"x0":229,"x1":970,"y0":624,"y1":651},"conf":0.8538,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":141,"x1":980,"y0":627,"y1":648},"font_size":0.0,"text":"ao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 202"}],"source":"layout det","text":"ao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 202"},{"bbox":{"x0":137,"x1":892,"y0":658,"y1":684},"conf":0.8098,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":888,"y0":662,"y1":683},"font_size":0.0,"text":"[29] Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024."}],"source":"layout det","text":"[29] Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024."},{"bbox":{"x0":137,"x1":1022,"y0":692,"y1":720},"conf":0.6825,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1016,"y0":693,"y1":718},"font_size":0.0,"text":"[30] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020."}],"source":"layout det","text":"[30] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020."},{"bbox":{"x0":135,"x1":1083,"y0":726,"y1":774},"conf":0.9116,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":728,"y1":751},"font_size":0.0,"text":"[31] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human"},{"bbox":{"x0":178,"x1":587,"y0":751,"y1":772},"font_size":0.0,"text":"demonstrations. Nature, 625(7995):476–482, 2024."}],"source":"layout det","text":"[31] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476–482, 2024."},{"bbox":{"x0":229,"x1":969,"y0":780,"y1":808},"conf":0.8268,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":960,"y0":784,"y1":805},"font_size":0.0,"text":"Trinh and Thang Luong. Alphageometry: An olympiad-level ai system for geometry, 2024."}],"source":"layout det","text":"Trinh and Thang Luong. Alphageometry: An olympiad-level ai system for geometry, 2024."},{"bbox":{"x0":133,"x1":1091,"y0":815,"y1":863},"conf":0.5948,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":818,"y1":840},"font_size":0.0,"text":"[33] AlphaProof and AlphaGeometry Teams. Ai achieves silver-medal standard solving international mathematical"},{"bbox":{"x0":180,"x1":386,"y0":843,"y1":860},"font_size":0.0,"text":"olympiad problems, 2024."}],"source":"layout det","text":"[33] AlphaProof and AlphaGeometry Teams. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024."},{"bbox":{"x0":136,"x1":1088,"y0":871,"y1":942},"conf":0.9427,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":874,"y1":896},"font_size":0.0,"text":"[34] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering"},{"bbox":{"x0":178,"x1":1083,"y0":894,"y1":919},"font_size":0.0,"text":"code generation through pretrained models and deep reinforcement learning. Advances in Neural Information"},{"bbox":{"x0":178,"x1":527,"y0":917,"y1":939},"font_size":0.0,"text":"Processing Systems, 35:21314–21328, 2022."}],"source":"layout det","text":"[34] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314–21328, 2022."},{"bbox":{"x0":135,"x1":1091,"y0":948,"y1":997},"conf":0.9177,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1086,"y0":950,"y1":975},"font_size":0.0,"text":"[35] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao."},{"bbox":{"x0":178,"x1":738,"y0":974,"y1":995},"font_size":0.0,"text":"Refexion: Language agents with verbal reinforcement learning, 2023.l"}],"source":"layout det","text":"[35] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.Refexion: Language agents with verbal reinforcement learning, 2023.l"},{"bbox":{"x0":135,"x1":1090,"y0":1004,"y1":1052},"conf":0.9145,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1006,"y1":1030},"font_size":0.0,"text":"[36] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug,"},{"bbox":{"x0":178,"x1":225,"y0":1030,"y1":1051},"font_size":0.0,"text":"2023."}],"source":"layout det","text":"[36] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug,2023."},{"bbox":{"x0":136,"x1":1090,"y0":1060,"y1":1109},"conf":0.9182,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1063,"y1":1086},"font_size":0.0,"text":"[37] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel"},{"bbox":{"x0":178,"x1":941,"y0":1082,"y1":1107},"font_size":0.0,"text":"Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025."}],"source":"layout det","text":"[37] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025."},{"bbox":{"x0":136,"x1":1089,"y0":1116,"y1":1187},"conf":0.9428,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":1119,"y1":1140},"font_size":0.0,"text":"[38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya"},{"bbox":{"x0":176,"x1":1083,"y0":1138,"y1":1163},"font_size":0.0,"text":"Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint"},{"bbox":{"x0":178,"x1":374,"y0":1163,"y1":1185},"font_size":0.0,"text":"arXiv:2402.03300, 2024."}],"source":"layout det","text":"[38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024."},{"bbox":{"x0":136,"x1":1090,"y0":1194,"y1":1243},"conf":0.9086,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1086,"y0":1193,"y1":1223},"font_size":0.0,"text":"[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning"},{"bbox":{"x0":178,"x1":363,"y0":1219,"y1":1241},"font_size":0.0,"text":"Representations, 2019."}],"source":"layout det","text":"[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":138,"x1":1089,"y0":257,"y1":393},"conf":0.9707,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1089,"y0":1116,"y1":1187},"conf":0.9428,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1088,"y0":871,"y1":942},"conf":0.9427,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1088,"y0":512,"y1":561},"conf":0.9306,"label":"Text","label_id":1},{"bbox":{"x0":137,"x1":1089,"y0":201,"y1":250},"conf":0.9214,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1090,"y0":401,"y1":450},"conf":0.9208,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1090,"y0":1060,"y1":1109},"conf":0.9182,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1091,"y0":948,"y1":997},"conf":0.9177,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1090,"y0":1004,"y1":1052},"conf":0.9145,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1083,"y0":726,"y1":774},"conf":0.9116,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1090,"y0":456,"y1":504},"conf":0.9108,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":1090,"y0":1194,"y1":1243},"conf":0.9086,"label":"Text","label_id":1},{"bbox":{"x0":229,"x1":970,"y0":624,"y1":651},"conf":0.8538,"label":"Text","label_id":1},{"bbox":{"x0":229,"x1":969,"y0":780,"y1":808},"conf":0.8268,"label":"Text","label_id":1},{"bbox":{"x0":137,"x1":892,"y0":658,"y1":684},"conf":0.8098,"label":"Text","label_id":1},{"bbox":{"x0":597,"x1":626,"y0":1528,"y1":1551},"conf":0.7177,"label":"Abandon","label_id":2},{"bbox":{"x0":137,"x1":1022,"y0":692,"y1":720},"conf":0.6825,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1088,"y0":567,"y1":617},"conf":0.6407,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1091,"y0":815,"y1":863},"conf":0.5948,"label":"Text","label_id":1},{"bbox":{"x0":328,"x1":1070,"y0":817,"y1":862},"conf":0.3062,"label":"Text","label_id":1},{"bbox":{"x0":328,"x1":1069,"y0":570,"y1":616},"conf":0.2876,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1530],[625,1530],[625,1553],[599,1553]],"score":0.8504},{"poly":[[178,1219],[363,1219],[363,1241],[178,1241]],"score":0.8891},{"poly":[[136,1193],[1086,1195],[1086,1223],[136,1221]],"score":0.6615},{"poly":[[178,1163],[374,1163],[374,1185],[178,1185]],"score":0.7786},{"poly":[[176,1138],[1083,1140],[1083,1163],[176,1162]],"score":0.7538},{"poly":[[140,1119],[1084,1119],[1084,1140],[140,1140]],"score":0.8053},{"poly":[[178,1082],[941,1084],[941,1107],[178,1105]],"score":0.6765},{"poly":[[140,1063],[1084,1063],[1084,1086],[140,1086]],"score":0.6842},{"poly":[[178,1030],[225,1030],[225,1051],[178,1051]],"score":0.8061},{"poly":[[140,1006],[1084,1006],[1084,1030],[140,1030]],"score":0.6665},{"poly":[[178,974],[738,974],[738,995],[178,995]],"score":0.8155},{"poly":[[138,952],[1086,950],[1086,973],[138,975]],"score":0.6536},{"poly":[[178,917],[527,917],[527,939],[178,939]],"score":0.8485},{"poly":[[178,894],[1083,896],[1083,919],[178,917]],"score":0.6868},{"poly":[[140,874],[1084,874],[1084,896],[140,896]],"score":0.8172},{"poly":[[180,843],[386,843],[386,860],[180,860]],"score":0.9824},{"poly":[[140,818],[1084,818],[1084,840],[140,840]],"score":0.8379},{"poly":[[140,784],[960,784],[960,805],[140,805]],"score":0.8327},{"poly":[[178,751],[587,751],[587,772],[178,772]],"score":0.8084},{"poly":[[140,728],[1084,728],[1084,751],[140,751]],"score":0.6954},{"poly":[[138,693],[1016,695],[1016,718],[138,716]],"score":0.7413},{"poly":[[140,662],[888,662],[888,683],[140,683]],"score":0.8118},{"poly":[[141,627],[980,627],[980,648],[141,648]],"score":0.811},{"poly":[[176,594],[763,594],[763,615],[176,615]],"score":0.813},{"poly":[[140,569],[1083,571],[1083,594],[140,592]],"score":0.6873},{"poly":[[176,535],[928,536],[928,559],[176,558]],"score":0.7501},{"poly":[[140,515],[1084,515],[1084,538],[140,538]],"score":0.6977},{"poly":[[178,482],[441,482],[441,503],[178,503]],"score":0.8235},{"poly":[[141,459],[1084,459],[1084,480],[141,480]],"score":0.8117},{"poly":[[178,426],[323,426],[323,447],[178,447]],"score":0.8115},{"poly":[[140,404],[1084,404],[1084,426],[140,426]],"score":0.8114},{"poly":[[176,368],[264,368],[264,391],[176,391]],"score":0.7817},{"poly":[[178,348],[1084,348],[1084,371],[178,371]],"score":0.6799},{"poly":[[176,323],[1086,325],[1086,348],[176,346]],"score":0.7504},{"poly":[[176,302],[1083,304],[1083,327],[176,325]],"score":0.6992},{"poly":[[176,280],[1084,282],[1084,305],[176,304]],"score":0.7194},{"poly":[[140,259],[1084,257],[1084,280],[140,282]],"score":0.7059},{"poly":[[178,224],[610,224],[610,246],[178,246]],"score":0.6603},{"poly":[[140,203],[1083,203],[1083,226],[140,226]],"score":0.7111}],"page_no":13,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1527,"y1":1553},"conf":0.7365,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":595,"x1":629,"y0":1526,"y1":1553},"font_size":0.0,"text":"15"}],"source":"layout det","text":""},{"bbox":{"x0":167,"x1":197,"y0":1363,"y1":1386},"conf":0.267,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":168,"x1":196,"y0":1363,"y1":1386},"font_size":0.0,"text":"19"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":310,"y0":191,"y1":240},"conf":0.8953,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":134,"x1":306,"y0":191,"y1":236},"font_size":-9.853999999999998e+21,"text":"Appendix"}],"source":"layout det","text":"Appendix"},{"bbox":{"x0":135,"x1":477,"y0":258,"y1":291},"conf":0.8739,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":471,"y0":261,"y1":284},"font_size":-9.853999999999998e+21,"text":"ADataset Transformation"}],"source":"layout det","text":"ADataset Transformation"},{"bbox":{"x0":135,"x1":1090,"y0":305,"y1":384},"conf":0.9487,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":307,"y1":330},"font_size":-9.853999999999998e+21,"text":"Here is an example of data transformation, where we demonstrate the original problem, the transformed"},{"bbox":{"x0":138,"x1":1084,"y0":332,"y1":355},"font_size":-9.853999999999998e+21,"text":"problem, and the model outputs during the transformation process (the original is in markdown text, which"},{"bbox":{"x0":136,"x1":442,"y0":356,"y1":378},"font_size":-9.853999999999998e+21,"text":"has been rendered for readability)."}],"source":"layout det","text":"Here is an example of data transformation, where we demonstrate the original problem, the transformed problem, and the model outputs during the transformation process (the original is in markdown text, which has been rendered for readability)."},{"bbox":{"x0":135,"x1":1089,"y0":388,"y1":540},"conf":0.9693,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1084,"y0":389,"y1":414},"font_size":-9.853999999999998e+21,"text":"To ensure the model can complete this task with high accuracy, our goal is to have it perform comprehensive"},{"bbox":{"x0":136,"x1":1084,"y0":416,"y1":439},"font_size":-9.853999999999998e+21,"text":"reasoning while avoiding hallucinations. We achieve this by providing a clear thinking framework that"},{"bbox":{"x0":136,"x1":1084,"y0":441,"y1":462},"font_size":-9.853999999999998e+21,"text":"encourages thorough reasoning. Specifcally, we employ Chain of Thought (CoT) reasoning along with fouri"},{"bbox":{"x0":138,"x1":1084,"y0":464,"y1":485},"font_size":-9.853999999999998e+21,"text":"well-defned steps: extract the answer format, rewrite the problem statement, solve the modifed problem andii"},{"bbox":{"x0":136,"x1":1084,"y0":487,"y1":512},"font_size":-9.853999999999998e+21,"text":"provide an integer as the fnal answer. For each step, we provide few-shot solutions or detailed guidelines toi"},{"bbox":{"x0":138,"x1":391,"y0":513,"y1":536},"font_size":-9.853999999999998e+21,"text":"guide the model’s reasoning."}],"source":"layout det","text":"To ensure the model can complete this task with high accuracy, our goal is to have it perform comprehensive reasoning while avoiding hallucinations. We achieve this by providing a clear thinking framework that encourages thorough reasoning. Specifcally, we employ Chain of Thought (CoT) reasoning along with fouri well-defned steps: extract the answer format, rewrite the problem statement, solve the modifed problem andii provide an integer as the fnal answer. For each step, we provide few-shot solutions or detailed guidelines toi guide the model’s reasoning."},{"bbox":{"x0":134,"x1":1087,"y0":543,"y1":599},"conf":0.9279,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1084,"y0":546,"y1":569},"font_size":-9.853999999999998e+21,"text":"Through this approach, we have observed that in most cases, the LLM can generate reformulations with both"},{"bbox":{"x0":136,"x1":492,"y0":569,"y1":594},"font_size":-9.853999999999998e+21,"text":"format and quality that are satisfactory."}],"source":"layout det","text":"Through this approach, we have observed that in most cases, the LLM can generate reformulations with both format and quality that are satisfactory."},{"bbox":{"x0":166,"x1":361,"y0":622,"y1":651},"conf":0.7483,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":173,"x1":358,"y0":624,"y1":647},"font_size":-9.853999999999998e+21,"text":"Original Problem"}],"source":"layout det","text":"Original Problem"},{"bbox":{"x0":166,"x1":1057,"y0":650,"y1":703},"conf":0.9285,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1053,"y0":650,"y1":677},"font_size":-9.853999999999998e+21,"text":"Let x and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible"},{"bbox":{"x0":168,"x1":421,"y0":673,"y1":700},"font_size":-9.853999999999998e+21,"text":"value of x. Answer: $11-2\\sqrt{6}$"}],"source":"layout det","text":"Let x and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible value of x. Answer: $11-2\\sqrt{6}$"},{"bbox":{"x0":168,"x1":417,"y0":733,"y1":760},"conf":0.8999,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":171,"x1":412,"y0":736,"y1":757},"font_size":-9.853999999999998e+21,"text":"Transformed Problem"}],"source":"layout det","text":"Transformed Problem"},{"bbox":{"x0":166,"x1":1058,"y0":760,"y1":837},"conf":0.9464,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1053,"y0":762,"y1":786},"font_size":-9.853999999999998e+21,"text":"Let x and be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible"},{"bbox":{"x0":261,"x1":273,"y0":773,"y1":787},"font_size":-9.853999999999998e+21,"text":"$y$"},{"bbox":{"x0":170,"x1":1053,"y0":787,"y1":812},"font_size":-9.853999999999998e+21,"text":"value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$ and $n$  are integers. Please fndi"},{"bbox":{"x0":168,"x1":482,"y0":808,"y1":833},"font_size":-9.853999999999998e+21,"text":"the value of $\\boldsymbol{k+m+n.}$  Answer: 19"}],"source":"layout det","text":"Let x and be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible $y$ value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$ and $n$  are integers. Please fndi the value of $\\boldsymbol{k+m+n.}$  Answer: 19"},{"bbox":{"x0":166,"x1":531,"y0":864,"y1":930},"conf":0.354,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":171,"x1":529,"y0":868,"y1":891},"font_size":-9.853999999999998e+21,"text":"Model’s output (Markdown Text)"},{"bbox":{"x0":170,"x1":501,"y0":901,"y1":926},"font_size":-9.853999999999998e+21,"text":"Transformation Thought Process"}],"source":"layout det","text":"Model’s output (Markdown Text)Transformation Thought Process"},{"bbox":{"x0":166,"x1":1059,"y0":932,"y1":1054},"conf":0.9564,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1053,"y0":931,"y1":957},"font_size":-9.853999999999998e+21,"text":"The original answer to the problem is an algebraic expression $11-2\\sqrt{6}.$  We need to transform this"},{"bbox":{"x0":170,"x1":1054,"y0":959,"y1":980},"font_size":-9.853999999999998e+21,"text":"into an integer form. First, note that the answer consists of an integer part and a square root term."},{"bbox":{"x0":170,"x1":1053,"y0":982,"y1":1005},"font_size":-9.853999999999998e+21,"text":"According to the given rule, we replace the integer and square root components with undetermined"},{"bbox":{"x0":170,"x1":1053,"y0":1006,"y1":1028},"font_size":-9.853999999999998e+21,"text":"coefcients. The original problem usesfi $x$  and as variab√les, so we choose other letters as undetermined"},{"bbox":{"x0":559,"x1":570,"y0":1016,"y1":1029},"font_size":-9.853999999999998e+21,"text":"$y$"},{"bbox":{"x0":168,"x1":1048,"y0":1030,"y1":1053},"font_size":-9.853999999999998e+21,"text":"coefcients. We express the answer in the formfi $\\boldsymbol{k-m\\sqrt{n}}$ and require the student to fndi $k+m+n$"}],"source":"layout det","text":"The original answer to the problem is an algebraic expression $11-2\\sqrt{6}.$  We need to transform this into an integer form. First, note that the answer consists of an integer part and a square root term.According to the given rule, we replace the integer and square root components with undetermined coefcients. The original problem usesfi $x$  and as variab√les, so we choose other letters as undetermined $y$  coefcients. We express the answer in the formfi $\\boldsymbol{k-m\\sqrt{n}}$ and require the student to fndi $k+m+n$"},{"bbox":{"x0":167,"x1":332,"y0":1054,"y1":1077},"conf":0.7806,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":329,"y0":1053,"y1":1076},"font_size":-9.853999999999998e+21,"text":"Answer Format"}],"source":"layout det","text":"Answer Format"},{"bbox":{"x0":167,"x1":260,"y0":1079,"y1":1104},"conf":0.8731,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":164,"x1":261,"y0":1074,"y1":1106},"font_size":-9.853999999999998e+21,"text":"k −mn"}],"source":"layout det","text":"k −mn"},{"bbox":{"x0":168,"x1":394,"y0":1105,"y1":1127},"conf":0.5778,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":170,"x1":392,"y0":1102,"y1":1125},"font_size":-9.853999999999998e+21,"text":"Transformed Problem"}],"source":"layout det","text":"Transformed Problem"},{"bbox":{"x0":166,"x1":1059,"y0":1128,"y1":1202},"conf":0.9452,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":166,"x1":1054,"y0":1125,"y1":1155},"font_size":-9.853999999999998e+21,"text":"Let $x$  and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible"},{"bbox":{"x0":170,"x1":1053,"y0":1155,"y1":1179},"font_size":-9.853999999999998e+21,"text":"value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$  and $n$  are integers. Please fndi"},{"bbox":{"x0":168,"x1":376,"y0":1176,"y1":1201},"font_size":-9.853999999999998e+21,"text":"the value of $\\boldsymbol{k+m+n.}$"}],"source":"layout det","text":"Let $x$  and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$  and $n$  are integers. Please fndi the value of $\\boldsymbol{k+m+n.}$"},{"bbox":{"x0":169,"x1":666,"y0":1203,"y1":1227},"conf":0.8597,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":171,"x1":662,"y0":1201,"y1":1224},"font_size":-9.853999999999998e+21,"text":"Answer Calculation for the Transformed Problem"}],"source":"layout det","text":"Answer Calculation for the Transformed Problem"},{"bbox":{"x0":216,"x1":518,"y0":1231,"y1":1257},"conf":0.9129,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":200,"x1":516,"y0":1231,"y1":1255},"font_size":-9.853999999999998e+21,"text":"The original answer is: $11-2\\sqrt{6}.$"}],"source":"layout det","text":"The original answer is: $11-2\\sqrt{6}.$"},{"bbox":{"x0":217,"x1":524,"y0":1257,"y1":1281},"conf":0.9058,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":198,"x1":521,"y0":1254,"y1":1280},"font_size":-9.853999999999998e+21,"text":"The required format is: $\\boldsymbol{k-m\\sqrt{n}}.$"}],"source":"layout det","text":"The required format is: $\\boldsymbol{k-m\\sqrt{n}}.$"},{"bbox":{"x0":216,"x1":648,"y0":1281,"y1":1304},"conf":0.9004,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":200,"x1":647,"y0":1280,"y1":1304},"font_size":-9.853999999999998e+21,"text":"Matching the coefcients:fi $k=11, m=2, n=6.$"}],"source":"layout det","text":"Matching the coefcients:fi $k=11, m=2, n=6.$"},{"bbox":{"x0":216,"x1":650,"y0":1306,"y1":1329},"conf":0.8979,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":198,"x1":647,"y0":1302,"y1":1328},"font_size":-9.853999999999998e+21,"text":"The fnal result is:i $k+m+n=11+2+6=19.$"}],"source":"layout det","text":"The fnal result is:i $k+m+n=11+2+6=19.$"},{"bbox":{"x0":167,"x1":307,"y0":1335,"y1":1360},"conf":0.3228,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":168,"x1":303,"y0":1333,"y1":1358},"font_size":-9.853999999999998e+21,"text":"Final Answer"}],"source":"layout det","text":"Final Answer"}],"formula_dets":[{"bbox":{"x0":576,"x1":665,"y0":789,"y1":812},"conf":0.8961,"label":"print_embedding","label_id":0},{"bbox":{"x0":576,"x1":664,"y0":1156,"y1":1179},"conf":0.892,"label":"print_embedding","label_id":0},{"bbox":{"x0":431,"x1":521,"y0":1258,"y1":1280},"conf":0.8876,"label":"print_embedding","label_id":0},{"bbox":{"x0":723,"x1":811,"y0":931,"y1":957},"conf":0.8826,"label":"print_embedding","label_id":0},{"bbox":{"x0":588,"x1":672,"y0":1031,"y1":1053},"conf":0.8825,"label":"print_embedding","label_id":0},{"bbox":{"x0":426,"x1":516,"y0":1232,"y1":1255},"conf":0.8805,"label":"print_embedding","label_id":0},{"bbox":{"x0":508,"x1":773,"y0":650,"y1":677},"conf":0.8797,"label":"print_embedding","label_id":0},{"bbox":{"x0":508,"x1":773,"y0":762,"y1":786},"conf":0.8793,"label":"print_embedding","label_id":0},{"bbox":{"x0":508,"x1":773,"y0":1129,"y1":1154},"conf":0.8732,"label":"print_embedding","label_id":0},{"bbox":{"x0":451,"x1":647,"y0":1282,"y1":1303},"conf":0.8686,"label":"print_embedding","label_id":0},{"bbox":{"x0":952,"x1":1048,"y0":1032,"y1":1052},"conf":0.8635,"label":"print_embedding","label_id":0},{"bbox":{"x0":279,"x1":376,"y0":1182,"y1":1200},"conf":0.8617,"label":"print_embedding","label_id":0},{"bbox":{"x0":279,"x1":375,"y0":814,"y1":832},"conf":0.8595,"label":"print_embedding","label_id":0},{"bbox":{"x0":388,"x1":647,"y0":1307,"y1":1328},"conf":0.8586,"label":"print_embedding","label_id":0},{"bbox":{"x0":345,"x1":421,"y0":678,"y1":699},"conf":0.8211,"label":"print_embedding","label_id":0},{"bbox":{"x0":727,"x1":776,"y0":1156,"y1":1178},"conf":0.7268,"label":"print_embedding","label_id":0},{"bbox":{"x0":171,"x1":255,"y0":1081,"y1":1104},"conf":0.7033,"label":"print_isolated","label_id":1},{"bbox":{"x0":261,"x1":273,"y0":1140,"y1":1154},"conf":0.6977,"label":"print_embedding","label_id":0},{"bbox":{"x0":727,"x1":777,"y0":789,"y1":811},"conf":0.6883,"label":"print_embedding","label_id":0},{"bbox":{"x0":820,"x1":833,"y0":1164,"y1":1175},"conf":0.6812,"label":"print_embedding","label_id":0},{"bbox":{"x0":206,"x1":218,"y0":1140,"y1":1150},"conf":0.6733,"label":"print_embedding","label_id":0},{"bbox":{"x0":261,"x1":273,"y0":662,"y1":676},"conf":0.6655,"label":"print_embedding","label_id":0},{"bbox":{"x0":261,"x1":273,"y0":773,"y1":787},"conf":0.6561,"label":"print_embedding","label_id":0},{"bbox":{"x0":504,"x1":516,"y0":1016,"y1":1025},"conf":0.6487,"label":"print_embedding","label_id":0},{"bbox":{"x0":559,"x1":570,"y0":1016,"y1":1029},"conf":0.6395,"label":"print_embedding","label_id":0},{"bbox":{"x0":820,"x1":833,"y0":797,"y1":808},"conf":0.6376,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":135,"x1":1089,"y0":388,"y1":540},"conf":0.9693,"label":"Text","label_id":1},{"bbox":{"x0":166,"x1":1059,"y0":932,"y1":1054},"conf":0.9564,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1090,"y0":305,"y1":384},"conf":0.9487,"label":"Text","label_id":1},{"bbox":{"x0":166,"x1":1058,"y0":760,"y1":837},"conf":0.9464,"label":"Text","label_id":1},{"bbox":{"x0":166,"x1":1059,"y0":1128,"y1":1202},"conf":0.9452,"label":"Text","label_id":1},{"bbox":{"x0":166,"x1":1057,"y0":650,"y1":703},"conf":0.9285,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1087,"y0":543,"y1":599},"conf":0.9279,"label":"Text","label_id":1},{"bbox":{"x0":216,"x1":518,"y0":1231,"y1":1257},"conf":0.9129,"label":"Text","label_id":1},{"bbox":{"x0":217,"x1":524,"y0":1257,"y1":1281},"conf":0.9058,"label":"Text","label_id":1},{"bbox":{"x0":216,"x1":648,"y0":1281,"y1":1304},"conf":0.9004,"label":"Text","label_id":1},{"bbox":{"x0":168,"x1":417,"y0":733,"y1":760},"conf":0.8999,"label":"Title","label_id":0},{"bbox":{"x0":216,"x1":650,"y0":1306,"y1":1329},"conf":0.8979,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":310,"y0":191,"y1":240},"conf":0.8953,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":477,"y0":258,"y1":291},"conf":0.8739,"label":"Title","label_id":0},{"bbox":{"x0":167,"x1":260,"y0":1079,"y1":1104},"conf":0.8731,"label":"Text","label_id":1},{"bbox":{"x0":169,"x1":666,"y0":1203,"y1":1227},"conf":0.8597,"label":"Text","label_id":1},{"bbox":{"x0":167,"x1":332,"y0":1054,"y1":1077},"conf":0.7806,"label":"Text","label_id":1},{"bbox":{"x0":166,"x1":361,"y0":622,"y1":651},"conf":0.7483,"label":"Title","label_id":0},{"bbox":{"x0":596,"x1":627,"y0":1527,"y1":1553},"conf":0.7365,"label":"Abandon","label_id":2},{"bbox":{"x0":167,"x1":504,"y0":901,"y1":929},"conf":0.6217,"label":"Title","label_id":0},{"bbox":{"x0":168,"x1":535,"y0":866,"y1":897},"conf":0.61,"label":"Title","label_id":0},{"bbox":{"x0":167,"x1":307,"y0":1335,"y1":1360},"conf":0.5814,"label":"Text","label_id":1},{"bbox":{"x0":168,"x1":394,"y0":1105,"y1":1127},"conf":0.5778,"label":"Title","label_id":0},{"bbox":{"x0":167,"x1":197,"y0":1363,"y1":1386},"conf":0.4383,"label":"Text","label_id":1},{"bbox":{"x0":168,"x1":394,"y0":1105,"y1":1127},"conf":0.3751,"label":"Text","label_id":1},{"bbox":{"x0":166,"x1":531,"y0":864,"y1":930},"conf":0.354,"label":"Title","label_id":0},{"bbox":{"x0":167,"x1":307,"y0":1335,"y1":1360},"conf":0.3228,"label":"Title","label_id":0},{"bbox":{"x0":167,"x1":197,"y0":1363,"y1":1386},"conf":0.267,"label":"Abandon","label_id":2},{"bbox":{"x0":167,"x1":534,"y0":865,"y1":930},"conf":0.2103,"label":"Title","label_id":0}],"ocr_all":false,"ocr_dets":[{"poly":[[595,1526],[629,1526],[629,1553],[595,1553]],"score":0.887},{"poly":[[168,1363],[196,1363],[196,1386],[168,1386]],"score":0.7773},{"poly":[[168,1333],[303,1335],[302,1358],[168,1356]],"score":0.8018},{"poly":[[198,1302],[647,1304],[647,1327],[198,1325]],"score":0.8332},{"poly":[[200,1280],[644,1280],[644,1304],[200,1304]],"score":0.7482},{"poly":[[198,1254],[521,1256],[520,1279],[198,1277]],"score":0.8101},{"poly":[[200,1231],[514,1231],[514,1254],[200,1254]],"score":0.8139},{"poly":[[171,1201],[662,1201],[662,1224],[171,1224]],"score":0.738},{"poly":[[168,1176],[378,1178],[377,1201],[168,1199]],"score":0.738},{"poly":[[170,1155],[1053,1155],[1053,1176],[170,1176]],"score":0.916},{"poly":[[166,1127],[1054,1125],[1054,1153],[166,1155]],"score":0.6657},{"poly":[[170,1102],[392,1102],[392,1125],[170,1125]],"score":0.7431},{"poly":[[164,1076],[261,1074],[261,1104],[165,1106]],"score":0.7052},{"poly":[[170,1053],[329,1053],[329,1076],[170,1076]],"score":0.8478},{"poly":[[168,1030],[1051,1030],[1051,1053],[168,1053]],"score":0.729},{"poly":[[170,1006],[1053,1006],[1053,1028],[170,1028]],"score":0.8711},{"poly":[[170,982],[1053,982],[1053,1005],[170,1005]],"score":0.7236},{"poly":[[170,959],[1054,959],[1054,980],[170,980]],"score":0.8758},{"poly":[[170,932],[1053,932],[1053,955],[170,955]],"score":0.7694},{"poly":[[170,901],[501,903],[500,926],[170,924]],"score":0.7878},{"poly":[[171,868],[529,868],[529,891],[171,891]],"score":0.8481},{"poly":[[168,808],[482,810],[482,833],[168,832]],"score":0.8047},{"poly":[[170,787],[1053,787],[1053,810],[170,810]],"score":0.7451},{"poly":[[170,762],[1053,762],[1053,785],[170,785]],"score":0.7847},{"poly":[[171,736],[412,736],[412,757],[171,757]],"score":0.9919},{"poly":[[168,677],[424,673],[424,696],[168,700]],"score":0.7294},{"poly":[[170,652],[1053,652],[1053,675],[170,675]],"score":0.7342},{"poly":[[173,624],[358,624],[358,647],[173,647]],"score":0.813},{"poly":[[136,569],[492,571],[492,594],[136,592]],"score":0.7449},{"poly":[[140,546],[1084,546],[1084,569],[140,569]],"score":0.7176},{"poly":[[138,513],[391,513],[391,536],[138,536]],"score":0.7157},{"poly":[[136,488],[1084,487],[1084,510],[136,512]],"score":0.7495},{"poly":[[138,464],[1084,464],[1084,485],[138,485]],"score":0.8583},{"poly":[[136,441],[1084,441],[1084,462],[136,462]],"score":0.8428},{"poly":[[136,416],[1084,416],[1084,439],[136,439]],"score":0.7216},{"poly":[[136,389],[1084,391],[1084,414],[136,412]],"score":0.7858},{"poly":[[136,356],[442,356],[442,378],[136,378]],"score":0.832},{"poly":[[138,332],[1084,332],[1084,355],[138,355]],"score":0.7413},{"poly":[[140,307],[1084,307],[1084,330],[140,330]],"score":0.7288},{"poly":[[138,261],[471,261],[471,284],[138,284]],"score":0.8316},{"poly":[[134,197],[305,191],[306,231],[136,236]],"score":0.8777}],"page_no":14,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1527,"y1":1553},"conf":0.727,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":595,"x1":629,"y0":1526,"y1":1553},"font_size":0.0,"text":"16"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":136,"x1":440,"y0":194,"y1":230},"conf":0.8666,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":434,"y0":198,"y1":226},"font_size":0.0,"text":"BSupplementary Case"}],"source":"layout det","text":"BSupplementary Case"},{"bbox":{"x0":183,"x1":276,"y0":263,"y1":286},"conf":0.7334,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":185,"x1":274,"y0":262,"y1":285},"font_size":0.0,"text":"Question:"}],"source":"layout det","text":"Question:"},{"bbox":{"x0":182,"x1":1043,"y0":287,"y1":408},"conf":0.9559,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":1038,"y0":287,"y1":310},"font_size":0.0,"text":"Among the 900 residents of Aimeville, there are 195 who own a diamond ring, 367 who own a set"},{"bbox":{"x0":181,"x1":1039,"y0":307,"y1":338},"font_size":0.0,"text":"of golf clubs, and 562 who own a garden spade. In addition, each of the 900 residents owns a bag"},{"bbox":{"x0":183,"x1":1038,"y0":335,"y1":358},"font_size":0.0,"text":"of candy hearts. There are 437 residents who own exactly two of these things, and 234 residents"},{"bbox":{"x0":183,"x1":1038,"y0":358,"y1":383},"font_size":0.0,"text":"who own exactly three of these things. Find the number of residents of Aimeville who own all four"},{"bbox":{"x0":181,"x1":324,"y0":381,"y1":406},"font_size":0.0,"text":"of these things."}],"source":"layout det","text":"Among the 900 residents of Aimeville, there are 195 who own a diamond ring, 367 who own a set of golf clubs, and 562 who own a garden spade. In addition, each of the 900 residents owns a bag of candy hearts. There are 437 residents who own exactly two of these things, and 234 residents who own exactly three of these things. Find the number of residents of Aimeville who own all four of these things."},{"bbox":{"x0":182,"x1":283,"y0":417,"y1":441},"conf":0.6858,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":181,"x1":281,"y0":414,"y1":442},"font_size":0.0,"text":"Response:"}],"source":"layout det","text":"Response:"},{"bbox":{"x0":183,"x1":389,"y0":442,"y1":465},"conf":0.8836,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":183,"x1":386,"y0":441,"y1":462},"font_size":0.0,"text":"Step by Step Solution:"}],"source":"layout det","text":"Step by Step Solution:"},{"bbox":{"x0":183,"x1":523,"y0":465,"y1":488},"conf":0.8521,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":183,"x1":522,"y0":462,"y1":487},"font_size":0.0,"text":"1. Let’s defne the following variables:i"}],"source":"layout det","text":"1.Let’s defne the following variables:i"},{"bbox":{"x0":184,"x1":670,"y0":489,"y1":511},"conf":0.9024,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":667,"y0":488,"y1":511},"font_size":0.0,"text":"Let $A$  be the set of residents who own a diamond ring."}],"source":"layout det","text":"Let $A$  be the set of residents who own a diamond ring."},{"bbox":{"x0":183,"x1":689,"y0":513,"y1":536},"conf":0.9039,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":685,"y0":512,"y1":533},"font_size":0.0,"text":"Let $B$  be the set of residents who own a set of golf clubs."}],"source":"layout det","text":"Let $B$  be the set of residents who own a set of golf clubs."},{"bbox":{"x0":184,"x1":668,"y0":537,"y1":560},"conf":0.9055,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":665,"y0":535,"y1":559},"font_size":0.0,"text":"Let $C$ be the set of residents who own a garden spade."}],"source":"layout det","text":"Let $C$ be the set of residents who own a garden spade."},{"bbox":{"x0":182,"x1":1041,"y0":562,"y1":608},"conf":0.918,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1039,"y0":561,"y1":584},"font_size":0.0,"text":"Let $D$  be the set of residents who own a bag of candy hearts. Since all 900 residents own a bag of"},{"bbox":{"x0":183,"x1":403,"y0":582,"y1":609},"font_size":0.0,"text":"candy hearts, $|D|=900.$"}],"source":"layout det","text":"Let $D$  be the set of residents who own a bag of candy hearts. Since all 900 residents own a bag of candy hearts, $|D|=900.$"},{"bbox":{"x0":182,"x1":864,"y0":610,"y1":632},"conf":0.805,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":851,"y0":609,"y1":631},"font_size":0.0,"text":"Let $a_{i}$ denote the number of residents who own exactly $i$  out of the 4 items."}],"source":"layout det","text":"Let $a_{i}$ denote the number of residents who own exactly $i$  out of the 4 items."},{"bbox":{"x0":182,"x1":876,"y0":633,"y1":659},"conf":0.8534,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":875,"y0":630,"y1":657},"font_size":0.0,"text":"Let’s use the principle of inclusion-exclusion for the union of 4 sets $A,B,C,D.$"}],"source":"layout det","text":"Let’s use the principle of inclusion-exclusion for the union of 4 sets $A,B,C,D.$"},{"bbox":{"x0":184,"x1":208,"y0":666,"y1":678},"conf":0.3334,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":190,"x1":205,"y0":665,"y1":678},"font_size":0.0,"text":"..."}],"source":"layout det","text":"..."},{"bbox":{"x0":187,"x1":410,"y0":678,"y1":706},"conf":0.8033,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$a_{4}=\\tfrac{-219}{-4}=\\tfrac{219}{4}=54.75.$$"},{"bbox":{"x0":182,"x1":1042,"y0":707,"y1":775},"conf":0.9209,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":1036,"y0":706,"y1":729},"font_size":0.0,"text":"Since $a_{4},$  the number of people owning all four items, must be a whole number, our current"},{"bbox":{"x0":185,"x1":1038,"y0":731,"y1":752},"font_size":0.0,"text":"approach needs to be reconsidered in terms of the combinatorial counting of people owning diferentf"},{"bbox":{"x0":185,"x1":422,"y0":754,"y1":776},"font_size":0.0,"text":"combinations of the items."}],"source":"layout det","text":"Since $a_{4},$  the number of people owning all four items, must be a whole number, our current approach needs to be reconsidered in terms of the combinatorial counting of people owning diferentf combinations of the items."},{"bbox":{"x0":185,"x1":1041,"y0":778,"y1":824},"conf":0.8692,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":183,"x1":1038,"y0":772,"y1":802},"font_size":0.0,"text":"Instead of directly using the inclusion-exclusion principle, we can use a diferent approach involvingf"},{"bbox":{"x0":185,"x1":827,"y0":799,"y1":824},"font_size":0.0,"text":"the number of people owning combinations of the items $A,B,C,$ and D."}],"source":"layout det","text":"Instead of directly using the inclusion-exclusion principle, we can use a diferent approach involvingf the number of people owning combinations of the items $A,B,C,$ and D."},{"bbox":{"x0":182,"x1":1040,"y0":826,"y1":871},"conf":0.8949,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":1034,"y0":825,"y1":848},"font_size":0.0,"text":"Let $x_{i}$ be the number of people owning exactly $i$  item among $A,B,C$ (excluding the item $D$  which"},{"bbox":{"x0":185,"x1":326,"y0":850,"y1":871},"font_size":0.0,"text":"everyone owns)."}],"source":"layout det","text":"Let $x_{i}$ be the number of people owning exactly $i$  item among $A,B,C$ (excluding the item $D$  which everyone owns)."},{"bbox":{"x0":183,"x1":1040,"y0":873,"y1":920},"conf":0.904,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":185,"x1":1036,"y0":868,"y1":898},"font_size":0.0,"text":"Then the total number of people owning exactly $k$  items among the $A,B,C,$ and $D$  can be broken"},{"bbox":{"x0":180,"x1":245,"y0":892,"y1":923},"font_size":0.0,"text":"down:"}],"source":"layout det","text":"Then the total number of people owning exactly $k$  items among the $A,B,C,$ and $D$  can be broken down:"},{"bbox":{"x0":407,"x1":814,"y0":967,"y1":994},"conf":0.6758,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":409,"x1":810,"y0":967,"y1":992},"font_size":0.0,"text":"Table 3 A case of refective behavior emergence.l"}],"source":"layout det","text":"Table 3 A case of refective behavior emergence.l"}],"formula_dets":[{"bbox":{"x0":311,"x1":403,"y0":586,"y1":609},"conf":0.8924,"label":"print_embedding","label_id":0},{"bbox":{"x0":222,"x1":240,"y0":565,"y1":580},"conf":0.8719,"label":"print_embedding","label_id":0},{"bbox":{"x0":776,"x1":875,"y0":631,"y1":657},"conf":0.8696,"label":"print_embedding","label_id":0},{"bbox":{"x0":770,"x1":849,"y0":868,"y1":898},"conf":0.8602,"label":"print_embedding","label_id":0},{"bbox":{"x0":223,"x1":239,"y0":517,"y1":532},"conf":0.8556,"label":"print_embedding","label_id":0},{"bbox":{"x0":223,"x1":239,"y0":541,"y1":557},"conf":0.8536,"label":"print_embedding","label_id":0},{"bbox":{"x0":678,"x1":758,"y0":799,"y1":824},"conf":0.8529,"label":"print_embedding","label_id":0},{"bbox":{"x0":223,"x1":239,"y0":493,"y1":508},"conf":0.8408,"label":"print_embedding","label_id":0},{"bbox":{"x0":961,"x1":978,"y0":829,"y1":844},"conf":0.8319,"label":"print_embedding","label_id":0},{"bbox":{"x0":712,"x1":785,"y0":825,"y1":848},"conf":0.8124,"label":"print_embedding","label_id":0},{"bbox":{"x0":890,"x1":908,"y0":877,"y1":893},"conf":0.811,"label":"print_embedding","label_id":0},{"bbox":{"x0":222,"x1":241,"y0":618,"y1":631},"conf":0.8052,"label":"print_embedding","label_id":0},{"bbox":{"x0":187,"x1":410,"y0":678,"y1":706},"conf":0.8033,"label":"print_isolated","label_id":1},{"bbox":{"x0":242,"x1":268,"y0":714,"y1":729},"conf":0.7702,"label":"print_embedding","label_id":0},{"bbox":{"x0":592,"x1":600,"y0":830,"y1":845},"conf":0.7534,"label":"print_embedding","label_id":0},{"bbox":{"x0":605,"x1":617,"y0":877,"y1":893},"conf":0.7371,"label":"print_embedding","label_id":0},{"bbox":{"x0":675,"x1":683,"y0":614,"y1":629},"conf":0.6873,"label":"print_embedding","label_id":0},{"bbox":{"x0":221,"x1":241,"y0":834,"y1":847},"conf":0.6458,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":182,"x1":1043,"y0":287,"y1":408},"conf":0.9559,"label":"Text","label_id":1},{"bbox":{"x0":182,"x1":1042,"y0":707,"y1":775},"conf":0.9209,"label":"Text","label_id":1},{"bbox":{"x0":182,"x1":1041,"y0":562,"y1":608},"conf":0.918,"label":"Text","label_id":1},{"bbox":{"x0":184,"x1":668,"y0":537,"y1":560},"conf":0.9055,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":1040,"y0":873,"y1":920},"conf":0.904,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":689,"y0":513,"y1":536},"conf":0.9039,"label":"Text","label_id":1},{"bbox":{"x0":184,"x1":670,"y0":489,"y1":511},"conf":0.9024,"label":"Text","label_id":1},{"bbox":{"x0":182,"x1":1040,"y0":826,"y1":871},"conf":0.8949,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":389,"y0":442,"y1":465},"conf":0.8836,"label":"Text","label_id":1},{"bbox":{"x0":185,"x1":1041,"y0":778,"y1":824},"conf":0.8692,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":440,"y0":194,"y1":230},"conf":0.8666,"label":"Title","label_id":0},{"bbox":{"x0":182,"x1":876,"y0":633,"y1":659},"conf":0.8534,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":523,"y0":465,"y1":488},"conf":0.8521,"label":"Text","label_id":1},{"bbox":{"x0":182,"x1":864,"y0":610,"y1":632},"conf":0.805,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":413,"y0":678,"y1":707},"conf":0.7515,"label":"Equation","label_id":8},{"bbox":{"x0":183,"x1":276,"y0":263,"y1":286},"conf":0.7334,"label":"Title","label_id":0},{"bbox":{"x0":596,"x1":627,"y0":1527,"y1":1553},"conf":0.727,"label":"Abandon","label_id":2},{"bbox":{"x0":182,"x1":283,"y0":417,"y1":441},"conf":0.6858,"label":"Title","label_id":0},{"bbox":{"x0":407,"x1":814,"y0":967,"y1":994},"conf":0.6758,"label":"Figure caption","label_id":4},{"bbox":{"x0":184,"x1":208,"y0":666,"y1":678},"conf":0.3334,"label":"Text","label_id":1},{"bbox":{"x0":182,"x1":283,"y0":417,"y1":441},"conf":0.2284,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[595,1526],[629,1526],[629,1553],[595,1553]],"score":0.9337},{"poly":[[409,967],[810,969],[810,992],[409,990]],"score":0.7654},{"poly":[[183,892],[245,898],[243,923],[180,917]],"score":0.868},{"poly":[[185,870],[1036,871],[1036,894],[185,893]],"score":0.8346},{"poly":[[185,850],[326,850],[326,871],[185,871]],"score":0.8531},{"poly":[[185,825],[1034,825],[1034,848],[185,848]],"score":0.769},{"poly":[[185,800],[827,800],[827,823],[185,823]],"score":0.747},{"poly":[[183,772],[1038,774],[1038,802],[183,800]],"score":0.7202},{"poly":[[185,754],[422,754],[422,776],[185,776]],"score":0.8671},{"poly":[[185,731],[1038,731],[1038,752],[185,752]],"score":0.902},{"poly":[[185,706],[1036,706],[1036,729],[185,729]],"score":0.7786},{"poly":[[190,665],[205,665],[205,678],[190,678]],"score":0.7627},{"poly":[[185,632],[873,630],[873,653],[185,655]],"score":0.805},{"poly":[[185,609],[851,609],[851,630],[185,630]],"score":0.9216},{"poly":[[183,584],[404,582],[404,605],[183,607]],"score":0.7797},{"poly":[[186,561],[1039,561],[1039,584],[186,584]],"score":0.7803},{"poly":[[185,535],[665,536],[665,559],[185,558]],"score":0.8018},{"poly":[[186,512],[685,512],[685,533],[186,533]],"score":0.8084},{"poly":[[185,488],[667,488],[667,511],[185,511]],"score":0.7375},{"poly":[[183,462],[522,464],[522,487],[183,485]],"score":0.8193},{"poly":[[183,441],[386,441],[386,462],[183,462]],"score":0.9627},{"poly":[[182,414],[281,418],[280,442],[181,439]],"score":0.7541},{"poly":[[181,381],[324,383],[324,406],[181,404]],"score":0.7606},{"poly":[[183,358],[1038,360],[1038,383],[183,381]],"score":0.8282},{"poly":[[183,335],[1038,335],[1038,358],[183,358]],"score":0.7213},{"poly":[[181,307],[1039,310],[1039,338],[181,335]],"score":0.705},{"poly":[[185,287],[1038,287],[1038,310],[185,310]],"score":0.7934},{"poly":[[185,262],[274,262],[274,285],[185,285]],"score":0.8257},{"poly":[[178,198],[434,198],[434,226],[178,226]],"score":0.8467},{"poly":[[140,200],[163,200],[163,221],[140,221]],"score":0.8296}],"page_no":15,"scale":2.0,"width":612}],"pages_success_ratio":0.0,"src_path":"oss://glm-data-ocr-data/services/maas/docs/d9bf8aa3-274b-4e31-a2e4-7425f3cc2ce8","text":"DAPO: An Open-Source LLM Reinforcement Learning System at Scale\n1ByteDance Seed2Institute for AI Industry Research (AIR), Tsinghua University 3The University of Hong Kong 4SIA-Lab of Tsinghua AIR and ByteDance Seed\nFull author list in Contributions\nAbstract\nInference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework a, along with a carefully curated and processed dataset.These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.\nDate: March 17, 2025\nCorrespondence: zhouhao@air.tsinghua.edu.cn, wangmingxuan.89@bytedance.com\nProject Page: https://dapo-sia.github.io/\nFigure 1 AIME 2024 scores of DAPO on the Qwen2.5-32B base model, outperforming the previous SoTA DeepSeekR1-Zero-Qwen-32B using $50\\%$ training steps. The x-axis represents the gradient update steps.\n\n1Introduction\nTest-time scaling such as OpenAI’s o1 [1] and DeepSeek’s R1 [2] brings a profound paradigm shift to Large Language Models (LLMs) [3–7]. Test-time scaling enables longer Chain-of-Thought thinking and induces sophisticated reasoning behaviors, which makes the models superior in competitive math and coding tasks like AIME and Codeforces.\nThe central technique driving the revolution is large-scale Reinforcement Learning (RL), which elicits complex reasoning behaviors such as self-verifcation and iterative refnement. However, the actual algorithm andii key recipe for scalable RL training remains a myth, hidden from technical reports of existing reasoning models [1, 2, 8–11]. In this paper, we reveal signifcant obstacles in large-scale RL training and open-source ai scalable RL system with fully open-sourced algorithm, training code and dataset that provides democratized solutions with industry-level RL results.\nWe experiment over Qwen2.5-32B [12] as the pretrained model for RL. In our initial GRPO run, we achieved only 30 points on AIME — a performance signifcantly below DeepSeek’s RL (47 points). A thorough analysisi reveals that the naive GRPO baseline sufers from several key issues such as entropy collapse, reward noise,f and training instability. The broader community has encountered similar challenges in reproducing DeepSeek’s results [13–19] suggesting that critical training details may have been omitted in the R1 paper that are required to develop an industry-level, large-scale, and reproducible RL system.\nTo close this gap, we release an open-source state-of-the-art system for large-scale LLM RL, which achieves 50 points on AIME 2024 based on Qwen2.5-32B model, outperforming previous state-of-the-art results achieved by DeepSeek-R1-Zero-Qwen-32B [2] (47 points) using $50\\%$ training steps (Figure 1). We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and introduce 4 key techniques to make RL shine in the long-CoT RL scenario. Details are presented in Section 3.\n1.Clip-Higher, which promotes the diversity of the system and avoids entropy collapse;\n2.Dynamic Sampling, which improves training efciency and stability;fi\n3.Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios;\n4.Overlong Reward Shaping, which reduces reward noise and stabilizes training.\nOur implementation is based on verl [20]. By fully releasing our state-of-the-art RL system including training code and data, we aim to reveal valuable insights to large-scale LLM RL that beneft the larger community.i\n2Preliminary\n2.1Proximal Policy Optimization (PPO)\nPPO [21] introduces a clipped surrogate objective for policy optimization. By constraining the policy updates within a proximal region of the previous policy using clip, PPO stabilizes training and improves sample efciency. Specifcally, PPO updates the policy by maximizing the following objective:fii\n$$\\mathcal{J}_{\\text{PPO}}(\\theta)=\\mathbb{E}_{(q,a)\\sim\\mathcal{D},o_{\\leq t} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid q)}\\Bigg[\\min\\Bigg(\\frac{\\pi_{\\theta}(o_{t}\\mid q,o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_{t}\\mid q,o_{<t})} \\hat{A}_{t},\\ \\text{clip}\\Bigg(\\frac{\\pi_{\\theta}(o_{t}\\mid q,o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_{t}\\mid q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\Bigg)\\hat{A}_{t}\\Bigg)\\Bigg]$$\n(1)\nwhere $(q,a)$  is a question-answer pair from the data distribution $\\mathcal{D}, \\varepsilon$ is the clipping range of importance sampling ratio, and $\\hat{A}_{t}$ is an estimator of the advantage at time step $t$  Given the value function $V$ and the reward function $R, \\hat{A}_{t}$ is computed using the Generalized Advantage Estimation (GAE) [22]:\n$$\\hat{A}_{t}^{\\text{GAE}(\\gamma,\\lambda)}=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^{l}\\delta_{t+l},$$\n(2)\nwhere\n$$\\delta_{l}=R_{l}+\\gamma V(s_{l+1})-V(s_{l}),\\quad0\\leq\\gamma,\\lambda\\leq1.$$\n(3)\n\nFigure 2 The accuracy on the AIME test set and the entropy of the actor model’s generated probabilities during the RL training process, both before and after applying Clip-Higher strategy.\n2.2Group Relative Policy Optimization (GRPO)\nCompared to PPO, GRPO eliminates the value function and estimates the advantage in a group-relative manner. For a specifc question-answer pairi $(q,a),$  the behavior policy $\\pi_{\\theta_{\\text{old}}}$  samples a group of $G$  individual responses $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}.$  Then, the advantage of the $i$ th response is calculated by normalizing the group-level rewards $\\{R_{i} \\}_{i=1}^{G} :$\n$$\\hat{A}_{i,t}=\\frac{r_{i}-\\mathrm{mean}(\\{R_{i}\\}_{i=1}^{G})}{\\mathrm{std}(\\{R_{i}\\}_{i=1}^{G})}.$$\n(4)\nSimilar to PPO, GRPO adopts a clipped objective, together with a directly imposed KL penalty term:\n$$\\mathcal{J}_{\\text{GRPO}}(\\theta)=\\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n$$\\Bigg[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\Bigg( \\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\Big)\\hat{A}_{i,t}\\Big)-\\beta D_{\\text{KL}}(\\pi_{\\theta}||\\pi_{\\text{ref}})\\Bigg)\\Bigg].$$\n(5)\nwhere\n$$r_{i , t}(\\theta)=\\frac{\\pi_{\\theta}(o_{i , t}\\mid q,o_{i ,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i , t}\\mid q,o_{i ,<t})}.$$\n(6)\nIt is also worth noting that GRPO computes the objective at the sample-level. To be exact, GRPO frsti calculates the mean loss within each generated sequence, before averaging the loss of diferent samples. As wef will be discussing in Section 3.3, such diference may have an impact on the performance of the algorithm.f\n2.3Removing KL Divergence\nThe KL penalty term is used to regulate the divergence between the online policy and the frozen reference policy. In the RLHF scenario [23], the goal of RL is to align the model behavior without diverging too far from the initial model. However, during training the long-CoT reasoning model, the model distribution can diverge signifcantly from the initial model, thus this restriction is not necessary. Therefore, we will excludei the KL term from our proposed algorithm.\n\n2.4Rule-based Reward Modeling\nThe use of reward model usually sufers from the reward hacking problem [24–29]. Instead, we directly usef the fnal accuracy of a verifable task as the outcome reward, computed using the following rule:ii\n$$R(\\hat{y},y)=\\begin{cases}1,&\\texttt{is\\_equivalent}(\\hat{y},y)\\\\ -1,&\\text{otherwise}\\end{cases}$$\n(7)\nwhere $y$ is the ground-truth answer and $\\widehat{y}$  is the predicted answer. This is proved to be an efective approachf to activating the base model’s reasoning capability, as shown in multiple domains such as automated theorem proving [30–33], computer programming [34–37], and mathematics competition [2].\n3DAPO\nWe propose the Decouple Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm. DAPO samples a group of outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ for each question $4$  paired with the answer a, and optimizes the policy via the following objective:\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$\n(8)\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G,$$\nwhere\n$$r_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(o_{i,t}\\mid q,o_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t}\\mid q,o_{i,<t})},\\quad\\hat{A}_{i,t}=\\frac{R_{i}-\\mathrm{mean}(\\{R_{i}\\}_{i=1}^{G})}{\\mathrm{std}(\\{R_{i}\\}_{i=1}^{G})}.$$\n(9)\nThe full algorithm can be found in Algorithm 1. In this section, we will introduce the key techniques associated with DAPO.\n3.1Raise the Ceiling: Clip-Higher\nIn our initial experiments using naive PPO [21] or GRPO [38], we observed the entropy collapse phenomenon:the entropy of the policy decreases quickly as training progresses (Figure 2b). The sampled responses of certain groups tend to be nearly identical. This indicates limited exploration and early deterministic policy,which can hinder the scaling process.\nWe propose the Clip-Higher strategy to address this issue. Clipping over the importance sampling ratio is introduced in Clipped Proximal Policy Optimization (PPO-Clip) [21] to restrict the trust region and enhance the stability of RL. We identify that the upper clip can restrict the exploration of the policy, where making an ‘exploitation’ token more probable is much easier yet the probability of an unlikely ‘exploration’ token is too tightly bounded to be uplifted.\nConcretely, when $\\varepsilon=0.2$ (the default value of most algorithms) and $\\hat{A}_{i , t}>0$ (the system tries to increase the probability), consider two actions with probabilities $\\pi_{\\theta_{\\text{old}}}(o_{i} | q)=0.01$  and 0.9. The upper bounds of the increased probabilities $\\pi_{\\boldsymbol{\\theta}}\\big(o_{\\boldsymbol{i}}\\ \\big|\\ q\\big)$  are 0.012 and 1.08, respectively $(\\pi_{\\theta_{\\text{old}}}\\cdot(1+\\epsilon)).$  This implies that‘exploitation’ tokens with a higher probability $\\left(\\text{\\textit{e.g.}, 0.9}\\right)$  are not constrained to get even extremely larger probabilities like 0.999. Conversely, for low-probability ‘exploration’ tokens, achieving a non-trivial increase in probability is considerably more challenging. Empirically, we also observe that the mean probability of up-clipped tokens is low: $\\pi_{\\theta}(o_{i}\\mid q)<0.2$ (Figure 3a). This fnding supports our intuition that the upperi clipping threshold indeed restricts the probability increase of low-probability ‘exploration’ tokens, thereby potentially constraining the exploration of the system.\n\nFigure 3 The mean up-clipped probability as well as the ratio of prompts with accuracy=1.\nAdhering to the Clip-Higher strategy, we decouple the lower and higher clipping range as $\\varepsilon_{\\text{low}}$  and $\\varepsilon_{\\text{high}},$  as highlighted in Equation 10:\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$\n(10)\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$\nWe increase the value of $\\varepsilon_{\\mathbf{high}}$  to leave more room for the increase of low-probability tokens. As shown in Figure 2, this adjustment efectively enhances the policy’s entropy and facilitates the generation of moref diverse samples. We keep $\\varepsilon_{\\text{low}}$ as it is, because increasing it will suppress the probability of these tokens to 0,resulting in the collapse of the sampling space.\n3.2The More the Merrier: Dynamic Sampling\nExisting RL algorithm sufers from the gradient-decreasing problem when some prompts have accuracy equalf to 1. For example for GRPO, if all outputs $\\boldsymbol{\\{o_{i}\\}_{i=1}^{G}}$ of a particular prompt are correct and receive the same reward, the resulting advantage for this group is zero. A zero advantage results in zero policy gradients,shrinking the magnitude and increasing the noise sensitivity of the batch gradient, thereby degrading sample efciency. Empirically, the number of samples with accuracy equal to 1 continues to increase, as shown infi Figure 3b. This means that the efective number of prompts in each batch keeps decreasing, which can leadf to larger variance in gradient and dampens the gradient signals for model training.\nTo this end, we propose to over-sample and filter out prompts with the accuracy equal to 1 and 0 as illustrated in Equation 11, leaving all prompts in the batch with efective gradients and keeping a consistent number off prompts. The sampling cost for each batch is dynamic. Before training, we keep sampling until the batch is fully flled with samples whose accuracy is neither 0 nor 1.i\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n$$\\left[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)\\right]$$\n(11)\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$\n\n(a) Entropy of actor model’s generation probabilities.(b) Average length of actor model-generated responses\nFigure 4 The entropy of the probability distribution of the actor model, as well as the changes in response length.\nNote that this strategy does not necessarily impede training efciency, because the generation time is typicallyfi dominated by the generation of long-tail samples if the RL system is synchronized and the generation stage is not pipelined. Besides, we fnd that with dynamic sampling the experiment achieves the same performancei faster as shown in Figure 6.\n3.3Rebalancing Act: Token-Level Policy Gradient Loss\nThe original GRPO algorithm employs a sample-level loss calculation, which involves frst averaging the lossesi by token within each sample and then aggregating the losses across samples. In this approach, each sample is assigned an equal weight in the fnal loss computation. However, we fnd that this method of loss reductionii introduces several challenges in the context of long-CoT RL scenarios.\nSince all samples are assigned the same weight in the loss calculation, tokens within longer responses (which contain more tokens) may have a disproportionately lower contribution to the overall loss, which can lead to two adverse efects. First, for high-quality long samples, this efect can impede the model’s ability to learnff reasoning-relevant patterns within them. Second, we observe that excessively long samples often exhibit low-quality patterns such as gibberish and repetitive words. Thus, sample-level loss calculation, due to its inability to efectively penalize those undesirable patterns in long samples, leads to an unhealthy increase inf entropy and response length, as shown in Figure 4a and Figure 4b.\nWe introduce a Token-level Policy Gradient Loss in the long-CoT RL scenario to address the above limitations:\n$$\\mathcal{J}_{\\text{DAPO}}(\\theta)=\\ \\ \\mathbb{E}_{(q,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)}$$\n$$\\Bigg[\\frac{1}{\\sum_{i=1}^{G}|o_{i}|}{\\sum_{i=1}^{G}\\sum_{t=1}^{|o_{i}|}\\min\\Big(r_{i,t}(\\theta)\\hat{A}_{i,t},\\ \\text{clip}\\Big(r_{i,t}(\\theta),1-\\varepsilon_{\\text{low}},1+\\varepsilon_{\\text{high}}\\Big)\\hat{A}_{i,t}\\Big)}\\Bigg],$$\n(12)\n$$\\text{s.t.}\\quad0<\\Big|\\{o_{i}\\mid\\texttt{is\\_equivalent}(a,o_{i}) \\}\\Big|<G.$$\nIn this setting, longer sequences can have more infuence on the overall gradient update compared to shorterl sequences. Moreover, from the perspective of individual tokens, if a particular generation pattern can lead to an increase or decrease in reward, it will be equally prompted or suppressed, regardless of the length of the response in which it appears.\n3.4Hide and Seek: Overlong Reward Shaping\nIn RL training, we typically set a maximum length for generation, with overlong samples truncated accordingly.We fnd that improper reward shaping for truncated samples can introduce reward noise and signifcantlyii disrupt the training process.\n\nFigure 5 The accuracy of the actor model on AIME and the entropy of its generation probabilities, both before and after applying Overlong Reward Shaping strategy.\nBy default, we assign a punitive reward to truncated samples. This approach may introduce noise into the training process, as a sound reasoning process can be penalized solely due to its excessive length. Such penalties can potentially confuse the model regarding the validity of its reasoning process.\nTo investigate the impact of this reward noise, we frst apply an Overlong Filtering strategy which masksi the loss of truncated samples. We fnd that this approach signifcantly stabilizes training and enhancesii performance, as demonstrated in Figure 5.\nAlgorithm 1DAPO: Decoupled Clip and Dynamic sAmpling Policy Optimization\nitial policy model $\\pi_{\\theta};$  reawrd model $R$  task prompts D; hyperparameters εlow, εhigh\n1: for step $=1\\text{,}...,\\text{M}$ do\n2:Sample a batch $\\mathcal{D}_{b}$  from $\\mathcal{D}$\n3:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$\n4:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(\\cdot|q)$  for each question $q\\in\\mathcal{D}_{b}$\n5:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$  by running $R$\n6:Filter out $o_{i}$  and add the remaining to the dynamic sampling bufer (Dynamic Sampling Equation (11))f 7:if bufer sizef $\\boldsymbol{n_{b}<N:}$\n8:continue\n9：For each $O_{2}$ in the bufer, computef $\\hat{A}_{i,t}$ for the $t$ th token of $o_{i}$  (Equation (9))\n10:for iteration $=1, ..., \\mu$ do\n11:Update the policy model $\\pi_{\\theta}$  by maximizing the DAPO objective (Equation (8))\nOutput $\\pi_{\\theta}$\nFurthermore, we propose Soft Overlong Punishment (Equation 13), a length-aware penalty mechanism designed to shape the reward for truncated samples. Specifcally, when the response length exceeds the predefnedii maximum value, we defne a punishment interval. Within this interval, the longer the response, the greater thei punishment it receives. This penalty is added to the original rule-based correctness reward, thereby signaling to the model to avoid excessively long responses.\n$$R_{\\text{length}}(y)=\\begin{cases}0,&|y|\\leq L_{\\text{max}}-L_{\\text{cache}}\\\\ \\frac{(L_{\\text{max}}-L_{\\text{cache}})-|y|}{L_{\\text{cache}}},&L_{\\text{max}}-L_{\\text{cache}}<|y|\\leq L_{\\text{max}}\\\\ -1,&L_{\\text{max}}<|y|\\end{cases}$$\n(13)\n\nFigure 6 The training progress before and after applying dynamic sampling on a baseline setting.\n3.5Dataset Transformation\nOur dataset is sourced from the web and ofcial competition homepages through a combination of webfi scraping and manual annotation. The answers of math dataset typically come in a variety of formats, such as expression, formula and number, which makes it challenging to design comprehensive rules to parse them.To provide accurate reward signals using rules and minimize errors introduced by formula parsers, inspired by AIME, we select and transform the answer√s into integers, which are easy to parse. For example, if the original answer is expressed in the form of $\\tfrac{a+\\sqrt{b}}{c},$  we instruct the LLM to modify the question so that the expected answer becomes $a+b+c.$  After selection and transformation, we obtained the DAPO-Math-17K dataset, which consists of 17K prompts, each paired with an integer as the answer.\n4Experiments\n4.1Training Details\nIn this work, we focus specifcally on mathematical tasks to evaluate our algorithm, which can be readilyi transferred to other tasks. We adopt the verl framework [20] for training. We use naive GRPO [38] as our baseline algorithm and estimate advantages using group reward normalization.\nFor hyper-parameters, we utilize the AdamW [39] optimizer with a constant learning rate of $1\\times10^{-6},$ incorporating a linear warm-up over 20 rollout steps. For rollout, the prompt batch size is 512 and we sample 16 responses for each prompt. For training, the mini-batch size is set to 512, i.e., 16 gradient updates for each rollout step. For Overlong Reward Shaping, we set the expected maximum length as 16,384 tokens and allocate additional 4,096 tokens as the soft punish cache. Therefore, the maximum number of tokens for generation is set to 20,480 tokens. As for the Clip-Higher mechanism, we set the clipping parameter $\\varepsilon_{\\text{low}}$ to 0.2 and $\\varepsilon_{\\text{high}}$  to 0.28, which efectively balance the trade-of between exploration and exploitation. Forff evaluation on AIME, we repeat the evaluation set for 32 times and report avg@32 for results stability. The inference hyperparameters of evaluation are set to temperature 1.0 and topp 0.7.\n4.2Main Results\nExperiments on AIME 2024 demonstrate that DAPO has successfully trained the Qwen-32B Base model into a powerful reasoning model, achieving performance superior to DeepSeek’s experiments on Qwen2.5-32B using the R1 approach. In Figure 1, we observe a substantial improvement of performance on AIME 2024,with accuracy increasing from near $0\\%$ to $50\\%.$  Notably, this improvement is achieved with only $50\\%$ of the training steps required by DeepSeek-R1-Zero-Qwen-32B.\nWe analyze the contributions of each training technique in our methodology, as detailed in Table 1. The observed improvements demonstrate the efectiveness of these techniques in RL training, each contributingf\n\nTable 1 Main results of progressive techniques applied to DAPO\n\n\n<html><body><table><tr><td>Model</td><td>AIME24avg@32</td></tr><tr><td>DeepSeek-R1-Zero-Qwen-32B</td><td>47</td></tr><tr><td>Naive GRPO</td><td>30</td></tr><tr><td>+ Overlong Filtering</td><td>36</td></tr><tr><td>+ Clip-Higher</td><td>38</td></tr><tr><td>+ Soft Overlong Punishment</td><td>41</td></tr><tr><td>+ Token-level Loss</td><td>42</td></tr><tr><td>+ Dynamic Sampling (DAPO)</td><td>50</td></tr></table></body></html>\n\nseveral accuracy points in AIME 2024. Notably, given the vanilla GRPO setting, only $30\\%$ accuracy can be reached by training from a Qwen2.5-32B base model.\nFor token-level loss, although it brings less performance improvement, we fnd it enhances training stabilityi and makes the length increase more healthily.\nWhen applying Dynamic Sampling, although more data needs to be sampled due to the fltering out ofi zero-gradient data, the overall training time is not signifcantly afected. As shown in Figure 6, although theif number of sampling instances increases, the model’s convergence time is even reduced, due to fewer training steps required.\n4.3Training Dynamics\nReinforcement learning on large language models is not only a cutting-edge research direction but also an intrinsically complex systems engineering challenge, characterized by the interdependence of its various subsystems. Modifcations to any single subsystem can propagate through the system, leading to unforeseeni consequences due to the intricate interplay among these components. Even seemingly minor changes in initial conditions, such as variations in data and hyperparameters, can amplify through iterative reinforcement learning processes, yielding substantial deviations in outcomes. This complexity often confronts researchers with a dilemma: even after meticulous analysis and well-founded expectations that a modifcation will enhancei specifc aspects of the training process, the actual results frequently diverge from the anticipated trajectory.i Therefore, monitoring of key intermediate results during experimentation is essential for swiftly identifying the sources of discrepancies and, ultimately, for refning the system.i\nThe Length of Generated Responses is a metric closely related to training stability and performance,as shown in Figure 7a. The increase in length provides the model with a larger space for exploration,allowing more complex reasoning behaviors to be sampled and gradually reinforced through training.However, it is important to note that length does not always maintain a continuous upward trend during training. In some considerable periods, it can exhibit a trend of stagnation or even decline, which has also been demonstrated in [2]. We typically use length in conjunction with validation accuracy as indicators to assess whether an experiment is deteriorating.\nThe Dynamics of Reward during training has always been one of the crucial monitoring indicators in reinforcement learning, as shown in Figure 7b. In the majority of our experiments, the trend of reward increase is relatively stable and does not fuctuate or decline signifcantly due to adjustments inli experimental settings. This indicates that, given a reliable reward signal, language models can robustly ft the distribution of training set. However, we fnd that the fnal reward on the training set ofteniii exhibits little correlation with the accuracy on the validation set, which indicates overftting to thei training set.\nThe Entropy of the Actor Model and Generation Probability are related to the model’s exploration capability and are key metrics that we closely monitor in our experiments. Intuitively, the model’s entropy needs to be maintained within an appropriate range. An excessively low entropy indicates that the probability distribution is overly sharp, leading to a loss of exploration capability. Conversely, an\n\nFigure 7 The metric curves of response length, reward score, generation entropy, and the mean probability of DAPO,which show the dynamics of RL training and serve as essential monitoring indicators to identify potential issues.\nexcessively high entropy is often associated with issues of over-exploration such as gibberish and repetitive generation. For the generation probability, the situation is exactly the opposite. As demonstrated in Section 3.1, by applying the Clip-Higher strategy, we efectively addressed the issue of entropy collapse.f In subsequent experiments, we fnd that maintaining a slow upward trend in entropy is conducive to thei improvement of model performance, shown in Figure 7c and Figure 7d.\n4.4Case Study\nDuring the RL training process, we observe an interesting phenomenon: the reasoning patterns of the actor model evolve dynamically over time. Specifcally, the algorithm not only reinforces existing reasoning patternsi that facilitate correct problem-solving but also gradually gives rise to entirely new modes of reasoning that were initially absent. This fnding reveals the adaptability and exploration capability of RL algorithms andi ofers new insights into the learning mechanisms of the model.f\nFor example, in the early stages of model training, there was virtually no occurrence of checking and refectingl on previous reasoning steps. However, as training progresses, the model exhibits distinct behaviors of refectionl and backtracking, as shown in Table 2. This observation sheds light on further exploration into interpreting the emergence of reasoning abilities during RL, which we leave for future research.\n5Conclusion\nIn this paper, we release a fully open-sourced system for large-scale LLM RL, including algorithm, code infrastructure, and dataset. The system achieves state-of-the-art large-scale LLM RL performance (AIME 50 using Qwen-32B pretrained model).We propose the Decoupled Clip and Dynamic sAmpling Policy\n\nQuestion:\nGiven that the base $ABC$ of the tetrahedron $\\boldsymbol{S-ABC}$ is an equilateral triangle, the projection $H$  of point $A$  on the face $\\mathit{SBC}$ is the orthocenter of $\\triangle SBC,$  the dihedral angle $H-AB-C$ is $30^{\\circ}$ and $SA=2,$  fnd the volume of the tetrahedron. The answer is in the formi $\\frac{k}{m}.$  so give the value of $k+m.$\nResponse:\nLet us consider the given problem: We need to determine ...\nNext, let $S$ have coordinates $(x_{0},y_{0},z_{0}).$ Since $\\it SA\\ =\\ 2,$ the distance formula gives us $\\sqrt{(x_{0})^{2}+(y_{0})^{2}+(z_{0})^{2}}=2,$ so we can write $x_{0}^{2}+y_{0}^{2}+z_{0}^{2}=4.$ Now, remember that $H$ is the orthogonal projection of\n...\nHowever, wait a moment, let’s rethink about the dihedral angle involving planes in a more thoughtful geometric way. Consider the plane $\\alpha_{1}=ABC,$  which lies entirely on the xy coordinate plane (as $z=0\\text{)}$  Let the plane $\\alpha_{2}=SBC.$  The point $A$  projected perpendicularly to plane $\\alpha_{2}$  lands on H.  The line $l=AB\\ \\ldots$\nTable 2 Emergence of Refective Behavior in Reinforcement Learningl\nOptimization (DAPO) algorithm, and introduce 4 key techniques to make RL powerfully efective and efcientffi in the long-CoT RL scenario. Additionally, by open-sourcing the training code and dataset, we provide the broader research community and society with practical access to a scalable reinforcement learning solution,enabling all to beneft from these advancements.i\n\nContributions\nProject Lead\nQiying $\\mathrm{Yu}^{1,2,4}$\nAlgorithm\nQiying $\\mathrm{Yu}^{1,2,4},$  Zheng Zhang1, Ruofei Zhu1, Yufeng Yuan1, Xiaochen $\\mathrm{Zuo}^{1},$  Yu $\\mathrm{Yue}^{1}$\nInfrastructure∗\nWeinan Dai1,2,4, Tiantian Fan1, Gaohong Liu1, Juncai $\\mathrm{Liu}^{1},$  Lingjun Liu1, Xin Liu1, Haibin Lin1, Zhiqi Lin1,Bole Ma1, Guangming Sheng1,3, Yuxuan Tong1,2,4, Qiying $\\mathrm{Yu}^{1,2,4},$  Chi Zhang1, Mofan Zhang1, Ru Zhang1, Wang Zhang1, Hang Zhu1, Jinhua $\\mathrm{Zhu}^{1}$\n∗Last-Name in Alphabetical Order\nDataset\nJiaze Chen1, Jiangjie Chen1,4, Chengyi Wang1, Hongli $\\mathrm{Yu}^{1,2,4},$  Yuxuan Song1,2,4, Xiangpeng Wei1, Qiying $\\mathrm{Yu}^{1,2,4}$\nSupervision\nHao Zhou2,4, Jingjing $\\text{Liu}^{\\text{\\tiny{2}},\\text{\\tiny{4}}},$ Wei-Ying Ma2,4, Ya-Qin Zhang2,4, Lin $\\text{Yan}^{1,4},$  Mu Qiao1,4, Yonghui Wu1,Mingxuan Wang1,4\nAffiliation\n1ByteDance Seed\n2Institute for AI Industry Research (AIR), Tsinghua University\n3The University of Hong Kong\n4SIA-Lab of Tsinghua AIR and ByteDance Seed\nAcknowledgments\nWe thank Zhengyin Du, Shengding Hu, Kai Shen, Tianyang Zhan, Zhen Xiao, Renjie Zheng, Li Han, Kaihua Jiang as well as other colleagues at ByteDance for their support for the DAPO project.\n\nReferences\n[1] OpenAI. Learning to reason with llms, 2024.\n[2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[3] OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[4] Anthropic. Claude 3.5 sonnet, 2024.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models are few-shot learners.Advances in neural information processing systems, 33:1877–1901, 2020.\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023.\n[7] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.\n[8] XAI. Grok 3 beta — the age of reasoning agents, 2024.\n[9] Google DeepMind. Gemini 2.0 fash thinking, 2024.l\n[10] Qwen. Qwq-32b: Embracing the power of reinforcement learning, 2024.\n[11] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025.\n[12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.\n[13] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, et al. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548, 2025.\n[14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasonerzero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025.\n[15] Jian Hu. Reinforce++: A simple and efcient approach for aligning large language models. arXiv preprintfi arXiv:2501.03262, 2025.\n[16] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu,Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025.\n[17] Jung Hyun Lee, June Yong Yang, Byeongho Heo, Dongyoon Han, and Kang Min Yoo.Token-supervised value models for enhancing mathematical reasoning capabilities of large language models.arXiv preprint arXiv:2407.12863, 2024.\n[18] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville,and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refned credit assignment. arXivi preprint arXiv:2410.01679, 2024.\n[19] Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. What’s behind ppo’s collapse in long-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025.\n[20] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridfow: A fexible and efcient rlhf framework. arXiv preprint arXiv:2409.19256, 2024.llfi\n[21] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n[22] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation, 2018.\n[23] Long Ouyang, Jefrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhinif Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,editors, Advances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran Associates,Inc., 2022.\n[24] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety, 2016.\n[25] Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg. Reinforcement learning with a corrupted reward channel, 2017.\n[26] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specifcation gaming: the fip side of ai ingenuity, 2020.il\n[27] Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward tampering problems and solutions in reinforcement learning: A causal infuence diagram perspective, 2021.l\nao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 202\n[29] Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024.\n[30] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020.\n[31] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476–482, 2024.\nTrinh and Thang Luong. Alphageometry: An olympiad-level ai system for geometry, 2024.\n[33] AlphaProof and AlphaGeometry Teams. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024.\n[34] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314–21328, 2022.\n[35] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.Refexion: Language agents with verbal reinforcement learning, 2023.l\n[36] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug,2023.\n[37] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025.\n[38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\n\nAppendix\nADataset Transformation\nHere is an example of data transformation, where we demonstrate the original problem, the transformed problem, and the model outputs during the transformation process (the original is in markdown text, which has been rendered for readability).\nTo ensure the model can complete this task with high accuracy, our goal is to have it perform comprehensive reasoning while avoiding hallucinations. We achieve this by providing a clear thinking framework that encourages thorough reasoning. Specifcally, we employ Chain of Thought (CoT) reasoning along with fouri well-defned steps: extract the answer format, rewrite the problem statement, solve the modifed problem andii provide an integer as the fnal answer. For each step, we provide few-shot solutions or detailed guidelines toi guide the model’s reasoning.\nThrough this approach, we have observed that in most cases, the LLM can generate reformulations with both format and quality that are satisfactory.\nOriginal Problem\nLet x and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible value of x. Answer: $11-2\\sqrt{6}$\nTransformed Problem\nLet x and be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible $y$ value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$ and $n$  are integers. Please fndi the value of $\\boldsymbol{k+m+n.}$  Answer: 19\nModel’s output (Markdown Text)Transformation Thought Process\nThe original answer to the problem is an algebraic expression $11-2\\sqrt{6}.$  We need to transform this into an integer form. First, note that the answer consists of an integer part and a square root term.According to the given rule, we replace the integer and square root components with undetermined coefcients. The original problem usesfi $x$  and as variab√les, so we choose other letters as undetermined $y$  coefcients. We express the answer in the formfi $\\boldsymbol{k-m\\sqrt{n}}$ and require the student to fndi $k+m+n$\nAnswer Format\nk −mn\nTransformed Problem\nLet $x$  and $y$  be real numbers such that $x^{2}+y^{2}-22x-16y+113=0.$  Determine the smallest possible value of x. The original answer is in the form $k-m \\sqrt{n},$  where $k,m,$  and $n$  are integers. Please fndi the value of $\\boldsymbol{k+m+n.}$\nAnswer Calculation for the Transformed Problem\nThe original answer is: $11-2\\sqrt{6}.$\nThe required format is: $\\boldsymbol{k-m\\sqrt{n}}.$\nMatching the coefcients:fi $k=11, m=2, n=6.$\nThe fnal result is:i $k+m+n=11+2+6=19.$\nFinal Answer\n\nBSupplementary Case\nQuestion:\nAmong the 900 residents of Aimeville, there are 195 who own a diamond ring, 367 who own a set of golf clubs, and 562 who own a garden spade. In addition, each of the 900 residents owns a bag of candy hearts. There are 437 residents who own exactly two of these things, and 234 residents who own exactly three of these things. Find the number of residents of Aimeville who own all four of these things.\nResponse:\nStep by Step Solution:\n1.Let’s defne the following variables:i\nLet $A$  be the set of residents who own a diamond ring.\nLet $B$  be the set of residents who own a set of golf clubs.\nLet $C$ be the set of residents who own a garden spade.\nLet $D$  be the set of residents who own a bag of candy hearts. Since all 900 residents own a bag of candy hearts, $|D|=900.$\nLet $a_{i}$ denote the number of residents who own exactly $i$  out of the 4 items.\nLet’s use the principle of inclusion-exclusion for the union of 4 sets $A,B,C,D.$\n...\n$$a_{4}=\\tfrac{-219}{-4}=\\tfrac{219}{4}=54.75.$$\nSince $a_{4},$  the number of people owning all four items, must be a whole number, our current approach needs to be reconsidered in terms of the combinatorial counting of people owning diferentf combinations of the items.\nInstead of directly using the inclusion-exclusion principle, we can use a diferent approach involvingf the number of people owning combinations of the items $A,B,C,$ and D.\nLet $x_{i}$ be the number of people owning exactly $i$  item among $A,B,C$ (excluding the item $D$  which everyone owns).\nThen the total number of people owning exactly $k$  items among the $A,B,C,$ and $D$  can be broken down:\nTable 3 A case of refective behavior emergence.l\n"}