{"code":0,"doc_size":1844642,"doc_type":"pdf","dst_path":"oss://glm-data-ocr-data/services/maas/pa/ad3c0df6-78b4-46dd-b563-53286a11b2cb.tar","markdown":"## DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n\nZhihong Shao1,2âˆ—â€ , Peiyi Wang1,3âˆ—â€ , Qihao Zhu1,3âˆ—â€ , Runxin Xu1, Junxiao Song1 Xiao $\\mathbf{Bi^{1}},$  Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1âˆ—\n\n1DeepSeek-AI, 2Tsinghua University, 3Peking University\n\nzhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com https://github.com/deepseek-ai/DeepSeek-Math\n\n## Abstract\n\nMathematical reasoning poses a signifcant challenge for language models due to its complexi and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pretraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of $51.7\\%$ on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves $60.9\\%$ on MATH.The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,we harness the signifcant potential of publicly available web data through a meticulouslyi engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.\n\n![269f628ad3495f211caccdc1d7ba12a4](imgs/269f628ad3495f211caccdc1d7ba12a4.jpg)\n\nFigure 1 | Top1 accuracy of open-source models on the competition-level MATH benchmark(Hendrycks et al., 2021) without the use of external toolkits and voting techniques.\n\n\n\n## 1.Introduction\n\nLarge language models (LLM) have revolutionized the approach to mathematical reasoning in artifcial intelligence, spurring signifcant advancements in both the quantitative reasoningii benchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024).Moreover, these models have proven instrumental in assisting humans in solving complex mathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible open-source models considerably trail behind in performance.\n\nIn this study, we introduce DeepSeekMath, a domain-specifc language model that signif-ii cantly outperforms the mathematical capabilities of open-source models and approaches the performance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeekMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This dataset is extracted from the Common Crawl (CC) using a fastText-based classifer (Joulin et al.,i 2016). In the initial iteration, the classifer is trained using instances from OpenWebMath (Pasteri et al., 2023) as positive examples, while incorporating a diverse selection of other web pages to serve as negative examples. Subsequently, we employ the classifer to mine additional positivei instances from the CC, which are further refned through human annotation. The classifer isii then updated with this enhanced dataset to improve its performance. The evaluation results indicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base 7B achieves $64.2\\%$ on GSM8K (Cobbe et al., 2021) and $36.2\\%$ on the competition-level MATH dataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In addition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese mathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience in mathematical data processing is a starting point for the research community, and there is signifcant room for improvement in the future.i\n\nDeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as we notice that starting from a code training model is a better choice compared to a general LLM. Furthermore, we observe the math training also improves model capability on MMLU(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only enhance the modelâ€™s mathematical abilities but also amplifes general reasoning capabilities.i\n\nAfter pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with chain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and tool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct 7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models.\n\nFurthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant reinforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).GRPO foregoes the critic model, instead estimating the baseline from group scores, signifcantlyi reducing training resources. By solely using a subset of English instruction tuning data, GRPO obtains a substantial improvement over the strong DeepSeekMath-Instruct, including both in-domain (GSM8K: $82.9\\%\\to88.2\\%,$ MATH: $46.8\\%\\to51.7\\%\\big)$  and out-of-domain mathematical tasks (e.g., CMATH: $84.6\\%\\to88.8\\%)$  during the reinforcement learning phase. We also provide a unifed paradigm to understand different methods, such as Rejection Sampling Fine-Tuningi(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and GRPO. Based on such a unifed paradigm, we fnd that all these methods are conceptualized asii either direct or simplifed RL techniques. We also conduct extensive experiments, e.g., onlinei v.s. offine training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,lto deeply investigate the essential elements of this paradigm. At last, we explain why our RL boosts the performance of instruction-tuned models, and further summarize potential directions to achieve more effective RL based on this unifed paradigm.i\n\n## 1.1. Contributions\n\nOur contribution includes scalable math pre-training, along with the exploration and analysis of reinforcement learning.\n\n## Math Pre-Training at Scale\n\nOur research provides compelling evidence that the publicly accessible Common Crawl data contains valuable information for mathematical purposes. By implementing a meticulously designed data selection pipeline, we successfully construct the DeepSeekMath Corpus, a high-quality dataset of 120B tokens from web pages fltered for mathemati-i cal content, which is almost 7 times the size of the math web pages used by Minerva(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath(Paster et al., 2023).\n\nOur pre-trained base model DeepSeekMath-Base 7B achieves comparable performance with Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not the only key factor in mathematical reasoning capability. A smaller model pre-trained on high-quality data could achieve strong performance as well.\n\nWe share our fndings from math training experiments. Code training prior to mathi training improves modelsâ€™ ability to solve mathematical problems both with and without tool use. This offers a partial answer to the long-standing question: does code training improve reasoning abilities? We believe it does, at least for mathematical reasoning.\n\nAlthough training on arXiv papers is common, especially in many math-related papers, it brings no notable improvements on all mathematical benchmarks adopted in this paper.\n\n## Exploration and Analysis of Reinforcement Learning\n\nWe introduce Group Relative Policy Optimization (GRPO), an effcient and effectivei reinforcement learning algorithm. GRPO foregoes the critic model, instead estimating the baseline from group scores, signifcantly reducing training resources compared toi Proximal Policy Optimization (PPO).\n\nWe demonstrate that GRPO signifcantly enhances the performance of our instruction-i tuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Furthermore, we observe enhancements in the out-of-domain performance during the reinforcement learning process.\n\nWe provide a unifed paradigm to understand different methods, such as RFT, DPO,i PPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offine training,l outcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so on to deeply investigate the essential elements of this paradigm.\n\nBased on our unifed paradigm, we explore the reasons behind the effectiveness of rein-i forcement learning, and summarize several potential directions to achieve more effective reinforcement learning of LLMs.\n\n## 1.2. Summary of Evaluations and Metrics\n\nEnglish and Chinese Mathematical Reasoning: We conduct comprehensive assessments of our models on English and Chinese benchmarks, covering mathematical problemsfrom grade-school level to college level. English benchmarks include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks include MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong et al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate modelsâ€™ ability to generate self-contained text solutions without tool use, and also the ability to solve problems using Python.\n\nOn English benchmarks, DeepSeekMath-Base is competitive with the closed-source Minerva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mistral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether theyâ€™ve undergone math pre-training or not, often by a signifcant margin. Notably,i DeepSeekMath-Base is superior on Chinese benchmarks, likely because we donâ€™t follow previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only math pre-training data, and also include high-quality non-English ones. With mathematical instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct and DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over $50\\%$ on the competition-level MATH dataset for the frst time within the open-sourcei community.\n\nFormal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal theorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates strong few-shot autoformalization performance.\n\nNatural Language Understanding, Reasoning, and Code: To build a comprehensive profle of modelsâ€™ general understanding, reasoning, and coding capabilities, we eval-i uate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)benchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering diverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 challenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code language models. Math pre-training benefts both language understanding and reasoningi performance.\n\n## 2.Math Pre-Training\n\n## 2.1. Data Collection and Decontamination\n\nIn this section, we will outline the process of constructing the DeepSeekMath Corpus from Common Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates how to systematically gather a large-scale mathematical corpus from Common Crawl, starting with a seed corpus (e.g., a small but high-quality collection of math-related dataset). Itâ€™s worth noting that this approach is also applicable to other domains, such as coding.\n\nFirst, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical web texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,2016) to recall more OpenWebMath-like mathematical web pages. Specifcally, we randomlyi select 500,000 data points from the seed corpus as positive training examples and another 500,000 web pages from Common Crawl as negative ones. We employ an open-source library1 for training, confguring the vector dimension to 256, learning rate to 0.1, the maximum lengthi\n\n\n\n![40b310208c8a4721da91ffc55b732b34](imgs/40b310208c8a4721da91ffc55b732b34.jpg)\n\n\n\nFigure 2 | An iterative pipeline that collects mathematical web pages from Common Crawl.\n\nof word n-gram to $3,$ the minimum number of word occurrences to 3, and the number of training epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based deduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then recall mathematical web pages from deduplicated Common Crawl with the fastText model.To flter out low-quality mathematical content, we rank the collected pages according to theiri scores predicted by the fastText model, and only preserve the top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the frst iteration, we choose to keep the top 40B tokens.i\n\nAfter the frst iteration of data collection, numerous mathematical web pages remain un-i collected, mainly because the fastText model is trained on a set of positive examples that lacks suffcient diversity. We therefore identify additional mathematical web sources to enrich the seedi corpus, so that we can optimize the fastText model. Specifcally, we frst organize the entire Com-ii mon Crawl into disjoint domains; a domain is defned as web pages sharing the same base URL.i For each domain, we calculate the percentage of web pages that are collected in the frst iteration.i Domains where over $10\\%$ of the web pages have been collected are classifed as math-relatedi(e.g., mathoverflow.net). Subsequently, we manually annotate the URLs associated with mathematical content within these identifed domains (e.g., mathoverflow.net/questions).i Web pages linked to these URLs, yet uncollected, will be added to the seed corpus. This approach enables us to gather more positive examples, thereby training an improved fastText model capable of recalling more mathematical data in the subsequent iteration. After four iterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B tokens. In the fourth iteration, we notice that nearly $98\\%$ of the data has already been collected in the third iteration, so we decide to cease data collection.\n\nTo avoid benchmark contamination, we follow Guo et al. (2024) to flter out web pagesi containing questions or answers from English mathematical benchmarks such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The fltering criteria are as follows: anyi text segment containing a 10-gram string that matches exactly with any sub-string from the evaluation benchmarks is removed from our math training corpus. For benchmark texts that are shorter than 10 grams but have at least 3 grams, we employ exact matching to flter outi contaminated web pages.\n\n\n\n## 2.2. Validating the Quality of the DeepSeekMath Corpus\n\nWe run pre-training experiments to investigate how the DeepSeekMath Corpus is compared with the recently released math-training corpora:\n\nMathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from textbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the majority (over $85\\%$ ) sourced from arXiv;\n\nOpenWebMath (Paster et al., 2023): CommonCrawl data fltered for mathematical content,i totaling 13.6B tokens;\n\nProof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWebMath, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B tokens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an arXiv:Web:Code ratio of 2:4:1.\n\n## 2.2.1. Training Setting\n\nWe apply math training to a general pre-trained language model with 1.3B parameters, which shares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeekLLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the effcient and light-weight HAI-LLM (High-fyer, 2023)il training framework. Following the training practice of DeepSeek LLMs, we use the AdamW optimizer (Loshchilov and Hutter, 2017) with $\\beta_{1}=0.9,\\beta_{2}=0.95,$ and weight_decay $=0.1,$  along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its $31.6\\%$ after $80\\%$ of the training process, and further decreases to $10.0\\%$ of the peak after $90\\%$ of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Math Corpus</td><td rowspan=\"2\">Size</td><td colspan=\"5\">English Benchmarks</td><td colspan=\"3\">Chinese Benchmarks</td></tr><tr><td>GSM8K MATH </td><td></td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathClozeMathQA </td><td>Gaokao</td></tr></thead><tbody><tr><td>No Math Training</td><td>N/A</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $2.9\\%$ </td><td> $15.6\\%$ </td><td> $19.5\\%$ </td><td> $12.3\\%$ </td><td> $0.8\\%$ </td><td> $17.9\\%$ </td></tr><tr><td>MathPile</td><td>8.9B</td><td> $2.7\\%$ </td><td> $3.3\\%$ </td><td> $2.2\\%$ </td><td> $12.5\\%$ </td><td> $15.7\\%$ </td><td> $1.2\\%$ </td><td> $0.0\\%$ </td><td> $2.8\\%$ </td></tr><tr><td>OpenWebMath</td><td>13.6B</td><td> $11.5\\%$ </td><td> $8.9\\%$ </td><td> $3.7\\%$ </td><td> $31.3\\%$ </td><td> $29.6\\%$ </td><td> $16.8\\%$ </td><td> $0.0\\%$ </td><td> $14.2\\%$ </td></tr><tr><td>Proof-Pile-2</td><td>51.9B</td><td> $14.3\\%$ </td><td> $11.2\\%$ </td><td> $3.7\\%$ </td><td> $43.8\\%$ </td><td> $29.2\\%$ </td><td> $19.9\\%$ </td><td> $5.1\\%$ </td><td> $11.7\\%$ </td></tr><tr><td>DeepSeekMath Corpus </td><td>120.2B</td><td> $\\mathbf{23.8\\%}$ </td><td> $\\mathbf{13.6\\%}$ </td><td> $4.8\\%$ </td><td> $\\textbf{56.3\\%}$ </td><td> $\\mathbf{33.1\\%}$ </td><td> $\\textbf{41.5\\%}$ </td><td> $5.9\\%$ </td><td> $\\mathbf{23.6\\%}$ </td></tr></tbody></table></body></html>\n\nTable 1 | Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evaluated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer with a vocabulary size of 100K.\n\n## 2.2.2. Evaluation Results\n\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and is the largest in size.\n\nHigh-quality: We evaluate downstream performance on 8 mathematical benchmarks using few-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear performance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that the model trained on the DeepSeekMath Corpus demonstrates better performance than\n\n\n\n![83e41fb90c1a2a879edc840e102f4e78](imgs/83e41fb90c1a2a879edc840e102f4e78.jpg)\n\nFigure 3 | Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.\n\nProof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of DeepSeekMath Corpus is higher.\n\nMultilingual: The DeepSeekMath Corpus encompasses data in multiple languages, predominantly featuring English and Chinese as the two most represented languages. As shown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning performance in both English and Chinese. In contrast, existing mathematical corpora,which are primarily English-centric, show limited improvement and may even hinder performance in Chinese mathematical reasoning.\n\nLarge-scale: The DeepSeekMath Corpus is several times larger than existing mathematical corpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeekMath Corpus, shows a steeper learning curve along with more lasting improvements. In contrast, the baseline corpora are much smaller, and have already been repeated multiple rounds during training, with the resulting model performance quickly reaching a plateau.\n\n## 2.3. Training and Evaluating DeepSeekMath-Base 7B\n\nIn this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning abilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: $56\\%$ is from the DeepSeekMath Corpus, $4\\%$ from AlgebraicStack, $10\\%$ from arXiv, $20\\%$ is Github code, and the remaining $10\\%$ is natural language data from Common Crawl in both English and Chinese. We mainly adopt the training setting specifed in Section 2.2.1, except that we set thei maximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens.\n\nWe conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMathBase 7B, focusing on its ability to produce self-contained mathematical solutions without relying on external tools, solve mathematical problems using tools, and conduct formal theorem proving.Beyond mathematics, we also provide a more general profle of the base model, including itsi performance of natural language understanding, reasoning, and programming skills.\n\nMathematical Problem Solving with Step-by-Step ReasoningWe evaluate DeepSeekMathBaseâ€™s performance of solving mathematical problems using few-shot chain-of-thought prompting (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encompass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),and CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks et al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse felds of mathematicsi from elementary to college-level complexity.\n\nAs shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight benchmarks among the open-source base models (including the widely-used general model Mistral 7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competitionlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over $10\\%$ absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base model 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained on mathematical texts.\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"4\">English Benchmarks</td><td colspan=\"3\">Chinese Benchmarks</td></tr><tr><td>GSM8K MATH </td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathClozeMathQA</td><td>Gaokao</td></tr></thead><tbody><tr><td colspan=\"10\">Closed-Source Base Model</td></tr><tr><td>Minerva</td><td>7B</td><td> $16.2\\%$ </td><td> $14.1\\%$ </td><td> $7.7\\%$ </td><td>-  $35.6\\%$ </td><td>-</td><td></td><td>-</td><td>-</td></tr><tr><td>Minerva</td><td>62B</td><td> $52.4\\%$ </td><td> $27.6\\%$ </td><td> $12.0\\%$ </td><td>-</td><td> $53.9\\%$ </td><td>-</td><td>-</td><td>-</td></tr><tr><td>Minerva</td><td>540B</td><td> $58.8\\%$ </td><td> $33.6\\%$ </td><td> $17.6\\%$ </td><td>-</td><td> $63.9\\%$ </td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan=\"10\">Open-Source Base Model</td></tr><tr><td>Mistral</td><td>7B</td><td> $40.3\\%$ </td><td> $14.3\\%$ </td><td> $9.2\\%$ </td><td> $71.9\\%$ </td><td> $51.1\\%$ </td><td> $44.9\\%$ </td><td> $5.1\\%$ </td><td> $23.4\\%$ </td></tr><tr><td>Llemma</td><td>7B</td><td> $37.4\\%$ </td><td> $18.1\\%$ </td><td> $6.3\\%$ </td><td> $59.4\\%$ </td><td> $43.1\\%$ </td><td> $43.4\\%$ </td><td> $11.9\\%$ </td><td> $23.6\\%$ </td></tr><tr><td>Llemma</td><td>34B</td><td> $54.0\\%$ </td><td> $25.3\\%$ </td><td> $10.3\\%$ </td><td> $71.9\\%$ </td><td> $52.9\\%$ </td><td> $56.1\\%$ </td><td> $11.9\\%$ </td><td> $26.2\\%$ </td></tr><tr><td>DeepSeekMath-Base 7B</td><td></td><td> $64.2\\%$ </td><td> $\\mathbf{36.2\\%}$ </td><td> $\\mathbf{15.4\\%}$ </td><td> $\\textbf{84.4\\%}$ </td><td> $\\mathbf{56.5\\%}$ </td><td> $\\textbf{71.7\\%}$ </td><td> $\\mathbf{20.3\\%}$ </td><td> $\\mathbf{35.3\\%}$ </td></tr></tbody></table></body></html>\n\nTable 2 | Comparisons between DeepSeekMath-Base 7B and strong base models on English and Chinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting.Minerva results are quoted from Lewkowycz et al. (2022a).\n\n\n\nMathematical Problem Solving with Tool UseWe evaluate program-aided mathematical reasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program where libraries such as math and sympy can be utilized for intricate computations. The execution result of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B outperforms the prior state-of-the-art Llemma 34B.\n\n\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"2\">Problem Solving w/ Tools</td><td colspan=\"2\">Informal-to-Formal Proving</td></tr><tr><td>GSM8K+Python</td><td> MATH+Python</td><td> miniF2F-valid miniF2F-test</td><td></td></tr><tr><td>Mistral</td><td>7B</td><td> $48.5\\%$ </td><td> $18.2\\%$ </td><td> $18.9\\%$ </td><td> $18.0\\%$ </td></tr><tr><td>CodeLlama</td><td>7B</td><td> $27.1\\%$ </td><td> $17.2\\%$ </td><td> $16.3\\%$ </td><td> $17.6\\%$ </td></tr><tr><td>CodeLlama</td><td>34B</td><td> $52.7\\%$ </td><td> $23.5\\%$ </td><td> $18.5\\%$ </td><td> $18.0\\%$ </td></tr><tr><td>Llemma</td><td>7B</td><td> $41.0\\%$ </td><td> $18.6\\%$ </td><td> $20.6\\%$ </td><td> $22.1\\%$ </td></tr><tr><td>Llemma</td><td>34B</td><td> $64.6\\%$ </td><td> $26.3\\%$ </td><td> $21.0\\%$ </td><td> $21.3\\%$ </td></tr><tr><td>DeepSeekMath-Base </td><td>7B</td><td> $66.9\\%$ </td><td> $\\textbf{31.4\\%}$ </td><td> $\\mathbf{25.8\\%}$ </td><td> $\\mathbf{24.6\\%}$ </td></tr></table></body></html>\n\nTable 3 | Few-shot evaluation of base modelsâ€™ ability to solve mathematical problems using tools and the ability to conduct informal-to-formal theorem proving in Isabelle.\n\nFormal MathematicsFormal proof automation is benefcial to ensure the accuracy and relia-i bility of mathematical proofs and enhance effciency, with increasing attention in recent years.i We evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,2022) which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a benchmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)to fll in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strongi performance in proof autoformalization.\n\n\n\n<html><body><table><thead><tr><td>Model</td><td>Size</td><td> MMLU</td><td>BBH</td><td>HumanEval (Pass@1) MBPP (Pass@1)</td><td></td></tr></thead><tbody><tr><td>Mistral</td><td>7B</td><td> $\\mathbf{62.4\\%}$ </td><td> $55.7\\%$ </td><td> $28.0\\%$ </td><td> $41.4\\%$ </td></tr><tr><td>DeepSeek-Coder-Base-v1.5â€ </td><td> 7B</td><td> $42.9\\%$ </td><td> $42.9\\%$ </td><td> $40.2\\%$ </td><td> $52.6\\%$ </td></tr><tr><td>DeepSeek-Coder-Base-v1.5</td><td>7B</td><td> $49.1\\%$ </td><td> $55.2\\%$ </td><td> $\\mathbf{43.2\\%}$ </td><td> $\\mathbf{60.4\\%}$ </td></tr><tr><td>DeepSeekMath-Base</td><td>7B</td><td> $54.9\\%$ </td><td> $\\textbf{59.5\\%}$ </td><td> $40.9\\%$ </td><td> $52.6\\%$ </td></tr></tbody></table></body></html>\n\nTable 4 | Evaluation on natural language understanding, reasoning, and code benchmarks.DeepSeek-Coder-Base-v1.5â€  is the checkpoint right before learning rate decay, which is used to train DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.On HumanEval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively.\n\nNatural Language Understanding, Reasoning, and CodeWe evaluate model performance of natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun et al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits signifcant enhancements in per-i formance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),illustrating the positive impact of math training on language understanding and reasoning.Additionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively maintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Overall, DeepSeekMath-Base 7B signifcantly outperforms the general model Mistral 7B (Jiang et al.,i 2023) on the three reasoning and coding benchmarks.\n\n## 3.Supervised Fine-Tuning\n\n## 3.1. SFT Data Curation\n\nWe construct a mathematical instruction-tuning dataset covering English and Chinese problems from different mathematical felds and of varying complexity levels: problems are paired withi solutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number of training examples is 776K.\n\nEnglish mathematical datasets: We annotate GSM8K and MATH problems with toolintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the training set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or PoT. Our English collection covers diverse felds of mathematics, e.g., algebra, probability,i number theory, calculus, and geometry.\n\nChinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning 76 sub-topics such as linear equations, with solutions annotated in both CoT and toolintegrated reasoning format.\n\n## 3.2. Training and Evaluating DeepSeekMath-Instruct 7B\n\nIn this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruction tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until reaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch size of 256 and a constant learning rate of 5e-5.\n\nWe evaluate modelsâ€™ mathematical performance both without and with tool use, on 4 quantitative reasoning benchmarks in English and Chinese. We benchmark our model against the leading models of the time:\n\nClosed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)and GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil et al., 2023), (3) Infection-2 (Infection AI, 2023), (4) Grok-1 3, as well as models recentlyll released by Chinese companies including (5) Baichuan $\\cdot3^{4},$  (6) the latest GLM-4 5 from the GLM family (Du et al., 2022). These models are for general purposes, most of which have undergone a series of alignment procedures.\n\nOpen-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeekAI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathematics including (5) InternLM2-Math $20\\mathrm{B}^{6}$ which builds on InternLM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised reward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,2023) which is Llama-2 70B fne-tuned on an augmented version of GSM8K and MATH,i(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fne-tuned to do tool-integratedi mathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on MathInstruct.\n\nAs shown in Table $5,$  under the evaluation setting where tool use is disallowed, DeepSeekMathInstruct 7B demonstrates strong performance of step-by-step reasoning.Notably, on the competition-level MATH dataset, our model surpasses all open-source models and the majority of proprietary models (e.g., Infection-2 and Gemini Pro) by at leastl $9\\%$ absolute. This is true even for models that are substantially larger (e.g., Qwen 72B) or have been specif-i cally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While DeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,it still underperforms GPT-4 and Gemini Ultra.\n\nUnder the evaluation setting where models are allowed to integrate natural language reasoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches an accuracy of $60\\%$ on MATH, surpassing all existing open-source models. On the other benchmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is 10 times larger.\n\n## 4.Reinforcement Learning\n\n## 4.1. Group Relative Policy Optimization\n\nReinforcement learning (RL) has been proven to be effective in further improving the mathematical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;Wang et al., 2023b). In this section, we introduce our effcient and effective RL algorithm, Groupi Relative Policy Optimization (GRPO).\n\n## 4.1.1. From PPO to GRPO\n\nProximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is widely used in the RL fne-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizesi LLMs by maximizing the following surrogate objective:\n\n$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}\\left[q\\sim P(Q),o\\sim\\pi_{\\theta_{old}}( O|q)\\right]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\min\\left[\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta} (o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\right)A_{t}\\right],$$\n\n(1)\n\nwhere $\\pi_{\\theta}$ and $\\pi_{\\theta_{old}}$ are the current and old policy models, and $q,o$ are questions and outputs sampled from the question dataset and the old policy $\\pi_{\\theta_{old}},$  respectively. ðœ€is a clipping-related hyper-parameter introduced in PPO for stabilizing training. $A_{t}$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based\n\n\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"2\">English Benchmarks</td><td colspan=\"2\"> Chinese Benchmarks</td></tr><tr><td>GSM8K</td><td>MATH</td><td>MGSM-zh </td><td>CMATH</td></tr></thead><tbody><tr><td colspan=\"7\">Chain-of-Thought Reasoning</td></tr><tr><td></td><td></td><td></td><td>Closed-Source Model</td><td></td><td></td><td></td></tr><tr><td>Gemini Ultra</td><td>-</td><td> $94.4\\%$ </td><td></td><td> $53.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>GPT-4</td><td>-</td><td></td><td> $92.0\\%$ </td><td> $52.9\\%$ </td><td>-</td><td> $86.0\\%$ </td></tr><tr><td>Infection-2</td><td>-</td><td></td><td> $81.4\\%$ </td><td> $34.8\\%$ </td><td>-</td><td>-</td></tr><tr><td>GPT-3.5</td><td>-</td><td></td><td> $80.8\\%$ </td><td> $34.1\\%$ </td><td>-</td><td> $73.8\\%$ </td></tr><tr><td>Gemini Pro</td><td>-</td><td></td><td> $86.5\\%$ </td><td> $32.6\\%$ </td><td>-</td><td>-</td></tr><tr><td>Grok-1</td><td>-</td><td></td><td> $62.9\\%$ </td><td> $23.9\\%$ </td><td>-</td><td>-</td></tr><tr><td>Baichuan-3</td><td>-</td><td></td><td> $88.2\\%$ </td><td> $49.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>GLM-4</td><td>-</td><td></td><td> $87.6\\%$ </td><td> $47.9\\%$ </td><td>-</td><td>-</td></tr><tr><td colspan=\"7\">Open-Source Model</td></tr><tr><td>InternLM2-Math</td><td>20B</td><td> $82.6\\%$ </td><td></td><td> $37.7\\%$ </td><td>-</td><td>-</td></tr><tr><td>Qwen</td><td></td><td>72B</td><td> $78.9\\%$ </td><td> $35.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>Math-Shepherd-Mistral</td><td>7B</td><td></td><td> $84.1\\%$ </td><td> $33.0\\%$ </td><td>-</td><td>-</td></tr><tr><td>WizardMath-v1.1</td><td>7B</td><td></td><td> $83.2\\%$ </td><td> $33.0\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeek-LLM-Chat</td><td>67B</td><td></td><td> $84.1\\%$ </td><td> $32.6\\%$ </td><td> $74.0\\%$ </td><td> $80.3\\%$ </td></tr><tr><td>MetaMath</td><td></td><td>70B</td><td> $82.3\\%$ </td><td> $26.6\\%$ </td><td> $66.4\\%$ </td><td> $70.9\\%$ </td></tr><tr><td>SeaLLM-v2</td><td>7B</td><td></td><td> $78.2\\%$ </td><td> $27.5\\%$ </td><td> $64.8\\%$ </td><td>-</td></tr><tr><td>ChatGLM3</td><td>6B</td><td></td><td> $72.3\\%$ </td><td> $25.7\\%$ </td><td>-</td><td>-</td></tr><tr><td>WizardMath-v1.0</td><td>70B</td><td></td><td> $81.6\\%$ </td><td> $22.7\\%$ </td><td> $64.8\\%$ </td><td> $65.4\\%$ </td></tr><tr><td>DeepSeekMath-Instruct</td><td> 7B</td><td></td><td> $82.9\\%$ </td><td> $46.8\\%$ </td><td> $73.2\\%$ </td><td> $84.6\\%$ </td></tr><tr><td>DeepSeekMath-RL</td><td>7B</td><td></td><td> $\\mathbf{88.2\\%}$ </td><td> $\\textbf{51.7\\%}$ </td><td> $79.6\\%$ </td><td> $\\mathbf{88.8\\%}$ </td></tr><tr><td colspan=\"7\">Tool-Integrated Reasoning</td></tr><tr><td></td><td></td><td></td><td>Closed-Source Model</td><td></td><td></td><td></td></tr><tr><td>GPT-4 Code Interpreter</td><td>-</td><td></td><td> $97.0\\%$ </td><td> $69.7\\%$ </td><td>-</td><td>-</td></tr><tr><td colspan=\"7\">Open-Source Model</td></tr><tr><td>InternLM2-Math</td><td></td><td>20B  $80.7\\%$ </td><td></td><td> $54.3\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeek-LLM-Chat</td><td></td><td>67B</td><td> $86.7\\%$ </td><td> $51.1\\%$ </td><td> $76.4\\%$ </td><td> $85.4\\%$ </td></tr><tr><td>ToRA</td><td></td><td>34B</td><td> $80.7\\%$ </td><td> $50.8\\%$ </td><td> $41.2\\%$ </td><td> $53.4\\%$ </td></tr><tr><td>MAmmoTH</td><td>70B</td><td></td><td> $76.9\\%$ </td><td> $41.8\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeekMath-Instruct </td><td>7B</td><td></td><td> $83.7\\%$ </td><td> $57.4\\%$ </td><td> $72.0\\%$ </td><td> $84.3\\%$ </td></tr><tr><td>DeepSeekMath-RL</td><td>7B</td><td></td><td> $\\mathbf{86.7\\%}$ </td><td> $\\textbf{58.8\\%}$ </td><td> $78.4\\%$ </td><td> $\\textbf{87.6\\%}$ </td></tr></tbody></table></body></html>\n\nTable 5 | Performance of Open- and Closed-Source models with both Chain-of-Thought and Tool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority votes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all opensource models from 7B to 70B, as well as the majority of closed-source models. Although DeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks.\n\n\n\n![38c1c84aacaba6c308ab5f1d6b92e40b](imgs/38c1c84aacaba6c308ab5f1d6b92e40b.jpg)\n\nFigure 4 | Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead estimating the baseline from group scores, signifcantly reducing training resources.i\n\non the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$  Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model,the standard approach is to add a per-token KL penalty from a reference model in the reward at each token (Ouyang et al., 2022), i.e.,\n\n$$r_{t}=r_{\\varphi}(\\mathit{q},\\mathit{o}_{\\leq t})-\\beta\\log\\frac{\\pi_{\\theta} (\\mathit{o}_{t}|\\mathit{q},\\mathit{o}_{<t})}{\\pi_{\\mathit{ref}}(\\mathit{o}_{t}|\\mathit{q},\\mathit{o}_{<t})},$$\n\n(2)\n\nwhere $r_{\\varphi}$ is the reward model, $\\pi_{ref}$ is the reference model, which is usually the initial SFT model,and $\\beta$ is the coeffcient of the KL penalty.i\n\nAs the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. Additionally,during RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token. To address this, as shown in Figure $4,$  we propose Group Relative Policy Optimization (GRPO), which obviates the need for additional value function approximation as in PPO, and instead uses the average reward of multiple sampled outputs, produced in response to the same question, as the baseline. More specifcally, for each questioni $q,$  GRPO samples a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model by maximizing the following objective:\n\n$$\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P( \\small{\\it Q}),\\{o_{i} \\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}( \\small{\\it O}|q)]$$\n\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|} \\left\\{\\min\\left[\\frac{\\pi_{\\theta}(o_{i,t}|\\mathsf{q},o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|\\mathsf{q},o_{i,<t})}\\hat{A}_{i,t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{i,t}|\\mathsf{q},o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|\\mathsf{q}, o_{i,<t})},1-\\varepsilon,1+\\varepsilon\\right)\\hat{A}_{i,t}\\right]-\\beta\\mathbb{D}_{KL}\\left[\\pi_{\\theta}||\\pi_{ref}\\right]\\right\\},$$\n\n(3)\n\nwhere ðœ€and $\\beta$ are hyper-parameters, and $\\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only, which will be detailed in the following subsections. The group relative way that GRPO leverages to calculate the advantages, aligns well with the comparative nature of rewards models, as reward models are typically trained on datasets of comparisons between outputs on the same question. Also note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}.$\n\n\n\n## Algorithm 1 Iterative Group Relative Policy Optimization\n\nput initial policy model $\\pi_{\\theta_{\\rm init}};$  reward models $r_{\\varphi};$  task prompts $\\mathcal{D}$  hyperparameters $\\varepsilon, \\beta, \\mu$\n\n1: policy model $\\pi_{\\theta}\\leftrightarrow\\pi_{\\theta_{\\text{init}}}$\n\n2: for iteration $=1,\\ldots,\\mathrm{I}$  do\n\n3:reference model $\\pi_{ref}\\leftrightarrow\\pi_{\\theta}$\n\n4:for step = 1, ..., M do\n\n5:Sample a batch $\\mathcal{D}_{b}$ from $\\mathcal{D}$\n\n6:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$\n\n7:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\cdot\\mid q)$  for each question $q\\in\\mathcal{D}_{b}$\n\n8:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$ by running $r_{\\varphi}$\n\n9:Compute $\\widehat{A}_{i,t}$ for the $t$ th token of $o_{i}$ through group relative advantage estimation.\n\n10:for GRPO iteration $=1,\\ldots,\\mu$ do\n\n11:Update the policy model $\\pi_{\\theta}$ by maximizing the GRPO objective (Equation 21)\n\n12:Update $r_{\\varphi}$ through continuous training using a replay mechanism.\n\n## Output $\\pi_{\\theta}$\n\nAnd different from the KL penalty term used in (2), we estimate the KL divergence with the following unbiased estimator (Schulman, 2020):\n\n$$\\text{I D}_{KL}\\left[\\pi_{\\theta}||\\pi_{ref}\\right]=\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-\\log\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-1,$$\n\n(4)\n\nwhich is guaranteed to be positive.\n\n## 4.1.2. Outcome Supervision RL with GRPO\n\nFormally, for each question $q,$  a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  are sampled from the old policy model $\\pi_{\\theta_{old}}.$  A reward model is then used to score the outputs, yielding $G$ rewards $\\mathbf{r}=\\{r_{1},r_{2},\\cdots,r_{G}\\}$  correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output $o_{i}$ and sets the advantages $\\hat{A}_{i,t}$ of all tokens in the output as the normalized reward, i.e., $\\hat{A}_{i,t}=\\widetilde{r}_{i}=\\tfrac{r_{i}-\\mathrm{mean}(\\mathbf{r})}{\\mathrm{std}(\\mathbf{r})},$ and then optimizes the policy by maximizing the objective defned in equation (3).i\n\n## 4.1.3. Process Supervision RL with GRPO\n\nOutcome supervision only provides a reward at the end of each output, which may not be suffcient and effcient to supervise the policy in complex mathematical tasks. Following Wangii et al. (2023b), we also explore process supervision, which provides a reward at the end of each reasoning step. Formally, given the question $q$ and $G$ sampled outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\},$  a process reward model is used to score each step of the outputs, yielding corresponding rewards: $\\mathbf{R}=\\{\\{r_{1}^{index}{}^{(1)},\\cdots,r_{1}^{index}{}^{(K_{1})}\\}, \\cdots,\\{r_{G}^{index}{}^{(1)},\\cdots,r_{G}^{index}{}^{(K_{G})}\\}\\},$  where $index(j)$  is the end token index of the $j$ th step, and $K_{i}$ is the total number of steps in the $i$ th output. We also normalize these rewards with the average and the standard deviation, i.e. $\\underline{\\widetilde{r}_{i}^{index(j)}}=\\underline{\\tfrac{r_{i}^{index(j)}-\\mathbf{mean(R)}}{\\mathbf{std(R)}}}.$  Subsequently,the process supervision calculates the advantage of each token as the sum of the normalized rewards from the following steps, i.e., $\\begin{array}{l}\\hat{A}_{i,t}=\\sum_{index(j)\\geq t}\\widetilde{r}_{i}^{index (j)},\\end{array}$  and then optimizes the policy by maximizing the objective defned in equation (3).i\n\n\n\n## 4.1.4. Iterative RL with GRPO\n\nAs the reinforcement learning training process progresses, the old reward model may not be suffcient to supervise the current policy model. Therefore, we also explore the iterative RLi with GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the reward model based on the sampling results from the policy model and continually train the old reward model using a replay mechanism that incorporates $10\\%$ of historical data. Then, we set the reference model as the policy model, and continually train the policy model with the new reward model.\n\n## 4.2. Training and Evaluating DeepSeekMath-RL\n\nWe conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-ofthought-format questions related to GSM8K and MATH from the SFT data, which consists of around 144K questions. We exclude other SFT questions to investigate the impact of RL on benchmarks that lack data throughout the RL phase. We construct the training set of reward models following (Wang et al., 2023b). We train our initial reward model based on the DeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the policy model as 1e-6. The KL coeffcient is 0.04. For each question, we sample 64 outputs. Thei max length is set to 1024, and the training batch size is 1024. The policy model only has a single update following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks following DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with chain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks can be regarded as out-of-domain tasks.\n\nTable 5 demonstrates the performance of open- and closed-source models with both chainof-thought and tool-integrated reasoning on English and Chinese benchmarks. We fnd that:i 1) DeepSeekMath-RL 7B attains accuracies of $88.2\\%$ and $51.7\\%$ on GSM8K and MATH, respectively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source models in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,DeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope of its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,showcasing the effectiveness of reinforcement learning.\n\n## 5.Discussion\n\nIn this section, we will share our fndings in pre-training and RL experiments.i\n\n## 5.1. Lessons Learnt in Pre-Training\n\nWe frst share our experience in pre-training. Unless otherwise specifed, we will adhere toii the training settings outlined in Section 2.2.1. It is worth noting that, when referring to the DeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of the data collection process.\n\n## 5.1.1. Code Training Benefts Mathematical Reasoningi\n\nA popular yet unverifed hypothesis suggests that code training improves reasoning. We attempti to offer a partial response to this, particularly within the mathematical domain: code training\n\n\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Training Setting</td><td colspan=\"3\">Training Tokens</td><td colspan=\"3\">w/o Tool Use</td><td colspan=\"2\">w/ Tool Use</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>General Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python</td><td></td></tr></thead><tbody><tr><td>No Continual Training</td><td>â€“</td><td>â€“</td><td>â€“</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $12.3\\%$ </td><td> $2.7\\%$ </td><td> $2.3\\%$ </td></tr><tr><td colspan=\"9\">Two-Stage Training</td></tr><tr><td>Stage 1: General Training</td><td>400B</td><td>â€“</td><td>â€“</td><td> $2.9\\%$ </td><td> $3.2\\%$ </td><td> $14.8\\%$ </td><td> $3.3\\%$ </td><td> $2.3\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $19.1\\%$ </td><td> $14.4\\%$ </td><td> $37.2\\%$ </td><td> $14.3\\%$ </td><td> $6.7\\%$ </td></tr><tr><td>Stage 1: Code Training</td><td>â€“</td><td>400B </td><td>â€“</td><td> $5.9\\%$ </td><td> $3.6\\%$ </td><td> $19.9\\%$ </td><td> $12.4\\%$ </td><td> $10.0\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>-</td><td>150B</td><td> $21.9\\%$ </td><td> $\\mathbf{15.3\\%}$ </td><td> $39.7 \\%$ </td><td> $17.4\\%$ </td><td> $9.4\\%$ </td></tr><tr><td colspan=\"9\">One-Stage Training</td></tr><tr><td>Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $20.5\\%$ </td><td> $13.1\\%$ </td><td> $37.6\\%$ </td><td> $11.4\\%$ </td><td> $6.5\\%$ </td></tr><tr><td>Code & Math Mixed Training â€“</td><td></td><td>400B</td><td> 150B</td><td> $17.6\\%$ </td><td> $12.1\\%$ </td><td> $36.3\\%$ </td><td> $\\mathbf{19.7\\%}$ </td><td> $\\mathbf{13.5\\%}$ </td></tr></tbody></table></body></html>\n\nTable 6 | Investigation of how code affects mathematical reasoning under different training settings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning performance without and with tool use via few-shot chain-of-thought prompting and few-shot program-of-thought prompting, respectively.\n\noves modelsâ€™ ability to do mathematical reasoning both with and without tool use\n\nTo study how code training affects mathematical reasoning, we experimented with the following two-stage training and one-stage training settings:\n\n## Two-Stage Training\n\nCode Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: We train DeepSeekLLM 1.3B for 400B code tokens followed by 150B math tokens;\n\nGeneral Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: As a control experiment, we also experiment with general tokens (sampled from a large-scale general corpus created by DeepSeek-AI) instead of code tokens in the frst stage of training, in ani attempt to investigate the advantages of code tokens over general tokens in improving mathematical reasoning.\n\n## One-Stage Training\n\nMath Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\n\nTraining on a mixture of 400B Code Tokens and 150B Math Tokens: Math training following code training degrades coding performance. We investigate whether code tokens,when mixed with math tokens for one-stage training, would still improve mathematical reasoning and also alleviate the problem of catastrophic forgetting.\n\nResultsTable 6 and Table 7 demonstrate the downstream performance under different training settings.\n\nCode training benefts program-aided mathematical reasoning, both under the two-stagei training and one-stage training settings. As shown in Table 6, under the two-stage training setting, code training alone already signifcantly enhances the ability to solve GSM8K andi MATH problems using Python. Math training in the second stage yields further improvements.Interestingly, under the one-stage training setting, mixing code tokens and math tokens effectively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also synergizes coding (Table 7) and program-aided mathematical reasoning (Table 6).\n\n\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Training Setting</td><td colspan=\"3\">Training Tokens</td><td rowspan=\"2\">MMLU</td><td rowspan=\"2\">BBHHumanEval (Pass@1) MBPP (Pass@1)</td><td rowspan=\"2\"></td></tr><tr><td>General Code Math</td><td></td></tr></thead><tbody><tr><td>No Continual Training</td><td>â€“</td><td>â€“ â€“</td><td> $24.5\\%$ </td><td> $28.1\\%$ </td><td> $12.2\\%$ </td><td> $13.0\\%$ </td></tr><tr><td colspan=\"7\">Two-Stage Training</td></tr><tr><td>Stage 1: General Training</td><td>400B</td><td>â€“</td><td>-</td><td> $25.9\\%$ </td><td> $27.7\\%$ </td><td> $15.2\\%$  $13.6\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $33.1\\%$ </td><td> $32.7\\%$  $12.8\\%$ </td><td> $13.2\\%$ </td></tr><tr><td>Stage 1: Code Training</td><td>â€“</td><td>400B -</td><td></td><td> $25.0\\%$ </td><td> $31.5\\%$  $25.0\\%$ </td><td> $\\mathbf{40.0\\%}$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $\\mathbf{36.2\\%}$ </td><td> $35.3\\%$  $12.2\\%$ </td><td> $17.0\\%$ </td></tr><tr><td colspan=\"7\">One-Stage Training</td></tr><tr><td>Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $32.3\\%$  $32.5\\%$ </td><td> $11.6\\%$  $\\mathbf{29.3\\%}$ </td><td> $13.2\\%$ </td></tr><tr><td>Code & Math Mixed Training </td><td>-</td><td>400B 150B</td><td></td><td> $33.5\\%$ </td><td> $\\mathbf{35.6\\%}$ </td><td> $39.4\\%$ </td></tr></tbody></table></body></html>\n\nTable 7 | Investigation of how different settings of code and math training affect model performance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM 1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting.On HumanEval and MBPP, we conduct zero-shot and few-shot evaluations, respectively.\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\"></td><td rowspan=\"2\">Size ArXiv Corpus</td><td colspan=\"4\">English Benchmarks</td><td colspan=\"4\">Chinese Benchmarks</td></tr><tr><td></td><td>GSM8K MATH </td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathCloze</td><td>Gaokao MathQA</td></tr></thead><tbody><tr><td rowspan=\"3\">DeepSeek-LLM</td><td rowspan=\"3\">1.3B</td><td>No Math Training</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $2.9\\%$ </td><td> $15.6\\%$ </td><td> $19.5\\%$ </td><td> $12.3\\%$ </td><td> $0.8\\%$ </td><td> $17.9\\%$ </td></tr><tr><td>MathPile</td><td> $2.7\\%$ </td><td> $3.3\\%$ </td><td> $2.2\\%$ </td><td> $12.5\\%$ </td><td> $15.7\\%$ </td><td> $1.2\\%$ </td><td> $0.0\\%$ </td><td> $2.8\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $3.3\\%$ </td><td> $3.4\\%$ </td><td> $4.0\\%$ </td><td> $9.4\\%$ </td><td> $9.0\\%$ </td><td> $7.4\\%$ </td><td> $0.8\\%$ </td><td> $2.3\\%$ </td></tr><tr><td rowspan=\"3\">DeepSeek-Coder-Base-v1.5 7B</td><td rowspan=\"3\"></td><td>No Math Training</td><td> $29.0\\%$ </td><td> $12.5\\%$ </td><td> $6.6\\%$ </td><td> $40.6\\%$ </td><td> $38.1\\%$ </td><td> $45.9\\%$ </td><td> $5.9\\%$ </td><td> $21.1\\%$ </td></tr><tr><td>MathPile</td><td> $23.6\\%$ </td><td> $11.5\\%$ </td><td> $7.0\\%$ </td><td> $46.9\\%$ </td><td> $35.8\\%$ </td><td> $37.9\\%$ </td><td> $4.2\\%$ </td><td> $25.6\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $28.1\\%$ </td><td>11.1%</td><td> $7.7\\%$ </td><td> $50.0\\%$ </td><td> $35.2\\%$ </td><td> $42.6\\%$ </td><td> $7.6\\%$ </td><td> $24.8\\%$ </td></tr></tbody></table></body></html>\n\nTable 8 | Effect of math training on different arXiv datasets. Model performance is evaluated with few-shot chain-of-thought prompting.\n\n\n\n<html><body><table><tr><td>ArXiv Corpus</td><td>miniF2F-valid</td><td> miniF2F-test</td></tr><tr><td>No Math Training</td><td> $20.1\\%$ </td><td> $21.7\\%$ </td></tr><tr><td>MathPile</td><td> $16.8\\%$ </td><td> $16.4\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $14.8\\%$ </td><td> $11.9\\%$ </td></tr></table></body></html>\n\nTable 9 | Effect of math training on different arXiv corpora, the base model being DeepSeekCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.\n\nCode training also improves mathematical reasoning without tool use. Under the two-stage training setting, the initial stage of code training already results in moderate enhancements.It also boosts the effciency of the subsequent math training, eventually leading to the besti performance. However, combining code tokens and math tokens for one-stage training compromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,due to its limited scale, lacks the capacity to fully assimilate both code and mathematical data simultaneously.\n\n## 5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\n\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev et al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,detailed analysis regarding their impact on mathematical reasoning has not been extensively conducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem ineffective in improving mathematical reasoning. We experiment with models of different sizes,including DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv corpora that underwent varied processing pipelines:\n\nMathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and flteringi heuristic rules, over $85\\%$ of which are scientifc arXiv papers;i\n\nArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX fles with preambles,i comments, macros, and bibliographies removed, totaling 28.0B tokens.\n\nIn our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeekCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective in improving mathematical reasoning. When trained on a arXiv-only corpus, both models display no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table 8), and formal mathematics like miniF2F (Table 9).\n\nHowever, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied:\n\nThe impact of arXiv tokens on specifc math-related tasks not included in this research,i such as informalization of theorems which is to convert formal statements or proofs to their informal versions;\n\nThe effect of arXiv tokens when combined with other types of data;\n\nâ€¢ Whether the benefts of arXiv papers would manifest themselves at a larger model scale.i Thus, further exploration is required, which we leave for future studies.\n\n## 5.2. Insights of Reinforcement Learning\n\n## 5.2.1. Towards to a Unifed Paradigmi\n\nIn this section, we provide a unifed paradigm to analyze different training methods, such asi SFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the unifed paradigm. Generally, the gradient with respect to the parameteri $\\theta$ of a training method can be written as:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathcal{A}}(\\theta)=\\mathbb{E}[\\underbrace{(q,o) \\sim\\mathcal{D}}_{Data Source}]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\underbrace{\\mathit{GC}_{\\mathcal{A}}(q,o,t,\\pi_{rf})}_{\\mathit{Gradient Coefficient}} \\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n\n(5)\n\nThere exist three key components: 1) Data Source $\\mathcal{D},$  which determines the training data; 2)Reward Function $\\pi_{rf},$  which is the source of the training reward signal; 3) Algorithm $\\mathcal{A}\\text{:}$  which processes the training data and the reward signal to the gradient coeffcienti $G\\mathcal{C}$ that determines the magnitude of the penalty or reinforcement for the data. We analyze several representative methods based on such a unifed paradigm:i\n\nSupervised Fine-tuning (SFT): SFT fne-tunes pretrained model on human selected SFTi data.\n\n\n\n\n\n<html><body><table><thead><tr><td>Methods</td><td>Data Source</td><td>Reward Function</td><td>Gradient Coeffcient</td></tr></thead><tbody><tr><td>SFT</td><td> $q,o\\sim P_{sft}(\\mathsf{Q},\\mathsf{O})$ </td><td>-</td><td>1</td></tr><tr><td>RFT</td><td> $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{sft}(\\mathsf{O}|q)$ </td><td>Rule</td><td>Equation 10</td></tr><tr><td>DPO</td><td> $q \\sim P_{sft}( Q ), o^{+}, o^{-}\\sim\\pi_{sft}(\\mathsf{O}| q )$ </td><td>Rule</td><td>Equation 14</td></tr><tr><td>Online RFT</td><td> $q\\sim P_{sft}(\\mathsf{Q}), o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ </td><td>Rule</td><td>Equation 10</td></tr><tr><td>PPO</td><td> $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ </td><td>Model</td><td>Equation 18</td></tr><tr><td>GRPO</td><td> $\\boldsymbol{q\\sim P_{sft}(Q),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta}(O|q)}$ </td><td>Model</td><td>Equation 21</td></tr></tbody></table></body></html>\n\nTable 10 | The data source and gradient coeffcient of different methods.i $P_{sft}$ denotes the data distribution of supervised fne-tuning datasets.i $\\pi_{\\theta_{sft}}$ and $\\pi_{\\theta}$ denote the supervised fne-tunedi model and the real-time policy model during the online training process, respectively.\n\n![fcd66559f5e33490d79f3a48343510e4](imgs/fcd66559f5e33490d79f3a48343510e4.jpg)\n\nFigure 5 | Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained using various methods, on two benchmarks.\n\nRejection Sampling Fine-tuning (RFT): RFT further fne-tunes the SFT model on thei fltered outputs sampled from the SFT model based on SFT questions. RFT flters theii outputs based on the correctness of their answers.\n\nDirect Preference Optimization (DPO): DPO further refnes the SFT model by fne-tuningii it on augmented outputs sampled from the SFT model, using pair-wise DPO loss.\n\nOnline Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT initiates the policy model using the SFT model and refnes it by fne-tuning with theii augmented outputs sampled from the real-time policy model.\n\nPPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces it with the outputs sampled from the real-time policy model.\n\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a more detailed derivation process.\n\nObservation about Data SourceWe divide the data source into two categories, online sampling, and offine sampling. Online sampling denotes that the training data is from the explo-l ration results of the real-time training policy model, while offine sampling denotes that thel\n\n\n\n![3e7f2cddcbded9f544f610a264d04d46](imgs/3e7f2cddcbded9f544f610a264d04d46.jpg)\n\nFigure 6 | Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on two benchmarks.\n\ntraining data is from the sampling results of the initial SFT model. RFT and DPO follow the offine style, while Online RFT and GRPO follow the online style.l\n\nAs shown in Figure 5, we fnd that the Online RFT signifcantly outperforms RFT on twoii benchmarks. Specifcally, Online RFT is comparable to RFT in the early stage of training buti gains an absolute advantage in the later stage, demonstrating the superiority of online training.This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more signifcant differences, and real-time data samplingi will offer greater advantages.\n\nObservation about Gradient CoeffcientThe algorithm processes the reward signal to thei gradient coeffcient to update the model parameter. We divide the reward function as â€˜Ruleâ€™i and â€˜Modelâ€™ in our experiments. Rule refers to judging the quality of a response based on the correctness of the answer, and Model denotes that we train a reward model to score each response. The training data of the reward model is based on the rule judgment. Equations 10 and 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its gradient coeffcient based on the reward value provided by the reward model. This allows fori differential reinforcement and penalization of responses according to their varying magnitudes.In contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly reinforces all responses with correct answers at the same level of intensity.\n\nAs demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the effciencyi of altering positive and negative gradient coeffcients. In addition, GRPO+PS shows superiori performance compared to GRPO+OS, indicating the benefts of using fne-grained, step-awareii gradient coeffcients. Furthermore, we explore the iterative RL, in our experiments, we conducti two rounds of iteration. As shown in Figure $6,$  we notice that the iterative RL signifcantlyi improves the performance, especially at the frst iteration.i\n\n\n\n![deb628a35fe29870d566481c44b7be39](imgs/deb628a35fe29870d566481c44b7be39.jpg)\n\nFigure 7 | The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K.\n\n## 5.2.2. Why RL Works?\n\nIn this paper, we conduct reinforcement learning based on a subset of instruction tuning data, and it achieves signifcant performance enhancement upon the instruction tuning model.i To further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K accuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances Maj@Kâ€™s performance but not Pass@K. These fndings indicate that RL enhances the modelâ€™si overall performance by rendering the output distribution more robust, in other words, it seems that the improvement is attributed to boosting the correct response from TopK rather than the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identifed ai misalignment problem in reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b).\n\n## 5.2.3. How to Achieve More Effective RL?\n\nWe demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unifedi paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplifed RL techniques. As summarized ini Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function.We provide some potential future directions about the three components.\n\nData SourceData source is the raw material of all training methods. In the context of $\\mathrm{RL},$ we specifcally refer to the data source as the unlabeled questions with the outputs sampled fromi the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction with advanced sampling (decoding)strategies, like those based on tree-search methods (Yao et al., 2023). Also, the effcient inferencei techniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determinesthe exploration effciency of policy models, also play an exceedingly important role.i\n\nAlgorithmsAlgorithms process the data and reward signal to the gradient coeffcient to updatei the model parameter. Based on Equation $5,$  to some extent, all methods now fully TRUST the signal of the reward function to increase or decrease the conditional probability of a certain token. However, it is impossible to ensure the reward signal is always reliable, especially in extremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023), which have been carefully annotated by well-trained annotators, still contain approximately $20\\%$ of incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm that is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,2023) alignment methods will bring a fundamental change to the learning algorithms.\n\nReward FunctionReward function is the source of the training signal. In RL, the reward function is usually the neural reward model. We think there exist three important directions for reward models: 1) How to enhance the generalization ability of the reward model. The reward model must be effectively generalized to handle out-of-distribution questions and advanced decoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of LLMs rather than improve their fundamental capabilities; 2) How to refect the uncertaintyl of reward model. The uncertainty could potentially act as a linking bridge between the weak reward model and the weak-to-strong learning algorithms; 3) How to effciently build high-i quality process reward models that can provide fne-grained training signals for the reasoningi process (Lightman et al., 2023; Wang et al., 2023b).\n\n## 6.Conclusion, Limitation, and Future Work\n\nWe present DeepSeekMath, which outperforms all open-source models on the competitionlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is initialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with a signifcant component of the training data being 120B math tokens sourced from Commoni Crawl. Our extensive ablation study shows web pages offer signifcant potential for high-qualityi mathematical data, while arXiv may not as benefcial as we expected. We introduce Groupi Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached a high score on benchmarks. We also provide a unifed paradigm to understand a series ofi methods and summarize several potential directions for more effective reinforcement learning.\n\nAlthough DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses,which may indicate data selection bias in pre-training and fne-tuning. In addition, restrictedi by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while DeepSeekMath shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs.\n\n\n\n## References\n\nR. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,E. Pitler, T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan,B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,M. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805.\n\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,2021.\n\nZ. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.\n\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n\nC. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,M. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.\n\nChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c om/THUDM/ChatGLM3.\n\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.URL https://arxiv.org/abs/2107.03374.\n\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\n\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,J. Hilton, R. Nakano, et al. Training verifers to solve math word problems. arXiv preprinti arXiv:2110.14168, 2021.\n\nT. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL https://github.com/togethercomputer/RedPajama-Data.\n\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,abs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485 50/arXiv.2401.02954.\n\n\n\nZ. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320â€“335,2022.\n\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. PAL: programaided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,pages 10764â€“10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html.\n\nZ. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A toolintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.doi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745 2.\n\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programmingâ€“ the rise of code intelligence, 2024.\n\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 2021.\n\nHigh-fyer. Hai-llm: é«˜æ•ˆä¸”è½»é‡çš„å¤§æ¨¡åž‹è®­ç»ƒå·¥å…·, 2023. URL https://www.high-flyer.cl n/en/blog/hai-llm.\n\nInfection AI. Infection-2, 2023. URL https://inflection.ai/inflection-2.ll\n\nA. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022.\n\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n\nA. Joulin, E. Grave, P. Bojanowski, M. Douze, H. JÃ©gou, and T. Mikolov. Fasttext. zip: Compressing text classifcation models. arXiv preprint arXiv:1612.03651, 2016.i\n\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.Effcient memory management for large language model serving with pagedattention. Ini Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\n\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274â€“19286. PMLR,2023.\n\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843â€“3857, 2022a.\n\n\n\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone,C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr act-Conference.html.\n\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,I. Sutskever, and K. Cobbe. Letâ€™s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n\nI. Loshchilov and F. Hutter.Decoupled weight decay regularization.arXiv preprint arXiv:1711.05101, 2017.\n\nH. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\n\nS. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sabharwal, P. Clark, and A. Kalyan. LILA: A unifed benchmark for mathematical reasoning.i In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5807â€“5832. Association for Computational Linguistics,2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/2022.emnlp-main.392.\n\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,C. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,abs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485 50/arXiv.2312.00738.\n\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.Advances in Neural Information Processing Systems, 35:27730â€“27744, 2022.\n\nK. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL https://doi.org/10.48550/arXiv.2310.06786.\n\nL. C. Paulson. Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,Proceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010,Edinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1â€“10.EasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd.\n\nS. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\n\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. 2023.\n\n\n\nJ. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app rox.html.\n\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=fR3wGCk-IXp.\n\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.\n\nM. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n\nT. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro soft.com/ai-anthology/terence-tao/.\n\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fne-tuned chat models. CoRR, abs/2307.09288,i 2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.\n\nT. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476â€“482, 2024.\n\nP. Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.\n\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b.\n\nZ. Wang, R. Xia, and P. Liu. Generative AI for math: Part I - mathpile: A billion-token-scale pretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120.URL https://doi.org/10.48550/arXiv.2312.17120.\n\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.URL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf 4f15af0f7b31abca4-Abstract-Conference.html.\n\n\n\nT. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.\n\nM. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A.MuÃ±oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International Conference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170 of Lecture Notes in Computer Science, pages 33â€“38. Springer, 2008. doi: 10.1007/978-3-540-7 1067-7\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7.\n\nH. Xia, T. Ge, P. Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909â€“3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20 23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.\n\nH. Xia, Z. Yang, Q. Dong, P. Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking effciencyi in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.\n\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffths, Y. Cao, and K. Narasimhan. Tree of thoughts:i Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,2023.\n\nL. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.Metamath: Bootstrap your own mathematical questions for large language models. CoRR,abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485 50/arXiv.2309.12284.\n\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a.\n\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b.\n\nX. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653.\n\nK. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021.\n\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.\n\n\n\n## A. Appendix\n\n## A.1. Analysis of Reinforcement Learning\n\nWe provide the detailed derivation of the data source and gradient coeffcient (algorithm andi reward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and GRPO.\n\n## A.1.1. Supervised Fine-tuning\n\nThe objective of Supervised Fine-tuning is maximizing the following objective:\n\n$$\\mathcal{J}_{S F T}(\\theta)=\\mathbb{E}[q,o\\sim P_{sft}(\\mathtt{Q},\\mathtt{O})]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n\n(6)\n\nThe gradient of $\\mathcal{J}_{{ST T}}(\\theta)$  is:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{SFT}=\\mathbb{E}\\left[q,o\\sim P_{sft}(Q,O)\\right] \\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n\n(7)\n\nData Source: The dataset employed for SFT. Reward Function: This can be regarded as human selection. Gradient Coeffcient: always set to 1.i\n\n## A.1.2. Rejection Sampling Fine-tuning\n\nRejection Sampling Fine-tuning frst samples multiple outputs from the supervised fne-tunedii LLMs for each question, and then trains LLMs on the sampled outputs with the correct answer.Formally, the objective of RFT is to maximize the following objectives:\n\n$$\\mathcal{J}_{\\mathit{RFT}}(\\theta)=\\mathbb{E}\\left[q\\sim P_{\\mathit{sft}}( \\mathit{Q}),o\\sim\\pi_{\\mathit{sft}}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|} \\sum_{t=1}^{|o|}\\mathbb{I}\\hskip-1.0pt\\mathbb{I}(o)\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n\n(8)\n\nThe gradient of $\\mathcal{J}_{{R} {F} {T}}(\\theta)$  is:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{RFT}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}( \\mathit{Q}),o\\sim\\pi_{sft}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\mathbb{I}\\hskip0.5pt(o)\\hskip0.5pt\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n\n(9)\n\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:Rule (whether the answer is correct or not). Gradient Coeffcient:i\n\n$$\\mathit{GC}_{\\mathit{RFT}}(\\mathit{q},\\mathit{o},t)=\\mathbb{I}(\\mathit{o})=\\left\\{\\begin{aligned} & 1&\\mathrm{the answer of o is correct}\\\\ & 0&\\mathrm{the answer of o is incorrect}\\end{aligned}\\right.$$\n\n(10)\n\n## A.1.3. Online Rejection Sampling Fine-tuning\n\nThe only difference between RFT and Online RFT is that the outputs of Online RFT are sampled from the real-time policy model $\\pi_{\\theta},$  rather than from the SFT model $\\pi_{\\theta_{sft}}.$  Therefore, the gradient of online RFT is:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{\\texttt{OnRFT}}(\\theta)=\\mathbb{E}\\left[q\\sim P_{\\mathit{sft}}(\\mathit{Q}),o\\sim\\pi_{\\theta}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\mathbb{I}\\hskip0.5pt(o)\\hskip0.5pt\\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}|q,\\mathit{o}_{<t})\\right).$$\n\n(11)\n\n\n\n## A.1.4. Direct Preference Optimization (DPO)\n\nThe objective of DPO is:\n\n$$\\mathcal{J}_{DPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(Q),o^{+},o^{-} \\sim\\pi_{sft}(O|q)]\\\\ \\log\\sigma\\left(\\beta\\frac{1}{|o^{+}|}\\sum_{t=1}^{|o^{+}|}\\log\\frac{\\pi_{\\theta}(o_{t}^{+}|q,o_{<t}^{+})}{\\pi_{\\mathrm{ref}}(o_{t}^{+}|q,o_{<t}^{+})}-\\beta\\frac{1}{|o^{-}|}\\sum_{t=1}^{|o^{-}|}\\log\\frac{\\pi_{\\theta}(o_{<t}^{-}|q,o_{<t}^{-})}{\\pi_{\\mathrm{ref}}(o_{<t}^{-}|q,o_{<t}^{-})}\\right)$$\n\n(12)\n\nThe gradient of $\\mathcal{J}_{DPO}(\\theta)$  is:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{DPO}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}( \\mathit{Q}),o^{+},o^{-}\\sim\\pi_{sft}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o^{+} |}\\sum_{t=1}^{|o^{+}|}\\mathit{GC}_{DPO}(\\mathit{q},\\mathit{o},t)\\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}^{+}|\\mathit{q},\\mathit{o}_{<t}^{+})\\right.$$\n\n(13)\n\n$$-\\left.\\frac{1}{|o^{-}|}\\sum_{t=1}^{|o^{-}|}GC_{DPO}(q,o,t)\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}^{-}|q,o_{<t}^{-})\\right)$$\n\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:human preference in the general domain (can be â€˜Ruleâ€™ in mathematical tasks). Gradient Coeffcient:iâˆ’âˆ’++\n\n$$\\mathit{GC}_{DPO}(q,o,t)=\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(o_{t}^{-}|q,o_{<t}^{-})}{\\pi_{ref}(o_{t}^{-}|q,o_{<t}^{-})}-\\beta\\log\\frac{\\pi_{\\theta}(o_{t}^{+}|q,o_{<t}^{+})}{\\pi_{ref}(o_{t}^{+}|q,o_{<t}^{+})}\\right)$$\n\n(14)\n\n## A.1.5. Proximal Policy Optimization (PPO)\n\nThe objective of PPO is:\n\n$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}(Q),o\\sim\\pi_{\\theta_{old}}(O|q)\\right]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\min\\left[\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t} )}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\right)A_{t}\\right].$$\n\n(15)\n\nTo simplify the analysis, it is assumed that the model only has a single update following each exploration stage, thereby ensuring that $\\pi_{\\theta_{old}}=\\pi_{\\theta}.$  In this case, we can remove the min and clip operation:\n\n$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\textit{Q}),o\\sim\\pi_{\\theta_{old}}(\\textit{O}|q)]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t}.$$\n\n(16)\n\nThe gradient of $\\mathcal{J}_{PPO}(\\theta)$  is:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\mathit{Q}),o\\sim\\pi_{\\theta_{old}}(\\mathit{O}|q)]\\frac{1}{|o|}\\sum_{t=1}^{|o|}A_{t} \\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}|q,o_{<t})$$\n\n(17)\n\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:reward model. Gradient Coeffcient:i\n\n$$GC_{PPO}(q,o,t,\\pi_{\\theta_{rm}})=A_{t},$$\n\n(18)\n\nwhere $A_{t}$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$\n\n## A.1.6. Group Relative Policy Optimization (GRPO)\n\nThe objective of GRPO is (assume $\\pi_{{{\\theta}_{old}}}=\\pi_{{{\\theta}}}$ for simplifed analysis):i\n\n$$\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\textit{Q}),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\textit{O}|q)]$$\n\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\left[\\frac{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta_{std}}(o_{i,t}|q,o_{i,<t})}\\hat{A}_{i,t}-\\beta(\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-\\log\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-1) \\right].$$\n\n(19)\n\n\n\nThe gradient of $\\mathcal{J}_{GRPO}(\\theta)$  is:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\mathsf{Q} ),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\mathsf{O}|q)]$$\n\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\left[\\hat{A}_{i,t}+\\beta\\left(\\frac{\\pi_{ref}(o_{i,t}|o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|o_{i,<t})}-1\\right)\\right]\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{i,t}|q,o_{i,<t}).$$\n\n(20)\n\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:reward model. Gradient Coeffcient:i\n\n$$\\textit{GC}_{GRPO}(\\textit{q},\\textit{o},\\textit{t},\\pi_{\\theta_{rm}})=\\hat{A}_{i,t}+\\beta\\left(\\frac{\\pi_{ref}(o_{i,t}|o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|o_{i,<t})}-1\\right),$$\n\n(21)\n\nwhere $\\hat{A}_{i,t}$ is computed based on the group reward scores.\n\n","msg":"","ocr_all":false,"page_count":30,"pages":[{"abandon_blocks":[{"bbox":{"x0":136,"x1":619,"y0":1551,"y1":1612},"conf":0.8517,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":144,"x1":612,"y0":1568,"y1":1605},"font_size":0.0,"text":"â€  Work done during internship at DeepSeek-AI."},{"bbox":{"x0":145,"x1":340,"y0":1549,"y1":1577},"font_size":0.0,"text":"âˆ—Core contributors."}],"source":"layout det","text":""},{"bbox":{"x0":128,"x1":1059,"y0":59,"y1":139},"conf":0.4179,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":207,"x1":386,"y0":75,"y1":112},"font_size":0.0,"text":""},{"bbox":{"x0":145,"x1":210,"y0":70,"y1":118},"font_size":0.0,"text":""}],"source":"layout det","text":""},{"bbox":{"x0":24,"x1":81,"y0":508,"y1":1230},"conf":0.3235,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":30,"x1":69,"y0":872,"y1":1223},"font_size":0.0,"text":".  3v00330a2042:viXr"},{"bbox":{"x0":30,"x1":73,"y0":519,"y1":867},"font_size":0.0,"text":"74202 rpA [2  ]LC.sc"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":203,"x1":988,"y0":222,"y1":316},"conf":0.9441,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":211,"x1":978,"y0":226,"y1":263},"font_size":-2.6289999999999996e-29,"text":"DeepSeekMath: Pushing the Limits of Mathematical"},{"bbox":{"x0":319,"x1":872,"y0":270,"y1":305},"font_size":-2.6289999999999996e-29,"text":"Reasoning in Open Language Models"}],"source":"layout det","text":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"},{"bbox":{"x0":237,"x1":941,"y0":349,"y1":413},"conf":0.6821,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":241,"x1":930,"y0":354,"y1":384},"font_size":-2.6289999999999996e-29,"text":"Zhihong Shao1,2âˆ—â€ , Peiyi Wang1,3âˆ—â€ , Qihao Zhu1,3âˆ—â€ , Runxin Xu1, Junxiao Song1"},{"bbox":{"x0":257,"x1":927,"y0":377,"y1":409},"font_size":-2.6289999999999996e-29,"text":"Xiao $\\mathbf{Bi^{1}},$  Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1âˆ—"}],"source":"layout det","text":"Zhihong Shao1,2âˆ—â€ , Peiyi Wang1,3âˆ—â€ , Qihao Zhu1,3âˆ—â€ , Runxin Xu1, Junxiao Song1 Xiao $\\mathbf{Bi^{1}},$  Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1âˆ—"},{"bbox":{"x0":338,"x1":854,"y0":415,"y1":454},"conf":0.6929,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":346,"x1":842,"y0":417,"y1":446},"font_size":-2.6289999999999996e-29,"text":"1DeepSeek-AI, 2Tsinghua University, 3Peking University"}],"source":"layout det","text":"1DeepSeek-AI, 2Tsinghua University, 3Peking University"},{"bbox":{"x0":321,"x1":882,"y0":461,"y1":523},"conf":0.7269,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":310,"x1":877,"y0":463,"y1":489},"font_size":-2.6289999999999996e-29,"text":"zhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com"},{"bbox":{"x0":335,"x1":853,"y0":488,"y1":514},"font_size":-2.6289999999999996e-29,"text":"https://github.com/deepseek-ai/DeepSeek-Math"}],"source":"layout det","text":"zhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com https://github.com/deepseek-ai/DeepSeek-Math"},{"bbox":{"x0":529,"x1":663,"y0":578,"y1":620},"conf":0.8627,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":533,"x1":658,"y0":579,"y1":614},"font_size":-2.6289999999999996e-29,"text":"Abstract"}],"source":"layout det","text":"Abstract"},{"bbox":{"x0":128,"x1":1062,"y0":652,"y1":1008},"conf":0.9839,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":656,"y1":686},"font_size":-2.6289999999999996e-29,"text":"Mathematical reasoning poses a signifcant challenge for language models due to its complexi"},{"bbox":{"x0":136,"x1":1053,"y0":684,"y1":714},"font_size":-2.6289999999999996e-29,"text":"and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-"},{"bbox":{"x0":140,"x1":1051,"y0":716,"y1":740},"font_size":-2.6289999999999996e-29,"text":"training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common"},{"bbox":{"x0":136,"x1":1053,"y0":740,"y1":772},"font_size":-2.6289999999999996e-29,"text":"Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an"},{"bbox":{"x0":138,"x1":1051,"y0":773,"y1":798},"font_size":-2.6289999999999996e-29,"text":"impressive score of $51.7\\%$ on the competition-level MATH benchmark without relying on"},{"bbox":{"x0":136,"x1":1051,"y0":800,"y1":826},"font_size":-2.6289999999999996e-29,"text":"external toolkits and voting techniques, approaching the performance level of Gemini-Ultra"},{"bbox":{"x0":136,"x1":1051,"y0":828,"y1":854},"font_size":-2.6289999999999996e-29,"text":"and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves $60.9\\%$ on MATH."},{"bbox":{"x0":140,"x1":1053,"y0":860,"y1":884},"font_size":-2.6289999999999996e-29,"text":"The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,"},{"bbox":{"x0":136,"x1":1049,"y0":888,"y1":914},"font_size":-2.6289999999999996e-29,"text":"we harness the signifcant potential of publicly available web data through a meticulouslyi"},{"bbox":{"x0":140,"x1":1051,"y0":917,"y1":940},"font_size":-2.6289999999999996e-29,"text":"engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization"},{"bbox":{"x0":136,"x1":1053,"y0":942,"y1":974},"font_size":-2.6289999999999996e-29,"text":"(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning"},{"bbox":{"x0":138,"x1":782,"y0":974,"y1":998},"font_size":-2.6289999999999996e-29,"text":"abilities while concurrently optimizing the memory usage of PPO."}],"source":"layout det","text":"Mathematical reasoning poses a signifcant challenge for language models due to its complexi and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pretraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of $51.7\\%$ on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves $60.9\\%$ on MATH.The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,we harness the signifcant potential of publicly available web data through a meticulouslyi engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO."},{"bbox":{"x0":277,"x1":915,"y0":1016,"y1":1397},"conf":0.9787,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![269f628ad3495f211caccdc1d7ba12a4](imgs/269f628ad3495f211caccdc1d7ba12a4.jpg)"},{"bbox":{"x0":131,"x1":1058,"y0":1403,"y1":1472},"conf":0.9474,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1409,"y1":1433},"font_size":-2.6289999999999996e-29,"text":"Figure 1 | Top1 accuracy of open-source models on the competition-level MATH benchmark"},{"bbox":{"x0":138,"x1":934,"y0":1435,"y1":1461},"font_size":-2.6289999999999996e-29,"text":"(Hendrycks et al., 2021) without the use of external toolkits and voting techniques."}],"source":"layout det","text":"Figure 1 | Top1 accuracy of open-source models on the competition-level MATH benchmark(Hendrycks et al., 2021) without the use of external toolkits and voting techniques."}],"formula_dets":[{"bbox":{"x0":887,"x1":946,"y0":830,"y1":854},"conf":0.8149,"label":"print_embedding","label_id":0},{"bbox":{"x0":341,"x1":403,"y0":773,"y1":797},"conf":0.8094,"label":"print_embedding","label_id":0},{"bbox":{"x0":306,"x1":337,"y0":384,"y1":405},"conf":0.6447,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":128,"x1":1062,"y0":652,"y1":1008},"conf":0.9839,"label":"Text","label_id":1},{"bbox":{"x0":277,"x1":915,"y0":1016,"y1":1397},"conf":0.9787,"label":"Figure","label_id":3},{"bbox":{"x0":131,"x1":1058,"y0":1403,"y1":1472},"conf":0.9474,"label":"Figure caption","label_id":4},{"bbox":{"x0":203,"x1":988,"y0":222,"y1":316},"conf":0.9441,"label":"Title","label_id":0},{"bbox":{"x0":529,"x1":663,"y0":578,"y1":620},"conf":0.8627,"label":"Title","label_id":0},{"bbox":{"x0":136,"x1":619,"y0":1551,"y1":1612},"conf":0.8517,"label":"Abandon","label_id":2},{"bbox":{"x0":321,"x1":882,"y0":461,"y1":523},"conf":0.7269,"label":"Text","label_id":1},{"bbox":{"x0":338,"x1":854,"y0":415,"y1":454},"conf":0.6929,"label":"Text","label_id":1},{"bbox":{"x0":237,"x1":941,"y0":349,"y1":413},"conf":0.6821,"label":"Text","label_id":1},{"bbox":{"x0":128,"x1":1059,"y0":59,"y1":139},"conf":0.4179,"label":"Abandon","label_id":2},{"bbox":{"x0":24,"x1":81,"y0":508,"y1":1230},"conf":0.3235,"label":"Abandon","label_id":2},{"bbox":{"x0":24,"x1":76,"y0":508,"y1":1230},"conf":0.3005,"label":"Abandon","label_id":2},{"bbox":{"x0":204,"x1":392,"y0":66,"y1":123},"conf":0.2983,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[144,1568],[612,1570],[611,1605],[144,1603]],"score":0.7206},{"poly":[[146,1549],[340,1553],[340,1577],[145,1573]],"score":0.7803},{"poly":[[138,1435],[934,1437],[934,1461],[138,1459]],"score":0.8653},{"poly":[[140,1409],[1049,1409],[1049,1433],[140,1433]],"score":0.7789},{"poly":[[588,1372],[638,1372],[638,1395],[588,1395]],"score":0.8403},{"poly":[[691,1352],[769,1352],[769,1377],[691,1377]],"score":0.8477},{"poly":[[549,1352],[626,1352],[626,1377],[549,1377]],"score":0.8092},{"poly":[[409,1352],[487,1352],[487,1377],[409,1377]],"score":0.8369},{"poly":[[831,1351],[906,1351],[906,1375],[831,1375]],"score":0.8977},{"poly":[[303,1331],[335,1331],[335,1354],[303,1354]],"score":0.8477},{"poly":[[351,1330],[369,1330],[369,1345],[351,1345]],"score":0.6154},{"poly":[[340,1291],[441,1291],[441,1310],[340,1310]],"score":0.9798},{"poly":[[627,1252],[638,1252],[638,1261],[627,1261]],"score":0.7622},{"poly":[[504,1224],[643,1226],[643,1251],[503,1249]],"score":0.796},{"poly":[[604,1209],[716,1209],[716,1233],[604,1233]],"score":0.7365},{"poly":[[648,1184],[792,1180],[793,1210],[649,1214]],"score":0.6895},{"poly":[[687,1162],[816,1157],[817,1187],[688,1191]],"score":0.7426},{"poly":[[744,1137],[836,1133],[837,1159],[745,1163]],"score":0.8412},{"poly":[[353,1095],[509,1095],[509,1119],[353,1119]],"score":0.7859},{"poly":[[303,1052],[333,1052],[333,1075],[303,1075]],"score":0.8197},{"poly":[[354,1045],[434,1045],[434,1065],[354,1065]],"score":0.9573},{"poly":[[705,1038],[884,1038],[884,1061],[705,1061]],"score":0.9349},{"poly":[[353,1023],[457,1023],[457,1047],[353,1047]],"score":0.8513},{"poly":[[138,974],[782,974],[782,998],[138,998]],"score":0.7743},{"poly":[[137,942],[1053,944],[1053,974],[136,972]],"score":0.6878},{"poly":[[140,917],[1051,917],[1051,940],[140,940]],"score":0.9487},{"poly":[[136,888],[1049,888],[1049,912],[136,912]],"score":0.7435},{"poly":[[30,872],[67,872],[69,1223],[32,1223]],"score":0.7427},{"poly":[[140,860],[1053,860],[1053,884],[140,884]],"score":0.7516},{"poly":[[136,830],[1051,828],[1051,853],[136,854]],"score":0.8114},{"poly":[[136,802],[1051,800],[1051,824],[136,826]],"score":0.799},{"poly":[[138,774],[1051,774],[1051,798],[138,798]],"score":0.7576},{"poly":[[137,740],[1053,742],[1053,772],[136,770]],"score":0.7179},{"poly":[[140,716],[1051,716],[1051,740],[140,740]],"score":0.7517},{"poly":[[136,684],[1053,684],[1053,714],[136,714]],"score":0.6725},{"poly":[[138,656],[1053,656],[1053,686],[138,686]],"score":0.6905},{"poly":[[534,579],[658,583],[657,614],[533,610]],"score":0.8308},{"poly":[[30,519],[73,519],[73,867],[30,867]],"score":0.6684},{"poly":[[335,489],[852,488],[853,512],[335,514]],"score":0.7911},{"poly":[[310,463],[877,465],[877,489],[310,488]],"score":0.8016},{"poly":[[346,417],[842,421],[842,446],[346,442]],"score":0.8011},{"poly":[[257,379],[927,377],[927,407],[257,409]],"score":0.7551},{"poly":[[241,354],[930,354],[930,384],[241,384]],"score":0.7172},{"poly":[[319,270],[872,270],[872,305],[319,305]],"score":0.7771},{"poly":[[211,228],[978,226],[978,261],[211,263]],"score":0.8083},{"poly":[[207,75],[386,75],[386,112],[207,112]],"score":0.9377},{"poly":[[145,83],[201,70],[210,105],[153,118]],"score":0.6326}],"page_no":0,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":607,"y0":1551,"y1":1579},"conf":0.6094,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1554,"y1":1573},"font_size":0.0,"text":"2"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":330,"y0":166,"y1":200},"conf":0.884,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":324,"y0":172,"y1":196},"font_size":0.0,"text":"1. Introduction"}],"source":"layout det","text":"1.Introduction"},{"bbox":{"x0":132,"x1":1059,"y0":219,"y1":417},"conf":0.9776,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":221,"y1":253},"font_size":0.0,"text":"Large language models (LLM) have revolutionized the approach to mathematical reasoning"},{"bbox":{"x0":138,"x1":1047,"y0":251,"y1":275},"font_size":0.0,"text":"in artifcial intelligence, spurring signifcant advancements in both the quantitative reasoningii"},{"bbox":{"x0":138,"x1":1051,"y0":279,"y1":303},"font_size":0.0,"text":"benchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024)."},{"bbox":{"x0":135,"x1":1051,"y0":302,"y1":333},"font_size":0.0,"text":"Moreover, these models have proven instrumental in assisting humans in solving complex"},{"bbox":{"x0":136,"x1":1053,"y0":330,"y1":360},"font_size":0.0,"text":"mathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,"},{"bbox":{"x0":140,"x1":1049,"y0":360,"y1":384},"font_size":0.0,"text":"2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible"},{"bbox":{"x0":136,"x1":743,"y0":382,"y1":416},"font_size":0.0,"text":"open-source models considerably trail behind in performance."}],"source":"layout det","text":"Large language models (LLM) have revolutionized the approach to mathematical reasoning in artifcial intelligence, spurring signifcant advancements in both the quantitative reasoningii benchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024).Moreover, these models have proven instrumental in assisting humans in solving complex mathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible open-source models considerably trail behind in performance."},{"bbox":{"x0":131,"x1":1060,"y0":422,"y1":889},"conf":0.9877,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1051,"y0":428,"y1":453},"font_size":0.0,"text":"In this study, we introduce DeepSeekMath, a domain-specifc language model that signif-ii"},{"bbox":{"x0":140,"x1":1049,"y0":454,"y1":479},"font_size":0.0,"text":"cantly outperforms the mathematical capabilities of open-source models and approaches the"},{"bbox":{"x0":138,"x1":1051,"y0":479,"y1":507},"font_size":0.0,"text":"performance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-"},{"bbox":{"x0":140,"x1":1049,"y0":509,"y1":533},"font_size":0.0,"text":"Math Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This"},{"bbox":{"x0":140,"x1":1051,"y0":535,"y1":560},"font_size":0.0,"text":"dataset is extracted from the Common Crawl (CC) using a fastText-based classifer (Joulin et al.,i"},{"bbox":{"x0":140,"x1":1049,"y0":563,"y1":588},"font_size":0.0,"text":"2016). In the initial iteration, the classifer is trained using instances from OpenWebMath (Pasteri"},{"bbox":{"x0":138,"x1":1049,"y0":588,"y1":616},"font_size":0.0,"text":"et al., 2023) as positive examples, while incorporating a diverse selection of other web pages to"},{"bbox":{"x0":140,"x1":1051,"y0":619,"y1":642},"font_size":0.0,"text":"serve as negative examples. Subsequently, we employ the classifer to mine additional positivei"},{"bbox":{"x0":138,"x1":1049,"y0":646,"y1":668},"font_size":0.0,"text":"instances from the CC, which are further refned through human annotation. The classifer isii"},{"bbox":{"x0":140,"x1":1049,"y0":672,"y1":696},"font_size":0.0,"text":"then updated with this enhanced dataset to improve its performance. The evaluation results"},{"bbox":{"x0":140,"x1":1049,"y0":698,"y1":723},"font_size":0.0,"text":"indicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base"},{"bbox":{"x0":140,"x1":1047,"y0":723,"y1":749},"font_size":0.0,"text":"7B achieves $64.2\\%$ on GSM8K (Cobbe et al., 2021) and $36.2\\%$ on the competition-level MATH"},{"bbox":{"x0":140,"x1":1049,"y0":753,"y1":777},"font_size":0.0,"text":"dataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In"},{"bbox":{"x0":140,"x1":1047,"y0":781,"y1":803},"font_size":0.0,"text":"addition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese"},{"bbox":{"x0":140,"x1":1049,"y0":807,"y1":831},"font_size":0.0,"text":"mathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience"},{"bbox":{"x0":138,"x1":1049,"y0":835,"y1":860},"font_size":0.0,"text":"in mathematical data processing is a starting point for the research community, and there is"},{"bbox":{"x0":140,"x1":597,"y0":861,"y1":886},"font_size":0.0,"text":"signifcant room for improvement in the future.i"}],"source":"layout det","text":"In this study, we introduce DeepSeekMath, a domain-specifc language model that signif-ii cantly outperforms the mathematical capabilities of open-source models and approaches the performance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeekMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This dataset is extracted from the Common Crawl (CC) using a fastText-based classifer (Joulin et al.,i 2016). In the initial iteration, the classifer is trained using instances from OpenWebMath (Pasteri et al., 2023) as positive examples, while incorporating a diverse selection of other web pages to serve as negative examples. Subsequently, we employ the classifer to mine additional positivei instances from the CC, which are further refned through human annotation. The classifer isii then updated with this enhanced dataset to improve its performance. The evaluation results indicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base 7B achieves $64.2\\%$ on GSM8K (Cobbe et al., 2021) and $36.2\\%$ on the competition-level MATH dataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In addition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese mathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience in mathematical data processing is a starting point for the research community, and there is signifcant room for improvement in the future.i"},{"bbox":{"x0":132,"x1":1057,"y0":896,"y1":1040},"conf":0.9698,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1049,"y0":902,"y1":926},"font_size":0.0,"text":"DeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as"},{"bbox":{"x0":138,"x1":1049,"y0":930,"y1":954},"font_size":0.0,"text":"we notice that starting from a code training model is a better choice compared to a general"},{"bbox":{"x0":140,"x1":1049,"y0":956,"y1":981},"font_size":0.0,"text":"LLM. Furthermore, we observe the math training also improves model capability on MMLU"},{"bbox":{"x0":138,"x1":1049,"y0":981,"y1":1007},"font_size":0.0,"text":"(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only"},{"bbox":{"x0":140,"x1":1031,"y0":1010,"y1":1035},"font_size":0.0,"text":"enhance the modelâ€™s mathematical abilities but also amplifes general reasoning capabilities.i"}],"source":"layout det","text":"DeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as we notice that starting from a code training model is a better choice compared to a general LLM. Furthermore, we observe the math training also improves model capability on MMLU(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only enhance the modelâ€™s mathematical abilities but also amplifes general reasoning capabilities.i"},{"bbox":{"x0":134,"x1":1059,"y0":1045,"y1":1161},"conf":0.9624,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1047,"y0":1049,"y1":1074},"font_size":0.0,"text":"After pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with"},{"bbox":{"x0":138,"x1":1051,"y0":1075,"y1":1105},"font_size":0.0,"text":"chain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and"},{"bbox":{"x0":140,"x1":1049,"y0":1105,"y1":1130},"font_size":0.0,"text":"tool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct"},{"bbox":{"x0":138,"x1":1051,"y0":1130,"y1":1156},"font_size":0.0,"text":"7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models."}],"source":"layout det","text":"After pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with chain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and tool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct 7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models."},{"bbox":{"x0":132,"x1":1060,"y0":1167,"y1":1501},"conf":0.9824,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1051,"y0":1170,"y1":1196},"font_size":0.0,"text":"Furthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant rein-"},{"bbox":{"x0":140,"x1":1051,"y0":1200,"y1":1224},"font_size":0.0,"text":"forcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017)."},{"bbox":{"x0":136,"x1":1051,"y0":1223,"y1":1254},"font_size":0.0,"text":"GRPO foregoes the critic model, instead estimating the baseline from group scores, signifcantlyi"},{"bbox":{"x0":138,"x1":1049,"y0":1254,"y1":1279},"font_size":0.0,"text":"reducing training resources. By solely using a subset of English instruction tuning data, GRPO"},{"bbox":{"x0":140,"x1":1051,"y0":1281,"y1":1305},"font_size":0.0,"text":"obtains a substantial improvement over the strong DeepSeekMath-Instruct, including both"},{"bbox":{"x0":140,"x1":1049,"y0":1306,"y1":1333},"font_size":0.0,"text":"in-domain (GSM8K: $82.9\\%\\to88.2\\%,$ MATH: $46.8\\%\\to51.7\\%\\big)$  and out-of-domain mathematical"},{"bbox":{"x0":140,"x1":1049,"y0":1334,"y1":1361},"font_size":0.0,"text":"tasks (e.g., CMATH: $84.6\\%\\to88.8\\%)$  during the reinforcement learning phase. We also provide"},{"bbox":{"x0":136,"x1":1051,"y0":1358,"y1":1391},"font_size":0.0,"text":"a unifed paradigm to understand different methods, such as Rejection Sampling Fine-Tuningi"},{"bbox":{"x0":140,"x1":1049,"y0":1389,"y1":1414},"font_size":0.0,"text":"(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and"},{"bbox":{"x0":136,"x1":1051,"y0":1412,"y1":1444},"font_size":0.0,"text":"GRPO. Based on such a unifed paradigm, we fnd that all these methods are conceptualized asii"},{"bbox":{"x0":138,"x1":1051,"y0":1442,"y1":1472},"font_size":0.0,"text":"either direct or simplifed RL techniques. We also conduct extensive experiments, e.g., onlinei"},{"bbox":{"x0":138,"x1":1053,"y0":1472,"y1":1496},"font_size":0.0,"text":"v.s. offine training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,l"}],"source":"layout det","text":"Furthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant reinforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).GRPO foregoes the critic model, instead estimating the baseline from group scores, signifcantlyi reducing training resources. By solely using a subset of English instruction tuning data, GRPO obtains a substantial improvement over the strong DeepSeekMath-Instruct, including both in-domain (GSM8K: $82.9\\%\\to88.2\\%,$ MATH: $46.8\\%\\to51.7\\%\\big)$  and out-of-domain mathematical tasks (e.g., CMATH: $84.6\\%\\to88.8\\%)$  during the reinforcement learning phase. We also provide a unifed paradigm to understand different methods, such as Rejection Sampling Fine-Tuningi(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and GRPO. Based on such a unifed paradigm, we fnd that all these methods are conceptualized asii either direct or simplifed RL techniques. We also conduct extensive experiments, e.g., onlinei v.s. offine training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,l"}],"formula_dets":[{"bbox":{"x0":572,"x1":725,"y0":1307,"y1":1331},"conf":0.8376,"label":"print_embedding","label_id":0},{"bbox":{"x0":337,"x1":490,"y0":1334,"y1":1358},"conf":0.8194,"label":"print_embedding","label_id":0},{"bbox":{"x0":260,"x1":320,"y0":724,"y1":747},"conf":0.8157,"label":"print_embedding","label_id":0},{"bbox":{"x0":669,"x1":730,"y0":723,"y1":747},"conf":0.8125,"label":"print_embedding","label_id":0},{"bbox":{"x0":337,"x1":491,"y0":1306,"y1":1330},"conf":0.8087,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":131,"x1":1060,"y0":422,"y1":889},"conf":0.9877,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":1167,"y1":1501},"conf":0.9824,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":219,"y1":417},"conf":0.9776,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":896,"y1":1040},"conf":0.9698,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1059,"y0":1045,"y1":1161},"conf":0.9624,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":330,"y0":166,"y1":200},"conf":0.884,"label":"Title","label_id":0},{"bbox":{"x0":582,"x1":607,"y0":1551,"y1":1579},"conf":0.6094,"label":"Abandon","label_id":2},{"bbox":{"x0":584,"x1":605,"y0":1553,"y1":1578},"conf":0.2618,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1554],[603,1554],[603,1573],[587,1573]],"score":0.726},{"poly":[[138,1472],[1053,1472],[1053,1496],[138,1496]],"score":0.7886},{"poly":[[138,1442],[1051,1442],[1051,1472],[138,1472]],"score":0.67},{"poly":[[137,1412],[1051,1414],[1051,1444],[136,1442]],"score":0.7144},{"poly":[[140,1389],[1049,1389],[1049,1414],[140,1414]],"score":0.7644},{"poly":[[137,1358],[1051,1361],[1051,1391],[136,1388]],"score":0.6539},{"poly":[[140,1337],[1049,1337],[1049,1361],[140,1361]],"score":0.7624},{"poly":[[140,1309],[1049,1309],[1049,1333],[140,1333]],"score":0.7355},{"poly":[[140,1281],[1051,1281],[1051,1305],[140,1305]],"score":0.7492},{"poly":[[138,1254],[1049,1254],[1049,1279],[138,1279]],"score":0.7593},{"poly":[[137,1223],[1051,1224],[1051,1254],[136,1252]],"score":0.6801},{"poly":[[140,1200],[1051,1200],[1051,1224],[140,1224]],"score":0.7908},{"poly":[[172,1170],[1051,1172],[1051,1196],[172,1195]],"score":0.7957},{"poly":[[138,1130],[1051,1131],[1051,1156],[138,1154]],"score":0.7831},{"poly":[[140,1105],[1049,1105],[1049,1130],[140,1130]],"score":0.7857},{"poly":[[138,1075],[1051,1075],[1051,1105],[138,1105]],"score":0.6883},{"poly":[[174,1049],[1047,1049],[1047,1074],[174,1074]],"score":0.7814},{"poly":[[140,1010],[1031,1010],[1031,1035],[140,1035]],"score":0.7711},{"poly":[[138,981],[1049,982],[1049,1007],[138,1005]],"score":0.7777},{"poly":[[140,956],[1049,956],[1049,981],[140,981]],"score":0.7256},{"poly":[[138,930],[1049,930],[1049,954],[138,954]],"score":0.7318},{"poly":[[175,902],[1049,902],[1049,926],[175,926]],"score":0.7111},{"poly":[[140,861],[597,861],[597,886],[140,886]],"score":0.7812},{"poly":[[138,835],[1049,835],[1049,860],[138,860]],"score":0.7686},{"poly":[[140,807],[1049,807],[1049,831],[140,831]],"score":0.7803},{"poly":[[140,781],[1047,781],[1047,803],[140,803]],"score":0.8957},{"poly":[[140,753],[1049,753],[1049,777],[140,777]],"score":0.7417},{"poly":[[140,724],[1047,724],[1047,749],[140,749]],"score":0.7412},{"poly":[[140,698],[1049,698],[1049,723],[140,723]],"score":0.7476},{"poly":[[140,672],[1049,672],[1049,696],[140,696]],"score":0.7757},{"poly":[[138,646],[1049,646],[1049,668],[138,668]],"score":0.8384},{"poly":[[140,619],[1051,619],[1051,642],[140,642]],"score":0.8508},{"poly":[[138,588],[1049,591],[1049,616],[138,612]],"score":0.707},{"poly":[[140,563],[1049,563],[1049,588],[140,588]],"score":0.7186},{"poly":[[140,535],[1051,535],[1051,560],[140,560]],"score":0.7166},{"poly":[[140,509],[1049,509],[1049,533],[140,533]],"score":0.7491},{"poly":[[138,482],[1051,479],[1051,503],[138,507]],"score":0.7303},{"poly":[[140,454],[1049,454],[1049,479],[140,479]],"score":0.7357},{"poly":[[172,428],[1051,428],[1051,453],[172,453]],"score":0.7577},{"poly":[[136,386],[742,382],[743,412],[137,416]],"score":0.6767},{"poly":[[140,360],[1049,360],[1049,384],[140,384]],"score":0.7474},{"poly":[[136,330],[1053,330],[1053,360],[136,360]],"score":0.6631},{"poly":[[135,302],[1051,303],[1051,333],[135,332]],"score":0.7088},{"poly":[[138,279],[1051,279],[1051,303],[138,303]],"score":0.7883},{"poly":[[138,251],[1047,251],[1047,275],[138,275]],"score":0.7554},{"poly":[[137,221],[1051,223],[1051,253],[136,251]],"score":0.7059},{"poly":[[138,172],[324,172],[324,196],[138,196]],"score":0.8706}],"page_no":1,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":608,"y0":1550,"y1":1579},"conf":0.6636,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1554,"y1":1575},"font_size":0.0,"text":"3"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":1057,"y0":167,"y1":259},"conf":0.9558,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":174,"y1":198},"font_size":0.0,"text":"to deeply investigate the essential elements of this paradigm. At last, we explain why our RL"},{"bbox":{"x0":138,"x1":1049,"y0":202,"y1":225},"font_size":0.0,"text":"boosts the performance of instruction-tuned models, and further summarize potential directions"},{"bbox":{"x0":136,"x1":725,"y0":226,"y1":253},"font_size":0.0,"text":"to achieve more effective RL based on this unifed paradigm.i"}],"source":"layout det","text":"to deeply investigate the essential elements of this paradigm. At last, we explain why our RL boosts the performance of instruction-tuned models, and further summarize potential directions to achieve more effective RL based on this unifed paradigm.i"},{"bbox":{"x0":133,"x1":333,"y0":288,"y1":321},"conf":0.8848,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":328,"y0":291,"y1":318},"font_size":0.0,"text":"1.1. Contributions"}],"source":"layout det","text":"1.1. Contributions"},{"bbox":{"x0":132,"x1":1058,"y0":333,"y1":396},"conf":0.9499,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":339,"y1":363},"font_size":0.0,"text":"Our contribution includes scalable math pre-training, along with the exploration and analysis of"},{"bbox":{"x0":138,"x1":365,"y0":363,"y1":393},"font_size":0.0,"text":"reinforcement learning."}],"source":"layout det","text":"Our contribution includes scalable math pre-training, along with the exploration and analysis of reinforcement learning."},{"bbox":{"x0":133,"x1":413,"y0":401,"y1":437},"conf":0.8932,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":408,"y0":407,"y1":430},"font_size":0.0,"text":"Math Pre-Training at Scale"}],"source":"layout det","text":"Math Pre-Training at Scale"},{"bbox":{"x0":187,"x1":1061,"y0":442,"y1":635},"conf":0.975,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":446,"y1":470},"font_size":0.0,"text":"Our research provides compelling evidence that the publicly accessible Common Crawl"},{"bbox":{"x0":193,"x1":1049,"y0":475,"y1":500},"font_size":0.0,"text":"data contains valuable information for mathematical purposes. By implementing a metic-"},{"bbox":{"x0":193,"x1":1049,"y0":500,"y1":526},"font_size":0.0,"text":"ulously designed data selection pipeline, we successfully construct the DeepSeekMath"},{"bbox":{"x0":195,"x1":1051,"y0":530,"y1":553},"font_size":0.0,"text":"Corpus, a high-quality dataset of 120B tokens from web pages fltered for mathemati-i"},{"bbox":{"x0":193,"x1":1049,"y0":556,"y1":581},"font_size":0.0,"text":"cal content, which is almost 7 times the size of the math web pages used by Minerva"},{"bbox":{"x0":193,"x1":1047,"y0":582,"y1":605},"font_size":0.0,"text":"(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath"},{"bbox":{"x0":193,"x1":383,"y0":609,"y1":633},"font_size":0.0,"text":"(Paster et al., 2023)."}],"source":"layout det","text":"Our research provides compelling evidence that the publicly accessible Common Crawl data contains valuable information for mathematical purposes. By implementing a meticulously designed data selection pipeline, we successfully construct the DeepSeekMath Corpus, a high-quality dataset of 120B tokens from web pages fltered for mathemati-i cal content, which is almost 7 times the size of the math web pages used by Minerva(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath(Paster et al., 2023)."},{"bbox":{"x0":187,"x1":1057,"y0":635,"y1":743},"conf":0.9582,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1049,"y0":637,"y1":661},"font_size":0.0,"text":"Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance"},{"bbox":{"x0":193,"x1":1049,"y0":663,"y1":686},"font_size":0.0,"text":"with Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not"},{"bbox":{"x0":195,"x1":1051,"y0":691,"y1":716},"font_size":0.0,"text":"the only key factor in mathematical reasoning capability. A smaller model pre-trained on"},{"bbox":{"x0":191,"x1":776,"y0":719,"y1":744},"font_size":0.0,"text":"high-quality data could achieve strong performance as well."}],"source":"layout det","text":"Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance with Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not the only key factor in mathematical reasoning capability. A smaller model pre-trained on high-quality data could achieve strong performance as well."},{"bbox":{"x0":187,"x1":1058,"y0":745,"y1":852},"conf":0.9548,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":744,"y1":774},"font_size":0.0,"text":"We share our fndings from math training experiments. Code training prior to mathi"},{"bbox":{"x0":195,"x1":1051,"y0":774,"y1":798},"font_size":0.0,"text":"training improves modelsâ€™ ability to solve mathematical problems both with and without"},{"bbox":{"x0":191,"x1":1051,"y0":798,"y1":828},"font_size":0.0,"text":"tool use. This offers a partial answer to the long-standing question: does code training"},{"bbox":{"x0":188,"x1":977,"y0":823,"y1":854},"font_size":0.0,"text":"improve reasoning abilities? We believe it does, at least for mathematical reasoning."}],"source":"layout det","text":"We share our fndings from math training experiments. Code training prior to mathi training improves modelsâ€™ ability to solve mathematical problems both with and without tool use. This offers a partial answer to the long-standing question: does code training improve reasoning abilities? We believe it does, at least for mathematical reasoning."},{"bbox":{"x0":185,"x1":1057,"y0":854,"y1":910},"conf":0.9479,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1051,"y0":854,"y1":879},"font_size":0.0,"text":"Although training on arXiv papers is common, especially in many math-related papers, it"},{"bbox":{"x0":193,"x1":1044,"y0":882,"y1":907},"font_size":0.0,"text":"brings no notable improvements on all mathematical benchmarks adopted in this paper."}],"source":"layout det","text":"Although training on arXiv papers is common, especially in many math-related papers, it brings no notable improvements on all mathematical benchmarks adopted in this paper."},{"bbox":{"x0":135,"x1":676,"y0":916,"y1":952},"conf":0.8568,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":672,"y0":917,"y1":951},"font_size":0.0,"text":"Exploration and Analysis of Reinforcement Learning"}],"source":"layout det","text":"Exploration and Analysis of Reinforcement Learning"},{"bbox":{"x0":187,"x1":1056,"y0":956,"y1":1068},"conf":0.9589,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":961,"y1":986},"font_size":0.0,"text":"We introduce Group Relative Policy Optimization (GRPO), an effcient and effectivei"},{"bbox":{"x0":191,"x1":1051,"y0":986,"y1":1016},"font_size":0.0,"text":"reinforcement learning algorithm. GRPO foregoes the critic model, instead estimating"},{"bbox":{"x0":195,"x1":1049,"y0":1017,"y1":1042},"font_size":0.0,"text":"the baseline from group scores, signifcantly reducing training resources compared toi"},{"bbox":{"x0":193,"x1":553,"y0":1044,"y1":1068},"font_size":0.0,"text":"Proximal Policy Optimization (PPO)."}],"source":"layout det","text":"We introduce Group Relative Policy Optimization (GRPO), an effcient and effectivei reinforcement learning algorithm. GRPO foregoes the critic model, instead estimating the baseline from group scores, signifcantly reducing training resources compared toi Proximal Policy Optimization (PPO)."},{"bbox":{"x0":187,"x1":1058,"y0":1070,"y1":1177},"conf":0.967,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":184,"x1":1051,"y0":1068,"y1":1095},"font_size":0.0,"text":"We demonstrate that GRPO signifcantly enhances the performance of our instruction-i"},{"bbox":{"x0":193,"x1":1051,"y0":1098,"y1":1123},"font_size":0.0,"text":"tuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Further-"},{"bbox":{"x0":193,"x1":1051,"y0":1126,"y1":1149},"font_size":0.0,"text":"more, we observe enhancements in the out-of-domain performance during the reinforce-"},{"bbox":{"x0":191,"x1":413,"y0":1154,"y1":1179},"font_size":0.0,"text":"ment learning process."}],"source":"layout det","text":"We demonstrate that GRPO signifcantly enhances the performance of our instruction-i tuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Furthermore, we observe enhancements in the out-of-domain performance during the reinforcement learning process."},{"bbox":{"x0":187,"x1":1058,"y0":1178,"y1":1286},"conf":0.9681,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1051,"y0":1179,"y1":1203},"font_size":0.0,"text":"We provide a unifed paradigm to understand different methods, such as RFT, DPO,i"},{"bbox":{"x0":188,"x1":1053,"y0":1202,"y1":1233},"font_size":0.0,"text":"PPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offine training,l"},{"bbox":{"x0":193,"x1":1049,"y0":1233,"y1":1258},"font_size":0.0,"text":"outcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so"},{"bbox":{"x0":193,"x1":812,"y0":1261,"y1":1284},"font_size":0.0,"text":"on to deeply investigate the essential elements of this paradigm."}],"source":"layout det","text":"We provide a unifed paradigm to understand different methods, such as RFT, DPO,i PPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offine training,l outcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so on to deeply investigate the essential elements of this paradigm."},{"bbox":{"x0":188,"x1":1059,"y0":1287,"y1":1373},"conf":0.9449,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":1288,"y1":1312},"font_size":0.0,"text":"Based on our unifed paradigm, we explore the reasons behind the effectiveness of rein-i"},{"bbox":{"x0":195,"x1":1049,"y0":1316,"y1":1338},"font_size":0.0,"text":"forcement learning, and summarize several potential directions to achieve more effective"},{"bbox":{"x0":193,"x1":509,"y0":1342,"y1":1366},"font_size":0.0,"text":"reinforcement learning of LLMs."}],"source":"layout det","text":"Based on our unifed paradigm, we explore the reasons behind the effectiveness of rein-i forcement learning, and summarize several potential directions to achieve more effective reinforcement learning of LLMs."},{"bbox":{"x0":134,"x1":562,"y0":1402,"y1":1437},"conf":0.878,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":555,"y0":1409,"y1":1431},"font_size":0.0,"text":"1.2. Summary of Evaluations and Metrics"}],"source":"layout det","text":"1.2. Summary of Evaluations and Metrics"},{"bbox":{"x0":188,"x1":1057,"y0":1447,"y1":1511},"conf":0.9237,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":168,"x1":1049,"y0":1451,"y1":1477},"font_size":0.0,"text":"English and Chinese Mathematical Reasoning: We conduct comprehensive assessments"},{"bbox":{"x0":193,"x1":1049,"y0":1479,"y1":1505},"font_size":0.0,"text":"of our models on English and Chinese benchmarks, covering mathematical problems"}],"source":"layout det","text":"English and Chinese Mathematical Reasoning: We conduct comprehensive assessments of our models on English and Chinese benchmarks, covering mathematical problems"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":187,"x1":1061,"y0":442,"y1":635},"conf":0.975,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1058,"y0":1178,"y1":1286},"conf":0.9681,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1058,"y0":1070,"y1":1177},"conf":0.967,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1056,"y0":956,"y1":1068},"conf":0.9589,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1057,"y0":635,"y1":743},"conf":0.9582,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":167,"y1":259},"conf":0.9558,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1058,"y0":745,"y1":852},"conf":0.9548,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":333,"y1":396},"conf":0.9499,"label":"Text","label_id":1},{"bbox":{"x0":185,"x1":1057,"y0":854,"y1":910},"conf":0.9479,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1059,"y0":1287,"y1":1373},"conf":0.9449,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1057,"y0":1447,"y1":1511},"conf":0.9237,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":413,"y0":401,"y1":437},"conf":0.8932,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":333,"y0":288,"y1":321},"conf":0.8848,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":562,"y0":1402,"y1":1437},"conf":0.878,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":676,"y0":916,"y1":952},"conf":0.8568,"label":"Title","label_id":0},{"bbox":{"x0":582,"x1":608,"y0":1550,"y1":1579},"conf":0.6636,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1554],[603,1554],[603,1575],[587,1575]],"score":0.7637},{"poly":[[193,1479],[1049,1481],[1049,1505],[193,1503]],"score":0.8094},{"poly":[[168,1451],[1049,1454],[1049,1477],[168,1473]],"score":0.8478},{"poly":[[138,1409],[555,1409],[555,1431],[138,1431]],"score":0.8656},{"poly":[[193,1342],[509,1342],[509,1366],[193,1366]],"score":0.7268},{"poly":[[195,1316],[1049,1316],[1049,1338],[195,1338]],"score":0.8911},{"poly":[[174,1288],[1051,1288],[1051,1312],[174,1312]],"score":0.7305},{"poly":[[193,1261],[812,1261],[812,1284],[193,1284]],"score":0.8647},{"poly":[[193,1233],[1049,1233],[1049,1258],[193,1258]],"score":0.7405},{"poly":[[188,1202],[1053,1205],[1053,1233],[188,1230]],"score":0.735},{"poly":[[186,1179],[1051,1179],[1051,1202],[186,1202]],"score":0.8853},{"poly":[[191,1154],[413,1154],[413,1179],[191,1179]],"score":0.7071},{"poly":[[193,1126],[1051,1126],[1051,1149],[193,1149]],"score":0.8388},{"poly":[[193,1098],[1051,1098],[1051,1123],[193,1123]],"score":0.7266},{"poly":[[184,1068],[1051,1070],[1051,1095],[184,1093]],"score":0.7647},{"poly":[[193,1044],[553,1044],[553,1068],[193,1068]],"score":0.7299},{"poly":[[195,1017],[1049,1017],[1049,1042],[195,1042]],"score":0.759},{"poly":[[191,986],[1051,986],[1051,1016],[191,1016]],"score":0.6509},{"poly":[[170,961],[1049,961],[1049,986],[170,986]],"score":0.7463},{"poly":[[137,917],[672,921],[672,951],[136,947]],"score":0.686},{"poly":[[193,882],[1044,882],[1044,907],[193,907]],"score":0.8105},{"poly":[[175,854],[1051,854],[1051,879],[175,879]],"score":0.7728},{"poly":[[188,823],[977,824],[977,854],[188,853]],"score":0.7044},{"poly":[[191,798],[1051,798],[1051,828],[191,828]],"score":0.658},{"poly":[[195,774],[1051,774],[1051,798],[195,798]],"score":0.7416},{"poly":[[172,744],[1049,744],[1049,774],[172,774]],"score":0.6271},{"poly":[[191,719],[776,719],[776,744],[191,744]],"score":0.7385},{"poly":[[195,691],[1051,691],[1051,716],[195,716]],"score":0.7355},{"poly":[[193,663],[1049,663],[1049,686],[193,686]],"score":0.8032},{"poly":[[175,637],[1049,637],[1049,661],[175,661]],"score":0.7501},{"poly":[[193,609],[383,609],[383,633],[193,633]],"score":0.7889},{"poly":[[193,582],[1047,582],[1047,605],[193,605]],"score":0.8232},{"poly":[[193,556],[1049,556],[1049,581],[193,581]],"score":0.7726},{"poly":[[195,530],[1051,530],[1051,553],[195,553]],"score":0.8454},{"poly":[[193,503],[1049,500],[1049,523],[193,526]],"score":0.8037},{"poly":[[193,475],[1049,475],[1049,500],[193,500]],"score":0.7599},{"poly":[[170,446],[1049,446],[1049,470],[170,470]],"score":0.8041},{"poly":[[140,407],[408,407],[408,430],[140,430]],"score":0.965},{"poly":[[139,363],[365,367],[365,393],[138,389]],"score":0.7141},{"poly":[[140,339],[1051,339],[1051,363],[140,363]],"score":0.7909},{"poly":[[138,291],[328,293],[328,318],[138,316]],"score":0.7417},{"poly":[[137,226],[725,228],[725,253],[136,251]],"score":0.7909},{"poly":[[138,202],[1049,202],[1049,225],[138,225]],"score":0.8885},{"poly":[[138,174],[1049,174],[1049,198],[138,198]],"score":0.7573}],"page_no":2,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":162,"x1":1009,"y0":1492,"y1":1582},"conf":0.286,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1556,"y1":1573},"font_size":0.0,"text":"4"},{"bbox":{"x0":172,"x1":379,"y0":1495,"y1":1519},"font_size":0.0,"text":"1https://fasttext.cc"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":186,"x1":1060,"y0":167,"y1":363},"conf":0.9739,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":193,"x1":1049,"y0":174,"y1":198},"font_size":0.0,"text":"from grade-school level to college level. English benchmarks include GSM8K (Cobbe"},{"bbox":{"x0":193,"x1":1049,"y0":200,"y1":225},"font_size":0.0,"text":"et al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses"},{"bbox":{"x0":193,"x1":1049,"y0":226,"y1":251},"font_size":0.0,"text":"(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks"},{"bbox":{"x0":190,"x1":1051,"y0":249,"y1":282},"font_size":0.0,"text":"include MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong"},{"bbox":{"x0":193,"x1":1049,"y0":282,"y1":305},"font_size":0.0,"text":"et al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate modelsâ€™ ability"},{"bbox":{"x0":193,"x1":1049,"y0":309,"y1":333},"font_size":0.0,"text":"to generate self-contained text solutions without tool use, and also the ability to solve"},{"bbox":{"x0":193,"x1":427,"y0":337,"y1":361},"font_size":0.0,"text":"problems using Python."}],"source":"layout det","text":"from grade-school level to college level. English benchmarks include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks include MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong et al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate modelsâ€™ ability to generate self-contained text solutions without tool use, and also the ability to solve problems using Python."},{"bbox":{"x0":187,"x1":1062,"y0":363,"y1":660},"conf":0.9567,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":193,"x1":1049,"y0":363,"y1":386},"font_size":0.0,"text":"On English benchmarks, DeepSeekMath-Base is competitive with the closed-source Min-"},{"bbox":{"x0":193,"x1":1051,"y0":391,"y1":416},"font_size":0.0,"text":"erva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mis-"},{"bbox":{"x0":195,"x1":1049,"y0":417,"y1":442},"font_size":0.0,"text":"tral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether"},{"bbox":{"x0":195,"x1":1051,"y0":444,"y1":470},"font_size":0.0,"text":"theyâ€™ve undergone math pre-training or not, often by a signifcant margin. Notably,i"},{"bbox":{"x0":195,"x1":1049,"y0":472,"y1":496},"font_size":0.0,"text":"DeepSeekMath-Base is superior on Chinese benchmarks, likely because we donâ€™t follow"},{"bbox":{"x0":190,"x1":1051,"y0":495,"y1":526},"font_size":0.0,"text":"previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only"},{"bbox":{"x0":193,"x1":1051,"y0":524,"y1":549},"font_size":0.0,"text":"math pre-training data, and also include high-quality non-English ones. With mathemati-"},{"bbox":{"x0":193,"x1":1049,"y0":551,"y1":575},"font_size":0.0,"text":"cal instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct"},{"bbox":{"x0":190,"x1":1051,"y0":575,"y1":607},"font_size":0.0,"text":"and DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over"},{"bbox":{"x0":193,"x1":1049,"y0":607,"y1":631},"font_size":0.0,"text":"$50\\%$ on the competition-level MATH dataset for the frst time within the open-sourcei"},{"bbox":{"x0":195,"x1":310,"y0":639,"y1":656},"font_size":0.0,"text":"community."}],"source":"layout det","text":"On English benchmarks, DeepSeekMath-Base is competitive with the closed-source Minerva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mistral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether theyâ€™ve undergone math pre-training or not, often by a signifcant margin. Notably,i DeepSeekMath-Base is superior on Chinese benchmarks, likely because we donâ€™t follow previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only math pre-training data, and also include high-quality non-English ones. With mathematical instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct and DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over $50\\%$ on the competition-level MATH dataset for the frst time within the open-sourcei community."},{"bbox":{"x0":187,"x1":1060,"y0":660,"y1":768},"conf":0.5812,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":1049,"y0":661,"y1":686},"font_size":0.0,"text":"Formal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal"},{"bbox":{"x0":193,"x1":1049,"y0":689,"y1":712},"font_size":0.0,"text":"theorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle"},{"bbox":{"x0":193,"x1":1049,"y0":716,"y1":740},"font_size":0.0,"text":"(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates"},{"bbox":{"x0":188,"x1":663,"y0":738,"y1":770},"font_size":0.0,"text":"strong few-shot autoformalization performance."}],"source":"layout det","text":"Formal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal theorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates strong few-shot autoformalization performance."},{"bbox":{"x0":187,"x1":1060,"y0":769,"y1":1019},"conf":0.9748,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":184,"x1":1051,"y0":765,"y1":796},"font_size":0.0,"text":"Natural Language Understanding, Reasoning, and Code: To build a comprehensive"},{"bbox":{"x0":193,"x1":1051,"y0":796,"y1":821},"font_size":0.0,"text":"profle of modelsâ€™ general understanding, reasoning, and coding capabilities, we eval-i"},{"bbox":{"x0":193,"x1":1051,"y0":824,"y1":849},"font_size":0.0,"text":"uate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)"},{"bbox":{"x0":190,"x1":1051,"y0":845,"y1":879},"font_size":0.0,"text":"benchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering"},{"bbox":{"x0":193,"x1":1051,"y0":879,"y1":903},"font_size":0.0,"text":"diverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 chal-"},{"bbox":{"x0":193,"x1":1051,"y0":907,"y1":931},"font_size":0.0,"text":"lenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval"},{"bbox":{"x0":193,"x1":1051,"y0":931,"y1":956},"font_size":0.0,"text":"(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code"},{"bbox":{"x0":190,"x1":1051,"y0":956,"y1":988},"font_size":0.0,"text":"language models. Math pre-training benefts both language understanding and reasoningi"},{"bbox":{"x0":193,"x1":324,"y0":988,"y1":1012},"font_size":0.0,"text":"performance."}],"source":"layout det","text":"Natural Language Understanding, Reasoning, and Code: To build a comprehensive profle of modelsâ€™ general understanding, reasoning, and coding capabilities, we eval-i uate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)benchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering diverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 challenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code language models. Math pre-training benefts both language understanding and reasoningi performance."},{"bbox":{"x0":133,"x1":400,"y0":1050,"y1":1090},"conf":0.8952,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":394,"y0":1049,"y1":1088},"font_size":0.0,"text":"2. Math Pre-Training"}],"source":"layout det","text":"2.Math Pre-Training"},{"bbox":{"x0":133,"x1":570,"y0":1107,"y1":1139},"conf":0.906,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":142,"x1":564,"y0":1114,"y1":1131},"font_size":0.0,"text":"2.1. Data Collection and Decontamination"}],"source":"layout det","text":"2.1. Data Collection and Decontamination"},{"bbox":{"x0":132,"x1":1057,"y0":1150,"y1":1296},"conf":0.9678,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1156,"y1":1181},"font_size":0.0,"text":"In this section, we will outline the process of constructing the DeepSeekMath Corpus from"},{"bbox":{"x0":140,"x1":1051,"y0":1184,"y1":1209},"font_size":0.0,"text":"Common Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates"},{"bbox":{"x0":135,"x1":1051,"y0":1207,"y1":1238},"font_size":0.0,"text":"how to systematically gather a large-scale mathematical corpus from Common Crawl, starting"},{"bbox":{"x0":135,"x1":1051,"y0":1233,"y1":1265},"font_size":0.0,"text":"with a seed corpus (e.g., a small but high-quality collection of math-related dataset). Itâ€™s worth"},{"bbox":{"x0":136,"x1":883,"y0":1261,"y1":1293},"font_size":0.0,"text":"noting that this approach is also applicable to other domains, such as coding."}],"source":"layout det","text":"In this section, we will outline the process of constructing the DeepSeekMath Corpus from Common Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates how to systematically gather a large-scale mathematical corpus from Common Crawl, starting with a seed corpus (e.g., a small but high-quality collection of math-related dataset). Itâ€™s worth noting that this approach is also applicable to other domains, such as coding."},{"bbox":{"x0":133,"x1":1059,"y0":1299,"y1":1472},"conf":0.9713,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":1305,"y1":1330},"font_size":0.0,"text":"First, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical"},{"bbox":{"x0":138,"x1":1053,"y0":1333,"y1":1356},"font_size":0.0,"text":"web texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,"},{"bbox":{"x0":140,"x1":1049,"y0":1359,"y1":1384},"font_size":0.0,"text":"2016) to recall more OpenWebMath-like mathematical web pages. Specifcally, we randomlyi"},{"bbox":{"x0":140,"x1":1049,"y0":1388,"y1":1412},"font_size":0.0,"text":"select 500,000 data points from the seed corpus as positive training examples and another"},{"bbox":{"x0":140,"x1":1046,"y0":1414,"y1":1438},"font_size":0.0,"text":"500,000 web pages from Common Crawl as negative ones. We employ an open-source library1"},{"bbox":{"x0":135,"x1":1051,"y0":1437,"y1":1468},"font_size":0.0,"text":"for training, confguring the vector dimension to 256, learning rate to 0.1, the maximum lengthi"}],"source":"layout det","text":"First, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical web texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,2016) to recall more OpenWebMath-like mathematical web pages. Specifcally, we randomlyi select 500,000 data points from the seed corpus as positive training examples and another 500,000 web pages from Common Crawl as negative ones. We employ an open-source library1 for training, confguring the vector dimension to 256, learning rate to 0.1, the maximum lengthi"}],"formula_dets":[{"bbox":{"x0":193,"x1":238,"y0":607,"y1":629},"conf":0.7947,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":187,"x1":1060,"y0":769,"y1":1019},"conf":0.9748,"label":"Text","label_id":1},{"bbox":{"x0":186,"x1":1060,"y0":167,"y1":363},"conf":0.9739,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":1299,"y1":1472},"conf":0.9713,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":1150,"y1":1296},"conf":0.9678,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1062,"y0":363,"y1":660},"conf":0.9567,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":570,"y0":1107,"y1":1139},"conf":0.906,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":400,"y0":1050,"y1":1090},"conf":0.8952,"label":"Title","label_id":0},{"bbox":{"x0":187,"x1":1059,"y0":660,"y1":767},"conf":0.6199,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1060,"y0":660,"y1":768},"conf":0.5812,"label":"Text","label_id":1},{"bbox":{"x0":162,"x1":1009,"y0":1492,"y1":1582},"conf":0.286,"label":"Abandon","label_id":2},{"bbox":{"x0":582,"x1":608,"y0":1550,"y1":1580},"conf":0.2515,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1556],[603,1556],[603,1573],[587,1573]],"score":0.7762},{"poly":[[172,1495],[379,1495],[379,1519],[172,1519]],"score":0.8071},{"poly":[[135,1437],[1051,1438],[1051,1468],[135,1466]],"score":0.718},{"poly":[[140,1414],[1046,1414],[1046,1438],[140,1438]],"score":0.7976},{"poly":[[140,1388],[1049,1388],[1049,1412],[140,1412]],"score":0.7975},{"poly":[[140,1359],[1049,1359],[1049,1384],[140,1384]],"score":0.7903},{"poly":[[138,1333],[1053,1333],[1053,1356],[138,1356]],"score":0.9141},{"poly":[[174,1305],[1049,1305],[1049,1330],[174,1330]],"score":0.7644},{"poly":[[137,1261],[883,1263],[883,1293],[136,1291]],"score":0.6746},{"poly":[[135,1235],[1051,1233],[1051,1263],[135,1265]],"score":0.6937},{"poly":[[135,1207],[1051,1209],[1051,1238],[135,1237]],"score":0.6818},{"poly":[[140,1184],[1051,1184],[1051,1209],[140,1209]],"score":0.7641},{"poly":[[138,1156],[1049,1156],[1049,1181],[138,1181]],"score":0.7617},{"poly":[[142,1114],[564,1114],[564,1131],[142,1131]],"score":0.994},{"poly":[[137,1049],[394,1056],[393,1088],[136,1080]],"score":0.6776},{"poly":[[193,988],[324,988],[324,1012],[193,1012]],"score":0.8464},{"poly":[[190,956],[1051,958],[1051,988],[190,986]],"score":0.7465},{"poly":[[193,931],[1051,931],[1051,956],[193,956]],"score":0.75},{"poly":[[193,907],[1051,907],[1051,931],[193,931]],"score":0.7536},{"poly":[[193,879],[1051,879],[1051,903],[193,903]],"score":0.7938},{"poly":[[190,845],[1051,849],[1051,879],[190,875]],"score":0.691},{"poly":[[193,824],[1051,824],[1051,849],[193,849]],"score":0.7481},{"poly":[[193,796],[1051,796],[1051,821],[193,821]],"score":0.7879},{"poly":[[184,767],[1051,765],[1051,795],[184,796]],"score":0.7412},{"poly":[[188,740],[663,738],[663,768],[188,770]],"score":0.6872},{"poly":[[193,716],[1049,716],[1049,740],[193,740]],"score":0.7647},{"poly":[[193,689],[1049,689],[1049,712],[193,712]],"score":0.9157},{"poly":[[188,661],[1049,661],[1049,686],[188,686]],"score":0.7595},{"poly":[[195,639],[310,639],[310,656],[195,656]],"score":0.9908},{"poly":[[195,609],[1049,609],[1049,631],[195,631]],"score":0.872},{"poly":[[190,575],[1051,577],[1051,607],[190,605]],"score":0.7025},{"poly":[[193,551],[1049,551],[1049,575],[193,575]],"score":0.733},{"poly":[[193,524],[1051,524],[1051,549],[193,549]],"score":0.7677},{"poly":[[190,496],[1051,495],[1051,524],[190,526]],"score":0.6798},{"poly":[[195,472],[1049,472],[1049,496],[195,496]],"score":0.7955},{"poly":[[195,444],[1051,446],[1051,470],[195,468]],"score":0.824},{"poly":[[195,417],[1049,417],[1049,442],[195,442]],"score":0.7646},{"poly":[[193,391],[1051,391],[1051,416],[193,416]],"score":0.7914},{"poly":[[193,363],[1049,363],[1049,386],[193,386]],"score":0.8904},{"poly":[[193,337],[427,337],[427,361],[193,361]],"score":0.8036},{"poly":[[193,309],[1049,309],[1049,333],[193,333]],"score":0.8101},{"poly":[[193,282],[1049,282],[1049,305],[193,305]],"score":0.8702},{"poly":[[190,249],[1051,253],[1051,282],[190,279]],"score":0.6649},{"poly":[[193,226],[1049,226],[1049,251],[193,251]],"score":0.7445},{"poly":[[193,200],[1049,200],[1049,225],[193,225]],"score":0.7491},{"poly":[[193,174],[1049,174],[1049,198],[193,198]],"score":0.8159}],"page_no":3,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1580},"conf":0.5767,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1554,"y1":1573},"font_size":0.0,"text":"5"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":144,"x1":1052,"y0":160,"y1":506},"conf":0.3403,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![40b310208c8a4721da91ffc55b732b34](imgs/40b310208c8a4721da91ffc55b732b34.jpg)"},{"bbox":{"x0":240,"x1":1029,"y0":517,"y1":552},"conf":0.2572,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[],"source":"layout det","text":""},{"bbox":{"x0":148,"x1":1043,"y0":516,"y1":555},"conf":0.8112,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":154,"x1":1032,"y0":519,"y1":547},"font_size":0.0,"text":"Figure 2 | An iterative pipeline that collects mathematical web pages from Common Crawl."}],"source":"layout det","text":"Figure 2 | An iterative pipeline that collects mathematical web pages from Common Crawl."},{"bbox":{"x0":133,"x1":1058,"y0":610,"y1":836},"conf":0.9716,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":610,"y1":642},"font_size":0.0,"text":"of word n-gram to $3,$ the minimum number of word occurrences to 3, and the number of"},{"bbox":{"x0":138,"x1":1049,"y0":644,"y1":667},"font_size":0.0,"text":"training epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based"},{"bbox":{"x0":140,"x1":1049,"y0":670,"y1":695},"font_size":0.0,"text":"deduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then"},{"bbox":{"x0":138,"x1":1049,"y0":698,"y1":721},"font_size":0.0,"text":"recall mathematical web pages from deduplicated Common Crawl with the fastText model."},{"bbox":{"x0":140,"x1":1049,"y0":724,"y1":749},"font_size":0.0,"text":"To flter out low-quality mathematical content, we rank the collected pages according to theiri"},{"bbox":{"x0":140,"x1":1049,"y0":753,"y1":775},"font_size":0.0,"text":"scores predicted by the fastText model, and only preserve the top-ranking ones. The volume"},{"bbox":{"x0":140,"x1":1049,"y0":779,"y1":803},"font_size":0.0,"text":"of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and"},{"bbox":{"x0":140,"x1":831,"y0":807,"y1":831},"font_size":0.0,"text":"160B tokens. In the frst iteration, we choose to keep the top 40B tokens.i"}],"source":"layout det","text":"of word n-gram to $3,$ the minimum number of word occurrences to 3, and the number of training epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based deduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then recall mathematical web pages from deduplicated Common Crawl with the fastText model.To flter out low-quality mathematical content, we rank the collected pages according to theiri scores predicted by the fastText model, and only preserve the top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the frst iteration, we choose to keep the top 40B tokens.i"},{"bbox":{"x0":131,"x1":1060,"y0":843,"y1":1253},"conf":0.9841,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1051,"y0":847,"y1":872},"font_size":0.0,"text":"After the frst iteration of data collection, numerous mathematical web pages remain un-i"},{"bbox":{"x0":138,"x1":1049,"y0":872,"y1":898},"font_size":0.0,"text":"collected, mainly because the fastText model is trained on a set of positive examples that lacks"},{"bbox":{"x0":140,"x1":1049,"y0":902,"y1":925},"font_size":0.0,"text":"suffcient diversity. We therefore identify additional mathematical web sources to enrich the seedi"},{"bbox":{"x0":138,"x1":1051,"y0":926,"y1":953},"font_size":0.0,"text":"corpus, so that we can optimize the fastText model. Specifcally, we frst organize the entire Com-ii"},{"bbox":{"x0":140,"x1":1053,"y0":956,"y1":981},"font_size":0.0,"text":"mon Crawl into disjoint domains; a domain is defned as web pages sharing the same base URL.i"},{"bbox":{"x0":140,"x1":1053,"y0":982,"y1":1007},"font_size":0.0,"text":"For each domain, we calculate the percentage of web pages that are collected in the frst iteration.i"},{"bbox":{"x0":140,"x1":1049,"y0":1008,"y1":1034},"font_size":0.0,"text":"Domains where over $10\\%$ of the web pages have been collected are classifed as math-relatedi"},{"bbox":{"x0":140,"x1":1049,"y0":1037,"y1":1060},"font_size":0.0,"text":"(e.g., mathoverflow.net). Subsequently, we manually annotate the URLs associated with"},{"bbox":{"x0":140,"x1":1051,"y0":1063,"y1":1088},"font_size":0.0,"text":"mathematical content within these identifed domains (e.g., mathoverflow.net/questions).i"},{"bbox":{"x0":136,"x1":1051,"y0":1086,"y1":1117},"font_size":0.0,"text":"Web pages linked to these URLs, yet uncollected, will be added to the seed corpus. This ap-"},{"bbox":{"x0":138,"x1":1049,"y0":1116,"y1":1144},"font_size":0.0,"text":"proach enables us to gather more positive examples, thereby training an improved fastText"},{"bbox":{"x0":138,"x1":1049,"y0":1145,"y1":1168},"font_size":0.0,"text":"model capable of recalling more mathematical data in the subsequent iteration. After four"},{"bbox":{"x0":140,"x1":1049,"y0":1172,"y1":1196},"font_size":0.0,"text":"iterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B"},{"bbox":{"x0":136,"x1":1049,"y0":1196,"y1":1223},"font_size":0.0,"text":"tokens. In the fourth iteration, we notice that nearly $98\\%$ of the data has already been collected"},{"bbox":{"x0":138,"x1":704,"y0":1226,"y1":1249},"font_size":0.0,"text":"in the third iteration, so we decide to cease data collection."}],"source":"layout det","text":"After the frst iteration of data collection, numerous mathematical web pages remain un-i collected, mainly because the fastText model is trained on a set of positive examples that lacks suffcient diversity. We therefore identify additional mathematical web sources to enrich the seedi corpus, so that we can optimize the fastText model. Specifcally, we frst organize the entire Com-ii mon Crawl into disjoint domains; a domain is defned as web pages sharing the same base URL.i For each domain, we calculate the percentage of web pages that are collected in the frst iteration.i Domains where over $10\\%$ of the web pages have been collected are classifed as math-relatedi(e.g., mathoverflow.net). Subsequently, we manually annotate the URLs associated with mathematical content within these identifed domains (e.g., mathoverflow.net/questions).i Web pages linked to these URLs, yet uncollected, will be added to the seed corpus. This approach enables us to gather more positive examples, thereby training an improved fastText model capable of recalling more mathematical data in the subsequent iteration. After four iterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B tokens. In the fourth iteration, we notice that nearly $98\\%$ of the data has already been collected in the third iteration, so we decide to cease data collection."},{"bbox":{"x0":133,"x1":1059,"y0":1262,"y1":1488},"conf":0.978,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":1265,"y1":1291},"font_size":0.0,"text":"To avoid benchmark contamination, we follow Guo et al. (2024) to flter out web pagesi"},{"bbox":{"x0":138,"x1":1049,"y0":1293,"y1":1319},"font_size":0.0,"text":"containing questions or answers from English mathematical benchmarks such as GSM8K (Cobbe"},{"bbox":{"x0":140,"x1":1049,"y0":1321,"y1":1344},"font_size":0.0,"text":"et al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH"},{"bbox":{"x0":138,"x1":1049,"y0":1345,"y1":1374},"font_size":0.0,"text":"(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The fltering criteria are as follows: anyi"},{"bbox":{"x0":140,"x1":1049,"y0":1375,"y1":1400},"font_size":0.0,"text":"text segment containing a 10-gram string that matches exactly with any sub-string from the"},{"bbox":{"x0":140,"x1":1051,"y0":1402,"y1":1426},"font_size":0.0,"text":"evaluation benchmarks is removed from our math training corpus. For benchmark texts that"},{"bbox":{"x0":138,"x1":1049,"y0":1430,"y1":1454},"font_size":0.0,"text":"are shorter than 10 grams but have at least 3 grams, we employ exact matching to flter outi"},{"bbox":{"x0":138,"x1":386,"y0":1458,"y1":1481},"font_size":0.0,"text":"contaminated web pages."}],"source":"layout det","text":"To avoid benchmark contamination, we follow Guo et al. (2024) to flter out web pagesi containing questions or answers from English mathematical benchmarks such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The fltering criteria are as follows: anyi text segment containing a 10-gram string that matches exactly with any sub-string from the evaluation benchmarks is removed from our math training corpus. For benchmark texts that are shorter than 10 grams but have at least 3 grams, we employ exact matching to flter outi contaminated web pages."}],"formula_dets":[{"bbox":{"x0":350,"x1":393,"y0":1008,"y1":1031},"conf":0.8306,"label":"print_embedding","label_id":0},{"bbox":{"x0":641,"x1":684,"y0":1197,"y1":1221},"conf":0.8292,"label":"print_embedding","label_id":0},{"bbox":{"x0":335,"x1":355,"y0":620,"y1":640},"conf":0.6611,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":131,"x1":1060,"y0":843,"y1":1253},"conf":0.9841,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":1262,"y1":1488},"conf":0.978,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":610,"y1":836},"conf":0.9716,"label":"Text","label_id":1},{"bbox":{"x0":148,"x1":1043,"y0":516,"y1":555},"conf":0.8112,"label":"Figure caption","label_id":4},{"bbox":{"x0":144,"x1":1052,"y0":160,"y1":506},"conf":0.6837,"label":"Figure","label_id":3},{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1580},"conf":0.5767,"label":"Abandon","label_id":2},{"bbox":{"x0":144,"x1":1052,"y0":160,"y1":506},"conf":0.3403,"label":"Figure","label_id":3},{"bbox":{"x0":240,"x1":1029,"y0":517,"y1":552},"conf":0.2572,"label":"Figure caption","label_id":4}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1554],[603,1554],[603,1573],[587,1573]],"score":0.8911},{"poly":[[138,1458],[386,1458],[386,1481],[138,1481]],"score":0.8564},{"poly":[[138,1430],[1049,1430],[1049,1452],[138,1452]],"score":0.9124},{"poly":[[140,1402],[1051,1402],[1051,1426],[140,1426]],"score":0.7469},{"poly":[[140,1375],[1049,1375],[1049,1400],[140,1400]],"score":0.7212},{"poly":[[138,1345],[1049,1349],[1049,1374],[138,1370]],"score":0.7149},{"poly":[[140,1321],[1049,1321],[1049,1344],[140,1344]],"score":0.8333},{"poly":[[138,1295],[1049,1293],[1049,1317],[138,1319]],"score":0.7658},{"poly":[[174,1265],[1049,1267],[1049,1291],[174,1289]],"score":0.7478},{"poly":[[138,1226],[704,1226],[704,1249],[138,1249]],"score":0.8394},{"poly":[[136,1196],[1049,1198],[1049,1223],[136,1221]],"score":0.7414},{"poly":[[140,1172],[1049,1172],[1049,1196],[140,1196]],"score":0.7235},{"poly":[[138,1145],[1049,1145],[1049,1168],[138,1168]],"score":0.8572},{"poly":[[138,1119],[1049,1116],[1049,1140],[138,1144]],"score":0.7201},{"poly":[[137,1086],[1051,1088],[1051,1117],[136,1116]],"score":0.6757},{"poly":[[140,1063],[1051,1063],[1051,1088],[140,1088]],"score":0.7252},{"poly":[[140,1037],[1049,1037],[1049,1060],[140,1060]],"score":0.8734},{"poly":[[140,1009],[1049,1009],[1049,1033],[140,1033]],"score":0.7426},{"poly":[[140,982],[1053,982],[1053,1007],[140,1007]],"score":0.6838},{"poly":[[140,956],[1053,956],[1053,981],[140,981]],"score":0.6987},{"poly":[[138,928],[1051,926],[1051,951],[138,953]],"score":0.723},{"poly":[[140,902],[1049,902],[1049,924],[140,924]],"score":0.8154},{"poly":[[138,872],[1049,874],[1049,898],[138,896]],"score":0.7127},{"poly":[[175,847],[1051,847],[1051,872],[175,872]],"score":0.7324},{"poly":[[140,807],[831,807],[831,830],[140,830]],"score":0.8619},{"poly":[[140,779],[1049,779],[1049,803],[140,803]],"score":0.7464},{"poly":[[140,753],[1049,753],[1049,775],[140,775]],"score":0.8732},{"poly":[[140,724],[1049,724],[1049,749],[140,749]],"score":0.7409},{"poly":[[138,698],[1049,698],[1049,721],[138,721]],"score":0.873},{"poly":[[140,670],[1049,670],[1049,695],[140,695]],"score":0.7455},{"poly":[[138,644],[1049,644],[1049,667],[138,667]],"score":0.8877},{"poly":[[138,612],[1051,610],[1051,640],[138,642]],"score":0.7497},{"poly":[[154,523],[1031,519],[1032,544],[154,547]],"score":0.7714},{"poly":[[307,468],[489,470],[489,493],[307,491]],"score":0.7726},{"poly":[[631,461],[879,461],[879,484],[631,484]],"score":0.7776},{"poly":[[307,453],[489,453],[489,470],[307,470]],"score":0.9432},{"poly":[[516,414],[635,418],[634,437],[515,433]],"score":0.7241},{"poly":[[477,396],[675,396],[675,419],[477,419]],"score":0.7331},{"poly":[[938,391],[1018,395],[1016,420],[937,415]],"score":0.7824},{"poly":[[151,375],[223,375],[223,395],[151,395]],"score":0.9169},{"poly":[[152,295],[222,295],[222,372],[152,372]],"score":0.7261},{"poly":[[573,303],[610,293],[615,311],[577,321]],"score":0.7622},{"poly":[[746,230],[759,230],[759,242],[746,242]],"score":0.9144},{"poly":[[670,196],[833,196],[833,219],[670,219]],"score":0.8126},{"poly":[[298,186],[482,186],[482,210],[298,210]],"score":0.7999},{"poly":[[631,177],[872,179],[872,202],[631,200]],"score":0.8285}],"page_no":4,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":608,"y0":1551,"y1":1579},"conf":0.6532,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1554,"y1":1573},"font_size":0.0,"text":"6"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":718,"y0":168,"y1":203},"conf":0.8755,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":709,"y0":174,"y1":196},"font_size":0.0,"text":"2.2. Validating the Quality of the DeepSeekMath Corpus"}],"source":"layout det","text":"2.2. Validating the Quality of the DeepSeekMath Corpus"},{"bbox":{"x0":132,"x1":1056,"y0":213,"y1":276},"conf":0.9329,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":218,"y1":242},"font_size":0.0,"text":"We run pre-training experiments to investigate how the DeepSeekMath Corpus is compared"},{"bbox":{"x0":136,"x1":615,"y0":244,"y1":270},"font_size":0.0,"text":"with the recently released math-training corpora:"}],"source":"layout det","text":"We run pre-training experiments to investigate how the DeepSeekMath Corpus is compared with the recently released math-training corpora:"},{"bbox":{"x0":188,"x1":1058,"y0":281,"y1":364},"conf":0.9549,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":284,"y1":309},"font_size":0.0,"text":"MathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from"},{"bbox":{"x0":193,"x1":1049,"y0":310,"y1":335},"font_size":0.0,"text":"textbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the"},{"bbox":{"x0":191,"x1":585,"y0":339,"y1":363},"font_size":0.0,"text":"majority (over $85\\%$ ) sourced from arXiv;"}],"source":"layout det","text":"MathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from textbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the majority (over $85\\%$ ) sourced from arXiv;"},{"bbox":{"x0":186,"x1":1058,"y0":366,"y1":419},"conf":0.9388,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":1051,"y0":367,"y1":391},"font_size":0.0,"text":"OpenWebMath (Paster et al., 2023): CommonCrawl data fltered for mathematical content,i"},{"bbox":{"x0":191,"x1":406,"y0":395,"y1":417},"font_size":0.0,"text":"totaling 13.6B tokens;"}],"source":"layout det","text":"OpenWebMath (Paster et al., 2023): CommonCrawl data fltered for mathematical content,i totaling 13.6B tokens;"},{"bbox":{"x0":189,"x1":1060,"y0":420,"y1":530},"conf":0.9555,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":190,"x1":1051,"y0":421,"y1":444},"font_size":0.0,"text":"Proof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWeb-"},{"bbox":{"x0":193,"x1":1051,"y0":447,"y1":472},"font_size":0.0,"text":"Math, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B to-"},{"bbox":{"x0":193,"x1":1051,"y0":475,"y1":498},"font_size":0.0,"text":"kens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an"},{"bbox":{"x0":193,"x1":484,"y0":502,"y1":524},"font_size":0.0,"text":"arXiv:Web:Code ratio of 2:4:1."}],"source":"layout det","text":"Proof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWebMath, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B tokens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an arXiv:Web:Code ratio of 2:4:1."},{"bbox":{"x0":133,"x1":366,"y0":558,"y1":594},"conf":0.8986,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":360,"y0":559,"y1":590},"font_size":0.0,"text":"2.2.1. Training Setting"}],"source":"layout det","text":"2.2.1. Training Setting"},{"bbox":{"x0":133,"x1":1060,"y0":603,"y1":883},"conf":0.9751,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":607,"y1":632},"font_size":0.0,"text":"We apply math training to a general pre-trained language model with 1.3B parameters, which"},{"bbox":{"x0":140,"x1":1051,"y0":635,"y1":658},"font_size":0.0,"text":"shares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeek-"},{"bbox":{"x0":140,"x1":1049,"y0":660,"y1":682},"font_size":0.0,"text":"LLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All"},{"bbox":{"x0":138,"x1":1049,"y0":689,"y1":714},"font_size":0.0,"text":"experiments are conducted using the effcient and light-weight HAI-LLM (High-fyer, 2023)il"},{"bbox":{"x0":140,"x1":1049,"y0":716,"y1":740},"font_size":0.0,"text":"training framework. Following the training practice of DeepSeek LLMs, we use the AdamW"},{"bbox":{"x0":136,"x1":1051,"y0":738,"y1":770},"font_size":0.0,"text":"optimizer (Loshchilov and Hutter, 2017) with $\\beta_{1}=0.9,\\beta_{2}=0.95,$ and weight_decay $=0.1,$  along"},{"bbox":{"x0":138,"x1":1047,"y0":770,"y1":793},"font_size":0.0,"text":"with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000"},{"bbox":{"x0":138,"x1":1049,"y0":796,"y1":823},"font_size":0.0,"text":"warmup steps, decreases to its $31.6\\%$ after $80\\%$ of the training process, and further decreases to"},{"bbox":{"x0":139,"x1":1051,"y0":824,"y1":849},"font_size":0.0,"text":"$10.0\\%$ of the peak after $90\\%$ of the training process. We set the maximum value of learning rate"},{"bbox":{"x0":138,"x1":801,"y0":853,"y1":875},"font_size":0.0,"text":"to 5.3e-4, and use a batch size of 4M tokens with a 4K context length."}],"source":"layout det","text":"We apply math training to a general pre-trained language model with 1.3B parameters, which shares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeekLLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the effcient and light-weight HAI-LLM (High-fyer, 2023)il training framework. Following the training practice of DeepSeek LLMs, we use the AdamW optimizer (Loshchilov and Hutter, 2017) with $\\beta_{1}=0.9,\\beta_{2}=0.95,$ and weight_decay $=0.1,$  along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its $31.6\\%$ after $80\\%$ of the training process, and further decreases to $10.0\\%$ of the peak after $90\\%$ of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length."},{"bbox":{"x0":138,"x1":1052,"y0":894,"y1":1150},"conf":0.9727,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":503,"x1":687,"y0":903,"y1":932},"font_size":0.0,"text":"English Benchmarks"},{"bbox":{"x0":814,"x1":997,"y0":904,"y1":928},"font_size":0.0,"text":"Chinese Benchmarks"},{"bbox":{"x0":146,"x1":271,"y0":926,"y1":957},"font_size":0.0,"text":"Math Corpus"},{"bbox":{"x0":354,"x1":402,"y0":927,"y1":955},"font_size":0.0,"text":"Size"},{"bbox":{"x0":693,"x1":765,"y0":937,"y1":963},"font_size":0.0,"text":"MMLU"},{"bbox":{"x0":866,"x1":942,"y0":938,"y1":963},"font_size":0.0,"text":"Gaokao"},{"bbox":{"x0":960,"x1":1039,"y0":936,"y1":964},"font_size":0.0,"text":"Gaokao"},{"bbox":{"x0":425,"x1":575,"y0":949,"y1":976},"font_size":0.0,"text":"GSM8K MATH "},{"bbox":{"x0":568,"x1":631,"y0":949,"y1":974},"font_size":0.0,"text":"OCW"},{"bbox":{"x0":636,"x1":686,"y0":948,"y1":977},"font_size":0.0,"text":"SAT"},{"bbox":{"x0":767,"x1":852,"y0":947,"y1":976},"font_size":0.0,"text":"CMATH"},{"bbox":{"x0":696,"x1":763,"y0":959,"y1":988},"font_size":0.0,"text":"STEM"},{"bbox":{"x0":850,"x1":1042,"y0":959,"y1":989},"font_size":0.0,"text":"MathClozeMathQA"},{"bbox":{"x0":953,"x1":961,"y0":974,"y1":980},"font_size":0.0,"text":""},{"bbox":{"x0":145,"x1":311,"y0":993,"y1":1025},"font_size":0.0,"text":"No Math Training"},{"bbox":{"x0":356,"x1":409,"y0":995,"y1":1022},"font_size":0.0,"text":"N/A"},{"bbox":{"x0":438,"x1":485,"y0":997,"y1":1019},"font_size":0.0,"text":" $2.9\\%$ "},{"bbox":{"x0":512,"x1":559,"y0":998,"y1":1019},"font_size":0.0,"text":" $3.0\\%$ "},{"bbox":{"x0":578,"x1":624,"y0":998,"y1":1019},"font_size":0.0,"text":" $2.9\\%$ "},{"bbox":{"x0":634,"x1":689,"y0":997,"y1":1020},"font_size":0.0,"text":" $15.6\\%$ "},{"bbox":{"x0":701,"x1":757,"y0":997,"y1":1020},"font_size":0.0,"text":" $19.5\\%$ "},{"bbox":{"x0":781,"x1":837,"y0":997,"y1":1020},"font_size":0.0,"text":" $12.3\\%$ "},{"bbox":{"x0":880,"x1":928,"y0":997,"y1":1019},"font_size":0.0,"text":" $0.8\\%$ "},{"bbox":{"x0":971,"x1":1029,"y0":997,"y1":1020},"font_size":0.0,"text":" $17.9\\%$ "},{"bbox":{"x0":145,"x1":236,"y0":1026,"y1":1056},"font_size":0.0,"text":"MathPile"},{"bbox":{"x0":356,"x1":403,"y0":1029,"y1":1055},"font_size":0.0,"text":"8.9B"},{"bbox":{"x0":438,"x1":485,"y0":1031,"y1":1053},"font_size":0.0,"text":" $2.7\\%$ "},{"bbox":{"x0":513,"x1":559,"y0":1032,"y1":1053},"font_size":0.0,"text":" $3.3\\%$ "},{"bbox":{"x0":578,"x1":623,"y0":1032,"y1":1053},"font_size":0.0,"text":" $2.2\\%$ "},{"bbox":{"x0":634,"x1":689,"y0":1032,"y1":1053},"font_size":0.0,"text":" $12.5\\%$ "},{"bbox":{"x0":701,"x1":757,"y0":1032,"y1":1053},"font_size":0.0,"text":" $15.7\\%$ "},{"bbox":{"x0":785,"x1":832,"y0":1032,"y1":1053},"font_size":0.0,"text":" $1.2\\%$ "},{"bbox":{"x0":880,"x1":927,"y0":1032,"y1":1053},"font_size":0.0,"text":" $0.0\\%$ "},{"bbox":{"x0":975,"x1":1024,"y0":1031,"y1":1053},"font_size":0.0,"text":" $2.8\\%$ "},{"bbox":{"x0":146,"x1":287,"y0":1051,"y1":1082},"font_size":0.0,"text":"OpenWebMath"},{"bbox":{"x0":357,"x1":412,"y0":1053,"y1":1078},"font_size":0.0,"text":"13.6B"},{"bbox":{"x0":434,"x1":490,"y0":1056,"y1":1076},"font_size":0.0,"text":" $11.5\\%$ "},{"bbox":{"x0":513,"x1":559,"y0":1057,"y1":1076},"font_size":0.0,"text":" $8.9\\%$ "},{"bbox":{"x0":578,"x1":623,"y0":1058,"y1":1076},"font_size":0.0,"text":" $3.7\\%$ "},{"bbox":{"x0":634,"x1":689,"y0":1057,"y1":1077},"font_size":0.0,"text":" $31.3\\%$ "},{"bbox":{"x0":701,"x1":757,"y0":1057,"y1":1076},"font_size":0.0,"text":" $29.6\\%$ "},{"bbox":{"x0":781,"x1":837,"y0":1057,"y1":1076},"font_size":0.0,"text":" $16.8\\%$ "},{"bbox":{"x0":880,"x1":927,"y0":1057,"y1":1076},"font_size":0.0,"text":" $0.0\\%$ "},{"bbox":{"x0":972,"x1":1029,"y0":1056,"y1":1076},"font_size":0.0,"text":" $14.2\\%$ "},{"bbox":{"x0":147,"x1":256,"y0":1077,"y1":1102},"font_size":0.0,"text":"Proof-Pile-2"},{"bbox":{"x0":356,"x1":413,"y0":1077,"y1":1103},"font_size":0.0,"text":"51.9B"},{"bbox":{"x0":434,"x1":490,"y0":1080,"y1":1101},"font_size":0.0,"text":" $14.3\\%$ "},{"bbox":{"x0":509,"x1":563,"y0":1081,"y1":1101},"font_size":0.0,"text":" $11.2\\%$ "},{"bbox":{"x0":578,"x1":623,"y0":1082,"y1":1101},"font_size":0.0,"text":" $3.7\\%$ "},{"bbox":{"x0":634,"x1":689,"y0":1081,"y1":1101},"font_size":0.0,"text":" $43.8\\%$ "},{"bbox":{"x0":701,"x1":757,"y0":1081,"y1":1101},"font_size":0.0,"text":" $29.2\\%$ "},{"bbox":{"x0":781,"x1":837,"y0":1080,"y1":1101},"font_size":0.0,"text":" $19.9\\%$ "},{"bbox":{"x0":879,"x1":928,"y0":1080,"y1":1101},"font_size":0.0,"text":" $5.1\\%$ "},{"bbox":{"x0":971,"x1":1030,"y0":1080,"y1":1102},"font_size":0.0,"text":" $11.7\\%$ "},{"bbox":{"x0":148,"x1":359,"y0":1112,"y1":1139},"font_size":0.0,"text":"DeepSeekMath Corpus "},{"bbox":{"x0":355,"x1":423,"y0":1112,"y1":1137},"font_size":0.0,"text":"120.2B"},{"bbox":{"x0":432,"x1":491,"y0":1113,"y1":1135},"font_size":0.0,"text":" $\\mathbf{23.8\\%}$ "},{"bbox":{"x0":508,"x1":564,"y0":1114,"y1":1135},"font_size":0.0,"text":" $\\mathbf{13.6\\%}$ "},{"bbox":{"x0":578,"x1":624,"y0":1114,"y1":1135},"font_size":0.0,"text":" $4.8\\%$ "},{"bbox":{"x0":634,"x1":689,"y0":1114,"y1":1135},"font_size":0.0,"text":" $\\textbf{56.3\\%}$ "},{"bbox":{"x0":701,"x1":757,"y0":1114,"y1":1135},"font_size":0.0,"text":" $\\mathbf{33.1\\%}$ "},{"bbox":{"x0":780,"x1":838,"y0":1113,"y1":1135},"font_size":0.0,"text":" $\\textbf{41.5\\%}$ "},{"bbox":{"x0":879,"x1":928,"y0":1113,"y1":1135},"font_size":0.0,"text":" $5.9\\%$ "},{"bbox":{"x0":970,"x1":1030,"y0":1113,"y1":1135},"font_size":0.0,"text":" $\\mathbf{23.6\\%}$ "}],"source":"layout det","text":"<html><body><table><thead><tr><td rowspan=\"2\">Math Corpus</td><td rowspan=\"2\">Size</td><td colspan=\"5\">English Benchmarks</td><td colspan=\"3\">Chinese Benchmarks</td></tr><tr><td>GSM8K MATH </td><td></td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathClozeMathQA </td><td>Gaokao</td></tr></thead><tbody><tr><td>No Math Training</td><td>N/A</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $2.9\\%$ </td><td> $15.6\\%$ </td><td> $19.5\\%$ </td><td> $12.3\\%$ </td><td> $0.8\\%$ </td><td> $17.9\\%$ </td></tr><tr><td>MathPile</td><td>8.9B</td><td> $2.7\\%$ </td><td> $3.3\\%$ </td><td> $2.2\\%$ </td><td> $12.5\\%$ </td><td> $15.7\\%$ </td><td> $1.2\\%$ </td><td> $0.0\\%$ </td><td> $2.8\\%$ </td></tr><tr><td>OpenWebMath</td><td>13.6B</td><td> $11.5\\%$ </td><td> $8.9\\%$ </td><td> $3.7\\%$ </td><td> $31.3\\%$ </td><td> $29.6\\%$ </td><td> $16.8\\%$ </td><td> $0.0\\%$ </td><td> $14.2\\%$ </td></tr><tr><td>Proof-Pile-2</td><td>51.9B</td><td> $14.3\\%$ </td><td> $11.2\\%$ </td><td> $3.7\\%$ </td><td> $43.8\\%$ </td><td> $29.2\\%$ </td><td> $19.9\\%$ </td><td> $5.1\\%$ </td><td> $11.7\\%$ </td></tr><tr><td>DeepSeekMath Corpus </td><td>120.2B</td><td> $\\mathbf{23.8\\%}$ </td><td> $\\mathbf{13.6\\%}$ </td><td> $4.8\\%$ </td><td> $\\textbf{56.3\\%}$ </td><td> $\\mathbf{33.1\\%}$ </td><td> $\\textbf{41.5\\%}$ </td><td> $5.9\\%$ </td><td> $\\mathbf{23.6\\%}$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":132,"x1":1057,"y0":1158,"y1":1249},"conf":0.8579,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1161,"y1":1188},"font_size":0.0,"text":"Table 1 | Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evalu-"},{"bbox":{"x0":140,"x1":1049,"y0":1191,"y1":1216},"font_size":0.0,"text":"ated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer"},{"bbox":{"x0":138,"x1":441,"y0":1217,"y1":1240},"font_size":0.0,"text":"with a vocabulary size of 100K."}],"source":"layout det","text":"Table 1 | Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evaluated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer with a vocabulary size of 100K."},{"bbox":{"x0":134,"x1":395,"y0":1287,"y1":1319},"conf":0.8933,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":390,"y0":1291,"y1":1314},"font_size":0.0,"text":"2.2.2. Evaluation Results"}],"source":"layout det","text":"2.2.2. Evaluation Results"},{"bbox":{"x0":133,"x1":1056,"y0":1331,"y1":1393},"conf":0.9278,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1047,"y0":1333,"y1":1358},"font_size":0.0,"text":"The DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and"},{"bbox":{"x0":140,"x1":347,"y0":1363,"y1":1386},"font_size":0.0,"text":"is the largest in size."}],"source":"layout det","text":"The DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and is the largest in size."},{"bbox":{"x0":187,"x1":1058,"y0":1400,"y1":1517},"conf":0.9621,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":1400,"y1":1428},"font_size":0.0,"text":"High-quality: We evaluate downstream performance on 8 mathematical benchmarks using"},{"bbox":{"x0":193,"x1":1049,"y0":1431,"y1":1454},"font_size":0.0,"text":"few-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear"},{"bbox":{"x0":193,"x1":1051,"y0":1456,"y1":1482},"font_size":0.0,"text":"performance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that"},{"bbox":{"x0":193,"x1":1051,"y0":1482,"y1":1509},"font_size":0.0,"text":"the model trained on the DeepSeekMath Corpus demonstrates better performance than"}],"source":"layout det","text":"High-quality: We evaluate downstream performance on 8 mathematical benchmarks using few-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear performance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that the model trained on the DeepSeekMath Corpus demonstrates better performance than"}],"formula_dets":[{"bbox":{"x0":575,"x1":752,"y0":740,"y1":769},"conf":0.8537,"label":"print_embedding","label_id":0},{"bbox":{"x0":361,"x1":405,"y0":824,"y1":847},"conf":0.8281,"label":"print_embedding","label_id":0},{"bbox":{"x0":139,"x1":198,"y0":824,"y1":847},"conf":0.8053,"label":"print_embedding","label_id":0},{"bbox":{"x0":546,"x1":589,"y0":797,"y1":820},"conf":0.8007,"label":"print_embedding","label_id":0},{"bbox":{"x0":435,"x1":495,"y0":796,"y1":820},"conf":0.7953,"label":"print_embedding","label_id":0},{"bbox":{"x0":970,"x1":1030,"y0":1113,"y1":1135},"conf":0.7929,"label":"print_embedding","label_id":0},{"bbox":{"x0":880,"x1":927,"y0":1057,"y1":1076},"conf":0.7908,"label":"print_embedding","label_id":0},{"bbox":{"x0":880,"x1":928,"y0":997,"y1":1019},"conf":0.7723,"label":"print_embedding","label_id":0},{"bbox":{"x0":781,"x1":837,"y0":1057,"y1":1076},"conf":0.7688,"label":"print_embedding","label_id":0},{"bbox":{"x0":880,"x1":927,"y0":1032,"y1":1053},"conf":0.7678,"label":"print_embedding","label_id":0},{"bbox":{"x0":972,"x1":1029,"y0":1056,"y1":1076},"conf":0.7669,"label":"print_embedding","label_id":0},{"bbox":{"x0":971,"x1":1030,"y0":1080,"y1":1102},"conf":0.7667,"label":"print_embedding","label_id":0},{"bbox":{"x0":781,"x1":837,"y0":1080,"y1":1101},"conf":0.7664,"label":"print_embedding","label_id":0},{"bbox":{"x0":879,"x1":928,"y0":1080,"y1":1101},"conf":0.7635,"label":"print_embedding","label_id":0},{"bbox":{"x0":975,"x1":1024,"y0":1031,"y1":1053},"conf":0.7616,"label":"print_embedding","label_id":0},{"bbox":{"x0":701,"x1":757,"y0":1057,"y1":1076},"conf":0.7571,"label":"print_embedding","label_id":0},{"bbox":{"x0":971,"x1":1029,"y0":997,"y1":1020},"conf":0.7532,"label":"print_embedding","label_id":0},{"bbox":{"x0":781,"x1":837,"y0":997,"y1":1020},"conf":0.7458,"label":"print_embedding","label_id":0},{"bbox":{"x0":879,"x1":928,"y0":1113,"y1":1135},"conf":0.7422,"label":"print_embedding","label_id":0},{"bbox":{"x0":701,"x1":757,"y0":1081,"y1":1101},"conf":0.7378,"label":"print_embedding","label_id":0},{"bbox":{"x0":780,"x1":838,"y0":1113,"y1":1135},"conf":0.7298,"label":"print_embedding","label_id":0},{"bbox":{"x0":701,"x1":757,"y0":997,"y1":1020},"conf":0.7282,"label":"print_embedding","label_id":0},{"bbox":{"x0":578,"x1":623,"y0":1082,"y1":1101},"conf":0.7241,"label":"print_embedding","label_id":0},{"bbox":{"x0":578,"x1":624,"y0":998,"y1":1019},"conf":0.7223,"label":"print_embedding","label_id":0},{"bbox":{"x0":578,"x1":623,"y0":1058,"y1":1076},"conf":0.7171,"label":"print_embedding","label_id":0},{"bbox":{"x0":701,"x1":757,"y0":1032,"y1":1053},"conf":0.7144,"label":"print_embedding","label_id":0},{"bbox":{"x0":512,"x1":559,"y0":998,"y1":1019},"conf":0.713,"label":"print_embedding","label_id":0},{"bbox":{"x0":634,"x1":689,"y0":997,"y1":1020},"conf":0.7088,"label":"print_embedding","label_id":0},{"bbox":{"x0":785,"x1":832,"y0":1032,"y1":1053},"conf":0.7085,"label":"print_embedding","label_id":0},{"bbox":{"x0":337,"x1":379,"y0":339,"y1":362},"conf":0.7048,"label":"print_embedding","label_id":0},{"bbox":{"x0":432,"x1":491,"y0":1113,"y1":1135},"conf":0.7036,"label":"print_embedding","label_id":0},{"bbox":{"x0":434,"x1":490,"y0":1080,"y1":1101},"conf":0.7006,"label":"print_embedding","label_id":0},{"bbox":{"x0":509,"x1":563,"y0":1081,"y1":1101},"conf":0.6967,"label":"print_embedding","label_id":0},{"bbox":{"x0":438,"x1":485,"y0":997,"y1":1019},"conf":0.6943,"label":"print_embedding","label_id":0},{"bbox":{"x0":434,"x1":490,"y0":1056,"y1":1076},"conf":0.6855,"label":"print_embedding","label_id":0},{"bbox":{"x0":634,"x1":689,"y0":1081,"y1":1101},"conf":0.6852,"label":"print_embedding","label_id":0},{"bbox":{"x0":508,"x1":564,"y0":1114,"y1":1135},"conf":0.6784,"label":"print_embedding","label_id":0},{"bbox":{"x0":578,"x1":623,"y0":1032,"y1":1053},"conf":0.6782,"label":"print_embedding","label_id":0},{"bbox":{"x0":438,"x1":485,"y0":1031,"y1":1053},"conf":0.6775,"label":"print_embedding","label_id":0},{"bbox":{"x0":634,"x1":689,"y0":1057,"y1":1077},"conf":0.6581,"label":"print_embedding","label_id":0},{"bbox":{"x0":513,"x1":559,"y0":1057,"y1":1076},"conf":0.6565,"label":"print_embedding","label_id":0},{"bbox":{"x0":513,"x1":559,"y0":1032,"y1":1053},"conf":0.6529,"label":"print_embedding","label_id":0},{"bbox":{"x0":578,"x1":624,"y0":1114,"y1":1135},"conf":0.6508,"label":"print_embedding","label_id":0},{"bbox":{"x0":936,"x1":989,"y0":743,"y1":766},"conf":0.6345,"label":"print_embedding","label_id":0},{"bbox":{"x0":701,"x1":757,"y0":1114,"y1":1135},"conf":0.6219,"label":"print_embedding","label_id":0},{"bbox":{"x0":634,"x1":689,"y0":1032,"y1":1053},"conf":0.6196,"label":"print_embedding","label_id":0},{"bbox":{"x0":634,"x1":689,"y0":1114,"y1":1135},"conf":0.614,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1060,"y0":603,"y1":883},"conf":0.9751,"label":"Text","label_id":1},{"bbox":{"x0":138,"x1":1052,"y0":894,"y1":1150},"conf":0.9727,"label":"Table","label_id":5},{"bbox":{"x0":187,"x1":1058,"y0":1400,"y1":1517},"conf":0.9621,"label":"Text","label_id":1},{"bbox":{"x0":189,"x1":1060,"y0":420,"y1":530},"conf":0.9555,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1058,"y0":281,"y1":364},"conf":0.9549,"label":"Text","label_id":1},{"bbox":{"x0":186,"x1":1058,"y0":366,"y1":419},"conf":0.9388,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1056,"y0":213,"y1":276},"conf":0.9329,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":1331,"y1":1393},"conf":0.9278,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":366,"y0":558,"y1":594},"conf":0.8986,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":395,"y0":1287,"y1":1319},"conf":0.8933,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":718,"y0":168,"y1":203},"conf":0.8755,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":1057,"y0":1158,"y1":1249},"conf":0.8579,"label":"Text","label_id":1},{"bbox":{"x0":582,"x1":608,"y0":1551,"y1":1579},"conf":0.6532,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1554],[603,1554],[603,1573],[587,1573]],"score":0.9004},{"poly":[[193,1482],[1051,1484],[1051,1509],[193,1507]],"score":0.8072},{"poly":[[193,1458],[1051,1456],[1051,1481],[193,1482]],"score":0.7789},{"poly":[[193,1431],[1049,1431],[1049,1454],[193,1454]],"score":0.8441},{"poly":[[170,1400],[1049,1403],[1049,1428],[170,1424]],"score":0.7146},{"poly":[[140,1363],[347,1363],[347,1386],[140,1386]],"score":0.898},{"poly":[[140,1333],[1047,1333],[1047,1358],[140,1358]],"score":0.7275},{"poly":[[140,1291],[390,1291],[390,1314],[140,1314]],"score":0.8522},{"poly":[[138,1217],[441,1217],[441,1240],[138,1240]],"score":0.8902},{"poly":[[140,1191],[1049,1191],[1049,1216],[140,1216]],"score":0.7747},{"poly":[[138,1161],[1051,1163],[1051,1188],[138,1186]],"score":0.836},{"poly":[[971,1112],[1030,1112],[1030,1138],[971,1138]],"score":0.8506},{"poly":[[877,1112],[929,1112],[929,1138],[877,1138]],"score":0.8906},{"poly":[[776,1112],[838,1112],[838,1137],[776,1137]],"score":0.9586},{"poly":[[505,1112],[760,1112],[760,1137],[505,1137]],"score":0.7234},{"poly":[[416,1114],[491,1114],[491,1133],[416,1133]],"score":0.8405},{"poly":[[147,1112],[425,1110],[425,1135],[147,1137]],"score":0.8012},{"poly":[[629,1079],[691,1079],[691,1103],[629,1103]],"score":0.8317},{"poly":[[147,1079],[257,1079],[257,1103],[147,1103]],"score":0.8912},{"poly":[[971,1077],[1030,1077],[1030,1103],[971,1103]],"score":0.9706},{"poly":[[877,1077],[929,1077],[929,1103],[877,1103]],"score":0.9514},{"poly":[[780,1077],[838,1077],[838,1103],[780,1103]],"score":0.9734},{"poly":[[697,1077],[759,1077],[759,1103],[697,1103]],"score":0.8371},{"poly":[[574,1077],[631,1077],[631,1103],[574,1103]],"score":0.7504},{"poly":[[507,1077],[567,1077],[567,1103],[507,1103]],"score":0.8177},{"poly":[[432,1077],[493,1077],[493,1103],[432,1103]],"score":0.9307},{"poly":[[356,1077],[413,1077],[413,1103],[356,1103]],"score":0.9501},{"poly":[[629,1054],[693,1054],[693,1079],[629,1079]],"score":0.8074},{"poly":[[147,1054],[285,1054],[285,1079],[147,1079]],"score":0.8446},{"poly":[[971,1052],[1030,1052],[1030,1079],[971,1079]],"score":0.8639},{"poly":[[877,1052],[929,1052],[929,1079],[877,1079]],"score":0.867},{"poly":[[778,1052],[838,1052],[838,1079],[778,1079]],"score":0.8469},{"poly":[[698,1050],[759,1055],[757,1079],[696,1075]],"score":0.8054},{"poly":[[576,1052],[633,1052],[633,1079],[576,1079]],"score":0.7937},{"poly":[[510,1052],[564,1052],[564,1079],[510,1079]],"score":0.8599},{"poly":[[432,1052],[491,1052],[491,1079],[432,1079]],"score":0.8477},{"poly":[[356,1052],[413,1052],[413,1079],[356,1079]],"score":0.832},{"poly":[[697,1030],[760,1030],[760,1054],[697,1054]],"score":0.909},{"poly":[[147,1030],[234,1030],[234,1054],[147,1054]],"score":0.9642},{"poly":[[973,1028],[1026,1028],[1026,1056],[973,1056]],"score":0.8583},{"poly":[[877,1028],[930,1028],[930,1056],[877,1056]],"score":0.7898},{"poly":[[782,1028],[837,1028],[837,1056],[782,1056]],"score":0.8583},{"poly":[[572,1024],[695,1024],[695,1054],[572,1054]],"score":0.697},{"poly":[[510,1028],[564,1028],[564,1056],[510,1056]],"score":0.8511},{"poly":[[436,1028],[487,1028],[487,1054],[436,1054]],"score":0.9663},{"poly":[[354,1028],[406,1028],[406,1056],[354,1056]],"score":0.8222},{"poly":[[778,996],[840,996],[840,1021],[778,1021]],"score":0.9203},{"poly":[[572,996],[760,996],[760,1021],[572,1021]],"score":0.8057},{"poly":[[510,996],[580,996],[580,1021],[510,1021]],"score":0.7902},{"poly":[[354,996],[409,996],[409,1021],[354,1021]],"score":0.9643},{"poly":[[146,993],[310,998],[310,1023],[145,1017]],"score":0.7428},{"poly":[[971,995],[1031,995],[1031,1021],[971,1021]],"score":0.8674},{"poly":[[877,995],[930,995],[930,1021],[877,1021]],"score":0.8559},{"poly":[[434,995],[487,995],[487,1021],[434,1021]],"score":0.7875},{"poly":[[851,959],[1040,961],[1040,988],[851,986]],"score":0.7836},{"poly":[[698,961],[760,961],[760,988],[698,988]],"score":0.9356},{"poly":[[769,949],[849,949],[849,974],[769,974]],"score":0.8617},{"poly":[[425,949],[682,947],[682,972],[425,974]],"score":0.766},{"poly":[[960,935],[1037,939],[1036,963],[958,959]],"score":0.8132},{"poly":[[865,938],[946,938],[946,963],[865,963]],"score":0.8152},{"poly":[[693,938],[764,938],[764,963],[693,963]],"score":0.8262},{"poly":[[149,930],[269,930],[269,954],[149,954]],"score":0.8468},{"poly":[[356,926],[402,926],[402,954],[356,954]],"score":0.8119},{"poly":[[812,903],[998,905],[998,930],[812,928]],"score":0.7589},{"poly":[[505,905],[686,905],[686,930],[505,930]],"score":0.761},{"poly":[[138,853],[801,853],[801,875],[138,875]],"score":0.8447},{"poly":[[140,824],[1051,824],[1051,849],[140,849]],"score":0.7523},{"poly":[[138,798],[1049,798],[1049,823],[138,823]],"score":0.748},{"poly":[[138,770],[1047,770],[1047,793],[138,793]],"score":0.9087},{"poly":[[137,738],[1051,740],[1051,770],[136,768]],"score":0.6881},{"poly":[[140,716],[1049,716],[1049,740],[140,740]],"score":0.7466},{"poly":[[138,689],[1049,689],[1049,712],[138,712]],"score":0.8797},{"poly":[[140,660],[1049,660],[1049,682],[140,682]],"score":0.7609},{"poly":[[140,635],[1051,635],[1051,658],[140,658]],"score":0.8763},{"poly":[[140,607],[1049,607],[1049,632],[140,632]],"score":0.7796},{"poly":[[139,559],[360,563],[359,590],[138,586]],"score":0.7243},{"poly":[[193,502],[484,502],[484,524],[193,524]],"score":0.8963},{"poly":[[193,475],[1051,475],[1051,498],[193,498]],"score":0.9013},{"poly":[[193,447],[1051,447],[1051,472],[193,472]],"score":0.7992},{"poly":[[190,421],[1051,421],[1051,444],[190,444]],"score":0.8639},{"poly":[[191,395],[406,395],[406,417],[191,417]],"score":0.8505},{"poly":[[188,367],[1051,367],[1051,389],[188,389]],"score":0.8591},{"poly":[[191,340],[585,340],[585,363],[191,363]],"score":0.859},{"poly":[[193,310],[1049,310],[1049,335],[193,335]],"score":0.7368},{"poly":[[170,284],[1049,284],[1049,309],[170,309]],"score":0.7524},{"poly":[[137,244],[615,246],[615,270],[136,268]],"score":0.793},{"poly":[[140,218],[1049,218],[1049,242],[140,242]],"score":0.7536},{"poly":[[140,174],[709,174],[709,196],[140,196]],"score":0.845}],"page_no":5,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":609,"y0":1550,"y1":1580},"conf":0.6352,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":583,"x1":604,"y0":1551,"y1":1575},"font_size":0.0,"text":"7"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":189,"x1":1003,"y0":180,"y1":926},"conf":0.9746,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![83e41fb90c1a2a879edc840e102f4e78](imgs/83e41fb90c1a2a879edc840e102f4e78.jpg)"},{"bbox":{"x0":132,"x1":1056,"y0":938,"y1":977},"conf":0.925,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1047,"y0":944,"y1":968},"font_size":0.0,"text":"Figure 3 | Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora."}],"source":"layout det","text":"Figure 3 | Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora."},{"bbox":{"x0":186,"x1":1057,"y0":1010,"y1":1069},"conf":0.8701,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":193,"x1":1051,"y0":1014,"y1":1038},"font_size":0.0,"text":"Proof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of"},{"bbox":{"x0":193,"x1":516,"y0":1040,"y1":1065},"font_size":0.0,"text":"DeepSeekMath Corpus is higher."}],"source":"layout det","text":"Proof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of DeepSeekMath Corpus is higher."},{"bbox":{"x0":187,"x1":1060,"y0":1070,"y1":1230},"conf":0.9751,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":168,"x1":1055,"y0":1063,"y1":1095},"font_size":0.0,"text":"Multilingual: The DeepSeekMath Corpus encompasses data in multiple languages, pre-"},{"bbox":{"x0":193,"x1":1051,"y0":1095,"y1":1119},"font_size":0.0,"text":"dominantly featuring English and Chinese as the two most represented languages. As"},{"bbox":{"x0":191,"x1":1051,"y0":1121,"y1":1147},"font_size":0.0,"text":"shown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning"},{"bbox":{"x0":191,"x1":1053,"y0":1149,"y1":1174},"font_size":0.0,"text":"performance in both English and Chinese. In contrast, existing mathematical corpora,"},{"bbox":{"x0":190,"x1":1051,"y0":1174,"y1":1200},"font_size":0.0,"text":"which are primarily English-centric, show limited improvement and may even hinder"},{"bbox":{"x0":188,"x1":670,"y0":1200,"y1":1231},"font_size":0.0,"text":"performance in Chinese mathematical reasoning."}],"source":"layout det","text":"Multilingual: The DeepSeekMath Corpus encompasses data in multiple languages, predominantly featuring English and Chinese as the two most represented languages. As shown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning performance in both English and Chinese. In contrast, existing mathematical corpora,which are primarily English-centric, show limited improvement and may even hinder performance in Chinese mathematical reasoning."},{"bbox":{"x0":187,"x1":1060,"y0":1231,"y1":1373},"conf":0.9765,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":1049,"y0":1230,"y1":1254},"font_size":0.0,"text":"Large-scale: The DeepSeekMath Corpus is several times larger than existing mathematical"},{"bbox":{"x0":190,"x1":1053,"y0":1252,"y1":1284},"font_size":0.0,"text":"corpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeek-"},{"bbox":{"x0":191,"x1":1051,"y0":1282,"y1":1309},"font_size":0.0,"text":"Math Corpus, shows a steeper learning curve along with more lasting improvements. In"},{"bbox":{"x0":193,"x1":1051,"y0":1312,"y1":1337},"font_size":0.0,"text":"contrast, the baseline corpora are much smaller, and have already been repeated multiple"},{"bbox":{"x0":191,"x1":1051,"y0":1338,"y1":1361},"font_size":0.0,"text":"rounds during training, with the resulting model performance quickly reaching a plateau."}],"source":"layout det","text":"Large-scale: The DeepSeekMath Corpus is several times larger than existing mathematical corpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeekMath Corpus, shows a steeper learning curve along with more lasting improvements. In contrast, the baseline corpora are much smaller, and have already been repeated multiple rounds during training, with the resulting model performance quickly reaching a plateau."},{"bbox":{"x0":134,"x1":678,"y0":1401,"y1":1436},"conf":0.9131,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":672,"y0":1405,"y1":1428},"font_size":0.0,"text":"2.3. Training and Evaluating DeepSeekMath-Base 7B"}],"source":"layout det","text":"2.3. Training and Evaluating DeepSeekMath-Base 7B"},{"bbox":{"x0":132,"x1":1058,"y0":1446,"y1":1510},"conf":0.9302,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":1445,"y1":1477},"font_size":0.0,"text":"In this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning"},{"bbox":{"x0":138,"x1":1049,"y0":1475,"y1":1502},"font_size":0.0,"text":"abilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B"}],"source":"layout det","text":"In this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning abilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":187,"x1":1060,"y0":1231,"y1":1373},"conf":0.9765,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1060,"y0":1070,"y1":1230},"conf":0.9751,"label":"Text","label_id":1},{"bbox":{"x0":189,"x1":1003,"y0":180,"y1":926},"conf":0.9746,"label":"Figure","label_id":3},{"bbox":{"x0":132,"x1":1058,"y0":1446,"y1":1510},"conf":0.9302,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1056,"y0":938,"y1":977},"conf":0.925,"label":"Figure caption","label_id":4},{"bbox":{"x0":134,"x1":678,"y0":1401,"y1":1436},"conf":0.9131,"label":"Title","label_id":0},{"bbox":{"x0":186,"x1":1057,"y0":1010,"y1":1069},"conf":0.8701,"label":"Text","label_id":1},{"bbox":{"x0":581,"x1":609,"y0":1550,"y1":1580},"conf":0.6352,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[583,1551],[604,1551],[604,1575],[583,1575]],"score":0.7892},{"poly":[[138,1477],[1049,1475],[1049,1500],[138,1502]],"score":0.803},{"poly":[[135,1445],[1051,1447],[1051,1477],[135,1475]],"score":0.7075},{"poly":[[140,1405],[672,1405],[672,1428],[140,1428]],"score":0.8887},{"poly":[[191,1338],[1051,1338],[1051,1361],[191,1361]],"score":0.7565},{"poly":[[193,1312],[1051,1312],[1051,1337],[193,1337]],"score":0.7356},{"poly":[[191,1282],[1051,1284],[1051,1309],[191,1307]],"score":0.7776},{"poly":[[190,1254],[1053,1252],[1053,1282],[190,1284]],"score":0.7015},{"poly":[[188,1230],[1049,1230],[1049,1254],[188,1254]],"score":0.7315},{"poly":[[188,1200],[670,1202],[670,1231],[188,1230]],"score":0.7017},{"poly":[[190,1174],[1051,1175],[1051,1200],[190,1198]],"score":0.803},{"poly":[[191,1149],[1053,1149],[1053,1174],[191,1174]],"score":0.7487},{"poly":[[191,1121],[1051,1123],[1051,1147],[191,1145]],"score":0.7341},{"poly":[[193,1095],[1051,1095],[1051,1119],[193,1119]],"score":0.7327},{"poly":[[168,1063],[1055,1065],[1054,1095],[168,1093]],"score":0.6765},{"poly":[[193,1040],[516,1040],[516,1065],[193,1065]],"score":0.7521},{"poly":[[193,1014],[1051,1014],[1051,1038],[193,1038]],"score":0.8036},{"poly":[[140,944],[1047,944],[1047,968],[140,968]],"score":0.7587},{"poly":[[765,893],[871,897],[870,921],[764,917]],"score":0.8316},{"poly":[[363,895],[470,895],[470,919],[363,919]],"score":0.7979},{"poly":[[252,881],[268,881],[268,895],[252,895]],"score":0.984},{"poly":[[954,877],[989,877],[989,898],[954,898]],"score":0.8788},{"poly":[[652,879],[672,879],[672,896],[652,896]],"score":0.75},{"poly":[[551,877],[587,877],[587,898],[551,898]],"score":0.9335},{"poly":[[891,875],[929,875],[929,900],[891,900]],"score":0.8233},{"poly":[[835,875],[861,875],[861,898],[835,898]],"score":0.811},{"poly":[[771,875],[799,875],[799,898],[771,898]],"score":0.7673},{"poly":[[709,875],[737,875],[737,898],[709,898]],"score":0.8439},{"poly":[[491,875],[528,875],[528,900],[491,900]],"score":0.8688},{"poly":[[432,875],[461,875],[461,898],[432,898]],"score":0.7671},{"poly":[[367,875],[397,875],[397,898],[367,898]],"score":0.7556},{"poly":[[308,875],[335,875],[335,898],[308,898]],"score":0.8643},{"poly":[[622,863],[652,863],[652,886],[622,886]],"score":0.77},{"poly":[[230,853],[248,853],[248,870],[230,870]],"score":0.7821},{"poly":[[271,846],[308,846],[308,865],[271,865]],"score":0.9001},{"poly":[[620,807],[650,807],[650,831],[620,831]],"score":0.7773},{"poly":[[230,803],[246,803],[246,821],[230,821]],"score":0.8235},{"poly":[[780,760],[801,760],[801,775],[780,775]],"score":0.7047},{"poly":[[729,750],[754,755],[751,769],[726,764]],"score":0.6165},{"poly":[[501,751],[567,738],[575,777],[509,790]],"score":0.6313},{"poly":[[218,649],[248,649],[248,672],[218,672]],"score":0.7481},{"poly":[[622,644],[649,644],[649,668],[622,668]],"score":0.8324},{"poly":[[286,698],[562,606],[579,655],[302,747]],"score":0.6754},{"poly":[[220,600],[250,600],[250,621],[220,621]],"score":0.8186},{"poly":[[620,591],[650,591],[650,614],[620,614]],"score":0.741},{"poly":[[370,574],[463,574],[463,593],[370,593]],"score":0.8257},{"poly":[[790,572],[849,572],[849,596],[790,596]],"score":0.959},{"poly":[[762,539],[874,539],[874,568],[762,568]],"score":0.7247},{"poly":[[359,535],[472,539],[471,570],[358,566]],"score":0.7509},{"poly":[[954,523],[989,523],[989,544],[954,544]],"score":0.897},{"poly":[[893,523],[927,523],[927,544],[893,544]],"score":0.939},{"poly":[[709,523],[739,523],[739,544],[709,544]],"score":0.8274},{"poly":[[654,524],[672,524],[672,542],[654,542]],"score":0.8593},{"poly":[[551,523],[587,523],[587,544],[551,544]],"score":0.9204},{"poly":[[491,523],[526,523],[526,544],[491,544]],"score":0.8896},{"poly":[[308,523],[335,523],[335,544],[308,544]],"score":0.8934},{"poly":[[252,524],[268,524],[268,542],[252,542]],"score":0.8348},{"poly":[[835,521],[863,521],[863,544],[835,544]],"score":0.7686},{"poly":[[769,521],[799,521],[799,544],[769,544]],"score":0.7331},{"poly":[[431,521],[463,521],[463,544],[431,544]],"score":0.734},{"poly":[[367,521],[397,521],[397,544],[367,544]],"score":0.7328},{"poly":[[629,509],[654,509],[654,530],[629,530]],"score":0.6753},{"poly":[[230,509],[245,509],[245,526],[230,526]],"score":0.725},{"poly":[[273,484],[323,484],[323,502],[273,502]],"score":0.7442},{"poly":[[487,473],[575,480],[573,511],[485,504]],"score":0.6584},{"poly":[[633,458],[647,458],[647,474],[633,474]],"score":0.8185},{"poly":[[230,456],[245,456],[245,472],[230,472]],"score":0.7111},{"poly":[[858,451],[968,451],[968,470],[858,470]],"score":0.7925},{"poly":[[292,427],[397,397],[407,434],[303,464]],"score":0.6059},{"poly":[[631,403],[647,403],[647,419],[631,419]],"score":0.7115},{"poly":[[507,378],[556,356],[573,392],[523,414]],"score":0.6277},{"poly":[[600,342],[631,343],[627,427],[595,425]],"score":0.6493},{"poly":[[638,342],[652,356],[638,370],[624,356]],"score":0.7707},{"poly":[[218,293],[246,293],[246,314],[218,314]],"score":0.8392},{"poly":[[622,291],[649,291],[649,312],[622,312]],"score":0.8249},{"poly":[[316,358],[540,260],[569,325],[345,423]],"score":0.6437},{"poly":[[940,271],[972,266],[976,287],[944,293]],"score":0.7401},{"poly":[[537,261],[571,261],[571,281],[537,281]],"score":0.8172},{"poly":[[220,240],[246,240],[246,261],[220,261]],"score":0.8605},{"poly":[[622,239],[649,239],[649,260],[622,260]],"score":0.8439},{"poly":[[374,221],[459,221],[459,240],[374,240]],"score":0.928},{"poly":[[778,218],[858,218],[858,242],[778,242]],"score":0.8661},{"poly":[[773,177],[962,177],[962,200],[773,200]],"score":0.8262},{"poly":[[610,179],[711,179],[711,196],[610,196]],"score":0.9734},{"poly":[[420,179],[542,179],[542,196],[420,196]],"score":0.897},{"poly":[[252,179],[353,179],[353,196],[252,196]],"score":0.8443}],"page_no":6,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":607,"y0":1552,"y1":1579},"conf":0.6237,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1552,"y1":1573},"font_size":0.0,"text":"8"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":1057,"y0":169,"y1":310},"conf":0.9704,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1052,"y0":172,"y1":195},"font_size":0.0,"text":"(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: $56\\%$"},{"bbox":{"x0":138,"x1":1049,"y0":196,"y1":223},"font_size":0.0,"text":"is from the DeepSeekMath Corpus, $4\\%$ from AlgebraicStack, $10\\%$ from arXiv, $20\\%$ is Github"},{"bbox":{"x0":138,"x1":1049,"y0":226,"y1":251},"font_size":0.0,"text":"code, and the remaining $10\\%$ is natural language data from Common Crawl in both English and"},{"bbox":{"x0":136,"x1":1053,"y0":249,"y1":281},"font_size":0.0,"text":"Chinese. We mainly adopt the training setting specifed in Section 2.2.1, except that we set thei"},{"bbox":{"x0":138,"x1":916,"y0":279,"y1":302},"font_size":0.0,"text":"maximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens."}],"source":"layout det","text":"(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: $56\\%$ is from the DeepSeekMath Corpus, $4\\%$ from AlgebraicStack, $10\\%$ from arXiv, $20\\%$ is Github code, and the remaining $10\\%$ is natural language data from Common Crawl in both English and Chinese. We mainly adopt the training setting specifed in Section 2.2.1, except that we set thei maximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens."},{"bbox":{"x0":134,"x1":1057,"y0":318,"y1":461},"conf":0.9717,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":321,"y1":344},"font_size":0.0,"text":"We conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMath-"},{"bbox":{"x0":138,"x1":1049,"y0":346,"y1":374},"font_size":0.0,"text":"Base 7B, focusing on its ability to produce self-contained mathematical solutions without relying"},{"bbox":{"x0":135,"x1":1053,"y0":370,"y1":403},"font_size":0.0,"text":"on external tools, solve mathematical problems using tools, and conduct formal theorem proving."},{"bbox":{"x0":140,"x1":1049,"y0":403,"y1":428},"font_size":0.0,"text":"Beyond mathematics, we also provide a more general profle of the base model, including itsi"},{"bbox":{"x0":140,"x1":959,"y0":430,"y1":454},"font_size":0.0,"text":"performance of natural language understanding, reasoning, and programming skills."}],"source":"layout det","text":"We conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMathBase 7B, focusing on its ability to produce self-contained mathematical solutions without relying on external tools, solve mathematical problems using tools, and conduct formal theorem proving.Beyond mathematics, we also provide a more general profle of the base model, including itsi performance of natural language understanding, reasoning, and programming skills."},{"bbox":{"x0":134,"x1":1061,"y0":497,"y1":694},"conf":0.974,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":500,"y1":524},"font_size":0.0,"text":"Mathematical Problem Solving with Step-by-Step ReasoningWe evaluate DeepSeekMath-"},{"bbox":{"x0":140,"x1":1051,"y0":528,"y1":553},"font_size":0.0,"text":"Baseâ€™s performance of solving mathematical problems using few-shot chain-of-thought prompt-"},{"bbox":{"x0":138,"x1":1051,"y0":554,"y1":579},"font_size":0.0,"text":"ing (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encom-"},{"bbox":{"x0":135,"x1":1053,"y0":579,"y1":610},"font_size":0.0,"text":"pass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),"},{"bbox":{"x0":138,"x1":1051,"y0":609,"y1":633},"font_size":0.0,"text":"and CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks"},{"bbox":{"x0":138,"x1":1049,"y0":633,"y1":662},"font_size":0.0,"text":"et al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse felds of mathematicsi"},{"bbox":{"x0":138,"x1":569,"y0":661,"y1":688},"font_size":0.0,"text":"from elementary to college-level complexity."}],"source":"layout det","text":"Mathematical Problem Solving with Step-by-Step ReasoningWe evaluate DeepSeekMathBaseâ€™s performance of solving mathematical problems using few-shot chain-of-thought prompting (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encompass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),and CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks et al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse felds of mathematicsi from elementary to college-level complexity."},{"bbox":{"x0":134,"x1":1060,"y0":701,"y1":921},"conf":0.9635,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1051,"y0":703,"y1":728},"font_size":0.0,"text":"As shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight bench-"},{"bbox":{"x0":138,"x1":1047,"y0":731,"y1":756},"font_size":0.0,"text":"marks among the open-source base models (including the widely-used general model Mistral"},{"bbox":{"x0":140,"x1":1051,"y0":758,"y1":782},"font_size":0.0,"text":"7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which"},{"bbox":{"x0":138,"x1":1051,"y0":786,"y1":809},"font_size":0.0,"text":"underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition-"},{"bbox":{"x0":138,"x1":1051,"y0":812,"y1":837},"font_size":0.0,"text":"level MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over"},{"bbox":{"x0":138,"x1":1049,"y0":838,"y1":863},"font_size":0.0,"text":"$10\\%$ absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base"},{"bbox":{"x0":136,"x1":1049,"y0":863,"y1":889},"font_size":0.0,"text":"model 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained"},{"bbox":{"x0":136,"x1":356,"y0":893,"y1":916},"font_size":0.0,"text":"on mathematical texts."}],"source":"layout det","text":"As shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight benchmarks among the open-source base models (including the widely-used general model Mistral 7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competitionlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over $10\\%$ absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base model 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained on mathematical texts."},{"bbox":{"x0":138,"x1":1052,"y0":933,"y1":1327},"conf":0.9775,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":480,"x1":670,"y0":939,"y1":970},"font_size":0.0,"text":"English Benchmarks"},{"bbox":{"x0":804,"x1":996,"y0":942,"y1":967},"font_size":0.0,"text":"Chinese Benchmarks"},{"bbox":{"x0":147,"x1":215,"y0":967,"y1":993},"font_size":0.0,"text":"Model"},{"bbox":{"x0":341,"x1":388,"y0":966,"y1":994},"font_size":0.0,"text":"Size"},{"bbox":{"x0":618,"x1":669,"y0":985,"y1":1015},"font_size":0.0,"text":"SAT"},{"bbox":{"x0":680,"x1":753,"y0":977,"y1":1003},"font_size":0.0,"text":"MMLU"},{"bbox":{"x0":756,"x1":842,"y0":985,"y1":1016},"font_size":0.0,"text":"CMATH"},{"bbox":{"x0":860,"x1":937,"y0":977,"y1":1003},"font_size":0.0,"text":"Gaokao"},{"bbox":{"x0":957,"x1":1038,"y0":975,"y1":1004},"font_size":0.0,"text":"Gaokao"},{"bbox":{"x0":398,"x1":555,"y0":989,"y1":1014},"font_size":0.0,"text":"GSM8K MATH "},{"bbox":{"x0":549,"x1":612,"y0":989,"y1":1012},"font_size":0.0,"text":"OCW"},{"bbox":{"x0":684,"x1":751,"y0":1000,"y1":1029},"font_size":0.0,"text":"STEM"},{"bbox":{"x0":842,"x1":1042,"y0":1000,"y1":1030},"font_size":0.0,"text":"MathClozeMathQA"},{"bbox":{"x0":472,"x1":716,"y0":1036,"y1":1063},"font_size":0.0,"text":"Closed-Source Base Model"},{"bbox":{"x0":146,"x1":233,"y0":1069,"y1":1100},"font_size":0.0,"text":"Minerva"},{"bbox":{"x0":341,"x1":374,"y0":1073,"y1":1098},"font_size":0.0,"text":"7B"},{"bbox":{"x0":405,"x1":464,"y0":1074,"y1":1096},"font_size":0.0,"text":" $16.2\\%$ "},{"bbox":{"x0":484,"x1":541,"y0":1074,"y1":1096},"font_size":0.0,"text":" $14.1\\%$ "},{"bbox":{"x0":557,"x1":605,"y0":1075,"y1":1095},"font_size":0.0,"text":" $7.7\\%$ "},{"bbox":{"x0":639,"x1":652,"y0":1079,"y1":1093},"font_size":0.0,"text":"-"},{"bbox":{"x0":686,"x1":747,"y0":1074,"y1":1096},"font_size":0.0,"text":" $35.6\\%$ "},{"bbox":{"x0":796,"x1":803,"y0":1078,"y1":1098},"font_size":10.0,"text":"-"},{"bbox":{"x0":895,"x1":902,"y0":1078,"y1":1098},"font_size":10.0,"text":"-"},{"bbox":{"x0":995,"x1":1002,"y0":1078,"y1":1098},"font_size":10.0,"text":"-"},{"bbox":{"x0":146,"x1":233,"y0":1095,"y1":1124},"font_size":0.0,"text":"Minerva"},{"bbox":{"x0":342,"x1":384,"y0":1097,"y1":1122},"font_size":0.0,"text":"62B"},{"bbox":{"x0":405,"x1":465,"y0":1099,"y1":1121},"font_size":0.0,"text":" $52.4\\%$ "},{"bbox":{"x0":483,"x1":541,"y0":1100,"y1":1121},"font_size":0.0,"text":" $27.6\\%$ "},{"bbox":{"x0":553,"x1":610,"y0":1100,"y1":1121},"font_size":0.0,"text":" $12.0\\%$ "},{"bbox":{"x0":642,"x1":649,"y0":1103,"y1":1123},"font_size":10.0,"text":"-"},{"bbox":{"x0":686,"x1":747,"y0":1099,"y1":1121},"font_size":0.0,"text":" $53.9\\%$ "},{"bbox":{"x0":796,"x1":803,"y0":1103,"y1":1123},"font_size":10.0,"text":"-"},{"bbox":{"x0":895,"x1":902,"y0":1103,"y1":1123},"font_size":10.0,"text":"-"},{"bbox":{"x0":995,"x1":1002,"y0":1103,"y1":1123},"font_size":10.0,"text":"-"},{"bbox":{"x0":146,"x1":233,"y0":1120,"y1":1150},"font_size":0.0,"text":"Minerva"},{"bbox":{"x0":342,"x1":395,"y0":1122,"y1":1148},"font_size":0.0,"text":"540B"},{"bbox":{"x0":404,"x1":466,"y0":1124,"y1":1147},"font_size":0.0,"text":" $58.8\\%$ "},{"bbox":{"x0":482,"x1":542,"y0":1124,"y1":1146},"font_size":0.0,"text":" $33.6\\%$ "},{"bbox":{"x0":553,"x1":611,"y0":1124,"y1":1146},"font_size":0.0,"text":" $17.6\\%$ "},{"bbox":{"x0":640,"x1":650,"y0":1133,"y1":1141},"font_size":0.0,"text":"-"},{"bbox":{"x0":686,"x1":748,"y0":1124,"y1":1147},"font_size":0.0,"text":" $63.9\\%$ "},{"bbox":{"x0":796,"x1":803,"y0":1128,"y1":1149},"font_size":10.0,"text":"-"},{"bbox":{"x0":895,"x1":902,"y0":1128,"y1":1149},"font_size":10.0,"text":"-"},{"bbox":{"x0":995,"x1":1002,"y0":1128,"y1":1149},"font_size":10.0,"text":"-"},{"bbox":{"x0":477,"x1":713,"y0":1155,"y1":1189},"font_size":0.0,"text":"Open-Source Base Model"},{"bbox":{"x0":145,"x1":222,"y0":1192,"y1":1220},"font_size":0.0,"text":"Mistral"},{"bbox":{"x0":340,"x1":376,"y0":1192,"y1":1220},"font_size":0.0,"text":"7B"},{"bbox":{"x0":403,"x1":466,"y0":1193,"y1":1218},"font_size":0.0,"text":" $40.3\\%$ "},{"bbox":{"x0":483,"x1":542,"y0":1194,"y1":1218},"font_size":0.0,"text":" $14.3\\%$ "},{"bbox":{"x0":556,"x1":606,"y0":1194,"y1":1217},"font_size":0.0,"text":" $9.2\\%$ "},{"bbox":{"x0":616,"x1":675,"y0":1194,"y1":1218},"font_size":0.0,"text":" $71.9\\%$ "},{"bbox":{"x0":686,"x1":747,"y0":1194,"y1":1218},"font_size":0.0,"text":" $51.1\\%$ "},{"bbox":{"x0":769,"x1":830,"y0":1194,"y1":1218},"font_size":0.0,"text":" $44.9\\%$ "},{"bbox":{"x0":870,"x1":926,"y0":1193,"y1":1218},"font_size":0.0,"text":" $5.1\\%$ "},{"bbox":{"x0":966,"x1":1030,"y0":1193,"y1":1218},"font_size":0.0,"text":" $23.4\\%$ "},{"bbox":{"x0":145,"x1":231,"y0":1226,"y1":1254},"font_size":0.0,"text":"Llemma"},{"bbox":{"x0":341,"x1":374,"y0":1227,"y1":1253},"font_size":0.0,"text":"7B"},{"bbox":{"x0":404,"x1":466,"y0":1229,"y1":1252},"font_size":0.0,"text":" $37.4\\%$ "},{"bbox":{"x0":483,"x1":542,"y0":1229,"y1":1252},"font_size":0.0,"text":" $18.1\\%$ "},{"bbox":{"x0":556,"x1":606,"y0":1230,"y1":1252},"font_size":0.0,"text":" $6.3\\%$ "},{"bbox":{"x0":616,"x1":675,"y0":1230,"y1":1252},"font_size":0.0,"text":" $59.4\\%$ "},{"bbox":{"x0":685,"x1":745,"y0":1229,"y1":1252},"font_size":0.0,"text":" $43.1\\%$ "},{"bbox":{"x0":769,"x1":830,"y0":1229,"y1":1252},"font_size":0.0,"text":" $43.4\\%$ "},{"bbox":{"x0":867,"x1":930,"y0":1229,"y1":1252},"font_size":0.0,"text":" $11.9\\%$ "},{"bbox":{"x0":966,"x1":1030,"y0":1229,"y1":1252},"font_size":0.0,"text":" $23.6\\%$ "},{"bbox":{"x0":147,"x1":231,"y0":1253,"y1":1279},"font_size":0.0,"text":"Llemma"},{"bbox":{"x0":341,"x1":384,"y0":1252,"y1":1280},"font_size":0.0,"text":"34B"},{"bbox":{"x0":404,"x1":466,"y0":1254,"y1":1278},"font_size":0.0,"text":" $54.0\\%$ "},{"bbox":{"x0":482,"x1":542,"y0":1255,"y1":1278},"font_size":0.0,"text":" $25.3\\%$ "},{"bbox":{"x0":553,"x1":609,"y0":1255,"y1":1278},"font_size":0.0,"text":" $10.3\\%$ "},{"bbox":{"x0":617,"x1":675,"y0":1255,"y1":1278},"font_size":0.0,"text":" $71.9\\%$ "},{"bbox":{"x0":686,"x1":747,"y0":1255,"y1":1278},"font_size":0.0,"text":" $52.9\\%$ "},{"bbox":{"x0":768,"x1":831,"y0":1255,"y1":1278},"font_size":0.0,"text":" $56.1\\%$ "},{"bbox":{"x0":867,"x1":930,"y0":1254,"y1":1278},"font_size":0.0,"text":" $11.9\\%$ "},{"bbox":{"x0":966,"x1":1031,"y0":1254,"y1":1278},"font_size":0.0,"text":" $26.2\\%$ "},{"bbox":{"x0":148,"x1":376,"y0":1286,"y1":1317},"font_size":0.0,"text":"DeepSeekMath-Base 7B"},{"bbox":{"x0":404,"x1":466,"y0":1289,"y1":1313},"font_size":0.0,"text":" $64.2\\%$ "},{"bbox":{"x0":482,"x1":542,"y0":1289,"y1":1313},"font_size":0.0,"text":" $\\mathbf{36.2\\%}$ "},{"bbox":{"x0":552,"x1":610,"y0":1289,"y1":1313},"font_size":0.0,"text":" $\\mathbf{15.4\\%}$ "},{"bbox":{"x0":617,"x1":675,"y0":1289,"y1":1313},"font_size":0.0,"text":" $\\textbf{84.4\\%}$ "},{"bbox":{"x0":686,"x1":747,"y0":1289,"y1":1313},"font_size":0.0,"text":" $\\mathbf{56.5\\%}$ "},{"bbox":{"x0":768,"x1":831,"y0":1289,"y1":1313},"font_size":0.0,"text":" $\\textbf{71.7\\%}$ "},{"bbox":{"x0":866,"x1":930,"y0":1289,"y1":1313},"font_size":0.0,"text":" $\\mathbf{20.3\\%}$ "},{"bbox":{"x0":966,"x1":1030,"y0":1288,"y1":1313},"font_size":0.0,"text":" $\\mathbf{35.3\\%}$ "}],"source":"layout det","text":"<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"4\">English Benchmarks</td><td colspan=\"3\">Chinese Benchmarks</td></tr><tr><td>GSM8K MATH </td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathClozeMathQA</td><td>Gaokao</td></tr></thead><tbody><tr><td colspan=\"10\">Closed-Source Base Model</td></tr><tr><td>Minerva</td><td>7B</td><td> $16.2\\%$ </td><td> $14.1\\%$ </td><td> $7.7\\%$ </td><td>-  $35.6\\%$ </td><td>-</td><td></td><td>-</td><td>-</td></tr><tr><td>Minerva</td><td>62B</td><td> $52.4\\%$ </td><td> $27.6\\%$ </td><td> $12.0\\%$ </td><td>-</td><td> $53.9\\%$ </td><td>-</td><td>-</td><td>-</td></tr><tr><td>Minerva</td><td>540B</td><td> $58.8\\%$ </td><td> $33.6\\%$ </td><td> $17.6\\%$ </td><td>-</td><td> $63.9\\%$ </td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan=\"10\">Open-Source Base Model</td></tr><tr><td>Mistral</td><td>7B</td><td> $40.3\\%$ </td><td> $14.3\\%$ </td><td> $9.2\\%$ </td><td> $71.9\\%$ </td><td> $51.1\\%$ </td><td> $44.9\\%$ </td><td> $5.1\\%$ </td><td> $23.4\\%$ </td></tr><tr><td>Llemma</td><td>7B</td><td> $37.4\\%$ </td><td> $18.1\\%$ </td><td> $6.3\\%$ </td><td> $59.4\\%$ </td><td> $43.1\\%$ </td><td> $43.4\\%$ </td><td> $11.9\\%$ </td><td> $23.6\\%$ </td></tr><tr><td>Llemma</td><td>34B</td><td> $54.0\\%$ </td><td> $25.3\\%$ </td><td> $10.3\\%$ </td><td> $71.9\\%$ </td><td> $52.9\\%$ </td><td> $56.1\\%$ </td><td> $11.9\\%$ </td><td> $26.2\\%$ </td></tr><tr><td>DeepSeekMath-Base 7B</td><td></td><td> $64.2\\%$ </td><td> $\\mathbf{36.2\\%}$ </td><td> $\\mathbf{15.4\\%}$ </td><td> $\\textbf{84.4\\%}$ </td><td> $\\mathbf{56.5\\%}$ </td><td> $\\textbf{71.7\\%}$ </td><td> $\\mathbf{20.3\\%}$ </td><td> $\\mathbf{35.3\\%}$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":134,"x1":1056,"y0":1338,"y1":1424},"conf":0.6635,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1340,"y1":1365},"font_size":0.0,"text":"Table 2 | Comparisons between DeepSeekMath-Base 7B and strong base models on English and"},{"bbox":{"x0":136,"x1":1053,"y0":1363,"y1":1396},"font_size":0.0,"text":"Chinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting."},{"bbox":{"x0":138,"x1":707,"y0":1395,"y1":1419},"font_size":0.0,"text":"Minerva results are quoted from Lewkowycz et al. (2022a)."}],"source":"layout det","text":"Table 2 | Comparisons between DeepSeekMath-Base 7B and strong base models on English and Chinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting.Minerva results are quoted from Lewkowycz et al. (2022a)."}],"formula_dets":[{"bbox":{"x0":966,"x1":1031,"y0":1254,"y1":1278},"conf":0.8411,"label":"print_embedding","label_id":0},{"bbox":{"x0":966,"x1":1030,"y0":1229,"y1":1252},"conf":0.8362,"label":"print_embedding","label_id":0},{"bbox":{"x0":966,"x1":1030,"y0":1288,"y1":1313},"conf":0.8339,"label":"print_embedding","label_id":0},{"bbox":{"x0":907,"x1":952,"y0":199,"y1":222},"conf":0.8325,"label":"print_embedding","label_id":0},{"bbox":{"x0":768,"x1":831,"y0":1255,"y1":1278},"conf":0.8288,"label":"print_embedding","label_id":0},{"bbox":{"x0":374,"x1":416,"y0":227,"y1":249},"conf":0.8259,"label":"print_embedding","label_id":0},{"bbox":{"x0":866,"x1":930,"y0":1289,"y1":1313},"conf":0.8245,"label":"print_embedding","label_id":0},{"bbox":{"x0":493,"x1":526,"y0":200,"y1":222},"conf":0.8143,"label":"print_embedding","label_id":0},{"bbox":{"x0":768,"x1":831,"y0":1289,"y1":1313},"conf":0.8134,"label":"print_embedding","label_id":0},{"bbox":{"x0":769,"x1":830,"y0":1229,"y1":1252},"conf":0.8132,"label":"print_embedding","label_id":0},{"bbox":{"x0":743,"x1":787,"y0":199,"y1":222},"conf":0.813,"label":"print_embedding","label_id":0},{"bbox":{"x0":867,"x1":930,"y0":1254,"y1":1278},"conf":0.8078,"label":"print_embedding","label_id":0},{"bbox":{"x0":966,"x1":1030,"y0":1193,"y1":1218},"conf":0.8036,"label":"print_embedding","label_id":0},{"bbox":{"x0":867,"x1":930,"y0":1229,"y1":1252},"conf":0.7839,"label":"print_embedding","label_id":0},{"bbox":{"x0":138,"x1":182,"y0":840,"y1":862},"conf":0.7806,"label":"print_embedding","label_id":0},{"bbox":{"x0":404,"x1":466,"y0":1254,"y1":1278},"conf":0.7799,"label":"print_embedding","label_id":0},{"bbox":{"x0":769,"x1":830,"y0":1194,"y1":1218},"conf":0.77,"label":"print_embedding","label_id":0},{"bbox":{"x0":404,"x1":466,"y0":1229,"y1":1252},"conf":0.7699,"label":"print_embedding","label_id":0},{"bbox":{"x0":404,"x1":466,"y0":1124,"y1":1147},"conf":0.7603,"label":"print_embedding","label_id":0},{"bbox":{"x0":404,"x1":466,"y0":1289,"y1":1313},"conf":0.7575,"label":"print_embedding","label_id":0},{"bbox":{"x0":870,"x1":926,"y0":1193,"y1":1218},"conf":0.755,"label":"print_embedding","label_id":0},{"bbox":{"x0":686,"x1":747,"y0":1255,"y1":1278},"conf":0.753,"label":"print_embedding","label_id":0},{"bbox":{"x0":403,"x1":466,"y0":1193,"y1":1218},"conf":0.7525,"label":"print_embedding","label_id":0},{"bbox":{"x0":686,"x1":747,"y0":1289,"y1":1313},"conf":0.7349,"label":"print_embedding","label_id":0},{"bbox":{"x0":686,"x1":747,"y0":1074,"y1":1096},"conf":0.7325,"label":"print_embedding","label_id":0},{"bbox":{"x0":686,"x1":748,"y0":1124,"y1":1147},"conf":0.7301,"label":"print_embedding","label_id":0},{"bbox":{"x0":405,"x1":465,"y0":1099,"y1":1121},"conf":0.7292,"label":"print_embedding","label_id":0},{"bbox":{"x0":483,"x1":542,"y0":1194,"y1":1218},"conf":0.7243,"label":"print_embedding","label_id":0},{"bbox":{"x0":1006,"x1":1052,"y0":172,"y1":195},"conf":0.7242,"label":"print_embedding","label_id":0},{"bbox":{"x0":686,"x1":747,"y0":1099,"y1":1121},"conf":0.7232,"label":"print_embedding","label_id":0},{"bbox":{"x0":482,"x1":542,"y0":1124,"y1":1146},"conf":0.7211,"label":"print_embedding","label_id":0},{"bbox":{"x0":482,"x1":542,"y0":1255,"y1":1278},"conf":0.7179,"label":"print_embedding","label_id":0},{"bbox":{"x0":685,"x1":745,"y0":1229,"y1":1252},"conf":0.7078,"label":"print_embedding","label_id":0},{"bbox":{"x0":557,"x1":605,"y0":1075,"y1":1095},"conf":0.6999,"label":"print_embedding","label_id":0},{"bbox":{"x0":616,"x1":675,"y0":1194,"y1":1218},"conf":0.6997,"label":"print_embedding","label_id":0},{"bbox":{"x0":556,"x1":606,"y0":1230,"y1":1252},"conf":0.6965,"label":"print_embedding","label_id":0},{"bbox":{"x0":405,"x1":464,"y0":1074,"y1":1096},"conf":0.6965,"label":"print_embedding","label_id":0},{"bbox":{"x0":556,"x1":606,"y0":1194,"y1":1217},"conf":0.6952,"label":"print_embedding","label_id":0},{"bbox":{"x0":483,"x1":541,"y0":1100,"y1":1121},"conf":0.695,"label":"print_embedding","label_id":0},{"bbox":{"x0":482,"x1":542,"y0":1289,"y1":1313},"conf":0.6898,"label":"print_embedding","label_id":0},{"bbox":{"x0":553,"x1":610,"y0":1100,"y1":1121},"conf":0.6858,"label":"print_embedding","label_id":0},{"bbox":{"x0":617,"x1":675,"y0":1255,"y1":1278},"conf":0.6856,"label":"print_embedding","label_id":0},{"bbox":{"x0":616,"x1":675,"y0":1230,"y1":1252},"conf":0.6818,"label":"print_embedding","label_id":0},{"bbox":{"x0":553,"x1":611,"y0":1124,"y1":1146},"conf":0.6785,"label":"print_embedding","label_id":0},{"bbox":{"x0":686,"x1":747,"y0":1194,"y1":1218},"conf":0.6764,"label":"print_embedding","label_id":0},{"bbox":{"x0":483,"x1":542,"y0":1229,"y1":1252},"conf":0.6705,"label":"print_embedding","label_id":0},{"bbox":{"x0":553,"x1":609,"y0":1255,"y1":1278},"conf":0.6567,"label":"print_embedding","label_id":0},{"bbox":{"x0":484,"x1":541,"y0":1074,"y1":1096},"conf":0.6524,"label":"print_embedding","label_id":0},{"bbox":{"x0":617,"x1":675,"y0":1289,"y1":1313},"conf":0.5947,"label":"print_embedding","label_id":0},{"bbox":{"x0":552,"x1":610,"y0":1289,"y1":1313},"conf":0.5525,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":138,"x1":1052,"y0":933,"y1":1327},"conf":0.9775,"label":"Table","label_id":5},{"bbox":{"x0":134,"x1":1061,"y0":497,"y1":694},"conf":0.974,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1057,"y0":318,"y1":461},"conf":0.9717,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":169,"y1":310},"conf":0.9704,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":701,"y1":921},"conf":0.9635,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1056,"y0":1338,"y1":1424},"conf":0.6635,"label":"Table footnote","label_id":7},{"bbox":{"x0":582,"x1":607,"y0":1552,"y1":1579},"conf":0.6237,"label":"Abandon","label_id":2},{"bbox":{"x0":134,"x1":1056,"y0":1338,"y1":1424},"conf":0.3604,"label":"Text","label_id":1},{"bbox":{"x0":584,"x1":605,"y0":1553,"y1":1577},"conf":0.2692,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1552],[603,1552],[603,1573],[587,1573]],"score":0.8413},{"poly":[[138,1395],[707,1395],[707,1419],[138,1419]],"score":0.7736},{"poly":[[137,1363],[1053,1367],[1053,1396],[136,1393]],"score":0.7424},{"poly":[[140,1340],[1049,1340],[1049,1365],[140,1365]],"score":0.747},{"poly":[[477,1289],[748,1289],[748,1312],[477,1312]],"score":0.8895},{"poly":[[406,1291],[486,1291],[486,1310],[406,1310]],"score":0.8691},{"poly":[[149,1289],[374,1289],[374,1312],[149,1312]],"score":0.9298},{"poly":[[966,1288],[1030,1288],[1030,1314],[966,1314]],"score":0.7856},{"poly":[[867,1288],[929,1288],[929,1314],[867,1314]],"score":0.8554},{"poly":[[766,1288],[829,1288],[829,1312],[766,1312]],"score":0.8374},{"poly":[[147,1254],[229,1254],[229,1279],[147,1279]],"score":0.9007},{"poly":[[968,1252],[1030,1252],[1030,1279],[968,1279]],"score":0.9232},{"poly":[[868,1252],[927,1252],[927,1279],[868,1279]],"score":0.9799},{"poly":[[767,1252],[829,1252],[829,1279],[767,1279]],"score":0.914},{"poly":[[673,1252],[746,1252],[746,1279],[673,1279]],"score":0.7345},{"poly":[[482,1252],[681,1252],[681,1277],[482,1277]],"score":0.8242},{"poly":[[402,1252],[468,1252],[468,1279],[402,1279]],"score":0.9013},{"poly":[[340,1251],[392,1251],[392,1279],[340,1279]],"score":0.7771},{"poly":[[867,1228],[929,1228],[929,1254],[867,1254]],"score":0.8534},{"poly":[[767,1228],[829,1228],[829,1254],[767,1254]],"score":0.8566},{"poly":[[551,1228],[744,1228],[744,1252],[551,1252]],"score":0.8499},{"poly":[[480,1228],[548,1228],[548,1252],[480,1252]],"score":0.9228},{"poly":[[404,1228],[468,1228],[468,1254],[404,1254]],"score":0.8885},{"poly":[[146,1224],[229,1228],[228,1255],[145,1250]],"score":0.8218},{"poly":[[966,1224],[1031,1224],[1031,1256],[966,1256]],"score":0.8043},{"poly":[[340,1224],[383,1224],[383,1254],[340,1254]],"score":0.7468},{"poly":[[968,1193],[1030,1193],[1030,1219],[968,1219]],"score":0.8839},{"poly":[[872,1193],[925,1193],[925,1221],[872,1221]],"score":0.8461},{"poly":[[767,1193],[829,1193],[829,1219],[767,1219]],"score":0.8294},{"poly":[[477,1191],[744,1191],[744,1216],[477,1216]],"score":0.7194},{"poly":[[404,1193],[470,1193],[470,1219],[404,1219]],"score":0.8145},{"poly":[[145,1193],[222,1193],[222,1217],[145,1217]],"score":0.9294},{"poly":[[340,1191],[378,1191],[378,1219],[340,1219]],"score":0.8799},{"poly":[[477,1156],[712,1156],[712,1186],[477,1186]],"score":0.7689},{"poly":[[480,1123],[611,1123],[611,1147],[480,1147]],"score":0.8963},{"poly":[[399,1123],[470,1123],[470,1147],[399,1147]],"score":0.8398},{"poly":[[146,1119],[233,1123],[231,1149],[145,1145]],"score":0.783},{"poly":[[686,1121],[746,1121],[746,1147],[686,1147]],"score":0.9607},{"poly":[[342,1121],[409,1121],[409,1147],[342,1147]],"score":0.782},{"poly":[[686,1096],[746,1096],[746,1123],[686,1123]],"score":0.8881},{"poly":[[480,1096],[611,1096],[611,1121],[480,1121]],"score":0.9128},{"poly":[[404,1096],[468,1096],[468,1121],[404,1121]],"score":0.9319},{"poly":[[340,1096],[397,1096],[397,1123],[340,1123]],"score":0.7533},{"poly":[[147,1094],[233,1100],[231,1127],[145,1120]],"score":0.7811},{"poly":[[146,1068],[233,1072],[231,1099],[145,1094]],"score":0.7943},{"poly":[[684,1068],[748,1068],[748,1100],[684,1100]],"score":0.8238},{"poly":[[477,1064],[610,1069],[609,1100],[476,1096]],"score":0.7416},{"poly":[[404,1070],[470,1070],[470,1096],[404,1096]],"score":0.8843},{"poly":[[340,1070],[378,1070],[378,1098],[340,1098]],"score":0.9007},{"poly":[[473,1038],[718,1038],[718,1061],[473,1061]],"score":0.9809},{"poly":[[844,1000],[1040,1002],[1040,1026],[844,1024]],"score":0.8285},{"poly":[[684,1002],[750,1002],[750,1028],[684,1028]],"score":0.9303},{"poly":[[758,985],[842,990],[841,1016],[756,1012]],"score":0.7734},{"poly":[[399,989],[666,989],[666,1014],[399,1014]],"score":0.7612},{"poly":[[957,977],[1037,977],[1037,1002],[957,1002]],"score":0.832},{"poly":[[860,977],[939,977],[939,1002],[860,1002]],"score":0.8525},{"poly":[[679,977],[751,977],[751,1002],[679,1002]],"score":0.954},{"poly":[[340,967],[388,967],[388,993],[340,993]],"score":0.8653},{"poly":[[145,967],[214,967],[214,993],[145,993]],"score":0.8972},{"poly":[[482,944],[668,944],[668,967],[482,967]],"score":0.9887},{"poly":[[801,940],[996,944],[996,968],[801,965]],"score":0.7933},{"poly":[[136,893],[356,893],[356,916],[136,916]],"score":0.8183},{"poly":[[136,865],[1049,863],[1049,888],[136,889]],"score":0.7765},{"poly":[[138,838],[1049,838],[1049,863],[138,863]],"score":0.7369},{"poly":[[138,812],[1051,812],[1051,837],[138,837]],"score":0.7518},{"poly":[[138,786],[1051,786],[1051,809],[138,809]],"score":0.8427},{"poly":[[140,758],[1051,758],[1051,782],[140,782]],"score":0.7205},{"poly":[[138,731],[1047,731],[1047,756],[138,756]],"score":0.7517},{"poly":[[175,703],[1051,703],[1051,728],[175,728]],"score":0.7201},{"poly":[[138,661],[569,663],[569,688],[138,686]],"score":0.8058},{"poly":[[138,633],[1049,635],[1049,660],[138,658]],"score":0.8114},{"poly":[[138,609],[1051,609],[1051,633],[138,633]],"score":0.7716},{"poly":[[135,581],[1053,579],[1053,609],[135,610]],"score":0.7266},{"poly":[[138,554],[1051,554],[1051,579],[138,579]],"score":0.7431},{"poly":[[140,528],[1051,528],[1051,553],[140,553]],"score":0.7499},{"poly":[[140,500],[1051,500],[1051,524],[140,524]],"score":0.7714},{"poly":[[140,430],[959,430],[959,454],[140,454]],"score":0.7861},{"poly":[[140,403],[1049,403],[1049,428],[140,428]],"score":0.748},{"poly":[[135,370],[1053,374],[1053,403],[135,400]],"score":0.7015},{"poly":[[138,346],[1049,349],[1049,374],[138,370]],"score":0.7492},{"poly":[[174,321],[1051,321],[1051,344],[174,344]],"score":0.9146},{"poly":[[138,279],[916,279],[916,302],[138,302]],"score":0.7385},{"poly":[[137,249],[1053,251],[1053,281],[136,279]],"score":0.6811},{"poly":[[138,226],[1049,226],[1049,251],[138,251]],"score":0.7468},{"poly":[[138,196],[1049,198],[1049,223],[138,221]],"score":0.7691},{"poly":[[140,172],[1051,172],[1051,195],[140,195]],"score":0.9504}],"page_no":7,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.6365,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":583,"x1":604,"y0":1551,"y1":1575},"font_size":0.0,"text":"9"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":1060,"y0":165,"y1":340},"conf":0.9146,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":172,"y1":196},"font_size":0.0,"text":"Mathematical Problem Solving with Tool UseWe evaluate program-aided mathematical"},{"bbox":{"x0":138,"x1":1049,"y0":198,"y1":223},"font_size":0.0,"text":"reasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,"},{"bbox":{"x0":140,"x1":1051,"y0":226,"y1":251},"font_size":0.0,"text":"2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program"},{"bbox":{"x0":138,"x1":1051,"y0":254,"y1":277},"font_size":0.0,"text":"where libraries such as math and sympy can be utilized for intricate computations. The execution"},{"bbox":{"x0":135,"x1":1053,"y0":275,"y1":307},"font_size":0.0,"text":"result of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B"},{"bbox":{"x0":136,"x1":631,"y0":305,"y1":332},"font_size":0.0,"text":"outperforms the prior state-of-the-art Llemma 34B."}],"source":"layout det","text":"Mathematical Problem Solving with Tool UseWe evaluate program-aided mathematical reasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program where libraries such as math and sympy can be utilized for intricate computations. The execution result of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B outperforms the prior state-of-the-art Llemma 34B."},{"bbox":{"x0":180,"x1":1005,"y0":350,"y1":621},"conf":0.9776,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":463,"x1":703,"y0":360,"y1":388},"font_size":0.0,"text":"Problem Solving w/ Tools"},{"bbox":{"x0":737,"x1":991,"y0":358,"y1":390},"font_size":0.0,"text":"Informal-to-Formal Proving"},{"bbox":{"x0":191,"x1":257,"y0":372,"y1":400},"font_size":0.0,"text":"Model"},{"bbox":{"x0":382,"x1":430,"y0":370,"y1":401},"font_size":0.0,"text":"Size"},{"bbox":{"x0":433,"x1":591,"y0":394,"y1":423},"font_size":0.0,"text":"GSM8K+Python"},{"bbox":{"x0":579,"x1":743,"y0":396,"y1":424},"font_size":0.0,"text":" MATH+Python"},{"bbox":{"x0":729,"x1":991,"y0":397,"y1":422},"font_size":0.0,"text":" miniF2F-valid miniF2F-test"},{"bbox":{"x0":191,"x1":263,"y0":429,"y1":457},"font_size":0.0,"text":"Mistral"},{"bbox":{"x0":381,"x1":417,"y0":429,"y1":460},"font_size":0.0,"text":"7B"},{"bbox":{"x0":478,"x1":539,"y0":431,"y1":455},"font_size":0.0,"text":" $48.5\\%$ "},{"bbox":{"x0":632,"x1":692,"y0":431,"y1":455},"font_size":0.0,"text":" $18.2\\%$ "},{"bbox":{"x0":775,"x1":834,"y0":432,"y1":454},"font_size":0.0,"text":" $18.9\\%$ "},{"bbox":{"x0":904,"x1":963,"y0":432,"y1":454},"font_size":0.0,"text":" $18.0\\%$ "},{"bbox":{"x0":189,"x1":302,"y0":462,"y1":493},"font_size":0.0,"text":"CodeLlama"},{"bbox":{"x0":383,"x1":417,"y0":464,"y1":492},"font_size":0.0,"text":"7B"},{"bbox":{"x0":478,"x1":539,"y0":466,"y1":489},"font_size":0.0,"text":" $27.1\\%$ "},{"bbox":{"x0":631,"x1":692,"y0":466,"y1":489},"font_size":0.0,"text":" $17.2\\%$ "},{"bbox":{"x0":774,"x1":834,"y0":467,"y1":489},"font_size":0.0,"text":" $16.3\\%$ "},{"bbox":{"x0":904,"x1":963,"y0":467,"y1":489},"font_size":0.0,"text":" $17.6\\%$ "},{"bbox":{"x0":190,"x1":301,"y0":487,"y1":516},"font_size":0.0,"text":"CodeLlama"},{"bbox":{"x0":382,"x1":426,"y0":490,"y1":518},"font_size":0.0,"text":"34B"},{"bbox":{"x0":478,"x1":540,"y0":491,"y1":514},"font_size":0.0,"text":" $52.7\\%$ "},{"bbox":{"x0":631,"x1":692,"y0":491,"y1":514},"font_size":0.0,"text":" $23.5\\%$ "},{"bbox":{"x0":775,"x1":835,"y0":491,"y1":513},"font_size":0.0,"text":" $18.5\\%$ "},{"bbox":{"x0":905,"x1":963,"y0":492,"y1":513},"font_size":0.0,"text":" $18.0\\%$ "},{"bbox":{"x0":190,"x1":272,"y0":522,"y1":551},"font_size":0.0,"text":"Llemma"},{"bbox":{"x0":383,"x1":417,"y0":524,"y1":550},"font_size":0.0,"text":"7B"},{"bbox":{"x0":478,"x1":540,"y0":525,"y1":548},"font_size":0.0,"text":" $41.0\\%$ "},{"bbox":{"x0":631,"x1":693,"y0":525,"y1":548},"font_size":0.0,"text":" $18.6\\%$ "},{"bbox":{"x0":774,"x1":835,"y0":526,"y1":548},"font_size":0.0,"text":" $20.6\\%$ "},{"bbox":{"x0":903,"x1":964,"y0":526,"y1":548},"font_size":0.0,"text":" $22.1\\%$ "},{"bbox":{"x0":189,"x1":272,"y0":547,"y1":576},"font_size":0.0,"text":"Llemma"},{"bbox":{"x0":382,"x1":426,"y0":548,"y1":578},"font_size":0.0,"text":"34B"},{"bbox":{"x0":478,"x1":540,"y0":550,"y1":573},"font_size":0.0,"text":" $64.6\\%$ "},{"bbox":{"x0":631,"x1":693,"y0":550,"y1":573},"font_size":0.0,"text":" $26.3\\%$ "},{"bbox":{"x0":774,"x1":835,"y0":551,"y1":572},"font_size":0.0,"text":" $21.0\\%$ "},{"bbox":{"x0":904,"x1":964,"y0":551,"y1":572},"font_size":0.0,"text":" $21.3\\%$ "},{"bbox":{"x0":191,"x1":389,"y0":583,"y1":611},"font_size":0.0,"text":"DeepSeekMath-Base "},{"bbox":{"x0":378,"x1":414,"y0":584,"y1":609},"font_size":0.0,"text":"7B"},{"bbox":{"x0":478,"x1":540,"y0":584,"y1":608},"font_size":0.0,"text":" $66.9\\%$ "},{"bbox":{"x0":630,"x1":693,"y0":584,"y1":608},"font_size":0.0,"text":" $\\textbf{31.4\\%}$ "},{"bbox":{"x0":773,"x1":835,"y0":584,"y1":608},"font_size":0.0,"text":" $\\mathbf{25.8\\%}$ "},{"bbox":{"x0":903,"x1":963,"y0":585,"y1":608},"font_size":0.0,"text":" $\\mathbf{24.6\\%}$ "}],"source":"layout det","text":"<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"2\">Problem Solving w/ Tools</td><td colspan=\"2\">Informal-to-Formal Proving</td></tr><tr><td>GSM8K+Python</td><td> MATH+Python</td><td> miniF2F-valid miniF2F-test</td><td></td></tr><tr><td>Mistral</td><td>7B</td><td> $48.5\\%$ </td><td> $18.2\\%$ </td><td> $18.9\\%$ </td><td> $18.0\\%$ </td></tr><tr><td>CodeLlama</td><td>7B</td><td> $27.1\\%$ </td><td> $17.2\\%$ </td><td> $16.3\\%$ </td><td> $17.6\\%$ </td></tr><tr><td>CodeLlama</td><td>34B</td><td> $52.7\\%$ </td><td> $23.5\\%$ </td><td> $18.5\\%$ </td><td> $18.0\\%$ </td></tr><tr><td>Llemma</td><td>7B</td><td> $41.0\\%$ </td><td> $18.6\\%$ </td><td> $20.6\\%$ </td><td> $22.1\\%$ </td></tr><tr><td>Llemma</td><td>34B</td><td> $64.6\\%$ </td><td> $26.3\\%$ </td><td> $21.0\\%$ </td><td> $21.3\\%$ </td></tr><tr><td>DeepSeekMath-Base </td><td>7B</td><td> $66.9\\%$ </td><td> $\\textbf{31.4\\%}$ </td><td> $\\mathbf{25.8\\%}$ </td><td> $\\mathbf{24.6\\%}$ </td></tr></table></body></html>"},{"bbox":{"x0":131,"x1":1059,"y0":630,"y1":695},"conf":0.2102,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":635,"y1":660},"font_size":0.0,"text":"Table 3 | Few-shot evaluation of base modelsâ€™ ability to solve mathematical problems using tools"},{"bbox":{"x0":140,"x1":851,"y0":663,"y1":686},"font_size":0.0,"text":"and the ability to conduct informal-to-formal theorem proving in Isabelle."}],"source":"layout det","text":"Table 3 | Few-shot evaluation of base modelsâ€™ ability to solve mathematical problems using tools and the ability to conduct informal-to-formal theorem proving in Isabelle."},{"bbox":{"x0":131,"x1":1061,"y0":741,"y1":1024},"conf":0.9759,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":746,"y1":772},"font_size":0.0,"text":"Formal MathematicsFormal proof automation is benefcial to ensure the accuracy and relia-i"},{"bbox":{"x0":138,"x1":1053,"y0":774,"y1":799},"font_size":0.0,"text":"bility of mathematical proofs and enhance effciency, with increasing attention in recent years.i"},{"bbox":{"x0":136,"x1":1051,"y0":798,"y1":824},"font_size":0.0,"text":"We evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,"},{"bbox":{"x0":138,"x1":1051,"y0":824,"y1":851},"font_size":0.0,"text":"2022) which is to generate a formal proof based on an informal statement, a formal counterpart"},{"bbox":{"x0":138,"x1":1051,"y0":854,"y1":879},"font_size":0.0,"text":"of the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a bench-"},{"bbox":{"x0":136,"x1":1051,"y0":879,"y1":905},"font_size":0.0,"text":"mark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each"},{"bbox":{"x0":138,"x1":1049,"y0":909,"y1":931},"font_size":0.0,"text":"problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate"},{"bbox":{"x0":136,"x1":1051,"y0":935,"y1":961},"font_size":0.0,"text":"proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)"},{"bbox":{"x0":135,"x1":1053,"y0":960,"y1":991},"font_size":0.0,"text":"to fll in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strongi"},{"bbox":{"x0":138,"x1":533,"y0":991,"y1":1016},"font_size":0.0,"text":"performance in proof autoformalization."}],"source":"layout det","text":"Formal MathematicsFormal proof automation is benefcial to ensure the accuracy and relia-i bility of mathematical proofs and enhance effciency, with increasing attention in recent years.i We evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,2022) which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a benchmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)to fll in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strongi performance in proof autoformalization."},{"bbox":{"x0":184,"x1":999,"y0":1032,"y1":1213},"conf":0.979,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":193,"x1":262,"y0":1041,"y1":1070},"font_size":0.0,"text":"Model"},{"bbox":{"x0":453,"x1":500,"y0":1042,"y1":1069},"font_size":0.0,"text":"Size"},{"bbox":{"x0":497,"x1":577,"y0":1042,"y1":1069},"font_size":0.0,"text":" MMLU"},{"bbox":{"x0":582,"x1":637,"y0":1043,"y1":1068},"font_size":0.0,"text":"BBH"},{"bbox":{"x0":639,"x1":989,"y0":1042,"y1":1071},"font_size":0.0,"text":"HumanEval (Pass@1) MBPP (Pass@1)"},{"bbox":{"x0":193,"x1":269,"y0":1077,"y1":1105},"font_size":0.0,"text":"Mistral"},{"bbox":{"x0":451,"x1":487,"y0":1077,"y1":1106},"font_size":0.0,"text":"7B"},{"bbox":{"x0":509,"x1":567,"y0":1080,"y1":1102},"font_size":0.0,"text":" $\\mathbf{62.4\\%}$ "},{"bbox":{"x0":580,"x1":637,"y0":1080,"y1":1102},"font_size":0.0,"text":" $55.7\\%$ "},{"bbox":{"x0":712,"x1":771,"y0":1080,"y1":1102},"font_size":0.0,"text":" $28.0\\%$ "},{"bbox":{"x0":887,"x1":946,"y0":1081,"y1":1102},"font_size":0.0,"text":" $41.4\\%$ "},{"bbox":{"x0":195,"x1":451,"y0":1110,"y1":1143},"font_size":0.0,"text":"DeepSeek-Coder-Base-v1.5â€ "},{"bbox":{"x0":449,"x1":487,"y0":1113,"y1":1139},"font_size":0.0,"text":" 7B"},{"bbox":{"x0":509,"x1":566,"y0":1117,"y1":1137},"font_size":0.0,"text":" $42.9\\%$ "},{"bbox":{"x0":581,"x1":637,"y0":1117,"y1":1137},"font_size":0.0,"text":" $42.9\\%$ "},{"bbox":{"x0":712,"x1":771,"y0":1116,"y1":1137},"font_size":0.0,"text":" $40.2\\%$ "},{"bbox":{"x0":887,"x1":946,"y0":1117,"y1":1138},"font_size":0.0,"text":" $52.6\\%$ "},{"bbox":{"x0":194,"x1":445,"y0":1135,"y1":1165},"font_size":0.0,"text":"DeepSeek-Coder-Base-v1.5"},{"bbox":{"x0":449,"x1":487,"y0":1138,"y1":1164},"font_size":0.0,"text":"7B"},{"bbox":{"x0":509,"x1":566,"y0":1141,"y1":1161},"font_size":0.0,"text":" $49.1\\%$ "},{"bbox":{"x0":580,"x1":637,"y0":1141,"y1":1161},"font_size":0.0,"text":" $55.2\\%$ "},{"bbox":{"x0":712,"x1":771,"y0":1140,"y1":1161},"font_size":0.0,"text":" $\\mathbf{43.2\\%}$ "},{"bbox":{"x0":887,"x1":946,"y0":1141,"y1":1161},"font_size":0.0,"text":" $\\mathbf{60.4\\%}$ "},{"bbox":{"x0":196,"x1":384,"y0":1172,"y1":1198},"font_size":0.0,"text":"DeepSeekMath-Base"},{"bbox":{"x0":451,"x1":487,"y0":1172,"y1":1200},"font_size":0.0,"text":"7B"},{"bbox":{"x0":509,"x1":566,"y0":1174,"y1":1196},"font_size":0.0,"text":" $54.9\\%$ "},{"bbox":{"x0":580,"x1":638,"y0":1174,"y1":1197},"font_size":0.0,"text":" $\\textbf{59.5\\%}$ "},{"bbox":{"x0":712,"x1":771,"y0":1174,"y1":1197},"font_size":0.0,"text":" $40.9\\%$ "},{"bbox":{"x0":886,"x1":946,"y0":1175,"y1":1197},"font_size":0.0,"text":" $52.6\\%$ "}],"source":"layout det","text":"<html><body><table><thead><tr><td>Model</td><td>Size</td><td> MMLU</td><td>BBH</td><td>HumanEval (Pass@1) MBPP (Pass@1)</td><td></td></tr></thead><tbody><tr><td>Mistral</td><td>7B</td><td> $\\mathbf{62.4\\%}$ </td><td> $55.7\\%$ </td><td> $28.0\\%$ </td><td> $41.4\\%$ </td></tr><tr><td>DeepSeek-Coder-Base-v1.5â€ </td><td> 7B</td><td> $42.9\\%$ </td><td> $42.9\\%$ </td><td> $40.2\\%$ </td><td> $52.6\\%$ </td></tr><tr><td>DeepSeek-Coder-Base-v1.5</td><td>7B</td><td> $49.1\\%$ </td><td> $55.2\\%$ </td><td> $\\mathbf{43.2\\%}$ </td><td> $\\mathbf{60.4\\%}$ </td></tr><tr><td>DeepSeekMath-Base</td><td>7B</td><td> $54.9\\%$ </td><td> $\\textbf{59.5\\%}$ </td><td> $40.9\\%$ </td><td> $52.6\\%$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":131,"x1":1060,"y0":1220,"y1":1367},"conf":0.9326,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1223,"y1":1247},"font_size":0.0,"text":"Table 4 | Evaluation on natural language understanding, reasoning, and code benchmarks."},{"bbox":{"x0":140,"x1":1047,"y0":1251,"y1":1274},"font_size":0.0,"text":"DeepSeek-Coder-Base-v1.5â€  is the checkpoint right before learning rate decay, which is used to"},{"bbox":{"x0":138,"x1":1051,"y0":1275,"y1":1305},"font_size":0.0,"text":"train DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting."},{"bbox":{"x0":138,"x1":1049,"y0":1303,"y1":1330},"font_size":0.0,"text":"On HumanEval and MBPP, we evaluate model performance under the zero-shot setting and a"},{"bbox":{"x0":136,"x1":427,"y0":1331,"y1":1358},"font_size":0.0,"text":"few-shot setting, respectively."}],"source":"layout det","text":"Table 4 | Evaluation on natural language understanding, reasoning, and code benchmarks.DeepSeek-Coder-Base-v1.5â€  is the checkpoint right before learning rate decay, which is used to train DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.On HumanEval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively."},{"bbox":{"x0":131,"x1":1061,"y0":1412,"y1":1505},"conf":0.9406,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1053,"y0":1412,"y1":1444},"font_size":0.0,"text":"Natural Language Understanding, Reasoning, and CodeWe evaluate model performance of"},{"bbox":{"x0":138,"x1":1049,"y0":1445,"y1":1470},"font_size":0.0,"text":"natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun"},{"bbox":{"x0":140,"x1":1051,"y0":1472,"y1":1496},"font_size":0.0,"text":"et al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,"}],"source":"layout det","text":"Natural Language Understanding, Reasoning, and CodeWe evaluate model performance of natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun et al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,"}],"formula_dets":[{"bbox":{"x0":774,"x1":835,"y0":526,"y1":548},"conf":0.8069,"label":"print_embedding","label_id":0},{"bbox":{"x0":631,"x1":693,"y0":550,"y1":573},"conf":0.8055,"label":"print_embedding","label_id":0},{"bbox":{"x0":774,"x1":835,"y0":551,"y1":572},"conf":0.8039,"label":"print_embedding","label_id":0},{"bbox":{"x0":712,"x1":771,"y0":1116,"y1":1137},"conf":0.8008,"label":"print_embedding","label_id":0},{"bbox":{"x0":904,"x1":964,"y0":551,"y1":572},"conf":0.7935,"label":"print_embedding","label_id":0},{"bbox":{"x0":478,"x1":540,"y0":525,"y1":548},"conf":0.7924,"label":"print_embedding","label_id":0},{"bbox":{"x0":478,"x1":540,"y0":584,"y1":608},"conf":0.7922,"label":"print_embedding","label_id":0},{"bbox":{"x0":631,"x1":693,"y0":525,"y1":548},"conf":0.7915,"label":"print_embedding","label_id":0},{"bbox":{"x0":905,"x1":963,"y0":492,"y1":513},"conf":0.7838,"label":"print_embedding","label_id":0},{"bbox":{"x0":712,"x1":771,"y0":1140,"y1":1161},"conf":0.7833,"label":"print_embedding","label_id":0},{"bbox":{"x0":478,"x1":540,"y0":550,"y1":573},"conf":0.7796,"label":"print_embedding","label_id":0},{"bbox":{"x0":631,"x1":692,"y0":491,"y1":514},"conf":0.7751,"label":"print_embedding","label_id":0},{"bbox":{"x0":773,"x1":835,"y0":584,"y1":608},"conf":0.7749,"label":"print_embedding","label_id":0},{"bbox":{"x0":478,"x1":540,"y0":491,"y1":514},"conf":0.7731,"label":"print_embedding","label_id":0},{"bbox":{"x0":775,"x1":835,"y0":491,"y1":513},"conf":0.7703,"label":"print_embedding","label_id":0},{"bbox":{"x0":712,"x1":771,"y0":1080,"y1":1102},"conf":0.7684,"label":"print_embedding","label_id":0},{"bbox":{"x0":904,"x1":963,"y0":467,"y1":489},"conf":0.7675,"label":"print_embedding","label_id":0},{"bbox":{"x0":887,"x1":946,"y0":1141,"y1":1161},"conf":0.7674,"label":"print_embedding","label_id":0},{"bbox":{"x0":478,"x1":539,"y0":466,"y1":489},"conf":0.7651,"label":"print_embedding","label_id":0},{"bbox":{"x0":887,"x1":946,"y0":1117,"y1":1138},"conf":0.7646,"label":"print_embedding","label_id":0},{"bbox":{"x0":904,"x1":963,"y0":432,"y1":454},"conf":0.7643,"label":"print_embedding","label_id":0},{"bbox":{"x0":903,"x1":964,"y0":526,"y1":548},"conf":0.7629,"label":"print_embedding","label_id":0},{"bbox":{"x0":774,"x1":834,"y0":467,"y1":489},"conf":0.7592,"label":"print_embedding","label_id":0},{"bbox":{"x0":886,"x1":946,"y0":1175,"y1":1197},"conf":0.7592,"label":"print_embedding","label_id":0},{"bbox":{"x0":630,"x1":693,"y0":584,"y1":608},"conf":0.7585,"label":"print_embedding","label_id":0},{"bbox":{"x0":631,"x1":692,"y0":466,"y1":489},"conf":0.7557,"label":"print_embedding","label_id":0},{"bbox":{"x0":478,"x1":539,"y0":431,"y1":455},"conf":0.7551,"label":"print_embedding","label_id":0},{"bbox":{"x0":903,"x1":963,"y0":585,"y1":608},"conf":0.7521,"label":"print_embedding","label_id":0},{"bbox":{"x0":775,"x1":834,"y0":432,"y1":454},"conf":0.7517,"label":"print_embedding","label_id":0},{"bbox":{"x0":712,"x1":771,"y0":1174,"y1":1197},"conf":0.7505,"label":"print_embedding","label_id":0},{"bbox":{"x0":580,"x1":637,"y0":1141,"y1":1161},"conf":0.7443,"label":"print_embedding","label_id":0},{"bbox":{"x0":632,"x1":692,"y0":431,"y1":455},"conf":0.7419,"label":"print_embedding","label_id":0},{"bbox":{"x0":581,"x1":637,"y0":1117,"y1":1137},"conf":0.7327,"label":"print_embedding","label_id":0},{"bbox":{"x0":509,"x1":566,"y0":1117,"y1":1137},"conf":0.7225,"label":"print_embedding","label_id":0},{"bbox":{"x0":509,"x1":566,"y0":1141,"y1":1161},"conf":0.7182,"label":"print_embedding","label_id":0},{"bbox":{"x0":580,"x1":637,"y0":1080,"y1":1102},"conf":0.7161,"label":"print_embedding","label_id":0},{"bbox":{"x0":887,"x1":946,"y0":1081,"y1":1102},"conf":0.7047,"label":"print_embedding","label_id":0},{"bbox":{"x0":509,"x1":566,"y0":1174,"y1":1196},"conf":0.7033,"label":"print_embedding","label_id":0},{"bbox":{"x0":509,"x1":567,"y0":1080,"y1":1102},"conf":0.6893,"label":"print_embedding","label_id":0},{"bbox":{"x0":580,"x1":638,"y0":1174,"y1":1197},"conf":0.6821,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":184,"x1":999,"y0":1032,"y1":1213},"conf":0.979,"label":"Table","label_id":5},{"bbox":{"x0":180,"x1":1005,"y0":350,"y1":621},"conf":0.9776,"label":"Table","label_id":5},{"bbox":{"x0":131,"x1":1061,"y0":741,"y1":1024},"conf":0.9759,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1061,"y0":1412,"y1":1505},"conf":0.9406,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1060,"y0":1220,"y1":1367},"conf":0.9326,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":165,"y1":340},"conf":0.9146,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1059,"y0":630,"y1":695},"conf":0.7918,"label":"Text","label_id":1},{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.6365,"label":"Abandon","label_id":2},{"bbox":{"x0":131,"x1":1059,"y0":630,"y1":695},"conf":0.2102,"label":"Table footnote","label_id":7}],"ocr_all":false,"ocr_dets":[{"poly":[[583,1551],[604,1551],[604,1575],[583,1575]],"score":0.6964},{"poly":[[140,1472],[1051,1472],[1051,1496],[140,1496]],"score":0.8152},{"poly":[[138,1445],[1049,1445],[1049,1470],[138,1470]],"score":0.741},{"poly":[[136,1414],[1053,1412],[1053,1442],[137,1444]],"score":0.707},{"poly":[[137,1331],[427,1333],[427,1358],[136,1356]],"score":0.8134},{"poly":[[138,1303],[1049,1305],[1049,1330],[138,1328]],"score":0.8068},{"poly":[[138,1275],[1051,1279],[1051,1305],[138,1302]],"score":0.7044},{"poly":[[140,1251],[1047,1251],[1047,1274],[140,1274]],"score":0.7841},{"poly":[[140,1223],[1049,1223],[1049,1247],[140,1247]],"score":0.7626},{"poly":[[195,1174],[383,1174],[383,1196],[195,1196]],"score":0.9355},{"poly":[[884,1172],[946,1172],[946,1198],[884,1198]],"score":0.9284},{"poly":[[711,1172],[773,1172],[773,1198],[711,1198]],"score":0.8426},{"poly":[[507,1172],[638,1172],[638,1196],[507,1196]],"score":0.8911},{"poly":[[450,1170],[489,1170],[489,1198],[450,1198]],"score":0.7895},{"poly":[[884,1137],[946,1137],[946,1163],[884,1163]],"score":0.8649},{"poly":[[711,1137],[771,1137],[771,1163],[711,1163]],"score":0.9677},{"poly":[[571,1137],[640,1137],[640,1163],[571,1163]],"score":0.7746},{"poly":[[507,1137],[581,1137],[581,1161],[507,1161]],"score":0.7971},{"poly":[[195,1137],[486,1135],[486,1161],[195,1163]],"score":0.7334},{"poly":[[883,1110],[948,1110],[948,1142],[883,1142]],"score":0.8398},{"poly":[[709,1110],[775,1110],[775,1142],[709,1142]],"score":0.8442},{"poly":[[504,1107],[642,1111],[641,1142],[503,1138]],"score":0.6874},{"poly":[[193,1110],[487,1110],[487,1140],[193,1140]],"score":0.7018},{"poly":[[884,1079],[948,1079],[948,1105],[884,1105]],"score":0.8934},{"poly":[[505,1079],[640,1079],[640,1103],[505,1103]],"score":0.8981},{"poly":[[709,1075],[775,1075],[775,1107],[709,1107]],"score":0.8082},{"poly":[[193,1073],[268,1077],[266,1106],[191,1101]],"score":0.7708},{"poly":[[450,1075],[487,1075],[487,1105],[450,1105]],"score":0.8598},{"poly":[[452,1044],[987,1044],[987,1068],[452,1068]],"score":0.7617},{"poly":[[193,1042],[262,1042],[262,1068],[193,1068]],"score":0.926},{"poly":[[138,991],[533,991],[533,1016],[138,1016]],"score":0.7721},{"poly":[[135,960],[1053,961],[1053,991],[135,989]],"score":0.7078},{"poly":[[136,937],[1051,935],[1051,960],[137,961]],"score":0.8187},{"poly":[[138,909],[1049,909],[1049,931],[138,931]],"score":0.8235},{"poly":[[136,881],[1051,879],[1051,903],[136,905]],"score":0.7795},{"poly":[[138,854],[1051,854],[1051,879],[138,879]],"score":0.7882},{"poly":[[138,824],[1051,826],[1051,851],[138,849]],"score":0.7555},{"poly":[[136,798],[1051,800],[1051,824],[136,823]],"score":0.757},{"poly":[[138,774],[1053,774],[1053,798],[138,798]],"score":0.7623},{"poly":[[138,746],[1053,746],[1053,770],[138,770]],"score":0.7276},{"poly":[[140,663],[851,663],[851,686],[140,686]],"score":0.8778},{"poly":[[140,635],[1049,635],[1049,660],[140,660]],"score":0.7989},{"poly":[[191,584],[415,584],[415,609],[191,609]],"score":0.8538},{"poly":[[902,582],[964,582],[964,609],[902,609]],"score":0.9752},{"poly":[[773,582],[835,582],[835,609],[773,609]],"score":0.9854},{"poly":[[629,582],[693,582],[693,609],[629,609]],"score":0.9557},{"poly":[[477,582],[541,582],[541,609],[477,609]],"score":0.9415},{"poly":[[900,546],[966,546],[966,577],[900,577]],"score":0.8549},{"poly":[[771,546],[837,546],[837,577],[771,577]],"score":0.8448},{"poly":[[629,541],[695,546],[693,578],[627,573]],"score":0.8228},{"poly":[[475,546],[541,546],[541,577],[475,577]],"score":0.8437},{"poly":[[381,547],[427,547],[427,575],[381,575]],"score":0.8682},{"poly":[[191,545],[273,549],[272,576],[189,571]],"score":0.7682},{"poly":[[189,521],[272,525],[270,551],[187,547]],"score":0.863},{"poly":[[902,523],[964,523],[964,549],[902,549]],"score":0.9595},{"poly":[[773,523],[835,523],[835,549],[773,549]],"score":0.9304},{"poly":[[629,521],[695,521],[695,553],[629,553]],"score":0.8353},{"poly":[[477,523],[539,523],[539,549],[477,549]],"score":0.9533},{"poly":[[381,523],[420,523],[420,551],[381,551]],"score":0.8999},{"poly":[[904,489],[964,489],[964,516],[904,516]],"score":0.9343},{"poly":[[190,487],[302,491],[301,516],[189,512]],"score":0.8077},{"poly":[[771,486],[837,486],[837,517],[771,517]],"score":0.8488},{"poly":[[629,486],[695,486],[695,517],[629,517]],"score":0.863},{"poly":[[475,486],[541,486],[541,517],[475,517]],"score":0.8968},{"poly":[[381,488],[425,488],[425,516],[381,516]],"score":0.9381},{"poly":[[904,465],[964,465],[964,491],[904,491]],"score":0.9019},{"poly":[[773,465],[835,465],[835,491],[773,491]],"score":0.9021},{"poly":[[631,465],[693,465],[693,491],[631,491]],"score":0.9217},{"poly":[[189,461],[302,465],[301,491],[188,487]],"score":0.8305},{"poly":[[475,461],[541,461],[541,493],[475,493]],"score":0.8122},{"poly":[[379,463],[418,463],[418,491],[379,491]],"score":0.8841},{"poly":[[904,430],[962,430],[962,456],[904,456]],"score":0.9618},{"poly":[[775,430],[835,430],[835,456],[775,456]],"score":0.8803},{"poly":[[631,430],[691,430],[691,456],[631,456]],"score":0.9071},{"poly":[[479,430],[539,430],[539,456],[479,456]],"score":0.8879},{"poly":[[190,430],[262,430],[262,456],[190,456]],"score":0.9114},{"poly":[[381,428],[418,428],[418,456],[381,456]],"score":0.872},{"poly":[[432,395],[993,396],[992,421],[432,419]],"score":0.8197},{"poly":[[383,372],[431,372],[431,398],[383,398]],"score":0.8183},{"poly":[[190,372],[259,372],[259,398],[190,398]],"score":0.8585},{"poly":[[734,358],[993,361],[992,391],[734,388]],"score":0.7323},{"poly":[[464,363],[704,363],[704,386],[464,386]],"score":0.9237},{"poly":[[136,307],[631,305],[631,330],[137,332]],"score":0.7564},{"poly":[[135,277],[1053,275],[1053,305],[135,307]],"score":0.6983},{"poly":[[138,254],[1051,254],[1051,277],[138,277]],"score":0.8501},{"poly":[[140,226],[1051,226],[1051,251],[140,251]],"score":0.7476},{"poly":[[138,198],[1049,198],[1049,223],[138,223]],"score":0.7758},{"poly":[[140,172],[1049,172],[1049,196],[140,196]],"score":0.7894}],"page_no":8,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":163,"x1":769,"y0":1424,"y1":1524},"conf":0.8142,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":167,"x1":580,"y0":1491,"y1":1523},"font_size":0.0,"text":"5https://open.bigmodel.cn/dev/api#glm-4"},{"bbox":{"x0":168,"x1":464,"y0":1470,"y1":1498},"font_size":0.0,"text":"4https://www.baichuan-ai.com"},{"bbox":{"x0":168,"x1":422,"y0":1447,"y1":1474},"font_size":0.0,"text":"3https://x.ai/model-card"},{"bbox":{"x0":168,"x1":764,"y0":1423,"y1":1453},"font_size":0.0,"text":"2https://openai.com/blog/chatgpt-plugins#code-interpreter"}],"source":"layout det","text":""},{"bbox":{"x0":578,"x1":613,"y0":1551,"y1":1580},"conf":0.5717,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"10"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":1061,"y0":167,"y1":369},"conf":0.9757,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":172,"y1":198},"font_size":155300.0,"text":"2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits signifcant enhancements in per-i"},{"bbox":{"x0":140,"x1":1051,"y0":200,"y1":225},"font_size":155300.0,"text":"formance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),"},{"bbox":{"x0":136,"x1":1053,"y0":223,"y1":256},"font_size":155300.0,"text":"illustrating the positive impact of math training on language understanding and reasoning."},{"bbox":{"x0":140,"x1":1049,"y0":256,"y1":279},"font_size":155300.0,"text":"Additionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively"},{"bbox":{"x0":140,"x1":1051,"y0":282,"y1":305},"font_size":155300.0,"text":"maintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Over-"},{"bbox":{"x0":140,"x1":1053,"y0":309,"y1":333},"font_size":155300.0,"text":"all, DeepSeekMath-Base 7B signifcantly outperforms the general model Mistral 7B (Jiang et al.,i"},{"bbox":{"x0":140,"x1":654,"y0":337,"y1":361},"font_size":155300.0,"text":"2023) on the three reasoning and coding benchmarks."}],"source":"layout det","text":"2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits signifcant enhancements in per-i formance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),illustrating the positive impact of math training on language understanding and reasoning.Additionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively maintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Overall, DeepSeekMath-Base 7B signifcantly outperforms the general model Mistral 7B (Jiang et al.,i 2023) on the three reasoning and coding benchmarks."},{"bbox":{"x0":133,"x1":467,"y0":400,"y1":441},"conf":0.9039,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":461,"y0":402,"y1":437},"font_size":155300.0,"text":"3. Supervised Fine-Tuning"}],"source":"layout det","text":"3.Supervised Fine-Tuning"},{"bbox":{"x0":134,"x1":379,"y0":457,"y1":488},"conf":0.8877,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":372,"y0":460,"y1":482},"font_size":155300.0,"text":"3.1. SFT Data Curation"}],"source":"layout det","text":"3.1. SFT Data Curation"},{"bbox":{"x0":133,"x1":1060,"y0":500,"y1":645},"conf":0.9707,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":503,"y1":530},"font_size":155300.0,"text":"We construct a mathematical instruction-tuning dataset covering English and Chinese problems"},{"bbox":{"x0":138,"x1":1049,"y0":533,"y1":558},"font_size":155300.0,"text":"from different mathematical felds and of varying complexity levels: problems are paired withi"},{"bbox":{"x0":138,"x1":1051,"y0":560,"y1":584},"font_size":155300.0,"text":"solutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,"},{"bbox":{"x0":140,"x1":1049,"y0":588,"y1":612},"font_size":155300.0,"text":"2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number"},{"bbox":{"x0":138,"x1":420,"y0":612,"y1":639},"font_size":155300.0,"text":"of training examples is 776K."}],"source":"layout det","text":"We construct a mathematical instruction-tuning dataset covering English and Chinese problems from different mathematical felds and of varying complexity levels: problems are paired withi solutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number of training examples is 776K."},{"bbox":{"x0":188,"x1":1059,"y0":650,"y1":790},"conf":0.9704,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1051,"y0":654,"y1":679},"font_size":155300.0,"text":"English mathematical datasets: We annotate GSM8K and MATH problems with tool-"},{"bbox":{"x0":193,"x1":1049,"y0":682,"y1":707},"font_size":155300.0,"text":"integrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the"},{"bbox":{"x0":195,"x1":1051,"y0":709,"y1":733},"font_size":155300.0,"text":"training set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or"},{"bbox":{"x0":191,"x1":1053,"y0":735,"y1":765},"font_size":155300.0,"text":"PoT. Our English collection covers diverse felds of mathematics, e.g., algebra, probability,i"},{"bbox":{"x0":190,"x1":580,"y0":763,"y1":789},"font_size":155300.0,"text":"number theory, calculus, and geometry."}],"source":"layout det","text":"English mathematical datasets: We annotate GSM8K and MATH problems with toolintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the training set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or PoT. Our English collection covers diverse felds of mathematics, e.g., algebra, probability,i number theory, calculus, and geometry."},{"bbox":{"x0":188,"x1":1060,"y0":790,"y1":875},"conf":0.9522,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":1051,"y0":784,"y1":819},"font_size":155300.0,"text":"Chinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning"},{"bbox":{"x0":193,"x1":1053,"y0":817,"y1":842},"font_size":155300.0,"text":"76 sub-topics such as linear equations, with solutions annotated in both CoT and tool-"},{"bbox":{"x0":193,"x1":470,"y0":846,"y1":868},"font_size":155300.0,"text":"integrated reasoning format."}],"source":"layout det","text":"Chinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning 76 sub-topics such as linear equations, with solutions annotated in both CoT and toolintegrated reasoning format."},{"bbox":{"x0":134,"x1":710,"y0":905,"y1":941},"conf":0.8946,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":702,"y0":912,"y1":935},"font_size":155300.0,"text":"3.2. Training and Evaluating DeepSeekMath-Instruct 7B"}],"source":"layout det","text":"3.2. Training and Evaluating DeepSeekMath-Instruct 7B"},{"bbox":{"x0":133,"x1":1058,"y0":950,"y1":1066},"conf":0.9628,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":956,"y1":979},"font_size":155300.0,"text":"In this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruc-"},{"bbox":{"x0":140,"x1":1049,"y0":984,"y1":1007},"font_size":155300.0,"text":"tion tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until"},{"bbox":{"x0":140,"x1":1049,"y0":1010,"y1":1033},"font_size":155300.0,"text":"reaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch"},{"bbox":{"x0":140,"x1":590,"y0":1037,"y1":1061},"font_size":155300.0,"text":"size of 256 and a constant learning rate of 5e-5."}],"source":"layout det","text":"In this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruction tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until reaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch size of 256 and a constant learning rate of 5e-5."},{"bbox":{"x0":135,"x1":1058,"y0":1072,"y1":1161},"conf":0.9637,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1051,"y0":1077,"y1":1102},"font_size":155300.0,"text":"We evaluate modelsâ€™ mathematical performance both without and with tool use, on 4"},{"bbox":{"x0":140,"x1":1047,"y0":1103,"y1":1128},"font_size":155300.0,"text":"quantitative reasoning benchmarks in English and Chinese. We benchmark our model against"},{"bbox":{"x0":138,"x1":443,"y0":1130,"y1":1156},"font_size":155300.0,"text":"the leading models of the time:"}],"source":"layout det","text":"We evaluate modelsâ€™ mathematical performance both without and with tool use, on 4 quantitative reasoning benchmarks in English and Chinese. We benchmark our model against the leading models of the time:"},{"bbox":{"x0":187,"x1":1058,"y0":1168,"y1":1333},"conf":0.9697,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":1172,"y1":1196},"font_size":155300.0,"text":"Closed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)"},{"bbox":{"x0":193,"x1":1049,"y0":1200,"y1":1224},"font_size":155300.0,"text":"and GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil"},{"bbox":{"x0":190,"x1":1051,"y0":1223,"y1":1254},"font_size":155300.0,"text":"et al., 2023), (3) Infection-2 (Infection AI, 2023), (4) Grok-1 3, as well as models recentlyll"},{"bbox":{"x0":190,"x1":1051,"y0":1249,"y1":1281},"font_size":155300.0,"text":"released by Chinese companies including (5) Baichuan $\\cdot3^{4},$  (6) the latest GLM-4 5 from the"},{"bbox":{"x0":195,"x1":1049,"y0":1281,"y1":1305},"font_size":155300.0,"text":"GLM family (Du et al., 2022). These models are for general purposes, most of which have"},{"bbox":{"x0":193,"x1":626,"y0":1309,"y1":1333},"font_size":155300.0,"text":"undergone a series of alignment procedures."}],"source":"layout det","text":"Closed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)and GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil et al., 2023), (3) Infection-2 (Infection AI, 2023), (4) Grok-1 3, as well as models recentlyll released by Chinese companies including (5) Baichuan $\\cdot3^{4},$  (6) the latest GLM-4 5 from the GLM family (Du et al., 2022). These models are for general purposes, most of which have undergone a series of alignment procedures."},{"bbox":{"x0":185,"x1":1059,"y0":1334,"y1":1390},"conf":0.9093,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1049,"y0":1333,"y1":1358},"font_size":155300.0,"text":"Open-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeek-"},{"bbox":{"x0":193,"x1":1051,"y0":1363,"y1":1386},"font_size":155300.0,"text":"AI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)"}],"source":"layout det","text":"Open-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeekAI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)"}],"formula_dets":[{"bbox":{"x0":720,"x1":751,"y0":1252,"y1":1276},"conf":0.6746,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1061,"y0":167,"y1":369},"conf":0.9757,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":500,"y1":645},"conf":0.9707,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1059,"y0":650,"y1":790},"conf":0.9704,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1058,"y0":1168,"y1":1333},"conf":0.9697,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1058,"y0":1072,"y1":1161},"conf":0.9637,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":950,"y1":1066},"conf":0.9628,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1060,"y0":790,"y1":875},"conf":0.9522,"label":"Text","label_id":1},{"bbox":{"x0":185,"x1":1059,"y0":1334,"y1":1390},"conf":0.9093,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":467,"y0":400,"y1":441},"conf":0.9039,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":710,"y0":905,"y1":941},"conf":0.8946,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":379,"y0":457,"y1":488},"conf":0.8877,"label":"Title","label_id":0},{"bbox":{"x0":163,"x1":769,"y0":1424,"y1":1524},"conf":0.8142,"label":"Abandon","label_id":2},{"bbox":{"x0":578,"x1":613,"y0":1551,"y1":1580},"conf":0.5717,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.8584},{"poly":[[167,1491],[580,1493],[579,1523],[167,1521]],"score":0.6747},{"poly":[[169,1470],[464,1474],[464,1498],[168,1494]],"score":0.6911},{"poly":[[168,1447],[422,1449],[422,1474],[168,1472]],"score":0.7514},{"poly":[[168,1423],[764,1426],[764,1453],[168,1449]],"score":0.6896},{"poly":[[193,1363],[1051,1363],[1051,1386],[193,1386]],"score":0.8825},{"poly":[[186,1333],[1049,1333],[1049,1358],[186,1358]],"score":0.7455},{"poly":[[193,1309],[626,1309],[626,1333],[193,1333]],"score":0.7504},{"poly":[[195,1281],[1049,1281],[1049,1305],[195,1305]],"score":0.7593},{"poly":[[190,1251],[1051,1249],[1051,1279],[190,1281]],"score":0.6922},{"poly":[[190,1223],[1051,1224],[1051,1254],[190,1252]],"score":0.6963},{"poly":[[193,1200],[1049,1200],[1049,1224],[193,1224]],"score":0.7696},{"poly":[[170,1172],[1049,1172],[1049,1196],[170,1196]],"score":0.7536},{"poly":[[138,1130],[443,1131],[443,1156],[138,1154]],"score":0.7708},{"poly":[[140,1103],[1047,1103],[1047,1128],[140,1128]],"score":0.7886},{"poly":[[175,1077],[1051,1077],[1051,1102],[175,1102]],"score":0.7334},{"poly":[[140,1037],[590,1037],[590,1061],[140,1061]],"score":0.7367},{"poly":[[140,1010],[1049,1010],[1049,1033],[140,1033]],"score":0.8612},{"poly":[[140,984],[1049,984],[1049,1007],[140,1007]],"score":0.8361},{"poly":[[140,956],[1051,956],[1051,979],[140,979]],"score":0.8636},{"poly":[[140,912],[702,912],[702,935],[140,935]],"score":0.8416},{"poly":[[193,846],[470,846],[470,868],[193,868]],"score":0.9264},{"poly":[[193,817],[1053,817],[1053,842],[193,842]],"score":0.7475},{"poly":[[188,784],[1051,789],[1051,819],[188,814]],"score":0.6849},{"poly":[[190,763],[580,765],[579,789],[190,788]],"score":0.8039},{"poly":[[191,735],[1053,735],[1053,765],[191,765]],"score":0.7005},{"poly":[[195,709],[1051,709],[1051,733],[195,733]],"score":0.758},{"poly":[[193,682],[1049,682],[1049,707],[193,707]],"score":0.7696},{"poly":[[172,654],[1051,654],[1051,679],[172,679]],"score":0.7618},{"poly":[[138,614],[420,612],[420,637],[138,639]],"score":0.8186},{"poly":[[140,588],[1049,588],[1049,612],[140,612]],"score":0.7453},{"poly":[[138,560],[1051,560],[1051,584],[138,584]],"score":0.7466},{"poly":[[138,533],[1049,533],[1049,558],[138,558]],"score":0.7363},{"poly":[[137,503],[1049,505],[1049,530],[136,528]],"score":0.7746},{"poly":[[136,460],[372,460],[372,482],[136,482]],"score":0.7569},{"poly":[[137,402],[461,405],[461,437],[136,433]],"score":0.7419},{"poly":[[140,337],[654,337],[654,361],[140,361]],"score":0.8022},{"poly":[[140,309],[1053,309],[1053,333],[140,333]],"score":0.7709},{"poly":[[140,282],[1051,282],[1051,305],[140,305]],"score":0.8609},{"poly":[[140,256],[1049,256],[1049,279],[140,279]],"score":0.8425},{"poly":[[137,223],[1053,226],[1053,256],[136,253]],"score":0.6467},{"poly":[[140,200],[1051,200],[1051,225],[140,225]],"score":0.7417},{"poly":[[138,172],[1053,174],[1053,198],[138,196]],"score":0.7851}],"page_no":9,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":163,"x1":1020,"y0":1492,"y1":1581},"conf":0.29,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":608,"y0":1554,"y1":1577},"font_size":0.0,"text":"11"},{"bbox":{"x0":170,"x1":608,"y0":1496,"y1":1519},"font_size":0.0,"text":"6https://github.com/InternLM/InternLM-Math"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":186,"x1":1061,"y0":168,"y1":498},"conf":0.981,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":193,"x1":1053,"y0":174,"y1":196},"font_size":0.0,"text":"ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathemat-"},{"bbox":{"x0":193,"x1":1049,"y0":198,"y1":225},"font_size":0.0,"text":"ics including (5) InternLM2-Math $20\\mathrm{B}^{6}$ which builds on InternLM2 and underwent math"},{"bbox":{"x0":193,"x1":1049,"y0":228,"y1":253},"font_size":0.0,"text":"training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO"},{"bbox":{"x0":195,"x1":1049,"y0":256,"y1":281},"font_size":0.0,"text":"training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised"},{"bbox":{"x0":193,"x1":1049,"y0":282,"y1":305},"font_size":0.0,"text":"reward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical"},{"bbox":{"x0":193,"x1":1053,"y0":309,"y1":333},"font_size":0.0,"text":"reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,"},{"bbox":{"x0":191,"x1":1049,"y0":337,"y1":361},"font_size":0.0,"text":"a version of instruction tuning that uses AI-evolved instructions) and PPO training with"},{"bbox":{"x0":191,"x1":1051,"y0":361,"y1":388},"font_size":0.0,"text":"training problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,"},{"bbox":{"x0":195,"x1":1051,"y0":389,"y1":414},"font_size":0.0,"text":"2023) which is Llama-2 70B fne-tuned on an augmented version of GSM8K and MATH,i"},{"bbox":{"x0":193,"x1":1049,"y0":417,"y1":442},"font_size":0.0,"text":"(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fne-tuned to do tool-integratedi"},{"bbox":{"x0":191,"x1":1049,"y0":442,"y1":468},"font_size":0.0,"text":"mathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B"},{"bbox":{"x0":195,"x1":533,"y0":472,"y1":495},"font_size":0.0,"text":"instruction-tuned on MathInstruct."}],"source":"layout det","text":"ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathematics including (5) InternLM2-Math $20\\mathrm{B}^{6}$ which builds on InternLM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised reward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,2023) which is Llama-2 70B fne-tuned on an augmented version of GSM8K and MATH,i(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fne-tuned to do tool-integratedi mathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on MathInstruct."},{"bbox":{"x0":134,"x1":1065,"y0":507,"y1":730},"conf":0.9714,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1060,"y0":510,"y1":537},"font_size":0.0,"text":"As shown in Table $5,$  under the evaluation setting where tool use is disallowed, DeepSeekMath-"},{"bbox":{"x0":140,"x1":1051,"y0":540,"y1":565},"font_size":0.0,"text":"Instruct 7B demonstrates strong performance of step-by-step reasoning.Notably, on the"},{"bbox":{"x0":140,"x1":1053,"y0":567,"y1":591},"font_size":0.0,"text":"competition-level MATH dataset, our model surpasses all open-source models and the ma-"},{"bbox":{"x0":136,"x1":1051,"y0":588,"y1":621},"font_size":0.0,"text":"jority of proprietary models (e.g., Infection-2 and Gemini Pro) by at leastl $9\\%$ absolute. This"},{"bbox":{"x0":138,"x1":1051,"y0":621,"y1":646},"font_size":0.0,"text":"is true even for models that are substantially larger (e.g., Qwen 72B) or have been specif-i"},{"bbox":{"x0":140,"x1":1049,"y0":647,"y1":672},"font_size":0.0,"text":"cally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While"},{"bbox":{"x0":140,"x1":1051,"y0":675,"y1":700},"font_size":0.0,"text":"DeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,"},{"bbox":{"x0":140,"x1":594,"y0":702,"y1":726},"font_size":0.0,"text":"it still underperforms GPT-4 and Gemini Ultra."}],"source":"layout det","text":"As shown in Table $5,$  under the evaluation setting where tool use is disallowed, DeepSeekMathInstruct 7B demonstrates strong performance of step-by-step reasoning.Notably, on the competition-level MATH dataset, our model surpasses all open-source models and the majority of proprietary models (e.g., Infection-2 and Gemini Pro) by at leastl $9\\%$ absolute. This is true even for models that are substantially larger (e.g., Qwen 72B) or have been specif-i cally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While DeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,it still underperforms GPT-4 and Gemini Ultra."},{"bbox":{"x0":134,"x1":1060,"y0":737,"y1":880},"conf":0.9697,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1053,"y0":737,"y1":770},"font_size":0.0,"text":"Under the evaluation setting where models are allowed to integrate natural language rea-"},{"bbox":{"x0":140,"x1":1049,"y0":770,"y1":795},"font_size":0.0,"text":"soning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches"},{"bbox":{"x0":136,"x1":1053,"y0":795,"y1":824},"font_size":0.0,"text":"an accuracy of $60\\%$ on MATH, surpassing all existing open-source models. On the other bench-"},{"bbox":{"x0":138,"x1":1049,"y0":824,"y1":847},"font_size":0.0,"text":"marks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is"},{"bbox":{"x0":136,"x1":287,"y0":847,"y1":877},"font_size":0.0,"text":"10 times larger."}],"source":"layout det","text":"Under the evaluation setting where models are allowed to integrate natural language reasoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches an accuracy of $60\\%$ on MATH, surpassing all existing open-source models. On the other benchmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is 10 times larger."},{"bbox":{"x0":133,"x1":471,"y0":914,"y1":952},"conf":0.9031,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":134,"x1":465,"y0":914,"y1":949},"font_size":0.0,"text":"4. Reinforcement Learning"}],"source":"layout det","text":"4.Reinforcement Learning"},{"bbox":{"x0":133,"x1":551,"y0":970,"y1":1004},"conf":0.9056,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":546,"y0":975,"y1":998},"font_size":0.0,"text":"4.1. Group Relative Policy Optimization"}],"source":"layout det","text":"4.1. Group Relative Policy Optimization"},{"bbox":{"x0":134,"x1":1060,"y0":1013,"y1":1130},"conf":0.9672,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1019,"y1":1044},"font_size":0.0,"text":"Reinforcement learning (RL) has been proven to be effective in further improving the mathe-"},{"bbox":{"x0":140,"x1":1051,"y0":1047,"y1":1072},"font_size":0.0,"text":"matical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;"},{"bbox":{"x0":135,"x1":1051,"y0":1070,"y1":1102},"font_size":0.0,"text":"Wang et al., 2023b). In this section, we introduce our effcient and effective RL algorithm, Groupi"},{"bbox":{"x0":138,"x1":505,"y0":1098,"y1":1124},"font_size":0.0,"text":"Relative Policy Optimization (GRPO)."}],"source":"layout det","text":"Reinforcement learning (RL) has been proven to be effective in further improving the mathematical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;Wang et al., 2023b). In this section, we introduce our effcient and effective RL algorithm, Groupi Relative Policy Optimization (GRPO)."},{"bbox":{"x0":134,"x1":409,"y0":1156,"y1":1187},"conf":0.8879,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":402,"y0":1158,"y1":1184},"font_size":0.0,"text":"4.1.1. From PPO to GRPO"}],"source":"layout det","text":"4.1.1. From PPO to GRPO"},{"bbox":{"x0":134,"x1":1057,"y0":1200,"y1":1289},"conf":0.9556,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1205,"y1":1230},"font_size":0.0,"text":"Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is"},{"bbox":{"x0":138,"x1":1049,"y0":1233,"y1":1258},"font_size":0.0,"text":"widely used in the RL fne-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizesi"},{"bbox":{"x0":140,"x1":677,"y0":1259,"y1":1284},"font_size":0.0,"text":"LLMs by maximizing the following surrogate objective:"}],"source":"layout det","text":"Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is widely used in the RL fne-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizesi LLMs by maximizing the following surrogate objective:"},{"bbox":{"x0":191,"x1":999,"y0":1297,"y1":1360},"conf":0.8623,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}\\left[q\\sim P(Q),o\\sim\\pi_{\\theta_{old}}( O|q)\\right]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\min\\left[\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta} (o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\right)A_{t}\\right],$$"},{"bbox":{"x0":1023,"x1":1056,"y0":1314,"y1":1344},"conf":0.384,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1024,"x1":1053,"y0":1317,"y1":1342},"font_size":0.0,"text":"(1)"}],"source":"layout det","text":"(1)"},{"bbox":{"x0":131,"x1":1057,"y0":1368,"y1":1483},"conf":0.9567,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1374,"y1":1399},"font_size":0.0,"text":"where $\\pi_{\\theta}$ and $\\pi_{\\theta_{old}}$ are the current and old policy models, and $q,o$ are questions and outputs"},{"bbox":{"x0":136,"x1":1051,"y0":1398,"y1":1428},"font_size":0.0,"text":"sampled from the question dataset and the old policy $\\pi_{\\theta_{old}},$  respectively. ðœ€is a clipping-related"},{"bbox":{"x0":138,"x1":1051,"y0":1424,"y1":1454},"font_size":0.0,"text":"hyper-parameter introduced in PPO for stabilizing training. $A_{t}$ is the advantage, which is"},{"bbox":{"x0":140,"x1":1049,"y0":1454,"y1":1479},"font_size":0.0,"text":"computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based"}],"source":"layout det","text":"where $\\pi_{\\theta}$ and $\\pi_{\\theta_{old}}$ are the current and old policy models, and $q,o$ are questions and outputs sampled from the question dataset and the old policy $\\pi_{\\theta_{old}},$  respectively. ðœ€is a clipping-related hyper-parameter introduced in PPO for stabilizing training. $A_{t}$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based"}],"formula_dets":[{"bbox":{"x0":191,"x1":999,"y0":1297,"y1":1360},"conf":0.8623,"label":"print_isolated","label_id":1},{"bbox":{"x0":662,"x1":709,"y0":1406,"y1":1426},"conf":0.8582,"label":"print_embedding","label_id":0},{"bbox":{"x0":870,"x1":903,"y0":593,"y1":614},"conf":0.8319,"label":"print_embedding","label_id":0},{"bbox":{"x0":754,"x1":776,"y0":1431,"y1":1450},"conf":0.8299,"label":"print_embedding","label_id":0},{"bbox":{"x0":282,"x1":325,"y0":795,"y1":817},"conf":0.8171,"label":"print_embedding","label_id":0},{"bbox":{"x0":280,"x1":320,"y0":1380,"y1":1399},"conf":0.809,"label":"print_embedding","label_id":0},{"bbox":{"x0":207,"x1":230,"y0":1381,"y1":1396},"conf":0.7072,"label":"print_embedding","label_id":0},{"bbox":{"x0":753,"x1":785,"y0":1374,"y1":1398},"conf":0.6929,"label":"print_embedding","label_id":0},{"bbox":{"x0":348,"x1":366,"y0":515,"y1":536},"conf":0.629,"label":"print_embedding","label_id":0},{"bbox":{"x0":520,"x1":571,"y0":198,"y1":221},"conf":0.6161,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":186,"x1":1061,"y0":168,"y1":498},"conf":0.981,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1065,"y0":507,"y1":730},"conf":0.9714,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":737,"y1":880},"conf":0.9697,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":1013,"y1":1130},"conf":0.9672,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":1368,"y1":1483},"conf":0.9567,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1057,"y0":1200,"y1":1289},"conf":0.9556,"label":"Text","label_id":1},{"bbox":{"x0":183,"x1":1005,"y0":1297,"y1":1362},"conf":0.9517,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":551,"y0":970,"y1":1004},"conf":0.9056,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":471,"y0":914,"y1":952},"conf":0.9031,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":409,"y0":1156,"y1":1187},"conf":0.8879,"label":"Title","label_id":0},{"bbox":{"x0":578,"x1":611,"y0":1550,"y1":1579},"conf":0.4719,"label":"Abandon","label_id":2},{"bbox":{"x0":167,"x1":612,"y0":1495,"y1":1579},"conf":0.4269,"label":"Abandon","label_id":2},{"bbox":{"x0":1024,"x1":1055,"y0":1315,"y1":1343},"conf":0.3927,"label":"Equation caption","label_id":9},{"bbox":{"x0":1023,"x1":1056,"y0":1314,"y1":1344},"conf":0.384,"label":"Equation caption","label_id":9},{"bbox":{"x0":166,"x1":613,"y0":1495,"y1":1524},"conf":0.3595,"label":"Abandon","label_id":2},{"bbox":{"x0":163,"x1":1020,"y0":1492,"y1":1581},"conf":0.29,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[608,1554],[608,1577],[580,1577]],"score":0.8293},{"poly":[[170,1496],[608,1496],[608,1519],[170,1519]],"score":0.8472},{"poly":[[140,1454],[1049,1454],[1049,1479],[140,1479]],"score":0.7663},{"poly":[[138,1424],[1051,1424],[1051,1454],[138,1454]],"score":0.638},{"poly":[[136,1398],[1051,1398],[1051,1428],[136,1428]],"score":0.6661},{"poly":[[138,1374],[1049,1374],[1049,1398],[138,1398]],"score":0.7834},{"poly":[[751,1331],[870,1331],[870,1356],[751,1356]],"score":0.86},{"poly":[[569,1331],[691,1331],[691,1356],[569,1356]],"score":0.8194},{"poly":[[480,1331],[507,1331],[507,1352],[480,1352]],"score":0.7342},{"poly":[[186,1314],[489,1317],[489,1347],[186,1344]],"score":0.7137},{"poly":[[689,1317],[748,1317],[748,1342],[689,1342]],"score":0.871},{"poly":[[1024,1317],[1053,1317],[1053,1342],[1024,1342]],"score":0.8879},{"poly":[[868,1316],[962,1314],[963,1340],[869,1342]],"score":0.8231},{"poly":[[968,1308],[995,1329],[977,1352],[950,1331]],"score":0.6273},{"poly":[[757,1307],[865,1307],[865,1331],[757,1331]],"score":0.8915},{"poly":[[571,1305],[684,1305],[684,1330],[571,1330]],"score":0.7622},{"poly":[[509,1302],[530,1302],[530,1317],[509,1317]],"score":0.9165},{"poly":[[140,1259],[677,1259],[677,1284],[140,1284]],"score":0.7768},{"poly":[[138,1233],[1049,1233],[1049,1258],[138,1258]],"score":0.7803},{"poly":[[140,1205],[1049,1205],[1049,1230],[140,1230]],"score":0.7357},{"poly":[[138,1160],[402,1158],[402,1182],[138,1184]],"score":0.7435},{"poly":[[138,1098],[505,1100],[505,1124],[138,1123]],"score":0.8144},{"poly":[[135,1070],[1051,1072],[1051,1102],[135,1100]],"score":0.7207},{"poly":[[140,1047],[1051,1047],[1051,1072],[140,1072]],"score":0.7767},{"poly":[[140,1019],[1051,1019],[1051,1044],[140,1044]],"score":0.7561},{"poly":[[138,975],[546,975],[546,998],[138,998]],"score":0.9644},{"poly":[[135,914],[465,919],[464,949],[134,944]],"score":0.7199},{"poly":[[137,847],[287,853],[286,877],[136,872]],"score":0.7908},{"poly":[[138,824],[1049,824],[1049,847],[138,847]],"score":0.9037},{"poly":[[136,795],[1053,795],[1053,824],[136,824]],"score":0.6539},{"poly":[[140,770],[1049,770],[1049,795],[140,795]],"score":0.7938},{"poly":[[172,737],[1053,740],[1053,770],[172,767]],"score":0.6498},{"poly":[[140,702],[594,702],[594,726],[140,726]],"score":0.7399},{"poly":[[140,675],[1051,675],[1051,700],[140,700]],"score":0.7224},{"poly":[[140,647],[1049,647],[1049,672],[140,672]],"score":0.7265},{"poly":[[138,621],[1051,621],[1051,646],[138,646]],"score":0.7221},{"poly":[[136,591],[1051,588],[1051,617],[137,621]],"score":0.6708},{"poly":[[140,567],[1053,567],[1053,591],[140,591]],"score":0.7228},{"poly":[[140,540],[1051,540],[1051,565],[140,565]],"score":0.7432},{"poly":[[172,512],[1060,510],[1060,535],[172,537]],"score":0.7756},{"poly":[[195,472],[533,472],[533,495],[195,495]],"score":0.8294},{"poly":[[191,444],[1049,442],[1049,467],[191,468]],"score":0.7729},{"poly":[[193,417],[1049,417],[1049,442],[193,442]],"score":0.743},{"poly":[[195,389],[1051,389],[1051,414],[195,414]],"score":0.7186},{"poly":[[191,363],[1051,361],[1051,386],[191,388]],"score":0.7209},{"poly":[[191,337],[1049,337],[1049,361],[191,361]],"score":0.714},{"poly":[[193,309],[1053,309],[1053,333],[193,333]],"score":0.7029},{"poly":[[193,282],[1049,282],[1049,305],[193,305]],"score":0.804},{"poly":[[195,256],[1049,256],[1049,281],[195,281]],"score":0.7046},{"poly":[[193,228],[1049,228],[1049,253],[193,253]],"score":0.7032},{"poly":[[193,200],[1049,200],[1049,225],[193,225]],"score":0.7307},{"poly":[[193,174],[1053,174],[1053,196],[193,196]],"score":0.8806}],"page_no":10,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7891,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1575},"font_size":0.0,"text":"12"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":220,"x1":971,"y0":162,"y1":1275},"conf":0.9741,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":538,"x1":747,"y0":176,"y1":204},"font_size":0.0,"text":"English Benchmarks"},{"bbox":{"x0":737,"x1":953,"y0":177,"y1":200},"font_size":0.0,"text":" Chinese Benchmarks"},{"bbox":{"x0":234,"x1":304,"y0":189,"y1":214},"font_size":0.0,"text":"Model"},{"bbox":{"x0":485,"x1":533,"y0":188,"y1":214},"font_size":0.0,"text":"Size"},{"bbox":{"x0":538,"x1":620,"y0":213,"y1":239},"font_size":0.0,"text":"GSM8K"},{"bbox":{"x0":645,"x1":721,"y0":212,"y1":237},"font_size":0.0,"text":"MATH"},{"bbox":{"x0":747,"x1":857,"y0":213,"y1":237},"font_size":0.0,"text":"MGSM-zh "},{"bbox":{"x0":861,"x1":950,"y0":214,"y1":235},"font_size":0.0,"text":"CMATH"},{"bbox":{"x0":446,"x1":745,"y0":253,"y1":286},"font_size":0.0,"text":"Chain-of-Thought Reasoning"},{"bbox":{"x0":490,"x1":701,"y0":296,"y1":320},"font_size":0.0,"text":"Closed-Source Model"},{"bbox":{"x0":234,"x1":368,"y0":335,"y1":358},"font_size":0.0,"text":"Gemini Ultra"},{"bbox":{"x0":490,"x1":497,"y0":338,"y1":359},"font_size":10.0,"text":"-"},{"bbox":{"x0":545,"x1":612,"y0":332,"y1":357},"font_size":0.0,"text":" $94.4\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":332,"y1":357},"font_size":0.0,"text":" $53.2\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":338,"y1":359},"font_size":10.0,"text":"-"},{"bbox":{"x0":902,"x1":911,"y0":343,"y1":353},"font_size":0.0,"text":"-"},{"bbox":{"x0":234,"x1":303,"y0":360,"y1":385},"font_size":0.0,"text":"GPT-4"},{"bbox":{"x0":490,"x1":497,"y0":365,"y1":386},"font_size":10.0,"text":"-"},{"bbox":{"x0":545,"x1":612,"y0":360,"y1":384},"font_size":0.0,"text":" $92.0\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":360,"y1":384},"font_size":0.0,"text":" $52.9\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":365,"y1":386},"font_size":10.0,"text":"-"},{"bbox":{"x0":871,"x1":943,"y0":358,"y1":386},"font_size":0.0,"text":" $86.0\\%$ "},{"bbox":{"x0":235,"x1":349,"y0":389,"y1":410},"font_size":0.0,"text":"Infection-2"},{"bbox":{"x0":271,"x1":271,"y0":392,"y1":414},"font_size":10.0,"text":"l"},{"bbox":{"x0":490,"x1":497,"y0":392,"y1":414},"font_size":10.0,"text":"-"},{"bbox":{"x0":544,"x1":613,"y0":387,"y1":411},"font_size":0.0,"text":" $81.4\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":387,"y1":411},"font_size":0.0,"text":" $34.8\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":392,"y1":414},"font_size":10.0,"text":"-"},{"bbox":{"x0":899,"x1":913,"y0":395,"y1":408},"font_size":0.0,"text":"-"},{"bbox":{"x0":234,"x1":317,"y0":414,"y1":438},"font_size":0.0,"text":"GPT-3.5"},{"bbox":{"x0":490,"x1":497,"y0":419,"y1":441},"font_size":10.0,"text":"-"},{"bbox":{"x0":544,"x1":613,"y0":414,"y1":438},"font_size":0.0,"text":" $80.8\\%$ "},{"bbox":{"x0":649,"x1":718,"y0":414,"y1":438},"font_size":0.0,"text":" $34.1\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":419,"y1":441},"font_size":10.0,"text":"-"},{"bbox":{"x0":871,"x1":943,"y0":412,"y1":440},"font_size":0.0,"text":" $73.8\\%$ "},{"bbox":{"x0":234,"x1":350,"y0":441,"y1":466},"font_size":0.0,"text":"Gemini Pro"},{"bbox":{"x0":490,"x1":497,"y0":446,"y1":468},"font_size":10.0,"text":"-"},{"bbox":{"x0":544,"x1":613,"y0":441,"y1":465},"font_size":0.0,"text":" $86.5\\%$ "},{"bbox":{"x0":649,"x1":719,"y0":441,"y1":465},"font_size":0.0,"text":" $32.6\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":446,"y1":468},"font_size":10.0,"text":"-"},{"bbox":{"x0":902,"x1":911,"y0":451,"y1":461},"font_size":0.0,"text":"-"},{"bbox":{"x0":234,"x1":308,"y0":469,"y1":494},"font_size":0.0,"text":"Grok-1"},{"bbox":{"x0":490,"x1":497,"y0":473,"y1":495},"font_size":10.0,"text":"-"},{"bbox":{"x0":543,"x1":614,"y0":467,"y1":493},"font_size":0.0,"text":" $62.9\\%$ "},{"bbox":{"x0":648,"x1":719,"y0":467,"y1":493},"font_size":0.0,"text":" $23.9\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":473,"y1":495},"font_size":10.0,"text":"-"},{"bbox":{"x0":903,"x1":911,"y0":480,"y1":488},"font_size":0.0,"text":"-"},{"bbox":{"x0":234,"x1":348,"y0":505,"y1":530},"font_size":0.0,"text":"Baichuan-3"},{"bbox":{"x0":490,"x1":497,"y0":511,"y1":533},"font_size":10.0,"text":"-"},{"bbox":{"x0":543,"x1":614,"y0":505,"y1":530},"font_size":0.0,"text":" $88.2\\%$ "},{"bbox":{"x0":648,"x1":719,"y0":505,"y1":530},"font_size":0.0,"text":" $49.2\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":511,"y1":533},"font_size":10.0,"text":"-"},{"bbox":{"x0":903,"x1":911,"y0":511,"y1":533},"font_size":10.0,"text":"-"},{"bbox":{"x0":233,"x1":312,"y0":534,"y1":559},"font_size":0.0,"text":"GLM-4"},{"bbox":{"x0":490,"x1":497,"y0":538,"y1":560},"font_size":10.0,"text":"-"},{"bbox":{"x0":543,"x1":614,"y0":533,"y1":558},"font_size":0.0,"text":" $87.6\\%$ "},{"bbox":{"x0":648,"x1":720,"y0":533,"y1":558},"font_size":0.0,"text":" $47.9\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":538,"y1":560},"font_size":10.0,"text":"-"},{"bbox":{"x0":903,"x1":911,"y0":538,"y1":560},"font_size":10.0,"text":"-"},{"bbox":{"x0":496,"x1":695,"y0":571,"y1":599},"font_size":0.0,"text":"Open-Source Model"},{"bbox":{"x0":234,"x1":403,"y0":610,"y1":633},"font_size":0.0,"text":"InternLM2-Math"},{"bbox":{"x0":486,"x1":529,"y0":610,"y1":633},"font_size":0.0,"text":"20B"},{"bbox":{"x0":545,"x1":613,"y0":608,"y1":633},"font_size":0.0,"text":" $82.6\\%$ "},{"bbox":{"x0":649,"x1":718,"y0":608,"y1":634},"font_size":0.0,"text":" $37.7\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":614,"y1":636},"font_size":10.0,"text":"-"},{"bbox":{"x0":903,"x1":911,"y0":614,"y1":636},"font_size":10.0,"text":"-"},{"bbox":{"x0":234,"x1":300,"y0":639,"y1":664},"font_size":0.0,"text":"Qwen"},{"bbox":{"x0":485,"x1":530,"y0":636,"y1":663},"font_size":0.0,"text":"72B"},{"bbox":{"x0":545,"x1":612,"y0":637,"y1":661},"font_size":0.0,"text":" $78.9\\%$ "},{"bbox":{"x0":649,"x1":718,"y0":637,"y1":661},"font_size":0.0,"text":" $35.2\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":641,"y1":663},"font_size":10.0,"text":"-"},{"bbox":{"x0":903,"x1":911,"y0":641,"y1":663},"font_size":10.0,"text":"-"},{"bbox":{"x0":234,"x1":468,"y0":665,"y1":690},"font_size":0.0,"text":"Math-Shepherd-Mistral"},{"bbox":{"x0":485,"x1":519,"y0":665,"y1":688},"font_size":0.0,"text":"7B"},{"bbox":{"x0":545,"x1":613,"y0":664,"y1":688},"font_size":0.0,"text":" $84.1\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":664,"y1":688},"font_size":0.0,"text":" $33.0\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":669,"y1":690},"font_size":10.0,"text":"-"},{"bbox":{"x0":903,"x1":911,"y0":669,"y1":690},"font_size":10.0,"text":"-"},{"bbox":{"x0":235,"x1":408,"y0":692,"y1":716},"font_size":0.0,"text":"WizardMath-v1.1"},{"bbox":{"x0":485,"x1":518,"y0":691,"y1":716},"font_size":0.0,"text":"7B"},{"bbox":{"x0":545,"x1":613,"y0":691,"y1":715},"font_size":0.0,"text":" $83.2\\%$ "},{"bbox":{"x0":649,"x1":718,"y0":691,"y1":715},"font_size":0.0,"text":" $33.0\\%$ "},{"bbox":{"x0":795,"x1":806,"y0":701,"y1":710},"font_size":0.0,"text":"-"},{"bbox":{"x0":902,"x1":912,"y0":701,"y1":710},"font_size":0.0,"text":"-"},{"bbox":{"x0":235,"x1":443,"y0":720,"y1":744},"font_size":0.0,"text":"DeepSeek-LLM-Chat"},{"bbox":{"x0":486,"x1":527,"y0":720,"y1":742},"font_size":0.0,"text":"67B"},{"bbox":{"x0":546,"x1":613,"y0":718,"y1":742},"font_size":0.0,"text":" $84.1\\%$ "},{"bbox":{"x0":649,"x1":718,"y0":718,"y1":742},"font_size":0.0,"text":" $32.6\\%$ "},{"bbox":{"x0":766,"x1":835,"y0":717,"y1":742},"font_size":0.0,"text":" $74.0\\%$ "},{"bbox":{"x0":873,"x1":940,"y0":717,"y1":743},"font_size":0.0,"text":" $80.3\\%$ "},{"bbox":{"x0":235,"x1":341,"y0":747,"y1":768},"font_size":0.0,"text":"MetaMath"},{"bbox":{"x0":486,"x1":527,"y0":746,"y1":768},"font_size":0.0,"text":"70B"},{"bbox":{"x0":546,"x1":612,"y0":745,"y1":769},"font_size":0.0,"text":" $82.3\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":745,"y1":769},"font_size":0.0,"text":" $26.6\\%$ "},{"bbox":{"x0":766,"x1":834,"y0":745,"y1":769},"font_size":0.0,"text":" $66.4\\%$ "},{"bbox":{"x0":873,"x1":941,"y0":745,"y1":770},"font_size":0.0,"text":" $70.9\\%$ "},{"bbox":{"x0":234,"x1":351,"y0":773,"y1":797},"font_size":0.0,"text":"SeaLLM-v2"},{"bbox":{"x0":485,"x1":519,"y0":772,"y1":796},"font_size":0.0,"text":"7B"},{"bbox":{"x0":545,"x1":612,"y0":772,"y1":796},"font_size":0.0,"text":" $78.2\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":772,"y1":796},"font_size":0.0,"text":" $27.5\\%$ "},{"bbox":{"x0":766,"x1":835,"y0":772,"y1":797},"font_size":0.0,"text":" $64.8\\%$ "},{"bbox":{"x0":899,"x1":915,"y0":780,"y1":796},"font_size":0.0,"text":"-"},{"bbox":{"x0":235,"x1":349,"y0":800,"y1":824},"font_size":0.0,"text":"ChatGLM3"},{"bbox":{"x0":486,"x1":518,"y0":800,"y1":824},"font_size":0.0,"text":"6B"},{"bbox":{"x0":545,"x1":613,"y0":799,"y1":823},"font_size":0.0,"text":" $72.3\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":799,"y1":823},"font_size":0.0,"text":" $25.7\\%$ "},{"bbox":{"x0":791,"x1":809,"y0":805,"y1":822},"font_size":0.0,"text":"-"},{"bbox":{"x0":899,"x1":915,"y0":805,"y1":822},"font_size":0.0,"text":"-"},{"bbox":{"x0":234,"x1":409,"y0":825,"y1":852},"font_size":0.0,"text":"WizardMath-v1.0"},{"bbox":{"x0":486,"x1":527,"y0":827,"y1":851},"font_size":0.0,"text":"70B"},{"bbox":{"x0":545,"x1":613,"y0":826,"y1":851},"font_size":0.0,"text":" $81.6\\%$ "},{"bbox":{"x0":648,"x1":718,"y0":826,"y1":851},"font_size":0.0,"text":" $22.7\\%$ "},{"bbox":{"x0":765,"x1":836,"y0":824,"y1":852},"font_size":0.0,"text":" $64.8\\%$ "},{"bbox":{"x0":872,"x1":942,"y0":825,"y1":851},"font_size":0.0,"text":" $65.4\\%$ "},{"bbox":{"x0":236,"x1":485,"y0":866,"y1":890},"font_size":0.0,"text":"DeepSeekMath-Instruct"},{"bbox":{"x0":481,"x1":517,"y0":865,"y1":888},"font_size":0.0,"text":" 7B"},{"bbox":{"x0":545,"x1":613,"y0":863,"y1":888},"font_size":0.0,"text":" $82.9\\%$ "},{"bbox":{"x0":649,"x1":718,"y0":863,"y1":888},"font_size":0.0,"text":" $46.8\\%$ "},{"bbox":{"x0":765,"x1":836,"y0":863,"y1":888},"font_size":0.0,"text":" $73.2\\%$ "},{"bbox":{"x0":872,"x1":942,"y0":863,"y1":888},"font_size":0.0,"text":" $84.6\\%$ "},{"bbox":{"x0":236,"x1":431,"y0":894,"y1":918},"font_size":0.0,"text":"DeepSeekMath-RL"},{"bbox":{"x0":484,"x1":518,"y0":892,"y1":917},"font_size":0.0,"text":"7B"},{"bbox":{"x0":545,"x1":613,"y0":891,"y1":916},"font_size":0.0,"text":" $\\mathbf{88.2\\%}$ "},{"bbox":{"x0":649,"x1":719,"y0":891,"y1":916},"font_size":0.0,"text":" $\\textbf{51.7\\%}$ "},{"bbox":{"x0":765,"x1":835,"y0":891,"y1":916},"font_size":0.0,"text":" $79.6\\%$ "},{"bbox":{"x0":872,"x1":942,"y0":891,"y1":916},"font_size":0.0,"text":" $\\mathbf{88.8\\%}$ "},{"bbox":{"x0":458,"x1":729,"y0":932,"y1":963},"font_size":0.0,"text":"Tool-Integrated Reasoning"},{"bbox":{"x0":489,"x1":701,"y0":972,"y1":998},"font_size":0.0,"text":"Closed-Source Model"},{"bbox":{"x0":235,"x1":468,"y0":1012,"y1":1036},"font_size":0.0,"text":"GPT-4 Code Interpreter"},{"bbox":{"x0":490,"x1":497,"y0":1015,"y1":1037},"font_size":10.0,"text":"-"},{"bbox":{"x0":545,"x1":613,"y0":1010,"y1":1037},"font_size":0.0,"text":" $97.0\\%$ "},{"bbox":{"x0":650,"x1":718,"y0":1010,"y1":1037},"font_size":0.0,"text":" $69.7\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":1015,"y1":1037},"font_size":10.0,"text":"-"},{"bbox":{"x0":903,"x1":911,"y0":1015,"y1":1037},"font_size":10.0,"text":"-"},{"bbox":{"x0":493,"x1":695,"y0":1046,"y1":1078},"font_size":0.0,"text":"Open-Source Model"},{"bbox":{"x0":235,"x1":403,"y0":1088,"y1":1112},"font_size":0.0,"text":"InternLM2-Math"},{"bbox":{"x0":486,"x1":529,"y0":1087,"y1":1110},"font_size":0.0,"text":"20B"},{"bbox":{"x0":546,"x1":612,"y0":1086,"y1":1111},"font_size":0.0,"text":" $80.7\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":1086,"y1":1112},"font_size":0.0,"text":" $54.3\\%$ "},{"bbox":{"x0":796,"x1":804,"y0":1092,"y1":1113},"font_size":10.0,"text":"-"},{"bbox":{"x0":904,"x1":910,"y0":1099,"y1":1106},"font_size":0.0,"text":"-"},{"bbox":{"x0":234,"x1":445,"y0":1115,"y1":1139},"font_size":0.0,"text":"DeepSeek-LLM-Chat"},{"bbox":{"x0":486,"x1":527,"y0":1115,"y1":1138},"font_size":0.0,"text":"67B"},{"bbox":{"x0":546,"x1":612,"y0":1114,"y1":1138},"font_size":0.0,"text":" $86.7\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":1114,"y1":1138},"font_size":0.0,"text":" $51.1\\%$ "},{"bbox":{"x0":766,"x1":834,"y0":1113,"y1":1138},"font_size":0.0,"text":" $76.4\\%$ "},{"bbox":{"x0":873,"x1":941,"y0":1113,"y1":1138},"font_size":0.0,"text":" $85.4\\%$ "},{"bbox":{"x0":233,"x1":296,"y0":1141,"y1":1166},"font_size":0.0,"text":"ToRA"},{"bbox":{"x0":485,"x1":529,"y0":1141,"y1":1167},"font_size":0.0,"text":"34B"},{"bbox":{"x0":546,"x1":612,"y0":1141,"y1":1165},"font_size":0.0,"text":" $80.7\\%$ "},{"bbox":{"x0":650,"x1":717,"y0":1141,"y1":1165},"font_size":0.0,"text":" $50.8\\%$ "},{"bbox":{"x0":766,"x1":834,"y0":1141,"y1":1166},"font_size":0.0,"text":" $41.2\\%$ "},{"bbox":{"x0":873,"x1":941,"y0":1141,"y1":1166},"font_size":0.0,"text":" $53.4\\%$ "},{"bbox":{"x0":234,"x1":360,"y0":1168,"y1":1193},"font_size":0.0,"text":"MAmmoTH"},{"bbox":{"x0":485,"x1":527,"y0":1169,"y1":1193},"font_size":0.0,"text":"70B"},{"bbox":{"x0":545,"x1":613,"y0":1168,"y1":1193},"font_size":0.0,"text":" $76.9\\%$ "},{"bbox":{"x0":649,"x1":717,"y0":1168,"y1":1193},"font_size":0.0,"text":" $41.8\\%$ "},{"bbox":{"x0":793,"x1":808,"y0":1175,"y1":1192},"font_size":0.0,"text":"-"},{"bbox":{"x0":899,"x1":915,"y0":1175,"y1":1190},"font_size":0.0,"text":"-"},{"bbox":{"x0":234,"x1":489,"y0":1207,"y1":1234},"font_size":0.0,"text":"DeepSeekMath-Instruct "},{"bbox":{"x0":477,"x1":517,"y0":1208,"y1":1231},"font_size":0.0,"text":"7B"},{"bbox":{"x0":545,"x1":613,"y0":1205,"y1":1230},"font_size":0.0,"text":" $83.7\\%$ "},{"bbox":{"x0":649,"x1":718,"y0":1205,"y1":1230},"font_size":0.0,"text":" $57.4\\%$ "},{"bbox":{"x0":766,"x1":834,"y0":1205,"y1":1230},"font_size":0.0,"text":" $72.0\\%$ "},{"bbox":{"x0":874,"x1":941,"y0":1205,"y1":1230},"font_size":0.0,"text":" $84.3\\%$ "},{"bbox":{"x0":235,"x1":431,"y0":1236,"y1":1260},"font_size":0.0,"text":"DeepSeekMath-RL"},{"bbox":{"x0":484,"x1":520,"y0":1232,"y1":1260},"font_size":0.0,"text":"7B"},{"bbox":{"x0":545,"x1":612,"y0":1234,"y1":1258},"font_size":0.0,"text":" $\\mathbf{86.7\\%}$ "},{"bbox":{"x0":650,"x1":717,"y0":1233,"y1":1258},"font_size":0.0,"text":" $\\textbf{58.8\\%}$ "},{"bbox":{"x0":767,"x1":833,"y0":1234,"y1":1258},"font_size":0.0,"text":" $78.4\\%$ "},{"bbox":{"x0":874,"x1":941,"y0":1234,"y1":1258},"font_size":0.0,"text":" $\\textbf{87.6\\%}$ "}],"source":"layout det","text":"<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"2\">English Benchmarks</td><td colspan=\"2\"> Chinese Benchmarks</td></tr><tr><td>GSM8K</td><td>MATH</td><td>MGSM-zh </td><td>CMATH</td></tr></thead><tbody><tr><td colspan=\"7\">Chain-of-Thought Reasoning</td></tr><tr><td></td><td></td><td></td><td>Closed-Source Model</td><td></td><td></td><td></td></tr><tr><td>Gemini Ultra</td><td>-</td><td> $94.4\\%$ </td><td></td><td> $53.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>GPT-4</td><td>-</td><td></td><td> $92.0\\%$ </td><td> $52.9\\%$ </td><td>-</td><td> $86.0\\%$ </td></tr><tr><td>Infection-2</td><td>-</td><td></td><td> $81.4\\%$ </td><td> $34.8\\%$ </td><td>-</td><td>-</td></tr><tr><td>GPT-3.5</td><td>-</td><td></td><td> $80.8\\%$ </td><td> $34.1\\%$ </td><td>-</td><td> $73.8\\%$ </td></tr><tr><td>Gemini Pro</td><td>-</td><td></td><td> $86.5\\%$ </td><td> $32.6\\%$ </td><td>-</td><td>-</td></tr><tr><td>Grok-1</td><td>-</td><td></td><td> $62.9\\%$ </td><td> $23.9\\%$ </td><td>-</td><td>-</td></tr><tr><td>Baichuan-3</td><td>-</td><td></td><td> $88.2\\%$ </td><td> $49.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>GLM-4</td><td>-</td><td></td><td> $87.6\\%$ </td><td> $47.9\\%$ </td><td>-</td><td>-</td></tr><tr><td colspan=\"7\">Open-Source Model</td></tr><tr><td>InternLM2-Math</td><td>20B</td><td> $82.6\\%$ </td><td></td><td> $37.7\\%$ </td><td>-</td><td>-</td></tr><tr><td>Qwen</td><td></td><td>72B</td><td> $78.9\\%$ </td><td> $35.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>Math-Shepherd-Mistral</td><td>7B</td><td></td><td> $84.1\\%$ </td><td> $33.0\\%$ </td><td>-</td><td>-</td></tr><tr><td>WizardMath-v1.1</td><td>7B</td><td></td><td> $83.2\\%$ </td><td> $33.0\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeek-LLM-Chat</td><td>67B</td><td></td><td> $84.1\\%$ </td><td> $32.6\\%$ </td><td> $74.0\\%$ </td><td> $80.3\\%$ </td></tr><tr><td>MetaMath</td><td></td><td>70B</td><td> $82.3\\%$ </td><td> $26.6\\%$ </td><td> $66.4\\%$ </td><td> $70.9\\%$ </td></tr><tr><td>SeaLLM-v2</td><td>7B</td><td></td><td> $78.2\\%$ </td><td> $27.5\\%$ </td><td> $64.8\\%$ </td><td>-</td></tr><tr><td>ChatGLM3</td><td>6B</td><td></td><td> $72.3\\%$ </td><td> $25.7\\%$ </td><td>-</td><td>-</td></tr><tr><td>WizardMath-v1.0</td><td>70B</td><td></td><td> $81.6\\%$ </td><td> $22.7\\%$ </td><td> $64.8\\%$ </td><td> $65.4\\%$ </td></tr><tr><td>DeepSeekMath-Instruct</td><td> 7B</td><td></td><td> $82.9\\%$ </td><td> $46.8\\%$ </td><td> $73.2\\%$ </td><td> $84.6\\%$ </td></tr><tr><td>DeepSeekMath-RL</td><td>7B</td><td></td><td> $\\mathbf{88.2\\%}$ </td><td> $\\textbf{51.7\\%}$ </td><td> $79.6\\%$ </td><td> $\\mathbf{88.8\\%}$ </td></tr><tr><td colspan=\"7\">Tool-Integrated Reasoning</td></tr><tr><td></td><td></td><td></td><td>Closed-Source Model</td><td></td><td></td><td></td></tr><tr><td>GPT-4 Code Interpreter</td><td>-</td><td></td><td> $97.0\\%$ </td><td> $69.7\\%$ </td><td>-</td><td>-</td></tr><tr><td colspan=\"7\">Open-Source Model</td></tr><tr><td>InternLM2-Math</td><td></td><td>20B  $80.7\\%$ </td><td></td><td> $54.3\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeek-LLM-Chat</td><td></td><td>67B</td><td> $86.7\\%$ </td><td> $51.1\\%$ </td><td> $76.4\\%$ </td><td> $85.4\\%$ </td></tr><tr><td>ToRA</td><td></td><td>34B</td><td> $80.7\\%$ </td><td> $50.8\\%$ </td><td> $41.2\\%$ </td><td> $53.4\\%$ </td></tr><tr><td>MAmmoTH</td><td>70B</td><td></td><td> $76.9\\%$ </td><td> $41.8\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeekMath-Instruct </td><td>7B</td><td></td><td> $83.7\\%$ </td><td> $57.4\\%$ </td><td> $72.0\\%$ </td><td> $84.3\\%$ </td></tr><tr><td>DeepSeekMath-RL</td><td>7B</td><td></td><td> $\\mathbf{86.7\\%}$ </td><td> $\\textbf{58.8\\%}$ </td><td> $78.4\\%$ </td><td> $\\textbf{87.6\\%}$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":131,"x1":1062,"y0":1282,"y1":1456},"conf":0.9422,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1286,"y1":1310},"font_size":0.0,"text":"Table 5 | Performance of Open- and Closed-Source models with both Chain-of-Thought and"},{"bbox":{"x0":135,"x1":1051,"y0":1310,"y1":1342},"font_size":0.0,"text":"Tool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority"},{"bbox":{"x0":136,"x1":1051,"y0":1340,"y1":1365},"font_size":0.0,"text":"votes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all open-"},{"bbox":{"x0":136,"x1":1051,"y0":1365,"y1":1395},"font_size":0.0,"text":"source models from 7B to 70B, as well as the majority of closed-source models. Although"},{"bbox":{"x0":138,"x1":1051,"y0":1395,"y1":1419},"font_size":0.0,"text":"DeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data"},{"bbox":{"x0":138,"x1":978,"y0":1421,"y1":1445},"font_size":0.0,"text":"of GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks."}],"source":"layout det","text":"Table 5 | Performance of Open- and Closed-Source models with both Chain-of-Thought and Tool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority votes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all opensource models from 7B to 70B, as well as the majority of closed-source models. Although DeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks."}],"formula_dets":[{"bbox":{"x0":766,"x1":834,"y0":745,"y1":769},"conf":0.867,"label":"print_embedding","label_id":0},{"bbox":{"x0":872,"x1":942,"y0":863,"y1":888},"conf":0.867,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":718,"y0":718,"y1":742},"conf":0.8641,"label":"print_embedding","label_id":0},{"bbox":{"x0":765,"x1":836,"y0":863,"y1":888},"conf":0.8638,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":745,"y1":769},"conf":0.8626,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":718,"y0":691,"y1":715},"conf":0.8616,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":664,"y1":688},"conf":0.8566,"label":"print_embedding","label_id":0},{"bbox":{"x0":873,"x1":940,"y0":717,"y1":743},"conf":0.8562,"label":"print_embedding","label_id":0},{"bbox":{"x0":873,"x1":941,"y0":745,"y1":770},"conf":0.8551,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":719,"y0":441,"y1":465},"conf":0.8544,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":612,"y0":772,"y1":796},"conf":0.8543,"label":"print_embedding","label_id":0},{"bbox":{"x0":546,"x1":612,"y0":745,"y1":769},"conf":0.8543,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":772,"y1":796},"conf":0.8518,"label":"print_embedding","label_id":0},{"bbox":{"x0":648,"x1":719,"y0":467,"y1":493},"conf":0.8499,"label":"print_embedding","label_id":0},{"bbox":{"x0":650,"x1":717,"y0":1141,"y1":1165},"conf":0.8478,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":799,"y1":823},"conf":0.8474,"label":"print_embedding","label_id":0},{"bbox":{"x0":872,"x1":942,"y0":891,"y1":916},"conf":0.8454,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":863,"y1":888},"conf":0.8452,"label":"print_embedding","label_id":0},{"bbox":{"x0":766,"x1":835,"y0":772,"y1":797},"conf":0.8446,"label":"print_embedding","label_id":0},{"bbox":{"x0":546,"x1":612,"y0":1141,"y1":1165},"conf":0.8433,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":691,"y1":715},"conf":0.8422,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":799,"y1":823},"conf":0.8419,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":718,"y0":863,"y1":888},"conf":0.8418,"label":"print_embedding","label_id":0},{"bbox":{"x0":765,"x1":835,"y0":891,"y1":916},"conf":0.84,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":387,"y1":411},"conf":0.84,"label":"print_embedding","label_id":0},{"bbox":{"x0":546,"x1":613,"y0":718,"y1":742},"conf":0.8394,"label":"print_embedding","label_id":0},{"bbox":{"x0":648,"x1":720,"y0":533,"y1":558},"conf":0.8378,"label":"print_embedding","label_id":0},{"bbox":{"x0":544,"x1":613,"y0":441,"y1":465},"conf":0.8374,"label":"print_embedding","label_id":0},{"bbox":{"x0":543,"x1":614,"y0":467,"y1":493},"conf":0.8369,"label":"print_embedding","label_id":0},{"bbox":{"x0":766,"x1":835,"y0":717,"y1":742},"conf":0.8354,"label":"print_embedding","label_id":0},{"bbox":{"x0":544,"x1":613,"y0":414,"y1":438},"conf":0.8353,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":612,"y0":637,"y1":661},"conf":0.8346,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":360,"y1":384},"conf":0.8343,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":718,"y0":1205,"y1":1230},"conf":0.8336,"label":"print_embedding","label_id":0},{"bbox":{"x0":766,"x1":834,"y0":1205,"y1":1230},"conf":0.8334,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":718,"y0":414,"y1":438},"conf":0.8326,"label":"print_embedding","label_id":0},{"bbox":{"x0":765,"x1":836,"y0":824,"y1":852},"conf":0.8322,"label":"print_embedding","label_id":0},{"bbox":{"x0":546,"x1":612,"y0":1114,"y1":1138},"conf":0.8322,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":664,"y1":688},"conf":0.8321,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":1114,"y1":1138},"conf":0.8287,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":718,"y0":637,"y1":661},"conf":0.8283,"label":"print_embedding","label_id":0},{"bbox":{"x0":874,"x1":941,"y0":1205,"y1":1230},"conf":0.8271,"label":"print_embedding","label_id":0},{"bbox":{"x0":872,"x1":942,"y0":825,"y1":851},"conf":0.8262,"label":"print_embedding","label_id":0},{"bbox":{"x0":766,"x1":834,"y0":1141,"y1":1166},"conf":0.8258,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":612,"y0":360,"y1":384},"conf":0.8243,"label":"print_embedding","label_id":0},{"bbox":{"x0":648,"x1":719,"y0":505,"y1":530},"conf":0.8239,"label":"print_embedding","label_id":0},{"bbox":{"x0":873,"x1":941,"y0":1141,"y1":1166},"conf":0.8234,"label":"print_embedding","label_id":0},{"bbox":{"x0":648,"x1":718,"y0":826,"y1":851},"conf":0.8218,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":1168,"y1":1193},"conf":0.8211,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":891,"y1":916},"conf":0.8198,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":826,"y1":851},"conf":0.8191,"label":"print_embedding","label_id":0},{"bbox":{"x0":543,"x1":614,"y0":533,"y1":558},"conf":0.8189,"label":"print_embedding","label_id":0},{"bbox":{"x0":544,"x1":613,"y0":387,"y1":411},"conf":0.8188,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":1205,"y1":1230},"conf":0.8185,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":718,"y0":608,"y1":634},"conf":0.8175,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":1086,"y1":1112},"conf":0.8167,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":608,"y1":633},"conf":0.8156,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":719,"y0":891,"y1":916},"conf":0.8155,"label":"print_embedding","label_id":0},{"bbox":{"x0":766,"x1":834,"y0":1113,"y1":1138},"conf":0.8152,"label":"print_embedding","label_id":0},{"bbox":{"x0":649,"x1":717,"y0":332,"y1":357},"conf":0.8146,"label":"print_embedding","label_id":0},{"bbox":{"x0":543,"x1":614,"y0":505,"y1":530},"conf":0.8145,"label":"print_embedding","label_id":0},{"bbox":{"x0":873,"x1":941,"y0":1113,"y1":1138},"conf":0.8122,"label":"print_embedding","label_id":0},{"bbox":{"x0":874,"x1":941,"y0":1234,"y1":1258},"conf":0.8103,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":612,"y0":332,"y1":357},"conf":0.8089,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":1168,"y1":1193},"conf":0.8065,"label":"print_embedding","label_id":0},{"bbox":{"x0":546,"x1":612,"y0":1086,"y1":1111},"conf":0.8042,"label":"print_embedding","label_id":0},{"bbox":{"x0":650,"x1":717,"y0":1233,"y1":1258},"conf":0.8034,"label":"print_embedding","label_id":0},{"bbox":{"x0":871,"x1":943,"y0":358,"y1":386},"conf":0.7961,"label":"print_embedding","label_id":0},{"bbox":{"x0":650,"x1":718,"y0":1010,"y1":1037},"conf":0.7956,"label":"print_embedding","label_id":0},{"bbox":{"x0":767,"x1":833,"y0":1234,"y1":1258},"conf":0.7915,"label":"print_embedding","label_id":0},{"bbox":{"x0":871,"x1":943,"y0":412,"y1":440},"conf":0.7858,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":612,"y0":1234,"y1":1258},"conf":0.7843,"label":"print_embedding","label_id":0},{"bbox":{"x0":545,"x1":613,"y0":1010,"y1":1037},"conf":0.7788,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":220,"x1":971,"y0":162,"y1":1275},"conf":0.9741,"label":"Table","label_id":5},{"bbox":{"x0":131,"x1":1062,"y0":1282,"y1":1456},"conf":0.9422,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7891,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1575],[580,1575]],"score":0.8803},{"poly":[[138,1421],[978,1421],[978,1445],[138,1445]],"score":0.8228},{"poly":[[138,1395],[1051,1395],[1051,1419],[138,1419]],"score":0.806},{"poly":[[136,1365],[1051,1365],[1051,1395],[136,1395]],"score":0.709},{"poly":[[136,1340],[1051,1340],[1051,1365],[136,1365]],"score":0.7848},{"poly":[[135,1310],[1051,1312],[1051,1342],[135,1340]],"score":0.7665},{"poly":[[138,1286],[1049,1286],[1049,1310],[138,1310]],"score":0.8479},{"poly":[[232,1231],[432,1231],[432,1261],[232,1261]],"score":0.7503},{"poly":[[870,1230],[943,1230],[943,1261],[870,1261]],"score":0.895},{"poly":[[542,1230],[613,1230],[613,1261],[542,1261]],"score":0.8558},{"poly":[[764,1228],[835,1228],[835,1261],[764,1261]],"score":0.781},{"poly":[[647,1228],[716,1228],[716,1261],[647,1261]],"score":0.7739},{"poly":[[482,1230],[519,1230],[519,1259],[482,1259]],"score":0.8683},{"poly":[[874,1205],[941,1205],[941,1231],[874,1231]],"score":0.9483},{"poly":[[767,1205],[833,1205],[833,1231],[767,1231]],"score":0.9567},{"poly":[[650,1205],[716,1205],[716,1231],[650,1231]],"score":0.9555},{"poly":[[542,1203],[613,1203],[613,1235],[542,1235]],"score":0.861},{"poly":[[232,1203],[519,1203],[519,1233],[232,1233]],"score":0.7377},{"poly":[[796,1177],[805,1177],[805,1186],[796,1186]],"score":0.7463},{"poly":[[649,1167],[714,1167],[714,1193],[649,1193]],"score":0.9484},{"poly":[[540,1161],[614,1165],[612,1197],[538,1192]],"score":0.771},{"poly":[[484,1167],[533,1167],[533,1195],[484,1195]],"score":0.8472},{"poly":[[229,1163],[362,1163],[362,1193],[229,1193]],"score":0.7039},{"poly":[[528,1145],[544,1145],[544,1160],[528,1160]],"score":0.6021},{"poly":[[874,1140],[941,1140],[941,1167],[874,1167]],"score":0.9378},{"poly":[[766,1140],[833,1140],[833,1167],[766,1167]],"score":0.9537},{"poly":[[649,1140],[716,1140],[716,1167],[649,1167]],"score":0.9108},{"poly":[[542,1140],[611,1140],[611,1167],[542,1167]],"score":0.9408},{"poly":[[232,1140],[296,1140],[296,1167],[232,1167]],"score":0.9611},{"poly":[[482,1138],[535,1138],[535,1167],[482,1167]],"score":0.8165},{"poly":[[528,1119],[544,1119],[544,1133],[528,1133]],"score":0.633},{"poly":[[232,1112],[447,1112],[447,1142],[232,1142]],"score":0.819},{"poly":[[872,1110],[941,1110],[941,1142],[872,1142]],"score":0.872},{"poly":[[764,1106],[835,1111],[833,1143],[761,1138]],"score":0.8382},{"poly":[[647,1110],[718,1110],[718,1142],[647,1142]],"score":0.8373},{"poly":[[541,1110],[613,1110],[613,1142],[541,1142]],"score":0.7969},{"poly":[[482,1112],[535,1112],[535,1140],[482,1140]],"score":0.8633},{"poly":[[528,1093],[548,1093],[548,1105],[528,1105]],"score":0.7802},{"poly":[[540,1080],[614,1085],[612,1116],[538,1112]],"score":0.864},{"poly":[[232,1086],[402,1086],[402,1110],[232,1110]],"score":0.9043},{"poly":[[649,1082],[718,1082],[718,1114],[649,1114]],"score":0.8599},{"poly":[[484,1084],[535,1084],[535,1112],[484,1112]],"score":0.8206},{"poly":[[491,1047],[698,1045],[698,1075],[491,1077]],"score":0.8174},{"poly":[[650,1010],[716,1010],[716,1037],[650,1037]],"score":0.9537},{"poly":[[542,1009],[613,1009],[613,1040],[542,1040]],"score":0.8594},{"poly":[[231,1007],[472,1011],[471,1039],[230,1035]],"score":0.8339},{"poly":[[487,974],[700,974],[700,996],[487,996]],"score":0.9719},{"poly":[[458,930],[732,933],[732,963],[457,959]],"score":0.7894},{"poly":[[232,889],[432,889],[432,919],[232,919]],"score":0.8054},{"poly":[[872,888],[943,888],[943,919],[872,919]],"score":0.8651},{"poly":[[764,888],[835,888],[835,919],[764,919]],"score":0.8375},{"poly":[[647,888],[718,888],[718,919],[647,919]],"score":0.8638},{"poly":[[542,888],[613,888],[613,919],[542,919]],"score":0.846},{"poly":[[482,888],[521,888],[521,919],[482,919]],"score":0.7708},{"poly":[[542,861],[613,861],[613,893],[542,893]],"score":0.8637},{"poly":[[232,861],[521,861],[521,891],[232,891]],"score":0.7546},{"poly":[[872,860],[941,860],[941,891],[872,891]],"score":0.8679},{"poly":[[766,860],[833,860],[833,891],[766,891]],"score":0.9295},{"poly":[[647,860],[716,860],[716,891],[647,891]],"score":0.8705},{"poly":[[234,826],[409,826],[409,851],[234,851]],"score":0.9242},{"poly":[[872,823],[941,823],[941,854],[872,854]],"score":0.8511},{"poly":[[766,823],[833,823],[833,854],[766,854]],"score":0.872},{"poly":[[647,823],[716,823],[716,854],[647,854]],"score":0.9044},{"poly":[[540,819],[614,823],[612,855],[538,850]],"score":0.7395},{"poly":[[482,821],[535,821],[535,854],[482,854]],"score":0.8029},{"poly":[[230,796],[353,796],[353,826],[230,826]],"score":0.79},{"poly":[[649,795],[716,795],[716,826],[649,826]],"score":0.8924},{"poly":[[544,795],[613,795],[613,826],[544,826]],"score":0.86},{"poly":[[482,796],[521,796],[521,824],[482,824]],"score":0.8967},{"poly":[[766,768],[835,768],[835,800],[766,800]],"score":0.826},{"poly":[[230,768],[353,768],[353,798],[230,798]],"score":0.8625},{"poly":[[649,767],[716,767],[716,798],[649,798]],"score":0.8789},{"poly":[[542,764],[614,769],[612,802],[540,798]],"score":0.7864},{"poly":[[482,768],[521,768],[521,798],[482,798]],"score":0.8666},{"poly":[[232,746],[344,746],[344,770],[232,770]],"score":0.9342},{"poly":[[767,744],[831,744],[831,770],[767,770]],"score":0.9233},{"poly":[[650,744],[714,744],[714,770],[650,770]],"score":0.977},{"poly":[[542,744],[611,744],[611,770],[542,770]],"score":0.9256},{"poly":[[872,738],[942,743],[940,774],[870,769]],"score":0.8204},{"poly":[[484,742],[532,742],[532,770],[484,770]],"score":0.8603},{"poly":[[767,717],[831,717],[831,744],[767,744]],"score":0.8733},{"poly":[[649,717],[714,717],[714,744],[649,744]],"score":0.8743},{"poly":[[542,717],[611,717],[611,744],[542,744]],"score":0.8504},{"poly":[[482,717],[533,717],[533,744],[482,744]],"score":0.871},{"poly":[[232,716],[447,716],[447,746],[232,746]],"score":0.7792},{"poly":[[872,714],[941,714],[941,746],[872,746]],"score":0.8161},{"poly":[[234,691],[408,691],[408,716],[234,716]],"score":0.943},{"poly":[[650,689],[716,689],[716,716],[650,716]],"score":0.9141},{"poly":[[542,688],[613,688],[613,719],[542,719]],"score":0.8548},{"poly":[[482,688],[521,688],[521,717],[482,717]],"score":0.8506},{"poly":[[232,665],[470,665],[470,689],[232,689]],"score":0.8635},{"poly":[[649,663],[716,663],[716,689],[649,689]],"score":0.926},{"poly":[[542,663],[611,663],[611,689],[542,689]],"score":0.9449},{"poly":[[479,663],[521,663],[521,689],[479,689]],"score":0.8886},{"poly":[[232,637],[301,637],[301,663],[232,663]],"score":0.8844},{"poly":[[647,633],[718,633],[718,665],[647,665]],"score":0.8383},{"poly":[[540,629],[614,634],[612,665],[538,661]],"score":0.7938},{"poly":[[481,629],[538,634],[535,666],[478,660]],"score":0.7795},{"poly":[[542,609],[611,609],[611,635],[542,635]],"score":0.9091},{"poly":[[232,609],[402,609],[402,632],[232,632]],"score":0.9684},{"poly":[[647,605],[718,605],[718,637],[647,637]],"score":0.8532},{"poly":[[482,605],[549,605],[549,637],[482,637]],"score":0.7012},{"poly":[[493,570],[698,570],[698,600],[493,600]],"score":0.8218},{"poly":[[647,530],[716,530],[716,561],[647,561]],"score":0.9398},{"poly":[[544,530],[613,530],[613,561],[544,561]],"score":0.8972},{"poly":[[230,527],[312,532],[311,563],[228,559]],"score":0.8084},{"poly":[[234,505],[349,505],[349,530],[234,530]],"score":0.9088},{"poly":[[647,502],[718,502],[718,533],[647,533]],"score":0.8767},{"poly":[[544,498],[614,502],[612,536],[542,531]],"score":0.7931},{"poly":[[650,467],[714,467],[714,493],[650,493]],"score":0.9825},{"poly":[[544,465],[613,465],[613,496],[544,496]],"score":0.8142},{"poly":[[232,467],[308,467],[308,493],[232,493]],"score":0.8861},{"poly":[[649,440],[716,440],[716,467],[649,467]],"score":0.9375},{"poly":[[544,440],[611,440],[611,467],[544,467]],"score":0.8783},{"poly":[[231,438],[353,442],[352,467],[230,463]],"score":0.8221},{"poly":[[872,410],[943,410],[943,442],[872,442]],"score":0.8518},{"poly":[[647,406],[718,411],[716,443],[644,438]],"score":0.8027},{"poly":[[546,412],[611,412],[611,439],[546,439]],"score":0.8648},{"poly":[[234,412],[319,412],[319,439],[234,439]],"score":0.9787},{"poly":[[544,386],[611,386],[611,412],[544,412]],"score":0.9068},{"poly":[[229,384],[351,384],[351,414],[229,414]],"score":0.7796},{"poly":[[647,382],[718,382],[718,414],[647,414]],"score":0.8126},{"poly":[[650,360],[716,360],[716,386],[650,386]],"score":0.8693},{"poly":[[544,360],[611,360],[611,386],[544,386]],"score":0.8413},{"poly":[[232,360],[305,360],[305,386],[232,386]],"score":0.8489},{"poly":[[872,356],[941,356],[941,388],[872,388]],"score":0.8312},{"poly":[[902,342],[913,342],[913,351],[902,351]],"score":0.6623},{"poly":[[232,333],[369,333],[369,358],[232,358]],"score":0.8876},{"poly":[[650,332],[714,332],[714,358],[650,358]],"score":0.9666},{"poly":[[544,326],[614,330],[612,362],[542,357]],"score":0.7939},{"poly":[[486,293],[698,293],[698,318],[486,318]],"score":0.7084},{"poly":[[443,252],[746,256],[746,286],[443,282]],"score":0.7751},{"poly":[[743,212],[952,212],[952,237],[743,237]],"score":0.8577},{"poly":[[643,212],[723,212],[723,239],[643,239]],"score":0.8208},{"poly":[[537,212],[622,212],[622,237],[537,237]],"score":0.966},{"poly":[[486,188],[537,188],[537,214],[486,214]],"score":0.8394},{"poly":[[232,188],[305,188],[305,214],[232,214]],"score":0.9533},{"poly":[[535,175],[957,170],[957,200],[535,205]],"score":0.7294}],"page_no":11,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7136,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1577},"font_size":0.0,"text":"13"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":159,"x1":1030,"y0":172,"y1":544},"conf":0.9762,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![38c1c84aacaba6c308ab5f1d6b92e40b](imgs/38c1c84aacaba6c308ab5f1d6b92e40b.jpg)"},{"bbox":{"x0":133,"x1":1055,"y0":549,"y1":614},"conf":0.9561,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":551,"y1":577},"font_size":0.0,"text":"Figure 4 | Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead"},{"bbox":{"x0":140,"x1":948,"y0":582,"y1":607},"font_size":0.0,"text":"estimating the baseline from group scores, signifcantly reducing training resources.i"}],"source":"layout det","text":"Figure 4 | Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead estimating the baseline from group scores, signifcantly reducing training resources.i"},{"bbox":{"x0":133,"x1":1059,"y0":643,"y1":762},"conf":0.9632,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":647,"y1":676},"font_size":0.0,"text":"on the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$  Thus, in PPO, a value function needs to"},{"bbox":{"x0":138,"x1":1051,"y0":675,"y1":700},"font_size":0.0,"text":"be trained alongside the policy model and to mitigate over-optimization of the reward model,"},{"bbox":{"x0":138,"x1":1051,"y0":703,"y1":728},"font_size":0.0,"text":"the standard approach is to add a per-token KL penalty from a reference model in the reward at"},{"bbox":{"x0":138,"x1":494,"y0":731,"y1":754},"font_size":0.0,"text":"each token (Ouyang et al., 2022), i.e.,"}],"source":"layout det","text":"on the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$  Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model,the standard approach is to add a per-token KL penalty from a reference model in the reward at each token (Ouyang et al., 2022), i.e.,"},{"bbox":{"x0":425,"x1":762,"y0":772,"y1":830},"conf":0.9551,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$r_{t}=r_{\\varphi}(\\mathit{q},\\mathit{o}_{\\leq t})-\\beta\\log\\frac{\\pi_{\\theta} (\\mathit{o}_{t}|\\mathit{q},\\mathit{o}_{<t})}{\\pi_{\\mathit{ref}}(\\mathit{o}_{t}|\\mathit{q},\\mathit{o}_{<t})},$$"},{"bbox":{"x0":1019,"x1":1056,"y0":784,"y1":817},"conf":0.8209,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1019,"x1":1056,"y0":784,"y1":816},"font_size":0.0,"text":"(2)"}],"source":"layout det","text":"(2)"},{"bbox":{"x0":132,"x1":1059,"y0":841,"y1":904},"conf":0.9343,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":846,"y1":874},"font_size":0.0,"text":"where $r_{\\varphi}$ is the reward model, $\\pi_{ref}$ is the reference model, which is usually the initial SFT model,"},{"bbox":{"x0":138,"x1":533,"y0":872,"y1":898},"font_size":0.0,"text":"and $\\beta$ is the coeffcient of the KL penalty.i"}],"source":"layout det","text":"where $r_{\\varphi}$ is the reward model, $\\pi_{ref}$ is the reference model, which is usually the initial SFT model,and $\\beta$ is the coeffcient of the KL penalty.i"},{"bbox":{"x0":132,"x1":1059,"y0":909,"y1":1216},"conf":0.9781,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":910,"y1":935},"font_size":0.0,"text":"As the value function employed in PPO is typically another model of comparable size as"},{"bbox":{"x0":140,"x1":1051,"y0":940,"y1":965},"font_size":0.0,"text":"the policy model, it brings a substantial memory and computational burden. Additionally,"},{"bbox":{"x0":136,"x1":1051,"y0":963,"y1":995},"font_size":0.0,"text":"during RL training, the value function is treated as a baseline in the calculation of the advantage"},{"bbox":{"x0":138,"x1":1051,"y0":995,"y1":1017},"font_size":0.0,"text":"for variance reduction. While in the LLM context, usually only the last token is assigned a"},{"bbox":{"x0":138,"x1":1051,"y0":1021,"y1":1045},"font_size":0.0,"text":"reward score by the reward model, which may complicate the training of a value function that is"},{"bbox":{"x0":138,"x1":1049,"y0":1049,"y1":1074},"font_size":0.0,"text":"accurate at each token. To address this, as shown in Figure $4,$  we propose Group Relative Policy"},{"bbox":{"x0":136,"x1":1051,"y0":1072,"y1":1103},"font_size":0.0,"text":"Optimization (GRPO), which obviates the need for additional value function approximation as"},{"bbox":{"x0":138,"x1":1049,"y0":1102,"y1":1126},"font_size":0.0,"text":"in PPO, and instead uses the average reward of multiple sampled outputs, produced in response"},{"bbox":{"x0":138,"x1":1051,"y0":1130,"y1":1156},"font_size":0.0,"text":"to the same question, as the baseline. More specifcally, for each questioni $q,$  GRPO samples a"},{"bbox":{"x0":138,"x1":1051,"y0":1156,"y1":1186},"font_size":0.0,"text":"group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model"},{"bbox":{"x0":138,"x1":519,"y0":1184,"y1":1209},"font_size":0.0,"text":"by maximizing the following objective:"}],"source":"layout det","text":"As the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. Additionally,during RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token. To address this, as shown in Figure $4,$  we propose Group Relative Policy Optimization (GRPO), which obviates the need for additional value function approximation as in PPO, and instead uses the average reward of multiple sampled outputs, produced in response to the same question, as the baseline. More specifcally, for each questioni $q,$  GRPO samples a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model by maximizing the following objective:"},{"bbox":{"x0":160,"x1":506,"y0":1222,"y1":1251},"conf":0.8545,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P( \\small{\\it Q}),\\{o_{i} \\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}( \\small{\\it O}|q)]$$"},{"bbox":{"x0":226,"x1":1010,"y0":1258,"y1":1318},"conf":0.822,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|} \\left\\{\\min\\left[\\frac{\\pi_{\\theta}(o_{i,t}|\\mathsf{q},o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|\\mathsf{q},o_{i,<t})}\\hat{A}_{i,t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{i,t}|\\mathsf{q},o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|\\mathsf{q}, o_{i,<t})},1-\\varepsilon,1+\\varepsilon\\right)\\hat{A}_{i,t}\\right]-\\beta\\mathbb{D}_{KL}\\left[\\pi_{\\theta}||\\pi_{ref}\\right]\\right\\},$$"},{"bbox":{"x0":1023,"x1":1056,"y0":1256,"y1":1285},"conf":0.74,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1024,"x1":1053,"y0":1258,"y1":1282},"font_size":0.0,"text":"(3)"}],"source":"layout det","text":"(3)"},{"bbox":{"x0":133,"x1":1062,"y0":1326,"y1":1529},"conf":0.9768,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1330,"y1":1358},"font_size":0.0,"text":"where ðœ€and $\\beta$ are hyper-parameters, and $\\hat{A}_{i,t}$ is the advantage calculated based on relative"},{"bbox":{"x0":138,"x1":1049,"y0":1358,"y1":1382},"font_size":0.0,"text":"rewards of the outputs inside each group only, which will be detailed in the following subsec-"},{"bbox":{"x0":136,"x1":1053,"y0":1381,"y1":1412},"font_size":0.0,"text":"tions. The group relative way that GRPO leverages to calculate the advantages, aligns well with"},{"bbox":{"x0":140,"x1":1051,"y0":1412,"y1":1437},"font_size":0.0,"text":"the comparative nature of rewards models, as reward models are typically trained on datasets"},{"bbox":{"x0":138,"x1":1049,"y0":1440,"y1":1465},"font_size":0.0,"text":"of comparisons between outputs on the same question. Also note that, instead of adding KL"},{"bbox":{"x0":135,"x1":1053,"y0":1463,"y1":1495},"font_size":0.0,"text":"penalty in the reward, GRPO regularizes by directly adding the KL divergence between the"},{"bbox":{"x0":140,"x1":1051,"y0":1495,"y1":1521},"font_size":0.0,"text":"trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}.$"}],"source":"layout det","text":"where ðœ€and $\\beta$ are hyper-parameters, and $\\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only, which will be detailed in the following subsections. The group relative way that GRPO leverages to calculate the advantages, aligns well with the comparative nature of rewards models, as reward models are typically trained on datasets of comparisons between outputs on the same question. Also note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}.$"}],"formula_dets":[{"bbox":{"x0":425,"x1":762,"y0":772,"y1":830},"conf":0.9551,"label":"print_isolated","label_id":1},{"bbox":{"x0":293,"x1":338,"y0":651,"y1":675},"conf":0.8984,"label":"print_embedding","label_id":0},{"bbox":{"x0":311,"x1":450,"y0":1158,"y1":1183},"conf":0.8904,"label":"print_embedding","label_id":0},{"bbox":{"x0":561,"x1":592,"y0":1333,"y1":1358},"conf":0.8716,"label":"print_embedding","label_id":0},{"bbox":{"x0":160,"x1":506,"y0":1222,"y1":1251},"conf":0.8545,"label":"print_isolated","label_id":1},{"bbox":{"x0":226,"x1":1010,"y0":1258,"y1":1318},"conf":0.822,"label":"print_isolated","label_id":1},{"bbox":{"x0":646,"x1":686,"y0":1166,"y1":1185},"conf":0.8093,"label":"print_embedding","label_id":0},{"bbox":{"x0":429,"x1":466,"y0":853,"y1":874},"conf":0.8024,"label":"print_embedding","label_id":0},{"bbox":{"x0":181,"x1":195,"y0":877,"y1":898},"conf":0.7976,"label":"print_embedding","label_id":0},{"bbox":{"x0":268,"x1":283,"y0":1336,"y1":1358},"conf":0.797,"label":"print_embedding","label_id":0},{"bbox":{"x0":1016,"x1":1051,"y0":1496,"y1":1521},"conf":0.7887,"label":"print_embedding","label_id":0},{"bbox":{"x0":202,"x1":224,"y0":856,"y1":873},"conf":0.7364,"label":"print_embedding","label_id":0},{"bbox":{"x0":701,"x1":718,"y0":1054,"y1":1074},"conf":0.6809,"label":"print_embedding","label_id":0},{"bbox":{"x0":628,"x1":655,"y0":652,"y1":676},"conf":0.6611,"label":"print_embedding","label_id":0},{"bbox":{"x0":860,"x1":876,"y0":1140,"y1":1156},"conf":0.6083,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1059,"y0":909,"y1":1216},"conf":0.9781,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1062,"y0":1326,"y1":1529},"conf":0.9768,"label":"Text","label_id":1},{"bbox":{"x0":159,"x1":1030,"y0":172,"y1":544},"conf":0.9762,"label":"Figure","label_id":3},{"bbox":{"x0":133,"x1":1059,"y0":643,"y1":762},"conf":0.9632,"label":"Text","label_id":1},{"bbox":{"x0":154,"x1":1017,"y0":1219,"y1":1320},"conf":0.9569,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1055,"y0":549,"y1":614},"conf":0.9561,"label":"Figure caption","label_id":4},{"bbox":{"x0":132,"x1":1059,"y0":841,"y1":904},"conf":0.9343,"label":"Text","label_id":1},{"bbox":{"x0":421,"x1":768,"y0":771,"y1":834},"conf":0.9308,"label":"Equation","label_id":8},{"bbox":{"x0":1019,"x1":1056,"y0":784,"y1":817},"conf":0.8209,"label":"Equation caption","label_id":9},{"bbox":{"x0":1023,"x1":1056,"y0":1256,"y1":1285},"conf":0.74,"label":"Equation caption","label_id":9},{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7136,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1577],[580,1577]],"score":0.8367},{"poly":[[140,1495],[1051,1495],[1051,1519],[140,1519]],"score":0.7901},{"poly":[[135,1465],[1053,1463],[1053,1493],[135,1495]],"score":0.7063},{"poly":[[138,1440],[1049,1440],[1049,1465],[138,1465]],"score":0.7655},{"poly":[[140,1412],[1051,1412],[1051,1437],[140,1437]],"score":0.7552},{"poly":[[136,1382],[1053,1381],[1053,1410],[137,1412]],"score":0.7037},{"poly":[[138,1358],[1049,1358],[1049,1382],[138,1382]],"score":0.773},{"poly":[[138,1330],[1051,1330],[1051,1354],[138,1354]],"score":0.7996},{"poly":[[385,1288],[523,1288],[523,1312],[385,1312]],"score":0.8174},{"poly":[[583,1286],[726,1282],[727,1314],[584,1318]],"score":0.7152},{"poly":[[248,1286],[282,1286],[282,1319],[248,1319]],"score":0.7427},{"poly":[[230,1286],[253,1286],[253,1309],[230,1309]],"score":0.7801},{"poly":[[342,1275],[379,1275],[379,1296],[342,1296]],"score":0.8235},{"poly":[[842,1270],[1012,1270],[1012,1302],[842,1302]],"score":0.8286},{"poly":[[515,1268],[586,1272],[583,1304],[513,1299]],"score":0.7303},{"poly":[[720,1267],[852,1267],[852,1303],[720,1303]],"score":0.6917},{"poly":[[234,1267],[250,1267],[250,1282],[234,1282]],"score":0.8956},{"poly":[[594,1257],[716,1261],[715,1288],[593,1284]],"score":0.7714},{"poly":[[389,1257],[516,1261],[515,1288],[388,1284]],"score":0.7254},{"poly":[[270,1257],[304,1262],[300,1285],[267,1280]],"score":0.6033},{"poly":[[1024,1258],[1053,1258],[1053,1282],[1024,1282]],"score":0.7641},{"poly":[[253,1258],[276,1258],[276,1274],[253,1274]],"score":0.6465},{"poly":[[308,1254],[337,1254],[337,1277],[308,1277]],"score":0.7581},{"poly":[[154,1221],[507,1223],[507,1253],[154,1251]],"score":0.7283},{"poly":[[138,1184],[519,1184],[519,1209],[138,1209]],"score":0.7738},{"poly":[[138,1156],[1051,1156],[1051,1186],[138,1186]],"score":0.6869},{"poly":[[138,1130],[1051,1130],[1051,1154],[138,1154]],"score":0.7734},{"poly":[[138,1102],[1049,1102],[1049,1126],[138,1126]],"score":0.7827},{"poly":[[137,1072],[1051,1074],[1051,1103],[136,1102]],"score":0.6959},{"poly":[[138,1049],[1049,1049],[1049,1074],[138,1074]],"score":0.7546},{"poly":[[138,1021],[1051,1021],[1051,1045],[138,1045]],"score":0.7475},{"poly":[[138,995],[1051,995],[1051,1017],[138,1017]],"score":0.9311},{"poly":[[137,963],[1051,965],[1051,995],[136,993]],"score":0.6625},{"poly":[[140,940],[1051,940],[1051,965],[140,965]],"score":0.7544},{"poly":[[174,910],[1049,910],[1049,935],[174,935]],"score":0.7118},{"poly":[[138,872],[533,872],[533,896],[138,896]],"score":0.7599},{"poly":[[138,846],[1053,846],[1053,870],[138,870]],"score":0.7515},{"poly":[[624,800],[760,800],[760,830],[624,830]],"score":0.7644},{"poly":[[422,784],[629,786],[629,816],[422,814]],"score":0.7616},{"poly":[[1019,784],[1056,784],[1056,816],[1019,816]],"score":0.9062},{"poly":[[627,770],[750,770],[750,800],[627,800]],"score":0.7714},{"poly":[[138,731],[494,731],[494,754],[138,754]],"score":0.902},{"poly":[[138,703],[1051,703],[1051,728],[138,728]],"score":0.8021},{"poly":[[138,675],[1051,675],[1051,700],[138,700]],"score":0.7574},{"poly":[[140,647],[1049,647],[1049,672],[140,672]],"score":0.7263},{"poly":[[140,582],[948,582],[948,607],[140,607]],"score":0.7832},{"poly":[[138,553],[1049,551],[1049,575],[138,577]],"score":0.7753},{"poly":[[631,502],[670,502],[670,532],[631,532]],"score":0.6636},{"poly":[[813,500],[852,500],[852,530],[813,530]],"score":0.857},{"poly":[[395,498],[440,498],[440,526],[395,526]],"score":0.7627},{"poly":[[819,481],[847,481],[847,495],[819,495]],"score":0.7966},{"poly":[[633,477],[666,477],[666,496],[633,496]],"score":0.6601},{"poly":[[401,475],[434,475],[434,495],[401,495]],"score":0.7412},{"poly":[[498,463],[557,463],[557,488],[498,488]],"score":0.8094},{"poly":[[274,461],[332,465],[330,492],[272,487]],"score":0.7548},{"poly":[[700,461],[785,461],[785,486],[700,486]],"score":0.8125},{"poly":[[190,456],[213,456],[213,481],[190,481]],"score":0.7228},{"poly":[[638,447],[668,447],[668,472],[638,472]],"score":0.8096},{"poly":[[399,446],[438,446],[438,474],[399,474]],"score":0.7726},{"poly":[[817,444],[854,444],[854,474],[817,474]],"score":0.8125},{"poly":[[718,446],[769,446],[769,465],[718,465]],"score":0.8328},{"poly":[[494,444],[560,444],[560,468],[494,468]],"score":0.7889},{"poly":[[273,440],[330,444],[328,471],[270,466]],"score":0.7949},{"poly":[[500,414],[557,414],[557,439],[500,439]],"score":0.8856},{"poly":[[817,407],[847,407],[847,432],[817,432]],"score":0.8516},{"poly":[[638,407],[665,407],[665,432],[638,432]],"score":0.7875},{"poly":[[404,407],[434,407],[434,432],[404,432]],"score":0.7651},{"poly":[[489,396],[569,396],[569,416],[489,416]],"score":0.8819},{"poly":[[930,391],[994,391],[994,416],[930,416]],"score":0.9195},{"poly":[[166,372],[250,368],[252,399],[168,404]],"score":0.8559},{"poly":[[932,366],[993,371],[991,397],[930,392]],"score":0.7545},{"poly":[[491,368],[521,368],[521,391],[491,391]],"score":0.9178},{"poly":[[936,319],[998,319],[998,344],[936,344]],"score":0.9521},{"poly":[[502,312],[555,312],[555,332],[502,332]],"score":0.9522},{"poly":[[934,298],[998,298],[998,323],[934,323]],"score":0.8509},{"poly":[[505,289],[555,293],[554,313],[503,308]],"score":0.8239},{"poly":[[276,279],[331,279],[331,303],[276,303]],"score":0.9523},{"poly":[[406,272],[431,272],[431,293],[406,293]],"score":0.7983},{"poly":[[188,268],[213,268],[213,295],[188,295]],"score":0.8018},{"poly":[[821,261],[845,261],[845,284],[821,284]],"score":0.7345},{"poly":[[500,260],[558,260],[558,284],[500,284]],"score":0.891},{"poly":[[721,258],[773,258],[773,284],[721,284]],"score":0.8429},{"poly":[[276,258],[330,258],[330,282],[276,282]],"score":0.9056},{"poly":[[494,239],[562,239],[562,263],[494,263]],"score":0.7548},{"poly":[[663,223],[682,223],[682,240],[663,240]],"score":0.8091},{"poly":[[596,218],[654,218],[654,244],[596,244]],"score":0.7162},{"poly":[[175,214],[236,214],[236,240],[175,240]],"score":0.939},{"poly":[[502,210],[557,210],[557,230],[502,230]],"score":0.9682},{"poly":[[583,195],[611,195],[611,216],[583,216]],"score":0.9145},{"poly":[[488,184],[571,188],[570,213],[487,208]],"score":0.7775}],"page_no":12,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7956,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1577},"font_size":0.0,"text":"14"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":135,"x1":707,"y0":170,"y1":201},"conf":0.3991,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":704,"y0":174,"y1":196},"font_size":10.0,"text":"Algorithm 1 Iterative Group Relative Policy Optimization"}],"source":"layout det","text":"Algorithm 1 Iterative Group Relative Policy Optimization"},{"bbox":{"x0":161,"x1":950,"y0":202,"y1":231},"conf":0.6982,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":949,"y0":203,"y1":228},"font_size":10.0,"text":"put initial policy model $\\pi_{\\theta_{\\rm init}};$  reward models $r_{\\varphi};$  task prompts $\\mathcal{D}$  hyperparameters $\\varepsilon, \\beta, \\mu$"}],"source":"layout det","text":"put initial policy model $\\pi_{\\theta_{\\rm init}};$  reward models $r_{\\varphi};$  task prompts $\\mathcal{D}$  hyperparameters $\\varepsilon, \\beta, \\mu$"},{"bbox":{"x0":142,"x1":392,"y0":230,"y1":255},"conf":0.6929,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":145,"x1":386,"y0":226,"y1":260},"font_size":10.0,"text":"1: policy model $\\pi_{\\theta}\\leftrightarrow\\pi_{\\theta_{\\text{init}}}$"}],"source":"layout det","text":"1: policy model $\\pi_{\\theta}\\leftrightarrow\\pi_{\\theta_{\\text{init}}}$"},{"bbox":{"x0":144,"x1":397,"y0":255,"y1":278},"conf":0.7468,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":147,"x1":395,"y0":254,"y1":279},"font_size":10.0,"text":"2: for iteration $=1,\\ldots,\\mathrm{I}$  do"}],"source":"layout det","text":"2: for iteration $=1,\\ldots,\\mathrm{I}$  do"},{"bbox":{"x0":144,"x1":441,"y0":279,"y1":302},"conf":0.7599,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":147,"x1":436,"y0":275,"y1":304},"font_size":10.0,"text":"3:reference model $\\pi_{ref}\\leftrightarrow\\pi_{\\theta}$"}],"source":"layout det","text":"3:reference model $\\pi_{ref}\\leftrightarrow\\pi_{\\theta}$"},{"bbox":{"x0":145,"x1":417,"y0":302,"y1":326},"conf":0.717,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":145,"x1":399,"y0":302,"y1":326},"font_size":10.0,"text":"4:for step = 1, ..., M do"}],"source":"layout det","text":"4:for step = 1, ..., M do"},{"bbox":{"x0":145,"x1":476,"y0":327,"y1":350},"conf":0.7936,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":147,"x1":468,"y0":323,"y1":349},"font_size":10.0,"text":"5:Sample a batch $\\mathcal{D}_{b}$ from $\\mathcal{D}$"}],"source":"layout det","text":"5:Sample a batch $\\mathcal{D}_{b}$ from $\\mathcal{D}$"},{"bbox":{"x0":149,"x1":586,"y0":350,"y1":374},"conf":0.8489,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":145,"x1":581,"y0":349,"y1":375},"font_size":10.0,"text":"6:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$"}],"source":"layout det","text":"6:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$"},{"bbox":{"x0":144,"x1":780,"y0":375,"y1":399},"conf":0.8867,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":147,"x1":776,"y0":372,"y1":402},"font_size":10.0,"text":"7:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\cdot\\mid q)$  for each question $q\\in\\mathcal{D}_{b}$"}],"source":"layout det","text":"7:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\cdot\\mid q)$  for each question $q\\in\\mathcal{D}_{b}$"},{"bbox":{"x0":142,"x1":823,"y0":400,"y1":425},"conf":0.8632,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":145,"x1":818,"y0":398,"y1":428},"font_size":10.0,"text":"8:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$ by running $r_{\\varphi}$"}],"source":"layout det","text":"8:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$ by running $r_{\\varphi}$"},{"bbox":{"x0":143,"x1":956,"y0":426,"y1":451},"conf":0.6355,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":145,"x1":950,"y0":425,"y1":451},"font_size":10.0,"text":"9:Compute $\\widehat{A}_{i,t}$ for the $t$ th token of $o_{i}$ through group relative advantage estimation."}],"source":"layout det","text":"9:Compute $\\widehat{A}_{i,t}$ for the $t$ th token of $o_{i}$ through group relative advantage estimation."},{"bbox":{"x0":138,"x1":523,"y0":450,"y1":472},"conf":0.7133,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":521,"y0":447,"y1":474},"font_size":10.0,"text":"10:for GRPO iteration $=1,\\ldots,\\mu$ do"}],"source":"layout det","text":"10:for GRPO iteration $=1,\\ldots,\\mu$ do"},{"bbox":{"x0":136,"x1":955,"y0":473,"y1":502},"conf":0.4411,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":948,"y0":474,"y1":498},"font_size":10.0,"text":"11:Update the policy model $\\pi_{\\theta}$ by maximizing the GRPO objective (Equation 21)"}],"source":"layout det","text":"11:Update the policy model $\\pi_{\\theta}$ by maximizing the GRPO objective (Equation 21)"},{"bbox":{"x0":136,"x1":795,"y0":503,"y1":530},"conf":0.2347,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":789,"y0":502,"y1":528},"font_size":10.0,"text":"12:Update $r_{\\varphi}$ through continuous training using a replay mechanism."}],"source":"layout det","text":"12:Update $r_{\\varphi}$ through continuous training using a replay mechanism."},{"bbox":{"x0":135,"x1":239,"y0":535,"y1":565},"conf":0.5757,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":233,"y0":535,"y1":563},"font_size":10.0,"text":"Output $\\pi_{\\theta}$"}],"source":"layout det","text":"Output $\\pi_{\\theta}$"},{"bbox":{"x0":132,"x1":1058,"y0":602,"y1":665},"conf":0.9331,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":609,"y1":632},"font_size":10.0,"text":"And different from the KL penalty term used in (2), we estimate the KL divergence with the"},{"bbox":{"x0":140,"x1":603,"y0":635,"y1":660},"font_size":10.0,"text":"following unbiased estimator (Schulman, 2020):"}],"source":"layout det","text":"And different from the KL penalty term used in (2), we estimate the KL divergence with the following unbiased estimator (Schulman, 2020):"},{"bbox":{"x0":345,"x1":843,"y0":678,"y1":732},"conf":0.9278,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\text{I D}_{KL}\\left[\\pi_{\\theta}||\\pi_{ref}\\right]=\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-\\log\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-1,$$"},{"bbox":{"x0":1022,"x1":1055,"y0":691,"y1":721},"conf":0.8102,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1056,"y0":689,"y1":721},"font_size":10.0,"text":"(4)"}],"source":"layout det","text":"(4)"},{"bbox":{"x0":133,"x1":481,"y0":744,"y1":780},"conf":0.9146,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":475,"y0":749,"y1":774},"font_size":10.0,"text":"which is guaranteed to be positive."}],"source":"layout det","text":"which is guaranteed to be positive."},{"bbox":{"x0":134,"x1":575,"y0":805,"y1":840},"conf":0.9025,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":567,"y0":810,"y1":833},"font_size":10.0,"text":"4.1.2. Outcome Supervision RL with GRPO"}],"source":"layout det","text":"4.1.2. Outcome Supervision RL with GRPO"},{"bbox":{"x0":133,"x1":1057,"y0":849,"y1":1053},"conf":0.9758,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":853,"y1":882},"font_size":10.0,"text":"Formally, for each question $q,$  a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  are sampled from the old"},{"bbox":{"x0":135,"x1":1051,"y0":877,"y1":910},"font_size":10.0,"text":"policy model $\\pi_{\\theta_{old}}.$  A reward model is then used to score the outputs, yielding $G$ rewards"},{"bbox":{"x0":139,"x1":1051,"y0":907,"y1":937},"font_size":10.0,"text":"$\\mathbf{r}=\\{r_{1},r_{2},\\cdots,r_{G}\\}$  correspondingly. Subsequently, these rewards are normalized by subtracting"},{"bbox":{"x0":140,"x1":1049,"y0":937,"y1":961},"font_size":10.0,"text":"the group average and dividing by the group standard deviation. Outcome supervision provides"},{"bbox":{"x0":140,"x1":1049,"y0":963,"y1":989},"font_size":10.0,"text":"the normalized reward at the end of each output $o_{i}$ and sets the advantages $\\hat{A}_{i,t}$ of all tokens in"},{"bbox":{"x0":141,"x1":1049,"y0":988,"y1":1024},"font_size":10.0,"text":"the output as the normalized reward, i.e., $\\hat{A}_{i,t}=\\widetilde{r}_{i}=\\tfrac{r_{i}-\\mathrm{mean}(\\mathbf{r})}{\\mathrm{std}(\\mathbf{r})},$ and then optimizes the policy by"},{"bbox":{"x0":140,"x1":613,"y0":1021,"y1":1045},"font_size":10.0,"text":"maximizing the objective defned in equation (3).i"}],"source":"layout det","text":"Formally, for each question $q,$  a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  are sampled from the old policy model $\\pi_{\\theta_{old}}.$  A reward model is then used to score the outputs, yielding $G$ rewards $\\mathbf{r}=\\{r_{1},r_{2},\\cdots,r_{G}\\}$  correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output $o_{i}$ and sets the advantages $\\hat{A}_{i,t}$ of all tokens in the output as the normalized reward, i.e., $\\hat{A}_{i,t}=\\widetilde{r}_{i}=\\tfrac{r_{i}-\\mathrm{mean}(\\mathbf{r})}{\\mathrm{std}(\\mathbf{r})},$ and then optimizes the policy by maximizing the objective defned in equation (3).i"},{"bbox":{"x0":133,"x1":560,"y0":1076,"y1":1110},"conf":0.8976,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":553,"y0":1079,"y1":1105},"font_size":10.0,"text":"4.1.3. Process Supervision RL with GRPO"}],"source":"layout det","text":"4.1.3. Process Supervision RL with GRPO"},{"bbox":{"x0":133,"x1":1059,"y0":1120,"y1":1446},"conf":0.9806,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1126,"y1":1151},"font_size":0.0,"text":"Outcome supervision only provides a reward at the end of each output, which may not be"},{"bbox":{"x0":138,"x1":1049,"y0":1151,"y1":1179},"font_size":0.0,"text":"suffcient and effcient to supervise the policy in complex mathematical tasks. Following Wangii"},{"bbox":{"x0":140,"x1":1051,"y0":1181,"y1":1205},"font_size":0.0,"text":"et al. (2023b), we also explore process supervision, which provides a reward at the end of"},{"bbox":{"x0":140,"x1":1051,"y0":1208,"y1":1233},"font_size":10.0,"text":"each reasoning step. Formally, given the question $q$ and $G$ sampled outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\},$  a"},{"bbox":{"x0":138,"x1":1051,"y0":1235,"y1":1259},"font_size":0.0,"text":"process reward model is used to score each step of the outputs, yielding corresponding rewards:"},{"bbox":{"x0":139,"x1":1049,"y0":1257,"y1":1293},"font_size":10.0,"text":"$\\mathbf{R}=\\{\\{r_{1}^{index}{}^{(1)},\\cdots,r_{1}^{index}{}^{(K_{1})}\\}, \\cdots,\\{r_{G}^{index}{}^{(1)},\\cdots,r_{G}^{index}{}^{(K_{G})}\\}\\},$  where $index(j)$  is the end token index"},{"bbox":{"x0":140,"x1":1049,"y0":1291,"y1":1316},"font_size":10.0,"text":"of the $j$ th step, and $K_{i}$ is the total number of steps in the $i$ th output. We also normalize these"},{"bbox":{"x0":141,"x1":1051,"y0":1317,"y1":1361},"font_size":10.0,"text":"rewards with the average and the standard deviation, i.e. $\\underline{\\widetilde{r}_{i}^{index(j)}}=\\underline{\\tfrac{r_{i}^{index(j)}-\\mathbf{mean(R)}}{\\mathbf{std(R)}}}.$  Subsequently,"},{"bbox":{"x0":140,"x1":1049,"y0":1358,"y1":1382},"font_size":0.0,"text":"the process supervision calculates the advantage of each token as the sum of the normalized"},{"bbox":{"x0":141,"x1":1049,"y0":1382,"y1":1416},"font_size":10.0,"text":"rewards from the following steps, i.e., $\\begin{array}{l}\\hat{A}_{i,t}=\\sum_{index(j)\\geq t}\\widetilde{r}_{i}^{index (j)},\\end{array}$  and then optimizes the policy by"},{"bbox":{"x0":140,"x1":613,"y0":1416,"y1":1439},"font_size":0.0,"text":"maximizing the objective defned in equation (3).i"}],"source":"layout det","text":"Outcome supervision only provides a reward at the end of each output, which may not be suffcient and effcient to supervise the policy in complex mathematical tasks. Following Wangii et al. (2023b), we also explore process supervision, which provides a reward at the end of each reasoning step. Formally, given the question $q$ and $G$ sampled outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\},$  a process reward model is used to score each step of the outputs, yielding corresponding rewards: $\\mathbf{R}=\\{\\{r_{1}^{index}{}^{(1)},\\cdots,r_{1}^{index}{}^{(K_{1})}\\}, \\cdots,\\{r_{G}^{index}{}^{(1)},\\cdots,r_{G}^{index}{}^{(K_{G})}\\}\\},$  where $index(j)$  is the end token index of the $j$ th step, and $K_{i}$ is the total number of steps in the $i$ th output. We also normalize these rewards with the average and the standard deviation, i.e. $\\underline{\\widetilde{r}_{i}^{index(j)}}=\\underline{\\tfrac{r_{i}^{index(j)}-\\mathbf{mean(R)}}{\\mathbf{std(R)}}}.$  Subsequently,the process supervision calculates the advantage of each token as the sum of the normalized rewards from the following steps, i.e., $\\begin{array}{l}\\hat{A}_{i,t}=\\sum_{index(j)\\geq t}\\widetilde{r}_{i}^{index (j)},\\end{array}$  and then optimizes the policy by maximizing the objective defned in equation (3).i"}],"formula_dets":[{"bbox":{"x0":345,"x1":843,"y0":678,"y1":732},"conf":0.9278,"label":"print_isolated","label_id":1},{"bbox":{"x0":678,"x1":913,"y0":1317,"y1":1361},"conf":0.9225,"label":"print_embedding","label_id":0},{"bbox":{"x0":399,"x1":450,"y0":402,"y1":427},"conf":0.9073,"label":"print_embedding","label_id":0},{"bbox":{"x0":542,"x1":729,"y0":988,"y1":1024},"conf":0.9057,"label":"print_embedding","label_id":0},{"bbox":{"x0":506,"x1":730,"y0":1382,"y1":1416},"conf":0.9006,"label":"print_embedding","label_id":0},{"bbox":{"x0":644,"x1":784,"y0":855,"y1":880},"conf":0.886,"label":"print_embedding","label_id":0},{"bbox":{"x0":397,"x1":557,"y0":375,"y1":400},"conf":0.8805,"label":"print_embedding","label_id":0},{"bbox":{"x0":139,"x1":306,"y0":910,"y1":934},"conf":0.8794,"label":"print_embedding","label_id":0},{"bbox":{"x0":886,"x1":1031,"y0":1208,"y1":1232},"conf":0.8772,"label":"print_embedding","label_id":0},{"bbox":{"x0":870,"x1":900,"y0":963,"y1":989},"conf":0.8736,"label":"print_embedding","label_id":0},{"bbox":{"x0":321,"x1":348,"y0":428,"y1":450},"conf":0.8625,"label":"print_embedding","label_id":0},{"bbox":{"x0":491,"x1":581,"y0":355,"y1":374},"conf":0.8591,"label":"print_embedding","label_id":0},{"bbox":{"x0":139,"x1":691,"y0":1257,"y1":1293},"conf":0.8582,"label":"print_embedding","label_id":0},{"bbox":{"x0":350,"x1":436,"y0":283,"y1":302},"conf":0.8555,"label":"print_embedding","label_id":0},{"bbox":{"x0":373,"x1":417,"y0":210,"y1":228},"conf":0.8424,"label":"print_embedding","label_id":0},{"bbox":{"x0":372,"x1":397,"y0":331,"y1":348},"conf":0.8396,"label":"print_embedding","label_id":0},{"bbox":{"x0":295,"x1":386,"y0":236,"y1":255},"conf":0.8396,"label":"print_embedding","label_id":0},{"bbox":{"x0":405,"x1":489,"y0":452,"y1":473},"conf":0.8391,"label":"print_embedding","label_id":0},{"bbox":{"x0":761,"x1":835,"y0":1266,"y1":1289},"conf":0.8335,"label":"print_embedding","label_id":0},{"bbox":{"x0":336,"x1":357,"y0":1296,"y1":1316},"conf":0.8251,"label":"print_embedding","label_id":0},{"bbox":{"x0":284,"x1":364,"y0":258,"y1":278},"conf":0.8205,"label":"print_embedding","label_id":0},{"bbox":{"x0":718,"x1":776,"y0":378,"y1":397},"conf":0.8201,"label":"print_embedding","label_id":0},{"bbox":{"x0":945,"x1":960,"y0":887,"y1":903},"conf":0.7951,"label":"print_embedding","label_id":0},{"bbox":{"x0":200,"x1":212,"y0":1295,"y1":1316},"conf":0.7678,"label":"print_embedding","label_id":0},{"bbox":{"x0":799,"x1":818,"y0":409,"y1":425},"conf":0.7634,"label":"print_embedding","label_id":0},{"bbox":{"x0":304,"x1":317,"y0":380,"y1":393},"conf":0.7609,"label":"print_embedding","label_id":0},{"bbox":{"x0":893,"x1":949,"y0":204,"y1":227},"conf":0.7477,"label":"print_embedding","label_id":0},{"bbox":{"x0":676,"x1":691,"y0":408,"y1":422},"conf":0.7474,"label":"print_embedding","label_id":0},{"bbox":{"x0":558,"x1":584,"y0":211,"y1":228},"conf":0.7451,"label":"print_embedding","label_id":0},{"bbox":{"x0":613,"x1":630,"y0":972,"y1":987},"conf":0.7313,"label":"print_embedding","label_id":0},{"bbox":{"x0":692,"x1":707,"y0":1212,"y1":1228},"conf":0.7304,"label":"print_embedding","label_id":0},{"bbox":{"x0":529,"x1":545,"y0":434,"y1":449},"conf":0.7263,"label":"print_embedding","label_id":0},{"bbox":{"x0":278,"x1":324,"y0":889,"y1":908},"conf":0.716,"label":"print_embedding","label_id":0},{"bbox":{"x0":274,"x1":293,"y0":512,"y1":528},"conf":0.7128,"label":"print_embedding","label_id":0},{"bbox":{"x0":451,"x1":468,"y0":331,"y1":346},"conf":0.7064,"label":"print_embedding","label_id":0},{"bbox":{"x0":633,"x1":644,"y0":1216,"y1":1232},"conf":0.6981,"label":"print_embedding","label_id":0},{"bbox":{"x0":487,"x1":509,"y0":482,"y1":497},"conf":0.6871,"label":"print_embedding","label_id":0},{"bbox":{"x0":692,"x1":702,"y0":1296,"y1":1312},"conf":0.6789,"label":"print_embedding","label_id":0},{"bbox":{"x0":415,"x1":425,"y0":433,"y1":445},"conf":0.6579,"label":"print_embedding","label_id":0},{"bbox":{"x0":709,"x1":731,"y0":207,"y1":224},"conf":0.6225,"label":"print_embedding","label_id":0},{"bbox":{"x0":212,"x1":233,"y0":546,"y1":560},"conf":0.5314,"label":"print_embedding","label_id":0},{"bbox":{"x0":421,"x1":438,"y0":863,"y1":881},"conf":0.5082,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1059,"y0":1120,"y1":1446},"conf":0.9806,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":849,"y1":1053},"conf":0.9758,"label":"Text","label_id":1},{"bbox":{"x0":340,"x1":851,"y0":674,"y1":738},"conf":0.9366,"label":"Equation","label_id":8},{"bbox":{"x0":132,"x1":1058,"y0":602,"y1":665},"conf":0.9331,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":481,"y0":744,"y1":780},"conf":0.9146,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":575,"y0":805,"y1":840},"conf":0.9025,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":560,"y0":1076,"y1":1110},"conf":0.8976,"label":"Title","label_id":0},{"bbox":{"x0":144,"x1":780,"y0":375,"y1":399},"conf":0.8867,"label":"Text","label_id":1},{"bbox":{"x0":142,"x1":823,"y0":400,"y1":425},"conf":0.8632,"label":"Text","label_id":1},{"bbox":{"x0":149,"x1":586,"y0":350,"y1":374},"conf":0.8489,"label":"Text","label_id":1},{"bbox":{"x0":1022,"x1":1055,"y0":691,"y1":721},"conf":0.8102,"label":"Equation caption","label_id":9},{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7956,"label":"Abandon","label_id":2},{"bbox":{"x0":145,"x1":476,"y0":327,"y1":350},"conf":0.7936,"label":"Text","label_id":1},{"bbox":{"x0":254,"x1":954,"y0":474,"y1":501},"conf":0.7784,"label":"Text","label_id":1},{"bbox":{"x0":144,"x1":441,"y0":279,"y1":302},"conf":0.7599,"label":"Text","label_id":1},{"bbox":{"x0":144,"x1":397,"y0":255,"y1":278},"conf":0.7468,"label":"Text","label_id":1},{"bbox":{"x0":145,"x1":417,"y0":302,"y1":326},"conf":0.717,"label":"Text","label_id":1},{"bbox":{"x0":138,"x1":523,"y0":450,"y1":472},"conf":0.7133,"label":"Text","label_id":1},{"bbox":{"x0":161,"x1":950,"y0":202,"y1":231},"conf":0.6982,"label":"Text","label_id":1},{"bbox":{"x0":142,"x1":392,"y0":230,"y1":255},"conf":0.6929,"label":"Text","label_id":1},{"bbox":{"x0":143,"x1":956,"y0":426,"y1":451},"conf":0.6355,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":239,"y0":535,"y1":565},"conf":0.5757,"label":"Title","label_id":0},{"bbox":{"x0":163,"x1":948,"y0":474,"y1":500},"conf":0.474,"label":"Text","label_id":1},{"bbox":{"x0":162,"x1":949,"y0":426,"y1":451},"conf":0.4676,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":955,"y0":473,"y1":502},"conf":0.4411,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":707,"y0":170,"y1":201},"conf":0.3991,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":707,"y0":170,"y1":201},"conf":0.2833,"label":"Text","label_id":1},{"bbox":{"x0":136,"x1":795,"y0":503,"y1":530},"conf":0.2347,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1577],[580,1577]],"score":0.7872},{"poly":[[140,1416],[613,1416],[613,1438],[140,1438]],"score":0.9441},{"poly":[[140,1358],[1049,1358],[1049,1382],[140,1382]],"score":0.7925},{"poly":[[815,1342],[865,1342],[865,1361],[815,1361]],"score":0.8869},{"poly":[[140,1291],[1049,1291],[1049,1316],[140,1316]],"score":0.731},{"poly":[[138,1235],[1051,1235],[1051,1259],[138,1259]],"score":0.7495},{"poly":[[140,1209],[1051,1209],[1051,1233],[140,1233]],"score":0.713},{"poly":[[140,1181],[1051,1181],[1051,1205],[140,1205]],"score":0.7286},{"poly":[[138,1151],[1049,1154],[1049,1179],[138,1175]],"score":0.7243},{"poly":[[140,1126],[1049,1126],[1049,1151],[140,1151]],"score":0.7189},{"poly":[[138,1081],[553,1079],[553,1103],[138,1105]],"score":0.7853},{"poly":[[140,1021],[613,1021],[613,1045],[140,1045]],"score":0.7915},{"poly":[[656,1005],[704,1005],[704,1024],[656,1024]],"score":0.9569},{"poly":[[140,963],[1049,963],[1049,988],[140,988]],"score":0.7694},{"poly":[[140,937],[1049,937],[1049,961],[140,961]],"score":0.8001},{"poly":[[136,907],[1051,907],[1051,937],[136,937]],"score":0.689},{"poly":[[135,881],[1051,877],[1051,907],[135,910]],"score":0.6774},{"poly":[[138,853],[1051,853],[1051,882],[138,882]],"score":0.6781},{"poly":[[140,810],[567,810],[567,833],[140,833]],"score":0.8937},{"poly":[[136,749],[475,749],[475,774],[136,774]],"score":0.7414},{"poly":[[677,707],[803,707],[803,731],[677,731]],"score":0.862},{"poly":[[496,707],[619,707],[619,731],[496,731]],"score":0.8537},{"poly":[[638,687],[679,700],[671,724],[630,712]],"score":0.6953},{"poly":[[342,689],[475,689],[475,719],[342,719]],"score":0.8589},{"poly":[[1021,689],[1056,689],[1056,721],[1021,721]],"score":0.8165},{"poly":[[809,685],[850,690],[846,718],[805,713]],"score":0.6244},{"poly":[[670,677],[813,673],[814,703],[671,707]],"score":0.7813},{"poly":[[487,677],[626,677],[626,707],[487,707]],"score":0.7639},{"poly":[[140,635],[603,635],[603,660],[140,660]],"score":0.7608},{"poly":[[140,609],[1049,609],[1049,632],[140,632]],"score":0.917},{"poly":[[137,535],[236,539],[235,563],[136,559]],"score":0.812},{"poly":[[204,503],[789,503],[789,528],[204,528]],"score":0.8161},{"poly":[[140,502],[168,502],[168,526],[140,526]],"score":0.8507},{"poly":[[266,474],[948,474],[948,498],[266,498]],"score":0.7454},{"poly":[[140,474],[170,474],[170,498],[140,498]],"score":0.81},{"poly":[[231,447],[521,449],[521,474],[230,472]],"score":0.7683},{"poly":[[140,451],[170,451],[170,474],[140,474]],"score":0.8496},{"poly":[[234,426],[950,426],[950,451],[234,451]],"score":0.7529},{"poly":[[145,425],[170,425],[170,447],[145,447]],"score":0.8152},{"poly":[[230,398],[822,398],[822,428],[230,428]],"score":0.685},{"poly":[[145,400],[170,400],[170,425],[145,425]],"score":0.7503},{"poly":[[230,372],[778,372],[778,402],[230,402]],"score":0.6879},{"poly":[[147,374],[170,374],[170,398],[147,398]],"score":0.8824},{"poly":[[232,349],[583,351],[583,375],[232,374]],"score":0.8154},{"poly":[[145,351],[170,351],[170,374],[145,374]],"score":0.8303},{"poly":[[230,325],[471,323],[472,347],[231,349]],"score":0.7903},{"poly":[[147,325],[170,325],[170,349],[147,349]],"score":0.8628},{"poly":[[202,302],[399,302],[399,326],[202,326]],"score":0.7892},{"poly":[[145,303],[170,303],[170,326],[145,326]],"score":0.8114},{"poly":[[201,275],[440,279],[439,304],[200,300]],"score":0.739},{"poly":[[147,279],[172,279],[172,300],[147,300]],"score":0.8491},{"poly":[[147,254],[395,254],[395,279],[147,279]],"score":0.752},{"poly":[[146,226],[390,230],[390,260],[145,256]],"score":0.6665},{"poly":[[138,203],[950,203],[950,228],[138,228]],"score":0.7485},{"poly":[[140,174],[704,174],[704,196],[140,196]],"score":0.838}],"page_no":13,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.791,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"15"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":451,"y0":167,"y1":200},"conf":0.87,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":445,"y0":174,"y1":196},"font_size":0.0,"text":"4.1.4. Iterative RL with GRPO"}],"source":"layout det","text":"4.1.4. Iterative RL with GRPO"},{"bbox":{"x0":133,"x1":1058,"y0":213,"y1":408},"conf":0.9772,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":219,"y1":244},"font_size":0.0,"text":"As the reinforcement learning training process progresses, the old reward model may not be"},{"bbox":{"x0":140,"x1":1051,"y0":246,"y1":270},"font_size":0.0,"text":"suffcient to supervise the current policy model. Therefore, we also explore the iterative RLi"},{"bbox":{"x0":140,"x1":1047,"y0":270,"y1":295},"font_size":0.0,"text":"with GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the"},{"bbox":{"x0":138,"x1":1049,"y0":300,"y1":325},"font_size":0.0,"text":"reward model based on the sampling results from the policy model and continually train the"},{"bbox":{"x0":140,"x1":1049,"y0":325,"y1":351},"font_size":0.0,"text":"old reward model using a replay mechanism that incorporates $10\\%$ of historical data. Then, we"},{"bbox":{"x0":140,"x1":1049,"y0":354,"y1":377},"font_size":0.0,"text":"set the reference model as the policy model, and continually train the policy model with the"},{"bbox":{"x0":140,"x1":331,"y0":381,"y1":403},"font_size":0.0,"text":"new reward model."}],"source":"layout det","text":"As the reinforcement learning training process progresses, the old reward model may not be suffcient to supervise the current policy model. Therefore, we also explore the iterative RLi with GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the reward model based on the sampling results from the policy model and continually train the old reward model using a replay mechanism that incorporates $10\\%$ of historical data. Then, we set the reference model as the policy model, and continually train the policy model with the new reward model."},{"bbox":{"x0":132,"x1":629,"y0":441,"y1":477},"conf":0.8989,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":624,"y0":444,"y1":470},"font_size":0.0,"text":"4.2. Training and Evaluating DeepSeekMath-RL"}],"source":"layout det","text":"4.2. Training and Evaluating DeepSeekMath-RL"},{"bbox":{"x0":132,"x1":1059,"y0":485,"y1":817},"conf":0.9836,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":491,"y1":516},"font_size":0.0,"text":"We conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-of-"},{"bbox":{"x0":140,"x1":1049,"y0":517,"y1":542},"font_size":0.0,"text":"thought-format questions related to GSM8K and MATH from the SFT data, which consists"},{"bbox":{"x0":140,"x1":1051,"y0":546,"y1":570},"font_size":0.0,"text":"of around 144K questions. We exclude other SFT questions to investigate the impact of RL"},{"bbox":{"x0":138,"x1":1051,"y0":572,"y1":596},"font_size":0.0,"text":"on benchmarks that lack data throughout the RL phase. We construct the training set of"},{"bbox":{"x0":140,"x1":1049,"y0":600,"y1":624},"font_size":0.0,"text":"reward models following (Wang et al., 2023b). We train our initial reward model based on the"},{"bbox":{"x0":140,"x1":1047,"y0":626,"y1":651},"font_size":0.0,"text":"DeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the"},{"bbox":{"x0":136,"x1":1051,"y0":651,"y1":682},"font_size":0.0,"text":"policy model as 1e-6. The KL coeffcient is 0.04. For each question, we sample 64 outputs. Thei"},{"bbox":{"x0":138,"x1":1049,"y0":681,"y1":707},"font_size":0.0,"text":"max length is set to 1024, and the training batch size is 1024. The policy model only has a single"},{"bbox":{"x0":140,"x1":1047,"y0":709,"y1":733},"font_size":0.0,"text":"update following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks"},{"bbox":{"x0":138,"x1":1049,"y0":735,"y1":760},"font_size":0.0,"text":"following DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with"},{"bbox":{"x0":140,"x1":1049,"y0":763,"y1":786},"font_size":0.0,"text":"chain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks"},{"bbox":{"x0":138,"x1":526,"y0":788,"y1":814},"font_size":0.0,"text":"can be regarded as out-of-domain tasks."}],"source":"layout det","text":"We conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-ofthought-format questions related to GSM8K and MATH from the SFT data, which consists of around 144K questions. We exclude other SFT questions to investigate the impact of RL on benchmarks that lack data throughout the RL phase. We construct the training set of reward models following (Wang et al., 2023b). We train our initial reward model based on the DeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the policy model as 1e-6. The KL coeffcient is 0.04. For each question, we sample 64 outputs. Thei max length is set to 1024, and the training batch size is 1024. The policy model only has a single update following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks following DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with chain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks can be regarded as out-of-domain tasks."},{"bbox":{"x0":133,"x1":1060,"y0":824,"y1":1079},"conf":0.9806,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1049,"y0":830,"y1":854},"font_size":0.0,"text":"Table 5 demonstrates the performance of open- and closed-source models with both chain-"},{"bbox":{"x0":140,"x1":1051,"y0":858,"y1":882},"font_size":0.0,"text":"of-thought and tool-integrated reasoning on English and Chinese benchmarks. We fnd that:i"},{"bbox":{"x0":140,"x1":1051,"y0":883,"y1":909},"font_size":0.0,"text":"1) DeepSeekMath-RL 7B attains accuracies of $88.2\\%$ and $51.7\\%$ on GSM8K and MATH, respec-"},{"bbox":{"x0":140,"x1":1049,"y0":910,"y1":933},"font_size":0.0,"text":"tively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source"},{"bbox":{"x0":140,"x1":1051,"y0":938,"y1":963},"font_size":0.0,"text":"models in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,"},{"bbox":{"x0":140,"x1":1051,"y0":965,"y1":989},"font_size":0.0,"text":"DeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of"},{"bbox":{"x0":140,"x1":1049,"y0":993,"y1":1017},"font_size":0.0,"text":"GSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope"},{"bbox":{"x0":140,"x1":1051,"y0":1021,"y1":1045},"font_size":0.0,"text":"of its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,"},{"bbox":{"x0":138,"x1":674,"y0":1045,"y1":1074},"font_size":0.0,"text":"showcasing the effectiveness of reinforcement learning."}],"source":"layout det","text":"Table 5 demonstrates the performance of open- and closed-source models with both chainof-thought and tool-integrated reasoning on English and Chinese benchmarks. We fnd that:i 1) DeepSeekMath-RL 7B attains accuracies of $88.2\\%$ and $51.7\\%$ on GSM8K and MATH, respectively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source models in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,DeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope of its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,showcasing the effectiveness of reinforcement learning."},{"bbox":{"x0":134,"x1":312,"y0":1111,"y1":1146},"conf":0.8938,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":305,"y0":1116,"y1":1142},"font_size":0.0,"text":"5. Discussion"}],"source":"layout det","text":"5.Discussion"},{"bbox":{"x0":133,"x1":898,"y0":1164,"y1":1200},"conf":0.8961,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":888,"y0":1166,"y1":1194},"font_size":0.0,"text":"In this section, we will share our fndings in pre-training and RL experiments.i"}],"source":"layout det","text":"In this section, we will share our fndings in pre-training and RL experiments.i"},{"bbox":{"x0":134,"x1":497,"y0":1232,"y1":1268},"conf":0.8993,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":491,"y0":1235,"y1":1263},"font_size":0.0,"text":"5.1. Lessons Learnt in Pre-Training"}],"source":"layout det","text":"5.1. Lessons Learnt in Pre-Training"},{"bbox":{"x0":134,"x1":1058,"y0":1276,"y1":1394},"conf":0.9679,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1281,"y1":1305},"font_size":0.0,"text":"We frst share our experience in pre-training. Unless otherwise specifed, we will adhere toii"},{"bbox":{"x0":140,"x1":1049,"y0":1309,"y1":1333},"font_size":0.0,"text":"the training settings outlined in Section 2.2.1. It is worth noting that, when referring to the"},{"bbox":{"x0":140,"x1":1051,"y0":1335,"y1":1359},"font_size":0.0,"text":"DeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of"},{"bbox":{"x0":138,"x1":401,"y0":1361,"y1":1388},"font_size":0.0,"text":"the data collection process."}],"source":"layout det","text":"We frst share our experience in pre-training. Unless otherwise specifed, we will adhere toii the training settings outlined in Section 2.2.1. It is worth noting that, when referring to the DeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of the data collection process."},{"bbox":{"x0":133,"x1":679,"y0":1418,"y1":1454},"conf":0.9133,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":675,"y0":1417,"y1":1453},"font_size":0.0,"text":"5.1.1. Code Training Benefts Mathematical Reasoningi"}],"source":"layout det","text":"5.1.1. Code Training Benefts Mathematical Reasoningi"},{"bbox":{"x0":132,"x1":1058,"y0":1462,"y1":1525},"conf":0.9286,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1468,"y1":1493},"font_size":0.0,"text":"A popular yet unverifed hypothesis suggests that code training improves reasoning. We attempti"},{"bbox":{"x0":136,"x1":1051,"y0":1491,"y1":1523},"font_size":0.0,"text":"to offer a partial response to this, particularly within the mathematical domain: code training"}],"source":"layout det","text":"A popular yet unverifed hypothesis suggests that code training improves reasoning. We attempti to offer a partial response to this, particularly within the mathematical domain: code training"}],"formula_dets":[{"bbox":{"x0":741,"x1":783,"y0":325,"y1":348},"conf":0.837,"label":"print_embedding","label_id":0},{"bbox":{"x0":580,"x1":641,"y0":883,"y1":906},"conf":0.7946,"label":"print_embedding","label_id":0},{"bbox":{"x0":685,"x1":745,"y0":883,"y1":906},"conf":0.7892,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1059,"y0":485,"y1":817},"conf":0.9836,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":824,"y1":1079},"conf":0.9806,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":213,"y1":408},"conf":0.9772,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1058,"y0":1276,"y1":1394},"conf":0.9679,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1462,"y1":1525},"conf":0.9286,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":679,"y0":1418,"y1":1454},"conf":0.9133,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":497,"y0":1232,"y1":1268},"conf":0.8993,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":629,"y0":441,"y1":477},"conf":0.8989,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":898,"y0":1164,"y1":1200},"conf":0.8961,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":312,"y0":1111,"y1":1146},"conf":0.8938,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":451,"y0":167,"y1":200},"conf":0.87,"label":"Title","label_id":0},{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.791,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.8145},{"poly":[[137,1491],[1051,1493],[1051,1523],[136,1521]],"score":0.6862},{"poly":[[140,1468],[1049,1468],[1049,1493],[140,1493]],"score":0.7674},{"poly":[[137,1417],[675,1423],[675,1453],[136,1447]],"score":0.6473},{"poly":[[138,1361],[401,1363],[400,1388],[138,1386]],"score":0.7995},{"poly":[[140,1335],[1051,1335],[1051,1359],[140,1359]],"score":0.7437},{"poly":[[140,1309],[1049,1309],[1049,1333],[140,1333]],"score":0.7347},{"poly":[[138,1281],[1049,1281],[1049,1305],[138,1305]],"score":0.7733},{"poly":[[138,1235],[491,1239],[491,1263],[138,1259]],"score":0.7461},{"poly":[[138,1166],[888,1168],[888,1193],[138,1191]],"score":0.7805},{"poly":[[137,1116],[305,1117],[305,1142],[136,1140]],"score":0.8908},{"poly":[[138,1045],[674,1049],[673,1074],[138,1070]],"score":0.7476},{"poly":[[140,1021],[1051,1021],[1051,1045],[140,1045]],"score":0.7682},{"poly":[[140,993],[1049,993],[1049,1017],[140,1017]],"score":0.7587},{"poly":[[140,965],[1051,965],[1051,989],[140,989]],"score":0.7768},{"poly":[[140,938],[1051,938],[1051,963],[140,963]],"score":0.7705},{"poly":[[140,910],[1049,910],[1049,933],[140,933]],"score":0.7734},{"poly":[[140,884],[1051,884],[1051,909],[140,909]],"score":0.7143},{"poly":[[140,858],[1051,858],[1051,882],[140,882]],"score":0.7575},{"poly":[[175,830],[1049,830],[1049,854],[175,854]],"score":0.7446},{"poly":[[138,789],[526,788],[526,812],[138,814]],"score":0.7547},{"poly":[[140,763],[1049,763],[1049,786],[140,786]],"score":0.8878},{"poly":[[138,735],[1049,735],[1049,760],[138,760]],"score":0.7795},{"poly":[[140,709],[1047,709],[1047,733],[140,733]],"score":0.7733},{"poly":[[138,681],[1049,682],[1049,707],[138,705]],"score":0.7875},{"poly":[[136,653],[1051,651],[1051,681],[137,682]],"score":0.686},{"poly":[[140,626],[1047,626],[1047,651],[140,651]],"score":0.7577},{"poly":[[140,600],[1049,600],[1049,624],[140,624]],"score":0.7831},{"poly":[[138,572],[1051,572],[1051,596],[138,596]],"score":0.7667},{"poly":[[140,546],[1051,546],[1051,570],[140,570]],"score":0.7731},{"poly":[[140,517],[1049,517],[1049,542],[140,542]],"score":0.741},{"poly":[[138,491],[1051,491],[1051,516],[138,516]],"score":0.7839},{"poly":[[138,446],[624,444],[624,468],[138,470]],"score":0.8083},{"poly":[[140,381],[331,381],[331,403],[140,403]],"score":0.8274},{"poly":[[140,354],[1049,354],[1049,377],[140,377]],"score":0.8929},{"poly":[[140,326],[1049,326],[1049,351],[140,351]],"score":0.7399},{"poly":[[138,300],[1049,300],[1049,325],[138,325]],"score":0.7638},{"poly":[[140,270],[1047,270],[1047,295],[140,295]],"score":0.699},{"poly":[[140,246],[1051,246],[1051,270],[140,270]],"score":0.727},{"poly":[[138,219],[1049,219],[1049,244],[138,244]],"score":0.741},{"poly":[[138,174],[445,174],[445,196],[138,196]],"score":0.8922}],"page_no":14,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1579},"conf":0.8048,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1575},"font_size":0.0,"text":"16"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":137,"x1":1051,"y0":163,"y1":508},"conf":0.979,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":387,"x1":522,"y0":171,"y1":200},"font_size":0.0,"text":"Training Tokens"},{"bbox":{"x0":146,"x1":280,"y0":181,"y1":211},"font_size":0.0,"text":"Training Setting"},{"bbox":{"x0":610,"x1":725,"y0":173,"y1":196},"font_size":0.0,"text":"w/o Tool Use"},{"bbox":{"x0":856,"x1":962,"y0":173,"y1":197},"font_size":0.0,"text":"w/ Tool Use"},{"bbox":{"x0":386,"x1":1042,"y0":200,"y1":228},"font_size":0.0,"text":"General Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python"},{"bbox":{"x0":146,"x1":332,"y0":230,"y1":260},"font_size":0.0,"text":"No Continual Training"},{"bbox":{"x0":387,"x1":405,"y0":239,"y1":257},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":460,"x1":475,"y0":240,"y1":255},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":511,"x1":529,"y0":239,"y1":256},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":573,"x1":618,"y0":235,"y1":257},"font_size":0.0,"text":" $2.9\\%$ "},{"bbox":{"x0":641,"x1":686,"y0":236,"y1":256},"font_size":0.0,"text":" $3.0\\%$ "},{"bbox":{"x0":708,"x1":762,"y0":235,"y1":257},"font_size":0.0,"text":" $12.3\\%$ "},{"bbox":{"x0":818,"x1":867,"y0":235,"y1":257},"font_size":0.0,"text":" $2.7\\%$ "},{"bbox":{"x0":953,"x1":1002,"y0":235,"y1":257},"font_size":0.0,"text":" $2.3\\%$ "},{"bbox":{"x0":513,"x1":678,"y0":264,"y1":299},"font_size":0.0,"text":"Two-Stage Training"},{"bbox":{"x0":145,"x1":353,"y0":297,"y1":327},"font_size":0.0,"text":"Stage 1: General Training"},{"bbox":{"x0":387,"x1":433,"y0":301,"y1":324},"font_size":0.0,"text":"400B"},{"bbox":{"x0":461,"x1":475,"y0":307,"y1":321},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":512,"x1":526,"y0":305,"y1":319},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":573,"x1":618,"y0":302,"y1":321},"font_size":0.0,"text":" $2.9\\%$ "},{"bbox":{"x0":641,"x1":686,"y0":302,"y1":321},"font_size":0.0,"text":" $3.2\\%$ "},{"bbox":{"x0":709,"x1":762,"y0":302,"y1":321},"font_size":0.0,"text":" $14.8\\%$ "},{"bbox":{"x0":818,"x1":867,"y0":301,"y1":322},"font_size":0.0,"text":" $3.3\\%$ "},{"bbox":{"x0":953,"x1":1001,"y0":301,"y1":322},"font_size":0.0,"text":" $2.3\\%$ "},{"bbox":{"x0":146,"x1":332,"y0":321,"y1":348},"font_size":0.0,"text":"Stage 2: Math Training"},{"bbox":{"x0":461,"x1":476,"y0":329,"y1":342},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":509,"x1":558,"y0":321,"y1":346},"font_size":0.0,"text":"150B"},{"bbox":{"x0":391,"x1":402,"y0":332,"y1":340},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":570,"x1":622,"y0":325,"y1":344},"font_size":0.0,"text":" $19.1\\%$ "},{"bbox":{"x0":639,"x1":690,"y0":325,"y1":344},"font_size":0.0,"text":" $14.4\\%$ "},{"bbox":{"x0":708,"x1":762,"y0":325,"y1":344},"font_size":0.0,"text":" $37.2\\%$ "},{"bbox":{"x0":816,"x1":871,"y0":324,"y1":344},"font_size":0.0,"text":" $14.3\\%$ "},{"bbox":{"x0":954,"x1":1001,"y0":325,"y1":344},"font_size":0.0,"text":" $6.7\\%$ "},{"bbox":{"x0":145,"x1":333,"y0":349,"y1":381},"font_size":0.0,"text":"Stage 1: Code Training"},{"bbox":{"x0":391,"x1":400,"y0":359,"y1":377},"font_size":8.0,"text":"â€“"},{"bbox":{"x0":458,"x1":507,"y0":352,"y1":377},"font_size":0.0,"text":"400B "},{"bbox":{"x0":509,"x1":527,"y0":356,"y1":373},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":573,"x1":619,"y0":355,"y1":375},"font_size":0.0,"text":" $5.9\\%$ "},{"bbox":{"x0":642,"x1":686,"y0":355,"y1":374},"font_size":0.0,"text":" $3.6\\%$ "},{"bbox":{"x0":709,"x1":763,"y0":354,"y1":375},"font_size":0.0,"text":" $19.9\\%$ "},{"bbox":{"x0":815,"x1":873,"y0":354,"y1":375},"font_size":0.0,"text":" $12.4\\%$ "},{"bbox":{"x0":950,"x1":1007,"y0":354,"y1":375},"font_size":0.0,"text":" $10.0\\%$ "},{"bbox":{"x0":145,"x1":333,"y0":372,"y1":403},"font_size":0.0,"text":"Stage 2: Math Training"},{"bbox":{"x0":391,"x1":400,"y0":381,"y1":399},"font_size":8.0,"text":"â€“"},{"bbox":{"x0":459,"x1":478,"y0":382,"y1":398},"font_size":0.0,"text":"-"},{"bbox":{"x0":510,"x1":558,"y0":376,"y1":398},"font_size":0.0,"text":"150B"},{"bbox":{"x0":569,"x1":622,"y0":378,"y1":397},"font_size":0.0,"text":" $21.9\\%$ "},{"bbox":{"x0":638,"x1":690,"y0":378,"y1":397},"font_size":0.0,"text":" $\\mathbf{15.3\\%}$ "},{"bbox":{"x0":708,"x1":762,"y0":378,"y1":397},"font_size":0.0,"text":" $39.7 \\%$ "},{"bbox":{"x0":814,"x1":873,"y0":377,"y1":398},"font_size":0.0,"text":" $17.4\\%$ "},{"bbox":{"x0":952,"x1":1004,"y0":377,"y1":398},"font_size":0.0,"text":" $9.4\\%$ "},{"bbox":{"x0":512,"x1":677,"y0":404,"y1":441},"font_size":0.0,"text":"One-Stage Training"},{"bbox":{"x0":145,"x1":266,"y0":437,"y1":469},"font_size":0.0,"text":"Math Training"},{"bbox":{"x0":391,"x1":400,"y0":447,"y1":465},"font_size":8.0,"text":"â€“"},{"bbox":{"x0":461,"x1":474,"y0":448,"y1":462},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":510,"x1":557,"y0":441,"y1":464},"font_size":0.0,"text":"150B"},{"bbox":{"x0":570,"x1":622,"y0":443,"y1":463},"font_size":0.0,"text":" $20.5\\%$ "},{"bbox":{"x0":638,"x1":690,"y0":443,"y1":463},"font_size":0.0,"text":" $13.1\\%$ "},{"bbox":{"x0":708,"x1":762,"y0":443,"y1":464},"font_size":0.0,"text":" $37.6\\%$ "},{"bbox":{"x0":815,"x1":872,"y0":441,"y1":464},"font_size":0.0,"text":" $11.4\\%$ "},{"bbox":{"x0":953,"x1":1002,"y0":441,"y1":464},"font_size":0.0,"text":" $6.5\\%$ "},{"bbox":{"x0":146,"x1":404,"y0":470,"y1":497},"font_size":0.0,"text":"Code & Math Mixed Training â€“"},{"bbox":{"x0":459,"x1":507,"y0":473,"y1":495},"font_size":0.0,"text":"400B"},{"bbox":{"x0":504,"x1":557,"y0":473,"y1":494},"font_size":0.0,"text":" 150B"},{"bbox":{"x0":570,"x1":622,"y0":474,"y1":494},"font_size":0.0,"text":" $17.6\\%$ "},{"bbox":{"x0":639,"x1":690,"y0":475,"y1":494},"font_size":0.0,"text":" $12.1\\%$ "},{"bbox":{"x0":709,"x1":762,"y0":474,"y1":494},"font_size":0.0,"text":" $36.3\\%$ "},{"bbox":{"x0":815,"x1":872,"y0":473,"y1":495},"font_size":0.0,"text":" $\\mathbf{19.7\\%}$ "},{"bbox":{"x0":949,"x1":1006,"y0":473,"y1":495},"font_size":0.0,"text":" $\\mathbf{13.5\\%}$ "}],"source":"layout det","text":"<html><body><table><thead><tr><td rowspan=\"2\">Training Setting</td><td colspan=\"3\">Training Tokens</td><td colspan=\"3\">w/o Tool Use</td><td colspan=\"2\">w/ Tool Use</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>General Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python</td><td></td></tr></thead><tbody><tr><td>No Continual Training</td><td>â€“</td><td>â€“</td><td>â€“</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $12.3\\%$ </td><td> $2.7\\%$ </td><td> $2.3\\%$ </td></tr><tr><td colspan=\"9\">Two-Stage Training</td></tr><tr><td>Stage 1: General Training</td><td>400B</td><td>â€“</td><td>â€“</td><td> $2.9\\%$ </td><td> $3.2\\%$ </td><td> $14.8\\%$ </td><td> $3.3\\%$ </td><td> $2.3\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $19.1\\%$ </td><td> $14.4\\%$ </td><td> $37.2\\%$ </td><td> $14.3\\%$ </td><td> $6.7\\%$ </td></tr><tr><td>Stage 1: Code Training</td><td>â€“</td><td>400B </td><td>â€“</td><td> $5.9\\%$ </td><td> $3.6\\%$ </td><td> $19.9\\%$ </td><td> $12.4\\%$ </td><td> $10.0\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>-</td><td>150B</td><td> $21.9\\%$ </td><td> $\\mathbf{15.3\\%}$ </td><td> $39.7 \\%$ </td><td> $17.4\\%$ </td><td> $9.4\\%$ </td></tr><tr><td colspan=\"9\">One-Stage Training</td></tr><tr><td>Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $20.5\\%$ </td><td> $13.1\\%$ </td><td> $37.6\\%$ </td><td> $11.4\\%$ </td><td> $6.5\\%$ </td></tr><tr><td>Code & Math Mixed Training â€“</td><td></td><td>400B</td><td> 150B</td><td> $17.6\\%$ </td><td> $12.1\\%$ </td><td> $36.3\\%$ </td><td> $\\mathbf{19.7\\%}$ </td><td> $\\mathbf{13.5\\%}$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":132,"x1":1057,"y0":516,"y1":637},"conf":0.9584,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":516,"y1":549},"font_size":0.0,"text":"Table 6 | Investigation of how code affects mathematical reasoning under different training"},{"bbox":{"x0":136,"x1":1049,"y0":547,"y1":574},"font_size":0.0,"text":"settings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning"},{"bbox":{"x0":138,"x1":1051,"y0":575,"y1":600},"font_size":0.0,"text":"performance without and with tool use via few-shot chain-of-thought prompting and few-shot"},{"bbox":{"x0":135,"x1":578,"y0":600,"y1":632},"font_size":0.0,"text":"program-of-thought prompting, respectively."}],"source":"layout det","text":"Table 6 | Investigation of how code affects mathematical reasoning under different training settings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning performance without and with tool use via few-shot chain-of-thought prompting and few-shot program-of-thought prompting, respectively."},{"bbox":{"x0":187,"x1":976,"y0":668,"y1":704},"conf":0.8283,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":982,"y0":670,"y1":696},"font_size":0.0,"text":"oves modelsâ€™ ability to do mathematical reasoning both with and without tool use"}],"source":"layout det","text":"oves modelsâ€™ ability to do mathematical reasoning both with and without tool use"},{"bbox":{"x0":133,"x1":1056,"y0":710,"y1":772},"conf":0.9339,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1053,"y0":709,"y1":740},"font_size":0.0,"text":"To study how code training affects mathematical reasoning, we experimented with the"},{"bbox":{"x0":138,"x1":725,"y0":738,"y1":767},"font_size":0.0,"text":"following two-stage training and one-stage training settings:"}],"source":"layout det","text":"To study how code training affects mathematical reasoning, we experimented with the following two-stage training and one-stage training settings:"},{"bbox":{"x0":134,"x1":344,"y0":779,"y1":812},"conf":0.6696,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":339,"y0":775,"y1":811},"font_size":0.0,"text":"Two-Stage Training"}],"source":"layout det","text":"Two-Stage Training"},{"bbox":{"x0":187,"x1":1058,"y0":817,"y1":875},"conf":0.9355,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":821,"y1":846},"font_size":0.0,"text":"Code Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: We train DeepSeek-"},{"bbox":{"x0":190,"x1":799,"y0":845,"y1":872},"font_size":0.0,"text":"LLM 1.3B for 400B code tokens followed by 150B math tokens;"}],"source":"layout det","text":"Code Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: We train DeepSeekLLM 1.3B for 400B code tokens followed by 150B math tokens;"},{"bbox":{"x0":188,"x1":1059,"y0":876,"y1":1015},"conf":0.9634,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1047,"y0":875,"y1":900},"font_size":0.0,"text":"General Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: As a control"},{"bbox":{"x0":191,"x1":1049,"y0":902,"y1":928},"font_size":0.0,"text":"experiment, we also experiment with general tokens (sampled from a large-scale general"},{"bbox":{"x0":193,"x1":1049,"y0":928,"y1":955},"font_size":0.0,"text":"corpus created by DeepSeek-AI) instead of code tokens in the frst stage of training, in ani"},{"bbox":{"x0":190,"x1":1051,"y0":951,"y1":986},"font_size":0.0,"text":"attempt to investigate the advantages of code tokens over general tokens in improving"},{"bbox":{"x0":188,"x1":436,"y0":977,"y1":1014},"font_size":0.0,"text":"mathematical reasoning."}],"source":"layout det","text":"General Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: As a control experiment, we also experiment with general tokens (sampled from a large-scale general corpus created by DeepSeek-AI) instead of code tokens in the frst stage of training, in ani attempt to investigate the advantages of code tokens over general tokens in improving mathematical reasoning."},{"bbox":{"x0":134,"x1":344,"y0":1021,"y1":1055},"conf":0.7253,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":341,"y0":1017,"y1":1053},"font_size":0.0,"text":"One-Stage Training"}],"source":"layout det","text":"One-Stage Training"},{"bbox":{"x0":187,"x1":1023,"y0":1061,"y1":1092},"conf":0.8949,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1016,"y0":1063,"y1":1089},"font_size":0.0,"text":"Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;"}],"source":"layout det","text":"Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;"},{"bbox":{"x0":187,"x1":1061,"y0":1094,"y1":1207},"conf":0.9532,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":168,"x1":1053,"y0":1089,"y1":1116},"font_size":0.0,"text":"Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training fol-"},{"bbox":{"x0":193,"x1":1053,"y0":1119,"y1":1144},"font_size":0.0,"text":"lowing code training degrades coding performance. We investigate whether code tokens,"},{"bbox":{"x0":193,"x1":1049,"y0":1147,"y1":1170},"font_size":0.0,"text":"when mixed with math tokens for one-stage training, would still improve mathematical"},{"bbox":{"x0":190,"x1":840,"y0":1168,"y1":1202},"font_size":0.0,"text":"reasoning and also alleviate the problem of catastrophic forgetting."}],"source":"layout det","text":"Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training following code training degrades coding performance. We investigate whether code tokens,when mixed with math tokens for one-stage training, would still improve mathematical reasoning and also alleviate the problem of catastrophic forgetting."},{"bbox":{"x0":132,"x1":1058,"y0":1238,"y1":1304},"conf":0.3512,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1238,"y1":1272},"font_size":0.0,"text":"ResultsTable 6 and Table 7 demonstrate the downstream performance under different training"},{"bbox":{"x0":138,"x1":225,"y0":1274,"y1":1298},"font_size":0.0,"text":"settings."}],"source":"layout det","text":"ResultsTable 6 and Table 7 demonstrate the downstream performance under different training settings."},{"bbox":{"x0":133,"x1":1060,"y0":1310,"y1":1509},"conf":0.9772,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1051,"y0":1307,"y1":1340},"font_size":0.0,"text":"Code training benefts program-aided mathematical reasoning, both under the two-stagei"},{"bbox":{"x0":135,"x1":1051,"y0":1335,"y1":1368},"font_size":0.0,"text":"training and one-stage training settings. As shown in Table 6, under the two-stage training"},{"bbox":{"x0":136,"x1":1049,"y0":1363,"y1":1395},"font_size":0.0,"text":"setting, code training alone already signifcantly enhances the ability to solve GSM8K andi"},{"bbox":{"x0":138,"x1":1051,"y0":1395,"y1":1419},"font_size":0.0,"text":"MATH problems using Python. Math training in the second stage yields further improvements."},{"bbox":{"x0":140,"x1":1051,"y0":1421,"y1":1445},"font_size":0.0,"text":"Interestingly, under the one-stage training setting, mixing code tokens and math tokens effec-"},{"bbox":{"x0":140,"x1":1051,"y0":1449,"y1":1474},"font_size":0.0,"text":"tively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also"},{"bbox":{"x0":136,"x1":927,"y0":1472,"y1":1503},"font_size":0.0,"text":"synergizes coding (Table 7) and program-aided mathematical reasoning (Table 6)."}],"source":"layout det","text":"Code training benefts program-aided mathematical reasoning, both under the two-stagei training and one-stage training settings. As shown in Table 6, under the two-stage training setting, code training alone already signifcantly enhances the ability to solve GSM8K andi MATH problems using Python. Math training in the second stage yields further improvements.Interestingly, under the one-stage training setting, mixing code tokens and math tokens effectively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also synergizes coding (Table 7) and program-aided mathematical reasoning (Table 6)."}],"formula_dets":[{"bbox":{"x0":815,"x1":873,"y0":354,"y1":375},"conf":0.8274,"label":"print_embedding","label_id":0},{"bbox":{"x0":950,"x1":1007,"y0":354,"y1":375},"conf":0.8141,"label":"print_embedding","label_id":0},{"bbox":{"x0":709,"x1":763,"y0":354,"y1":375},"conf":0.8129,"label":"print_embedding","label_id":0},{"bbox":{"x0":815,"x1":872,"y0":473,"y1":495},"conf":0.7986,"label":"print_embedding","label_id":0},{"bbox":{"x0":708,"x1":762,"y0":443,"y1":464},"conf":0.7969,"label":"print_embedding","label_id":0},{"bbox":{"x0":818,"x1":867,"y0":235,"y1":257},"conf":0.7965,"label":"print_embedding","label_id":0},{"bbox":{"x0":818,"x1":867,"y0":301,"y1":322},"conf":0.7949,"label":"print_embedding","label_id":0},{"bbox":{"x0":815,"x1":872,"y0":441,"y1":464},"conf":0.7929,"label":"print_embedding","label_id":0},{"bbox":{"x0":814,"x1":873,"y0":377,"y1":398},"conf":0.7905,"label":"print_embedding","label_id":0},{"bbox":{"x0":708,"x1":762,"y0":235,"y1":257},"conf":0.7897,"label":"print_embedding","label_id":0},{"bbox":{"x0":816,"x1":871,"y0":324,"y1":344},"conf":0.7876,"label":"print_embedding","label_id":0},{"bbox":{"x0":709,"x1":762,"y0":474,"y1":494},"conf":0.7783,"label":"print_embedding","label_id":0},{"bbox":{"x0":953,"x1":1002,"y0":441,"y1":464},"conf":0.7783,"label":"print_embedding","label_id":0},{"bbox":{"x0":949,"x1":1006,"y0":473,"y1":495},"conf":0.7772,"label":"print_embedding","label_id":0},{"bbox":{"x0":953,"x1":1001,"y0":301,"y1":322},"conf":0.7751,"label":"print_embedding","label_id":0},{"bbox":{"x0":708,"x1":762,"y0":378,"y1":397},"conf":0.7749,"label":"print_embedding","label_id":0},{"bbox":{"x0":953,"x1":1002,"y0":235,"y1":257},"conf":0.773,"label":"print_embedding","label_id":0},{"bbox":{"x0":641,"x1":686,"y0":236,"y1":256},"conf":0.7717,"label":"print_embedding","label_id":0},{"bbox":{"x0":708,"x1":762,"y0":325,"y1":344},"conf":0.7709,"label":"print_embedding","label_id":0},{"bbox":{"x0":573,"x1":618,"y0":235,"y1":257},"conf":0.7674,"label":"print_embedding","label_id":0},{"bbox":{"x0":709,"x1":762,"y0":302,"y1":321},"conf":0.7665,"label":"print_embedding","label_id":0},{"bbox":{"x0":641,"x1":686,"y0":302,"y1":321},"conf":0.7549,"label":"print_embedding","label_id":0},{"bbox":{"x0":954,"x1":1001,"y0":325,"y1":344},"conf":0.7499,"label":"print_embedding","label_id":0},{"bbox":{"x0":642,"x1":686,"y0":355,"y1":374},"conf":0.7474,"label":"print_embedding","label_id":0},{"bbox":{"x0":570,"x1":622,"y0":443,"y1":463},"conf":0.7459,"label":"print_embedding","label_id":0},{"bbox":{"x0":570,"x1":622,"y0":474,"y1":494},"conf":0.7434,"label":"print_embedding","label_id":0},{"bbox":{"x0":573,"x1":619,"y0":355,"y1":375},"conf":0.7407,"label":"print_embedding","label_id":0},{"bbox":{"x0":573,"x1":618,"y0":302,"y1":321},"conf":0.7396,"label":"print_embedding","label_id":0},{"bbox":{"x0":952,"x1":1004,"y0":377,"y1":398},"conf":0.7293,"label":"print_embedding","label_id":0},{"bbox":{"x0":551,"x1":577,"y0":880,"y1":897},"conf":0.7258,"label":"print_embedding","label_id":0},{"bbox":{"x0":569,"x1":622,"y0":378,"y1":397},"conf":0.7237,"label":"print_embedding","label_id":0},{"bbox":{"x0":638,"x1":690,"y0":378,"y1":397},"conf":0.7114,"label":"print_embedding","label_id":0},{"bbox":{"x0":639,"x1":690,"y0":475,"y1":494},"conf":0.6928,"label":"print_embedding","label_id":0},{"bbox":{"x0":570,"x1":622,"y0":325,"y1":344},"conf":0.6809,"label":"print_embedding","label_id":0},{"bbox":{"x0":639,"x1":690,"y0":325,"y1":344},"conf":0.6807,"label":"print_embedding","label_id":0},{"bbox":{"x0":638,"x1":690,"y0":443,"y1":463},"conf":0.675,"label":"print_embedding","label_id":0},{"bbox":{"x0":508,"x1":533,"y0":826,"y1":842},"conf":0.6728,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":137,"x1":1051,"y0":163,"y1":508},"conf":0.979,"label":"Table","label_id":5},{"bbox":{"x0":133,"x1":1060,"y0":1310,"y1":1509},"conf":0.9772,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1059,"y0":876,"y1":1015},"conf":0.9634,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":516,"y1":637},"conf":0.9584,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1061,"y0":1094,"y1":1207},"conf":0.9532,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1058,"y0":817,"y1":875},"conf":0.9355,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":710,"y1":772},"conf":0.9339,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1023,"y0":1061,"y1":1092},"conf":0.8949,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":976,"y0":668,"y1":704},"conf":0.8283,"label":"Text","label_id":1},{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1579},"conf":0.8048,"label":"Abandon","label_id":2},{"bbox":{"x0":132,"x1":1058,"y0":1238,"y1":1303},"conf":0.7766,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":344,"y0":1021,"y1":1055},"conf":0.7253,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":344,"y0":779,"y1":812},"conf":0.6696,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":1058,"y0":1238,"y1":1304},"conf":0.3512,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":344,"y0":779,"y1":812},"conf":0.2283,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1575],[580,1575]],"score":0.9404},{"poly":[[136,1474],[927,1472],[927,1502],[137,1503]],"score":0.7041},{"poly":[[140,1449],[1051,1449],[1051,1474],[140,1474]],"score":0.741},{"poly":[[140,1421],[1051,1421],[1051,1445],[140,1445]],"score":0.7368},{"poly":[[138,1395],[1051,1395],[1051,1419],[138,1419]],"score":0.7849},{"poly":[[136,1365],[1049,1363],[1049,1393],[137,1395]],"score":0.6885},{"poly":[[135,1335],[1051,1338],[1051,1368],[135,1365]],"score":0.6784},{"poly":[[170,1307],[1051,1310],[1051,1340],[170,1337]],"score":0.6703},{"poly":[[138,1274],[225,1274],[225,1298],[138,1298]],"score":0.8355},{"poly":[[138,1238],[1051,1242],[1051,1272],[138,1268]],"score":0.6946},{"poly":[[190,1168],[840,1172],[840,1202],[190,1198]],"score":0.6863},{"poly":[[193,1147],[1049,1147],[1049,1170],[193,1170]],"score":0.8628},{"poly":[[193,1119],[1053,1119],[1053,1144],[193,1144]],"score":0.7276},{"poly":[[168,1089],[1053,1091],[1053,1116],[168,1114]],"score":0.769},{"poly":[[170,1063],[1016,1065],[1016,1089],[170,1088]],"score":0.831},{"poly":[[137,1017],[341,1023],[340,1053],[136,1047]],"score":0.7124},{"poly":[[188,977],[436,984],[435,1014],[188,1007]],"score":0.6744},{"poly":[[190,951],[1051,956],[1051,986],[190,981]],"score":0.7171},{"poly":[[193,928],[1049,928],[1049,953],[193,953]],"score":0.7059},{"poly":[[191,903],[1049,902],[1049,926],[191,928]],"score":0.774},{"poly":[[186,875],[1047,875],[1047,900],[186,900]],"score":0.7761},{"poly":[[190,845],[799,847],[799,872],[190,870]],"score":0.7912},{"poly":[[172,821],[1049,821],[1049,846],[172,846]],"score":0.7744},{"poly":[[139,775],[339,781],[338,811],[138,805]],"score":0.6868},{"poly":[[138,738],[725,742],[725,767],[138,763]],"score":0.7995},{"poly":[[170,709],[1053,710],[1053,740],[170,738]],"score":0.7013},{"poly":[[138,672],[982,670],[982,695],[138,696]],"score":0.7822},{"poly":[[135,602],[578,600],[578,630],[135,632]],"score":0.6981},{"poly":[[138,575],[1051,575],[1051,600],[138,600]],"score":0.7653},{"poly":[[136,547],[1049,549],[1049,574],[136,572]],"score":0.7748},{"poly":[[137,516],[1051,519],[1051,549],[136,546]],"score":0.7007},{"poly":[[704,472],[764,472],[764,496],[704,496]],"score":0.8711},{"poly":[[636,474],[691,474],[691,493],[636,493]],"score":0.9219},{"poly":[[553,472],[624,472],[624,496],[553,496]],"score":0.684},{"poly":[[459,472],[558,472],[558,491],[459,491]],"score":0.7736},{"poly":[[144,468],[390,474],[390,498],[143,493]],"score":0.6963},{"poly":[[948,470],[1005,470],[1005,496],[948,496]],"score":0.8596},{"poly":[[810,468],[872,468],[872,500],[810,500]],"score":0.7647},{"poly":[[388,447],[402,447],[402,463],[388,463]],"score":0.6127},{"poly":[[636,444],[693,444],[693,463],[636,463]],"score":0.8674},{"poly":[[813,440],[872,440],[872,467],[813,467]],"score":0.8597},{"poly":[[688,440],[762,440],[762,465],[688,465]],"score":0.7887},{"poly":[[510,440],[626,440],[626,465],[510,465]],"score":0.7525},{"poly":[[144,436],[268,442],[267,467],[143,461]],"score":0.8071},{"poly":[[952,439],[1003,439],[1003,467],[952,467]],"score":0.8478},{"poly":[[514,410],[675,410],[675,435],[514,435]],"score":0.7859},{"poly":[[142,372],[333,374],[333,404],[142,402]],"score":0.7045},{"poly":[[815,374],[870,374],[870,400],[815,400]],"score":0.9787},{"poly":[[705,374],[762,374],[762,400],[705,400]],"score":0.9474},{"poly":[[634,374],[691,374],[691,400],[634,400]],"score":0.9231},{"poly":[[567,374],[624,374],[624,400],[567,400]],"score":0.9509},{"poly":[[509,374],[560,374],[560,400],[509,400]],"score":0.8018},{"poly":[[952,369],[1004,374],[1001,402],[949,397]],"score":0.8047},{"poly":[[705,353],[764,353],[764,377],[705,377]],"score":0.862},{"poly":[[459,353],[530,353],[530,377],[459,377]],"score":0.7878},{"poly":[[142,349],[333,351],[333,381],[142,379]],"score":0.7046},{"poly":[[950,351],[1007,351],[1007,377],[950,377]],"score":0.8583},{"poly":[[815,351],[872,351],[872,377],[815,377]],"score":0.8365},{"poly":[[638,351],[689,351],[689,377],[638,377]],"score":0.8248},{"poly":[[572,351],[620,351],[620,377],[572,377]],"score":0.9529},{"poly":[[145,323],[331,323],[331,347],[145,347]],"score":0.7528},{"poly":[[954,321],[1001,321],[1001,347],[954,347]],"score":0.9732},{"poly":[[815,321],[870,321],[870,347],[815,347]],"score":0.9683},{"poly":[[705,321],[764,321],[764,347],[705,347]],"score":0.8517},{"poly":[[636,321],[693,321],[693,347],[636,347]],"score":0.8645},{"poly":[[567,321],[626,321],[626,347],[567,347]],"score":0.8286},{"poly":[[510,319],[560,319],[560,346],[510,346]],"score":0.8301},{"poly":[[388,302],[432,302],[432,323],[388,323]],"score":0.9501},{"poly":[[142,296],[355,298],[354,328],[142,326]],"score":0.7343},{"poly":[[952,298],[1001,298],[1001,326],[952,326]],"score":0.834},{"poly":[[817,298],[867,298],[867,325],[817,325]],"score":0.9499},{"poly":[[705,298],[764,298],[764,325],[705,325]],"score":0.7748},{"poly":[[638,298],[688,298],[688,325],[638,325]],"score":0.9334},{"poly":[[572,298],[622,298],[622,325],[572,325]],"score":0.9156},{"poly":[[514,266],[675,270],[675,295],[514,291]],"score":0.7609},{"poly":[[707,233],[764,233],[764,258],[707,258]],"score":0.8685},{"poly":[[640,235],[689,235],[689,258],[640,258]],"score":0.8908},{"poly":[[144,231],[332,235],[331,260],[143,256]],"score":0.7543},{"poly":[[954,232],[1001,232],[1001,258],[954,258]],"score":0.8909},{"poly":[[819,232],[867,232],[867,260],[819,260]],"score":0.8563},{"poly":[[574,232],[624,232],[624,258],[574,258]],"score":0.8096},{"poly":[[386,202],[1042,204],[1042,228],[386,226]],"score":0.8123},{"poly":[[146,179],[279,185],[277,211],[145,205]],"score":0.7151},{"poly":[[856,175],[961,175],[961,195],[856,195]],"score":0.9467},{"poly":[[610,175],[721,175],[721,195],[610,195]],"score":0.9481},{"poly":[[390,175],[519,175],[519,195],[390,195]],"score":0.9245}],"page_no":15,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.8001,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1551,"y1":1575},"font_size":0.0,"text":"17"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":136,"x1":1052,"y0":168,"y1":517},"conf":0.9385,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":402,"x1":542,"y0":173,"y1":200},"font_size":0.0,"text":"Training Tokens"},{"bbox":{"x0":148,"x1":287,"y0":183,"y1":214},"font_size":0.0,"text":"Training Setting"},{"bbox":{"x0":586,"x1":670,"y0":186,"y1":207},"font_size":0.0,"text":"MMLU"},{"bbox":{"x0":658,"x1":1038,"y0":185,"y1":210},"font_size":0.0,"text":"BBHHumanEval (Pass@1) MBPP (Pass@1)"},{"bbox":{"x0":402,"x1":584,"y0":204,"y1":231},"font_size":0.0,"text":"General Code Math"},{"bbox":{"x0":147,"x1":342,"y0":236,"y1":264},"font_size":0.0,"text":"No Continual Training"},{"bbox":{"x0":481,"x1":492,"y0":247,"y1":257},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":533,"x1":548,"y0":245,"y1":260},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":592,"x1":648,"y0":239,"y1":262},"font_size":0.0,"text":" $24.5\\%$ "},{"bbox":{"x0":659,"x1":714,"y0":239,"y1":262},"font_size":0.0,"text":" $28.1\\%$ "},{"bbox":{"x0":782,"x1":839,"y0":239,"y1":262},"font_size":0.0,"text":" $12.2\\%$ "},{"bbox":{"x0":944,"x1":1003,"y0":238,"y1":262},"font_size":0.0,"text":" $13.0\\%$ "},{"bbox":{"x0":405,"x1":417,"y0":248,"y1":257},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":509,"x1":682,"y0":269,"y1":301},"font_size":0.0,"text":"Two-Stage Training"},{"bbox":{"x0":147,"x1":365,"y0":300,"y1":332},"font_size":0.0,"text":"Stage 1: General Training"},{"bbox":{"x0":401,"x1":451,"y0":303,"y1":329},"font_size":0.0,"text":"400B"},{"bbox":{"x0":479,"x1":494,"y0":311,"y1":325},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":531,"x1":550,"y0":308,"y1":327},"font_size":0.0,"text":"-"},{"bbox":{"x0":593,"x1":648,"y0":305,"y1":326},"font_size":0.0,"text":" $25.9\\%$ "},{"bbox":{"x0":659,"x1":714,"y0":305,"y1":325},"font_size":0.0,"text":" $27.7\\%$ "},{"bbox":{"x0":782,"x1":841,"y0":304,"y1":326},"font_size":0.0,"text":" $15.2\\%$ "},{"bbox":{"x0":944,"x1":1004,"y0":304,"y1":326},"font_size":0.0,"text":" $13.6\\%$ "},{"bbox":{"x0":146,"x1":344,"y0":325,"y1":356},"font_size":0.0,"text":"Stage 2: Math Training"},{"bbox":{"x0":402,"x1":420,"y0":332,"y1":350},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":479,"x1":496,"y0":333,"y1":349},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":531,"x1":580,"y0":328,"y1":350},"font_size":0.0,"text":"150B"},{"bbox":{"x0":592,"x1":648,"y0":330,"y1":350},"font_size":0.0,"text":" $33.1\\%$ "},{"bbox":{"x0":659,"x1":714,"y0":330,"y1":350},"font_size":0.0,"text":" $32.7\\%$ "},{"bbox":{"x0":782,"x1":840,"y0":329,"y1":350},"font_size":0.0,"text":" $12.8\\%$ "},{"bbox":{"x0":945,"x1":1003,"y0":330,"y1":350},"font_size":0.0,"text":" $13.2\\%$ "},{"bbox":{"x0":146,"x1":343,"y0":357,"y1":387},"font_size":0.0,"text":"Stage 1: Code Training"},{"bbox":{"x0":406,"x1":415,"y0":366,"y1":384},"font_size":9.0,"text":"â€“"},{"bbox":{"x0":477,"x1":534,"y0":359,"y1":384},"font_size":0.0,"text":"400B "},{"bbox":{"x0":522,"x1":551,"y0":362,"y1":381},"font_size":0.0,"text":"-"},{"bbox":{"x0":593,"x1":648,"y0":361,"y1":382},"font_size":0.0,"text":" $25.0\\%$ "},{"bbox":{"x0":659,"x1":715,"y0":360,"y1":382},"font_size":0.0,"text":" $31.5\\%$ "},{"bbox":{"x0":781,"x1":841,"y0":360,"y1":382},"font_size":0.0,"text":" $25.0\\%$ "},{"bbox":{"x0":944,"x1":1005,"y0":360,"y1":382},"font_size":0.0,"text":" $\\mathbf{40.0\\%}$ "},{"bbox":{"x0":146,"x1":343,"y0":380,"y1":412},"font_size":0.0,"text":"Stage 2: Math Training"},{"bbox":{"x0":406,"x1":415,"y0":389,"y1":408},"font_size":9.0,"text":"â€“"},{"bbox":{"x0":480,"x1":494,"y0":390,"y1":404},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":532,"x1":580,"y0":383,"y1":406},"font_size":0.0,"text":"150B"},{"bbox":{"x0":592,"x1":648,"y0":385,"y1":406},"font_size":0.0,"text":" $\\mathbf{36.2\\%}$ "},{"bbox":{"x0":659,"x1":715,"y0":385,"y1":406},"font_size":0.0,"text":" $35.3\\%$ "},{"bbox":{"x0":781,"x1":841,"y0":385,"y1":407},"font_size":0.0,"text":" $12.2\\%$ "},{"bbox":{"x0":944,"x1":1005,"y0":385,"y1":407},"font_size":0.0,"text":" $17.0\\%$ "},{"bbox":{"x0":510,"x1":680,"y0":413,"y1":445},"font_size":0.0,"text":"One-Stage Training"},{"bbox":{"x0":147,"x1":273,"y0":446,"y1":476},"font_size":0.0,"text":"Math Training"},{"bbox":{"x0":406,"x1":415,"y0":454,"y1":473},"font_size":9.0,"text":"â€“"},{"bbox":{"x0":531,"x1":581,"y0":448,"y1":473},"font_size":0.0,"text":"150B"},{"bbox":{"x0":593,"x1":647,"y0":450,"y1":471},"font_size":0.0,"text":" $32.3\\%$ "},{"bbox":{"x0":659,"x1":714,"y0":450,"y1":471},"font_size":0.0,"text":" $32.5\\%$ "},{"bbox":{"x0":781,"x1":841,"y0":448,"y1":472},"font_size":0.0,"text":" $11.6\\%$ "},{"bbox":{"x0":944,"x1":1004,"y0":448,"y1":472},"font_size":0.0,"text":" $13.2\\%$ "},{"bbox":{"x0":481,"x1":492,"y0":458,"y1":467},"font_size":0.0,"text":"â€“"},{"bbox":{"x0":148,"x1":408,"y0":479,"y1":507},"font_size":0.0,"text":"Code & Math Mixed Training "},{"bbox":{"x0":478,"x1":581,"y0":481,"y1":506},"font_size":0.0,"text":"400B 150B"},{"bbox":{"x0":593,"x1":647,"y0":484,"y1":504},"font_size":0.0,"text":" $33.5\\%$ "},{"bbox":{"x0":659,"x1":714,"y0":483,"y1":504},"font_size":0.0,"text":" $\\mathbf{35.6\\%}$ "},{"bbox":{"x0":781,"x1":840,"y0":482,"y1":505},"font_size":0.0,"text":" $\\mathbf{29.3\\%}$ "},{"bbox":{"x0":945,"x1":1003,"y0":482,"y1":505},"font_size":0.0,"text":" $39.4\\%$ "},{"bbox":{"x0":403,"x1":409,"y0":492,"y1":499},"font_size":0.0,"text":"-"}],"source":"layout det","text":"<html><body><table><thead><tr><td rowspan=\"2\">Training Setting</td><td colspan=\"3\">Training Tokens</td><td rowspan=\"2\">MMLU</td><td rowspan=\"2\">BBHHumanEval (Pass@1) MBPP (Pass@1)</td><td rowspan=\"2\"></td></tr><tr><td>General Code Math</td><td></td></tr></thead><tbody><tr><td>No Continual Training</td><td>â€“</td><td>â€“ â€“</td><td> $24.5\\%$ </td><td> $28.1\\%$ </td><td> $12.2\\%$ </td><td> $13.0\\%$ </td></tr><tr><td colspan=\"7\">Two-Stage Training</td></tr><tr><td>Stage 1: General Training</td><td>400B</td><td>â€“</td><td>-</td><td> $25.9\\%$ </td><td> $27.7\\%$ </td><td> $15.2\\%$  $13.6\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $33.1\\%$ </td><td> $32.7\\%$  $12.8\\%$ </td><td> $13.2\\%$ </td></tr><tr><td>Stage 1: Code Training</td><td>â€“</td><td>400B -</td><td></td><td> $25.0\\%$ </td><td> $31.5\\%$  $25.0\\%$ </td><td> $\\mathbf{40.0\\%}$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $\\mathbf{36.2\\%}$ </td><td> $35.3\\%$  $12.2\\%$ </td><td> $17.0\\%$ </td></tr><tr><td colspan=\"7\">One-Stage Training</td></tr><tr><td>Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $32.3\\%$  $32.5\\%$ </td><td> $11.6\\%$  $\\mathbf{29.3\\%}$ </td><td> $13.2\\%$ </td></tr><tr><td>Code & Math Mixed Training </td><td>-</td><td>400B 150B</td><td></td><td> $33.5\\%$ </td><td> $\\mathbf{35.6\\%}$ </td><td> $39.4\\%$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":133,"x1":1059,"y0":526,"y1":636},"conf":0.9091,"font_size":0.0,"label":"Table caption","label_id":6,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":530,"y1":554},"font_size":0.0,"text":"Table 7 | Investigation of how different settings of code and math training affect model perfor-"},{"bbox":{"x0":136,"x1":1051,"y0":558,"y1":582},"font_size":0.0,"text":"mance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM"},{"bbox":{"x0":135,"x1":1055,"y0":581,"y1":614},"font_size":0.0,"text":"1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting."},{"bbox":{"x0":140,"x1":998,"y0":612,"y1":637},"font_size":0.0,"text":"On HumanEval and MBPP, we conduct zero-shot and few-shot evaluations, respectively."}],"source":"layout det","text":"Table 7 | Investigation of how different settings of code and math training affect model performance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM 1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting.On HumanEval and MBPP, we conduct zero-shot and few-shot evaluations, respectively."},{"bbox":{"x0":137,"x1":1052,"y0":639,"y1":872},"conf":0.9775,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":594,"x1":746,"y0":640,"y1":661},"font_size":0.0,"text":"English Benchmarks"},{"bbox":{"x0":851,"x1":1007,"y0":639,"y1":660},"font_size":0.0,"text":"Chinese Benchmarks"},{"bbox":{"x0":146,"x1":200,"y0":658,"y1":681},"font_size":0.0,"text":"Model"},{"bbox":{"x0":349,"x1":495,"y0":658,"y1":682},"font_size":0.0,"text":"Size ArXiv Corpus"},{"bbox":{"x0":751,"x1":810,"y0":665,"y1":688},"font_size":0.0,"text":"MMLU"},{"bbox":{"x0":894,"x1":961,"y0":664,"y1":689},"font_size":0.0,"text":"Gaokao"},{"bbox":{"x0":975,"x1":1040,"y0":664,"y1":689},"font_size":0.0,"text":"Gaokao"},{"bbox":{"x0":527,"x1":654,"y0":676,"y1":697},"font_size":0.0,"text":"GSM8K MATH "},{"bbox":{"x0":648,"x1":700,"y0":675,"y1":697},"font_size":0.0,"text":"OCW"},{"bbox":{"x0":705,"x1":745,"y0":675,"y1":698},"font_size":0.0,"text":"SAT"},{"bbox":{"x0":813,"x1":883,"y0":676,"y1":698},"font_size":0.0,"text":"CMATH"},{"bbox":{"x0":755,"x1":808,"y0":686,"y1":709},"font_size":0.0,"text":"STEM"},{"bbox":{"x0":883,"x1":975,"y0":686,"y1":708},"font_size":0.0,"text":"MathCloze"},{"bbox":{"x0":969,"x1":1042,"y0":687,"y1":709},"font_size":0.0,"text":"MathQA"},{"bbox":{"x0":387,"x1":526,"y0":711,"y1":739},"font_size":0.0,"text":"No Math Training"},{"bbox":{"x0":538,"x1":577,"y0":717,"y1":734},"font_size":0.0,"text":" $2.9\\%$ "},{"bbox":{"x0":601,"x1":640,"y0":716,"y1":734},"font_size":0.0,"text":" $3.0\\%$ "},{"bbox":{"x0":655,"x1":694,"y0":716,"y1":735},"font_size":0.0,"text":" $2.9\\%$ "},{"bbox":{"x0":702,"x1":749,"y0":716,"y1":735},"font_size":0.0,"text":" $15.6\\%$ "},{"bbox":{"x0":758,"x1":805,"y0":716,"y1":735},"font_size":0.0,"text":" $19.5\\%$ "},{"bbox":{"x0":824,"x1":871,"y0":716,"y1":735},"font_size":0.0,"text":" $12.3\\%$ "},{"bbox":{"x0":907,"x1":947,"y0":716,"y1":734},"font_size":0.0,"text":" $0.8\\%$ "},{"bbox":{"x0":984,"x1":1032,"y0":716,"y1":735},"font_size":0.0,"text":" $17.9\\%$ "},{"bbox":{"x0":145,"x1":266,"y0":732,"y1":761},"font_size":0.0,"text":"DeepSeek-LLM"},{"bbox":{"x0":347,"x1":390,"y0":733,"y1":759},"font_size":0.0,"text":"1.3B"},{"bbox":{"x0":382,"x1":462,"y0":737,"y1":768},"font_size":0.0,"text":"MathPile"},{"bbox":{"x0":538,"x1":577,"y0":746,"y1":762},"font_size":0.0,"text":" $2.7\\%$ "},{"bbox":{"x0":601,"x1":640,"y0":745,"y1":762},"font_size":0.0,"text":" $3.3\\%$ "},{"bbox":{"x0":655,"x1":694,"y0":745,"y1":762},"font_size":0.0,"text":" $2.2\\%$ "},{"bbox":{"x0":703,"x1":748,"y0":744,"y1":762},"font_size":0.0,"text":" $12.5\\%$ "},{"bbox":{"x0":759,"x1":805,"y0":745,"y1":762},"font_size":0.0,"text":" $15.7\\%$ "},{"bbox":{"x0":828,"x1":868,"y0":745,"y1":762},"font_size":0.0,"text":" $1.2\\%$ "},{"bbox":{"x0":907,"x1":947,"y0":745,"y1":762},"font_size":0.0,"text":" $0.0\\%$ "},{"bbox":{"x0":988,"x1":1027,"y0":745,"y1":762},"font_size":0.0,"text":" $2.8\\%$ "},{"bbox":{"x0":388,"x1":525,"y0":764,"y1":786},"font_size":0.0,"text":"ArXiv-RedPajama"},{"bbox":{"x0":538,"x1":578,"y0":766,"y1":783},"font_size":0.0,"text":" $3.3\\%$ "},{"bbox":{"x0":601,"x1":640,"y0":767,"y1":783},"font_size":0.0,"text":" $3.4\\%$ "},{"bbox":{"x0":655,"x1":694,"y0":766,"y1":783},"font_size":0.0,"text":" $4.0\\%$ "},{"bbox":{"x0":705,"x1":745,"y0":766,"y1":783},"font_size":0.0,"text":" $9.4\\%$ "},{"bbox":{"x0":761,"x1":802,"y0":766,"y1":783},"font_size":0.0,"text":" $9.0\\%$ "},{"bbox":{"x0":828,"x1":868,"y0":766,"y1":783},"font_size":0.0,"text":" $7.4\\%$ "},{"bbox":{"x0":907,"x1":947,"y0":766,"y1":783},"font_size":0.0,"text":" $0.8\\%$ "},{"bbox":{"x0":988,"x1":1028,"y0":767,"y1":783},"font_size":0.0,"text":" $2.3\\%$ "},{"bbox":{"x0":387,"x1":526,"y0":788,"y1":816},"font_size":0.0,"text":"No Math Training"},{"bbox":{"x0":535,"x1":582,"y0":794,"y1":812},"font_size":0.0,"text":" $29.0\\%$ "},{"bbox":{"x0":598,"x1":643,"y0":795,"y1":811},"font_size":0.0,"text":" $12.5\\%$ "},{"bbox":{"x0":656,"x1":694,"y0":794,"y1":811},"font_size":0.0,"text":" $6.6\\%$ "},{"bbox":{"x0":703,"x1":748,"y0":794,"y1":811},"font_size":0.0,"text":" $40.6\\%$ "},{"bbox":{"x0":758,"x1":805,"y0":793,"y1":812},"font_size":0.0,"text":" $38.1\\%$ "},{"bbox":{"x0":824,"x1":871,"y0":794,"y1":811},"font_size":0.0,"text":" $45.9\\%$ "},{"bbox":{"x0":907,"x1":947,"y0":794,"y1":811},"font_size":0.0,"text":" $5.9\\%$ "},{"bbox":{"x0":983,"x1":1032,"y0":793,"y1":812},"font_size":0.0,"text":" $21.1\\%$ "},{"bbox":{"x0":148,"x1":375,"y0":812,"y1":834},"font_size":0.0,"text":"DeepSeek-Coder-Base-v1.5 7B"},{"bbox":{"x0":386,"x1":459,"y0":818,"y1":840},"font_size":0.0,"text":"MathPile"},{"bbox":{"x0":535,"x1":582,"y0":822,"y1":839},"font_size":0.0,"text":" $23.6\\%$ "},{"bbox":{"x0":598,"x1":643,"y0":823,"y1":839},"font_size":0.0,"text":" $11.5\\%$ "},{"bbox":{"x0":655,"x1":694,"y0":823,"y1":839},"font_size":0.0,"text":" $7.0\\%$ "},{"bbox":{"x0":703,"x1":748,"y0":823,"y1":839},"font_size":0.0,"text":" $46.9\\%$ "},{"bbox":{"x0":759,"x1":805,"y0":822,"y1":839},"font_size":0.0,"text":" $35.8\\%$ "},{"bbox":{"x0":824,"x1":871,"y0":823,"y1":839},"font_size":0.0,"text":" $37.9\\%$ "},{"bbox":{"x0":908,"x1":947,"y0":822,"y1":839},"font_size":0.0,"text":" $4.2\\%$ "},{"bbox":{"x0":984,"x1":1032,"y0":822,"y1":839},"font_size":0.0,"text":" $25.6\\%$ "},{"bbox":{"x0":388,"x1":525,"y0":841,"y1":862},"font_size":0.0,"text":"ArXiv-RedPajama"},{"bbox":{"x0":535,"x1":582,"y0":843,"y1":859},"font_size":0.0,"text":" $28.1\\%$ "},{"bbox":{"x0":595,"x1":648,"y0":839,"y1":862},"font_size":0.0,"text":"11.1%"},{"bbox":{"x0":656,"x1":693,"y0":844,"y1":859},"font_size":0.0,"text":" $7.7\\%$ "},{"bbox":{"x0":702,"x1":748,"y0":843,"y1":859},"font_size":0.0,"text":" $50.0\\%$ "},{"bbox":{"x0":758,"x1":804,"y0":843,"y1":859},"font_size":0.0,"text":" $35.2\\%$ "},{"bbox":{"x0":825,"x1":871,"y0":843,"y1":859},"font_size":0.0,"text":" $42.6\\%$ "},{"bbox":{"x0":908,"x1":946,"y0":844,"y1":859},"font_size":0.0,"text":" $7.6\\%$ "},{"bbox":{"x0":984,"x1":1032,"y0":843,"y1":859},"font_size":0.0,"text":" $24.8\\%$ "}],"source":"layout det","text":"<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\"></td><td rowspan=\"2\">Size ArXiv Corpus</td><td colspan=\"4\">English Benchmarks</td><td colspan=\"4\">Chinese Benchmarks</td></tr><tr><td></td><td>GSM8K MATH </td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathCloze</td><td>Gaokao MathQA</td></tr></thead><tbody><tr><td rowspan=\"3\">DeepSeek-LLM</td><td rowspan=\"3\">1.3B</td><td>No Math Training</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $2.9\\%$ </td><td> $15.6\\%$ </td><td> $19.5\\%$ </td><td> $12.3\\%$ </td><td> $0.8\\%$ </td><td> $17.9\\%$ </td></tr><tr><td>MathPile</td><td> $2.7\\%$ </td><td> $3.3\\%$ </td><td> $2.2\\%$ </td><td> $12.5\\%$ </td><td> $15.7\\%$ </td><td> $1.2\\%$ </td><td> $0.0\\%$ </td><td> $2.8\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $3.3\\%$ </td><td> $3.4\\%$ </td><td> $4.0\\%$ </td><td> $9.4\\%$ </td><td> $9.0\\%$ </td><td> $7.4\\%$ </td><td> $0.8\\%$ </td><td> $2.3\\%$ </td></tr><tr><td rowspan=\"3\">DeepSeek-Coder-Base-v1.5 7B</td><td rowspan=\"3\"></td><td>No Math Training</td><td> $29.0\\%$ </td><td> $12.5\\%$ </td><td> $6.6\\%$ </td><td> $40.6\\%$ </td><td> $38.1\\%$ </td><td> $45.9\\%$ </td><td> $5.9\\%$ </td><td> $21.1\\%$ </td></tr><tr><td>MathPile</td><td> $23.6\\%$ </td><td> $11.5\\%$ </td><td> $7.0\\%$ </td><td> $46.9\\%$ </td><td> $35.8\\%$ </td><td> $37.9\\%$ </td><td> $4.2\\%$ </td><td> $25.6\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $28.1\\%$ </td><td>11.1%</td><td> $7.7\\%$ </td><td> $50.0\\%$ </td><td> $35.2\\%$ </td><td> $42.6\\%$ </td><td> $7.6\\%$ </td><td> $24.8\\%$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":132,"x1":1055,"y0":881,"y1":945},"conf":0.8519,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":886,"y1":910},"font_size":0.0,"text":"Table 8 | Effect of math training on different arXiv datasets. Model performance is evaluated"},{"bbox":{"x0":133,"x1":560,"y0":909,"y1":942},"font_size":0.0,"text":"with few-shot chain-of-thought prompting."}],"source":"layout det","text":"Table 8 | Effect of math training on different arXiv datasets. Model performance is evaluated with few-shot chain-of-thought prompting."},{"bbox":{"x0":370,"x1":814,"y0":959,"y1":1101},"conf":0.9675,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":378,"x1":511,"y0":966,"y1":998},"font_size":0.0,"text":"ArXiv Corpus"},{"bbox":{"x0":549,"x1":686,"y0":968,"y1":995},"font_size":0.0,"text":"miniF2F-valid"},{"bbox":{"x0":680,"x1":806,"y0":966,"y1":996},"font_size":0.0,"text":" miniF2F-test"},{"bbox":{"x0":376,"x1":548,"y0":999,"y1":1035},"font_size":0.0,"text":"No Math Training"},{"bbox":{"x0":589,"x1":644,"y0":1009,"y1":1028},"font_size":0.0,"text":" $20.1\\%$ "},{"bbox":{"x0":718,"x1":773,"y0":1009,"y1":1028},"font_size":0.0,"text":" $21.7\\%$ "},{"bbox":{"x0":377,"x1":467,"y0":1034,"y1":1066},"font_size":0.0,"text":"MathPile"},{"bbox":{"x0":589,"x1":644,"y0":1045,"y1":1062},"font_size":0.0,"text":" $16.8\\%$ "},{"bbox":{"x0":719,"x1":773,"y0":1045,"y1":1063},"font_size":0.0,"text":" $16.4\\%$ "},{"bbox":{"x0":377,"x1":547,"y0":1060,"y1":1090},"font_size":0.0,"text":"ArXiv-RedPajama"},{"bbox":{"x0":589,"x1":644,"y0":1067,"y1":1085},"font_size":0.0,"text":" $14.8\\%$ "},{"bbox":{"x0":718,"x1":773,"y0":1067,"y1":1086},"font_size":0.0,"text":" $11.9\\%$ "}],"source":"layout det","text":"<html><body><table><tr><td>ArXiv Corpus</td><td>miniF2F-valid</td><td> miniF2F-test</td></tr><tr><td>No Math Training</td><td> $20.1\\%$ </td><td> $21.7\\%$ </td></tr><tr><td>MathPile</td><td> $16.8\\%$ </td><td> $16.4\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $14.8\\%$ </td><td> $11.9\\%$ </td></tr></table></body></html>"},{"bbox":{"x0":131,"x1":1060,"y0":1110,"y1":1175},"conf":0.8461,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1114,"y1":1138},"font_size":0.0,"text":"Table 9 | Effect of math training on different arXiv corpora, the base model being DeepSeek-"},{"bbox":{"x0":140,"x1":835,"y0":1142,"y1":1167},"font_size":0.0,"text":"Coder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle."}],"source":"layout det","text":"Table 9 | Effect of math training on different arXiv corpora, the base model being DeepSeekCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle."},{"bbox":{"x0":134,"x1":1060,"y0":1194,"y1":1393},"conf":0.9756,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":167,"x1":1055,"y0":1189,"y1":1228},"font_size":0.0,"text":"Code training also improves mathematical reasoning without tool use. Under the two-stage"},{"bbox":{"x0":138,"x1":1053,"y0":1224,"y1":1249},"font_size":0.0,"text":"training setting, the initial stage of code training already results in moderate enhancements."},{"bbox":{"x0":135,"x1":1055,"y0":1247,"y1":1279},"font_size":0.0,"text":"It also boosts the effciency of the subsequent math training, eventually leading to the besti"},{"bbox":{"x0":133,"x1":1055,"y0":1275,"y1":1307},"font_size":0.0,"text":"performance. However, combining code tokens and math tokens for one-stage training com-"},{"bbox":{"x0":138,"x1":1053,"y0":1305,"y1":1330},"font_size":0.0,"text":"promises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,"},{"bbox":{"x0":138,"x1":1051,"y0":1333,"y1":1358},"font_size":0.0,"text":"due to its limited scale, lacks the capacity to fully assimilate both code and mathematical data"},{"bbox":{"x0":138,"x1":296,"y0":1361,"y1":1384},"font_size":0.0,"text":"simultaneously."}],"source":"layout det","text":"Code training also improves mathematical reasoning without tool use. Under the two-stage training setting, the initial stage of code training already results in moderate enhancements.It also boosts the effciency of the subsequent math training, eventually leading to the besti performance. However, combining code tokens and math tokens for one-stage training compromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,due to its limited scale, lacks the capacity to fully assimilate both code and mathematical data simultaneously."},{"bbox":{"x0":133,"x1":888,"y0":1416,"y1":1454},"conf":0.7878,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":881,"y0":1416,"y1":1449},"font_size":0.0,"text":"5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning"}],"source":"layout det","text":"5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning"},{"bbox":{"x0":132,"x1":1056,"y0":1462,"y1":1525},"conf":0.9293,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1463,"y1":1489},"font_size":0.0,"text":"ArXiv papers are commonly included as a component of math pre-training data (Azerbayev"},{"bbox":{"x0":138,"x1":1053,"y0":1493,"y1":1517},"font_size":0.0,"text":"et al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,"}],"source":"layout det","text":"ArXiv papers are commonly included as a component of math pre-training data (Azerbayev et al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,"}],"formula_dets":[{"bbox":{"x0":944,"x1":1003,"y0":238,"y1":262},"conf":0.8163,"label":"print_embedding","label_id":0},{"bbox":{"x0":944,"x1":1005,"y0":360,"y1":382},"conf":0.8106,"label":"print_embedding","label_id":0},{"bbox":{"x0":781,"x1":841,"y0":360,"y1":382},"conf":0.8066,"label":"print_embedding","label_id":0},{"bbox":{"x0":782,"x1":839,"y0":239,"y1":262},"conf":0.7964,"label":"print_embedding","label_id":0},{"bbox":{"x0":782,"x1":841,"y0":304,"y1":326},"conf":0.7859,"label":"print_embedding","label_id":0},{"bbox":{"x0":781,"x1":841,"y0":385,"y1":407},"conf":0.7844,"label":"print_embedding","label_id":0},{"bbox":{"x0":589,"x1":644,"y0":1009,"y1":1028},"conf":0.7829,"label":"print_embedding","label_id":0},{"bbox":{"x0":944,"x1":1004,"y0":304,"y1":326},"conf":0.7817,"label":"print_embedding","label_id":0},{"bbox":{"x0":944,"x1":1005,"y0":385,"y1":407},"conf":0.7785,"label":"print_embedding","label_id":0},{"bbox":{"x0":782,"x1":840,"y0":329,"y1":350},"conf":0.7771,"label":"print_embedding","label_id":0},{"bbox":{"x0":781,"x1":841,"y0":448,"y1":472},"conf":0.7763,"label":"print_embedding","label_id":0},{"bbox":{"x0":589,"x1":644,"y0":1045,"y1":1062},"conf":0.7719,"label":"print_embedding","label_id":0},{"bbox":{"x0":719,"x1":773,"y0":1045,"y1":1063},"conf":0.7716,"label":"print_embedding","label_id":0},{"bbox":{"x0":589,"x1":644,"y0":1067,"y1":1085},"conf":0.7679,"label":"print_embedding","label_id":0},{"bbox":{"x0":659,"x1":714,"y0":239,"y1":262},"conf":0.7679,"label":"print_embedding","label_id":0},{"bbox":{"x0":944,"x1":1004,"y0":448,"y1":472},"conf":0.763,"label":"print_embedding","label_id":0},{"bbox":{"x0":781,"x1":840,"y0":482,"y1":505},"conf":0.7629,"label":"print_embedding","label_id":0},{"bbox":{"x0":945,"x1":1003,"y0":330,"y1":350},"conf":0.7628,"label":"print_embedding","label_id":0},{"bbox":{"x0":984,"x1":1032,"y0":843,"y1":859},"conf":0.7624,"label":"print_embedding","label_id":0},{"bbox":{"x0":984,"x1":1032,"y0":822,"y1":839},"conf":0.76,"label":"print_embedding","label_id":0},{"bbox":{"x0":907,"x1":947,"y0":716,"y1":734},"conf":0.7516,"label":"print_embedding","label_id":0},{"bbox":{"x0":718,"x1":773,"y0":1067,"y1":1086},"conf":0.7469,"label":"print_embedding","label_id":0},{"bbox":{"x0":592,"x1":648,"y0":239,"y1":262},"conf":0.746,"label":"print_embedding","label_id":0},{"bbox":{"x0":825,"x1":871,"y0":843,"y1":859},"conf":0.7447,"label":"print_embedding","label_id":0},{"bbox":{"x0":718,"x1":773,"y0":1009,"y1":1028},"conf":0.7393,"label":"print_embedding","label_id":0},{"bbox":{"x0":945,"x1":1003,"y0":482,"y1":505},"conf":0.7344,"label":"print_embedding","label_id":0},{"bbox":{"x0":659,"x1":715,"y0":385,"y1":406},"conf":0.7343,"label":"print_embedding","label_id":0},{"bbox":{"x0":758,"x1":804,"y0":843,"y1":859},"conf":0.7332,"label":"print_embedding","label_id":0},{"bbox":{"x0":907,"x1":947,"y0":745,"y1":762},"conf":0.7329,"label":"print_embedding","label_id":0},{"bbox":{"x0":593,"x1":647,"y0":450,"y1":471},"conf":0.7283,"label":"print_embedding","label_id":0},{"bbox":{"x0":659,"x1":714,"y0":330,"y1":350},"conf":0.7266,"label":"print_embedding","label_id":0},{"bbox":{"x0":601,"x1":640,"y0":716,"y1":734},"conf":0.7258,"label":"print_embedding","label_id":0},{"bbox":{"x0":659,"x1":714,"y0":450,"y1":471},"conf":0.7237,"label":"print_embedding","label_id":0},{"bbox":{"x0":655,"x1":694,"y0":716,"y1":735},"conf":0.7222,"label":"print_embedding","label_id":0},{"bbox":{"x0":908,"x1":947,"y0":822,"y1":839},"conf":0.7212,"label":"print_embedding","label_id":0},{"bbox":{"x0":759,"x1":805,"y0":822,"y1":839},"conf":0.7168,"label":"print_embedding","label_id":0},{"bbox":{"x0":592,"x1":648,"y0":385,"y1":406},"conf":0.7158,"label":"print_embedding","label_id":0},{"bbox":{"x0":659,"x1":715,"y0":360,"y1":382},"conf":0.7092,"label":"print_embedding","label_id":0},{"bbox":{"x0":824,"x1":871,"y0":823,"y1":839},"conf":0.7083,"label":"print_embedding","label_id":0},{"bbox":{"x0":659,"x1":714,"y0":305,"y1":325},"conf":0.7065,"label":"print_embedding","label_id":0},{"bbox":{"x0":759,"x1":805,"y0":745,"y1":762},"conf":0.7063,"label":"print_embedding","label_id":0},{"bbox":{"x0":907,"x1":947,"y0":794,"y1":811},"conf":0.7057,"label":"print_embedding","label_id":0},{"bbox":{"x0":758,"x1":805,"y0":716,"y1":735},"conf":0.7054,"label":"print_embedding","label_id":0},{"bbox":{"x0":593,"x1":647,"y0":484,"y1":504},"conf":0.7051,"label":"print_embedding","label_id":0},{"bbox":{"x0":824,"x1":871,"y0":716,"y1":735},"conf":0.7036,"label":"print_embedding","label_id":0},{"bbox":{"x0":659,"x1":714,"y0":483,"y1":504},"conf":0.6991,"label":"print_embedding","label_id":0},{"bbox":{"x0":907,"x1":947,"y0":766,"y1":783},"conf":0.6984,"label":"print_embedding","label_id":0},{"bbox":{"x0":538,"x1":577,"y0":717,"y1":734},"conf":0.6982,"label":"print_embedding","label_id":0},{"bbox":{"x0":984,"x1":1032,"y0":716,"y1":735},"conf":0.6973,"label":"print_embedding","label_id":0},{"bbox":{"x0":983,"x1":1032,"y0":793,"y1":812},"conf":0.6969,"label":"print_embedding","label_id":0},{"bbox":{"x0":908,"x1":946,"y0":844,"y1":859},"conf":0.6955,"label":"print_embedding","label_id":0},{"bbox":{"x0":593,"x1":648,"y0":361,"y1":382},"conf":0.6887,"label":"print_embedding","label_id":0},{"bbox":{"x0":703,"x1":748,"y0":823,"y1":839},"conf":0.6863,"label":"print_embedding","label_id":0},{"bbox":{"x0":702,"x1":749,"y0":716,"y1":735},"conf":0.6842,"label":"print_embedding","label_id":0},{"bbox":{"x0":702,"x1":748,"y0":843,"y1":859},"conf":0.684,"label":"print_embedding","label_id":0},{"bbox":{"x0":593,"x1":648,"y0":305,"y1":326},"conf":0.6833,"label":"print_embedding","label_id":0},{"bbox":{"x0":988,"x1":1027,"y0":745,"y1":762},"conf":0.6819,"label":"print_embedding","label_id":0},{"bbox":{"x0":761,"x1":802,"y0":766,"y1":783},"conf":0.6774,"label":"print_embedding","label_id":0},{"bbox":{"x0":656,"x1":694,"y0":794,"y1":811},"conf":0.6751,"label":"print_embedding","label_id":0},{"bbox":{"x0":988,"x1":1028,"y0":767,"y1":783},"conf":0.6721,"label":"print_embedding","label_id":0},{"bbox":{"x0":601,"x1":640,"y0":745,"y1":762},"conf":0.6682,"label":"print_embedding","label_id":0},{"bbox":{"x0":598,"x1":643,"y0":795,"y1":811},"conf":0.665,"label":"print_embedding","label_id":0},{"bbox":{"x0":703,"x1":748,"y0":794,"y1":811},"conf":0.6628,"label":"print_embedding","label_id":0},{"bbox":{"x0":703,"x1":748,"y0":744,"y1":762},"conf":0.6598,"label":"print_embedding","label_id":0},{"bbox":{"x0":538,"x1":577,"y0":746,"y1":762},"conf":0.6539,"label":"print_embedding","label_id":0},{"bbox":{"x0":535,"x1":582,"y0":822,"y1":839},"conf":0.6525,"label":"print_embedding","label_id":0},{"bbox":{"x0":824,"x1":871,"y0":794,"y1":811},"conf":0.6515,"label":"print_embedding","label_id":0},{"bbox":{"x0":601,"x1":640,"y0":767,"y1":783},"conf":0.6509,"label":"print_embedding","label_id":0},{"bbox":{"x0":598,"x1":643,"y0":823,"y1":839},"conf":0.6499,"label":"print_embedding","label_id":0},{"bbox":{"x0":655,"x1":694,"y0":823,"y1":839},"conf":0.6456,"label":"print_embedding","label_id":0},{"bbox":{"x0":592,"x1":648,"y0":330,"y1":350},"conf":0.6421,"label":"print_embedding","label_id":0},{"bbox":{"x0":705,"x1":745,"y0":766,"y1":783},"conf":0.6417,"label":"print_embedding","label_id":0},{"bbox":{"x0":828,"x1":868,"y0":745,"y1":762},"conf":0.6342,"label":"print_embedding","label_id":0},{"bbox":{"x0":538,"x1":578,"y0":766,"y1":783},"conf":0.6342,"label":"print_embedding","label_id":0},{"bbox":{"x0":758,"x1":805,"y0":793,"y1":812},"conf":0.6336,"label":"print_embedding","label_id":0},{"bbox":{"x0":656,"x1":693,"y0":844,"y1":859},"conf":0.6294,"label":"print_embedding","label_id":0},{"bbox":{"x0":655,"x1":694,"y0":745,"y1":762},"conf":0.6293,"label":"print_embedding","label_id":0},{"bbox":{"x0":535,"x1":582,"y0":794,"y1":812},"conf":0.6264,"label":"print_embedding","label_id":0},{"bbox":{"x0":655,"x1":694,"y0":766,"y1":783},"conf":0.6132,"label":"print_embedding","label_id":0},{"bbox":{"x0":535,"x1":582,"y0":843,"y1":859},"conf":0.598,"label":"print_embedding","label_id":0},{"bbox":{"x0":828,"x1":868,"y0":766,"y1":783},"conf":0.5923,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":137,"x1":1052,"y0":639,"y1":872},"conf":0.9775,"label":"Table","label_id":5},{"bbox":{"x0":134,"x1":1060,"y0":1194,"y1":1393},"conf":0.9756,"label":"Text","label_id":1},{"bbox":{"x0":370,"x1":814,"y0":959,"y1":1101},"conf":0.9675,"label":"Table","label_id":5},{"bbox":{"x0":136,"x1":1052,"y0":168,"y1":517},"conf":0.9385,"label":"Table","label_id":5},{"bbox":{"x0":132,"x1":1056,"y0":1462,"y1":1525},"conf":0.9293,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":526,"y1":636},"conf":0.9091,"label":"Table caption","label_id":6},{"bbox":{"x0":132,"x1":1055,"y0":881,"y1":945},"conf":0.8519,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1060,"y0":1110,"y1":1175},"conf":0.8461,"label":"Text","label_id":1},{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.8001,"label":"Abandon","label_id":2},{"bbox":{"x0":133,"x1":888,"y0":1416,"y1":1454},"conf":0.7878,"label":"Title","label_id":0},{"bbox":{"x0":140,"x1":1050,"y0":167,"y1":231},"conf":0.42,"label":"Table caption","label_id":6}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1551],[610,1551],[610,1575],[580,1575]],"score":0.8511},{"poly":[[138,1493],[1053,1493],[1053,1517],[138,1517]],"score":0.8132},{"poly":[[137,1463],[1051,1465],[1051,1489],[136,1488]],"score":0.8298},{"poly":[[137,1416],[881,1419],[881,1449],[136,1445]],"score":0.6841},{"poly":[[138,1361],[296,1361],[296,1384],[138,1384]],"score":0.9125},{"poly":[[138,1333],[1051,1333],[1051,1358],[138,1358]],"score":0.7701},{"poly":[[138,1305],[1053,1305],[1053,1330],[138,1330]],"score":0.7496},{"poly":[[133,1277],[1054,1275],[1055,1305],[133,1307]],"score":0.6938},{"poly":[[135,1247],[1055,1249],[1054,1279],[135,1277]],"score":0.7505},{"poly":[[138,1224],[1053,1224],[1053,1249],[138,1249]],"score":0.7454},{"poly":[[167,1189],[1055,1193],[1054,1228],[167,1224]],"score":0.6438},{"poly":[[140,1142],[835,1142],[835,1167],[140,1167]],"score":0.802},{"poly":[[140,1114],[1051,1114],[1051,1138],[140,1138]],"score":0.8142},{"poly":[[379,1063],[546,1063],[546,1088],[379,1088]],"score":0.9202},{"poly":[[714,1060],[778,1060],[778,1091],[714,1091]],"score":0.8896},{"poly":[[583,1060],[649,1060],[649,1091],[583,1091]],"score":0.8439},{"poly":[[377,1035],[468,1039],[467,1065],[375,1061]],"score":0.7973},{"poly":[[714,1035],[780,1035],[780,1067],[714,1067]],"score":0.8651},{"poly":[[583,1035],[650,1035],[650,1067],[583,1067]],"score":0.8415},{"poly":[[375,996],[546,1002],[545,1034],[374,1028]],"score":0.7599},{"poly":[[712,1000],[778,1000],[778,1031],[712,1031]],"score":0.9628},{"poly":[[583,1000],[650,1000],[650,1033],[583,1033]],"score":0.8536},{"poly":[[548,970],[806,968],[806,993],[548,995]],"score":0.8555},{"poly":[[378,964],[513,970],[511,998],[377,993]],"score":0.8623},{"poly":[[133,909],[560,912],[560,942],[133,938]],"score":0.6973},{"poly":[[140,886],[1049,886],[1049,910],[140,910]],"score":0.8453},{"poly":[[530,838],[587,838],[587,863],[530,863]],"score":0.8345},{"poly":[[386,838],[532,838],[532,861],[386,861]],"score":0.8721},{"poly":[[982,837],[1037,837],[1037,863],[982,863]],"score":0.9032},{"poly":[[904,837],[952,837],[952,863],[904,863]],"score":0.9211},{"poly":[[819,837],[876,837],[876,863],[819,863]],"score":0.8482},{"poly":[[753,837],[808,837],[808,863],[753,863]],"score":0.8255},{"poly":[[646,833],[755,837],[754,867],[645,863]],"score":0.6721},{"poly":[[594,837],[652,837],[652,863],[594,863]],"score":0.7551},{"poly":[[906,819],[948,819],[948,840],[906,840]],"score":0.9728},{"poly":[[819,817],[874,817],[874,842],[819,842]],"score":0.9418},{"poly":[[753,817],[808,817],[808,842],[753,842]],"score":0.8651},{"poly":[[697,817],[751,817],[751,842],[697,842]],"score":0.7919},{"poly":[[652,819],[700,819],[700,838],[652,838]],"score":0.9401},{"poly":[[594,817],[649,817],[649,842],[594,842]],"score":0.8616},{"poly":[[532,817],[585,817],[585,842],[532,842]],"score":0.9512},{"poly":[[386,819],[459,819],[459,838],[386,838]],"score":0.9464},{"poly":[[980,816],[1035,816],[1035,842],[980,842]],"score":0.8558},{"poly":[[147,810],[376,810],[376,833],[147,833]],"score":0.9553},{"poly":[[980,789],[1035,789],[1035,816],[980,816]],"score":0.8568},{"poly":[[902,789],[950,789],[950,816],[902,816]],"score":0.8413},{"poly":[[821,789],[876,789],[876,814],[821,814]],"score":0.9493},{"poly":[[744,789],[808,789],[808,814],[744,814]],"score":0.8658},{"poly":[[647,789],[751,789],[751,814],[647,814]],"score":0.8297},{"poly":[[594,789],[658,789],[658,814],[594,814]],"score":0.7916},{"poly":[[385,788],[587,788],[587,817],[385,817]],"score":0.6372},{"poly":[[904,761],[950,761],[950,788],[904,788]],"score":0.9639},{"poly":[[755,761],[806,761],[806,788],[755,788]],"score":0.8525},{"poly":[[698,761],[751,761],[751,786],[698,786]],"score":0.9096},{"poly":[[649,761],[704,761],[704,786],[649,786]],"score":0.8592},{"poly":[[596,761],[647,761],[647,788],[596,788]],"score":0.8128},{"poly":[[388,761],[528,761],[528,786],[388,786]],"score":0.8124},{"poly":[[984,760],[1031,760],[1031,788],[984,788]],"score":0.8756},{"poly":[[824,757],[873,762],[870,790],[822,785]],"score":0.8303},{"poly":[[533,760],[583,760],[583,786],[533,786]],"score":0.817},{"poly":[[824,740],[872,740],[872,767],[824,767]],"score":0.8799},{"poly":[[753,740],[808,740],[808,767],[753,767]],"score":0.8344},{"poly":[[697,740],[755,740],[755,765],[697,765]],"score":0.8708},{"poly":[[650,740],[707,740],[707,765],[650,765]],"score":0.8469},{"poly":[[596,740],[647,740],[647,767],[596,767]],"score":0.8373},{"poly":[[535,740],[583,740],[583,767],[535,767]],"score":0.8572},{"poly":[[984,739],[1033,739],[1033,767],[984,767]],"score":0.8062},{"poly":[[904,739],[952,739],[952,767],[904,767]],"score":0.8021},{"poly":[[346,729],[463,739],[461,769],[343,759]],"score":0.6969},{"poly":[[143,735],[265,731],[266,754],[144,758]],"score":0.8897},{"poly":[[385,712],[530,714],[530,739],[384,737]],"score":0.8215},{"poly":[[982,712],[1037,712],[1037,739],[982,739]],"score":0.8953},{"poly":[[904,712],[952,712],[952,739],[904,739]],"score":0.8753},{"poly":[[822,712],[876,712],[876,739],[822,739]],"score":0.8413},{"poly":[[744,712],[808,712],[808,737],[744,737]],"score":0.8831},{"poly":[[650,712],[755,712],[755,737],[650,737]],"score":0.8433},{"poly":[[596,712],[647,712],[647,739],[596,739]],"score":0.7696},{"poly":[[533,714],[580,714],[580,735],[533,735]],"score":0.8262},{"poly":[[883,682],[1044,686],[1043,712],[882,709]],"score":0.7835},{"poly":[[751,684],[810,684],[810,710],[751,710]],"score":0.8579},{"poly":[[813,670],[885,674],[883,700],[811,696]],"score":0.7948},{"poly":[[525,672],[746,672],[746,702],[525,702]],"score":0.7308},{"poly":[[971,665],[1040,665],[1040,689],[971,689]],"score":0.8351},{"poly":[[893,661],[965,665],[963,690],[891,685]],"score":0.8255},{"poly":[[750,663],[813,663],[813,689],[750,689]],"score":0.8308},{"poly":[[347,658],[498,658],[498,681],[347,681]],"score":0.9007},{"poly":[[144,656],[202,656],[202,681],[144,681]],"score":0.9701},{"poly":[[849,637],[1007,637],[1007,660],[849,660]],"score":0.8476},{"poly":[[592,637],[748,637],[748,661],[592,661]],"score":0.7434},{"poly":[[140,612],[998,612],[998,637],[140,637]],"score":0.7731},{"poly":[[135,581],[1055,584],[1054,614],[135,610]],"score":0.7477},{"poly":[[136,558],[1051,558],[1051,582],[136,582]],"score":0.7863},{"poly":[[138,530],[1051,530],[1051,554],[138,554]],"score":0.7773},{"poly":[[945,481],[1003,481],[1003,507],[945,507]],"score":0.8925},{"poly":[[780,481],[840,481],[840,507],[780,507]],"score":0.8843},{"poly":[[479,481],[714,481],[714,505],[479,505]],"score":0.7623},{"poly":[[146,479],[402,483],[402,507],[145,503]],"score":0.7562},{"poly":[[945,447],[1003,447],[1003,474],[945,474]],"score":0.9729},{"poly":[[780,447],[842,447],[842,474],[780,474]],"score":0.9424},{"poly":[[530,446],[712,446],[712,470],[530,470]],"score":0.6604},{"poly":[[144,443],[275,449],[274,476],[143,470]],"score":0.7678},{"poly":[[507,410],[681,414],[680,444],[507,440]],"score":0.7224},{"poly":[[142,381],[344,382],[344,412],[142,410]],"score":0.7647},{"poly":[[574,382],[714,382],[714,407],[574,407]],"score":0.8129},{"poly":[[532,382],[585,382],[585,407],[532,407]],"score":0.9337},{"poly":[[943,379],[1005,379],[1005,410],[943,410]],"score":0.8723},{"poly":[[778,379],[842,379],[842,410],[778,410]],"score":0.861},{"poly":[[142,356],[344,358],[344,388],[142,386]],"score":0.7603},{"poly":[[590,356],[718,356],[718,386],[590,386]],"score":0.8181},{"poly":[[475,358],[533,358],[533,384],[475,384]],"score":0.8505},{"poly":[[943,354],[1005,354],[1005,386],[943,386]],"score":0.8806},{"poly":[[778,350],[843,355],[840,388],[776,383]],"score":0.7818},{"poly":[[142,324],[346,326],[345,356],[142,354]],"score":0.7542},{"poly":[[945,326],[1003,326],[1003,353],[945,353]],"score":0.968},{"poly":[[587,325],[716,325],[716,354],[587,354]],"score":0.809},{"poly":[[778,323],[842,323],[842,354],[778,354]],"score":0.8155},{"poly":[[532,325],[585,325],[585,351],[532,351]],"score":0.8396},{"poly":[[142,300],[365,302],[365,332],[142,330]],"score":0.7559},{"poly":[[945,302],[1003,302],[1003,328],[945,328]],"score":0.9787},{"poly":[[590,300],[718,300],[718,330],[590,330]],"score":0.8395},{"poly":[[399,302],[452,302],[452,328],[399,328]],"score":0.8897},{"poly":[[780,298],[842,298],[842,330],[780,330]],"score":0.9037},{"poly":[[507,266],[683,270],[682,300],[507,296]],"score":0.7481},{"poly":[[535,247],[548,247],[548,260],[535,260]],"score":0.836},{"poly":[[404,244],[418,244],[418,260],[404,260]],"score":0.8743},{"poly":[[142,231],[344,237],[343,267],[141,261]],"score":0.7437},{"poly":[[943,237],[1005,237],[1005,263],[943,263]],"score":0.9386},{"poly":[[780,237],[842,237],[842,263],[780,263]],"score":0.9293},{"poly":[[592,235],[716,235],[716,260],[592,260]],"score":0.7185},{"poly":[[401,203],[583,203],[583,228],[401,228]],"score":0.8418},{"poly":[[587,184],[1040,184],[1040,209],[587,209]],"score":0.7478},{"poly":[[144,180],[291,184],[290,214],[143,210]],"score":0.7756},{"poly":[[402,174],[544,174],[544,198],[402,198]],"score":0.7875}],"page_no":16,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.7923,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1577},"font_size":0.0,"text":"18"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":1057,"y0":169,"y1":312},"conf":0.9703,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":174,"y1":198},"font_size":0.0,"text":"detailed analysis regarding their impact on mathematical reasoning has not been extensively"},{"bbox":{"x0":138,"x1":1051,"y0":198,"y1":225},"font_size":0.0,"text":"conducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem"},{"bbox":{"x0":140,"x1":1051,"y0":228,"y1":251},"font_size":0.0,"text":"ineffective in improving mathematical reasoning. We experiment with models of different sizes,"},{"bbox":{"x0":140,"x1":1049,"y0":254,"y1":279},"font_size":0.0,"text":"including DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv"},{"bbox":{"x0":140,"x1":647,"y0":282,"y1":307},"font_size":0.0,"text":"corpora that underwent varied processing pipelines:"}],"source":"layout det","text":"detailed analysis regarding their impact on mathematical reasoning has not been extensively conducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem ineffective in improving mathematical reasoning. We experiment with models of different sizes,including DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv corpora that underwent varied processing pipelines:"},{"bbox":{"x0":187,"x1":1057,"y0":318,"y1":375},"conf":0.9218,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1051,"y0":317,"y1":351},"font_size":0.0,"text":"MathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and flteringi"},{"bbox":{"x0":188,"x1":787,"y0":346,"y1":377},"font_size":0.0,"text":"heuristic rules, over $85\\%$ of which are scientifc arXiv papers;i"}],"source":"layout det","text":"MathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and flteringi heuristic rules, over $85\\%$ of which are scientifc arXiv papers;i"},{"bbox":{"x0":189,"x1":1058,"y0":376,"y1":433},"conf":0.9267,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1051,"y0":375,"y1":401},"font_size":0.0,"text":"ArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX fles with preambles,i"},{"bbox":{"x0":193,"x1":876,"y0":405,"y1":428},"font_size":0.0,"text":"comments, macros, and bibliographies removed, totaling 28.0B tokens."}],"source":"layout det","text":"ArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX fles with preambles,i comments, macros, and bibliographies removed, totaling 28.0B tokens."},{"bbox":{"x0":134,"x1":1059,"y0":439,"y1":636},"conf":0.9757,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":444,"y1":468},"font_size":0.0,"text":"In our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-"},{"bbox":{"x0":140,"x1":1049,"y0":472,"y1":496},"font_size":0.0,"text":"Coder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective"},{"bbox":{"x0":138,"x1":1051,"y0":500,"y1":523},"font_size":0.0,"text":"in improving mathematical reasoning. When trained on a arXiv-only corpus, both models dis-"},{"bbox":{"x0":136,"x1":1051,"y0":523,"y1":551},"font_size":0.0,"text":"play no notable improvements or even deterioration across various mathematical benchmarks of"},{"bbox":{"x0":136,"x1":1051,"y0":547,"y1":581},"font_size":0.0,"text":"different complexities employed in this study. These benchmarks include quantitative reasoning"},{"bbox":{"x0":140,"x1":1049,"y0":579,"y1":603},"font_size":0.0,"text":"datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table"},{"bbox":{"x0":138,"x1":624,"y0":605,"y1":632},"font_size":0.0,"text":"8), and formal mathematics like miniF2F (Table 9)."}],"source":"layout det","text":"In our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeekCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective in improving mathematical reasoning. When trained on a arXiv-only corpus, both models display no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table 8), and formal mathematics like miniF2F (Table 9)."},{"bbox":{"x0":133,"x1":1057,"y0":643,"y1":703},"conf":0.8274,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":647,"y1":672},"font_size":0.0,"text":"However, this conclusion has its limitations and should be taken with a grain of salt. We"},{"bbox":{"x0":140,"x1":346,"y0":675,"y1":698},"font_size":0.0,"text":"have not yet studied:"}],"source":"layout det","text":"However, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied:"},{"bbox":{"x0":189,"x1":1058,"y0":711,"y1":794},"conf":0.9461,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":716,"y1":740},"font_size":0.0,"text":"The impact of arXiv tokens on specifc math-related tasks not included in this research,i"},{"bbox":{"x0":191,"x1":1049,"y0":744,"y1":767},"font_size":0.0,"text":"such as informalization of theorems which is to convert formal statements or proofs to"},{"bbox":{"x0":193,"x1":425,"y0":770,"y1":793},"font_size":0.0,"text":"their informal versions;"}],"source":"layout det","text":"The impact of arXiv tokens on specifc math-related tasks not included in this research,i such as informalization of theorems which is to convert formal statements or proofs to their informal versions;"},{"bbox":{"x0":189,"x1":848,"y0":795,"y1":822},"conf":0.714,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":845,"y0":795,"y1":821},"font_size":0.0,"text":"The effect of arXiv tokens when combined with other types of data;"}],"source":"layout det","text":"The effect of arXiv tokens when combined with other types of data;"},{"bbox":{"x0":134,"x1":1051,"y0":827,"y1":894},"conf":0.4244,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":168,"x1":1046,"y0":821,"y1":851},"font_size":0.0,"text":"â€¢ Whether the benefts of arXiv papers would manifest themselves at a larger model scale.i"},{"bbox":{"x0":138,"x1":831,"y0":861,"y1":888},"font_size":0.0,"text":"Thus, further exploration is required, which we leave for future studies."}],"source":"layout det","text":"â€¢ Whether the benefts of arXiv papers would manifest themselves at a larger model scale.i Thus, further exploration is required, which we leave for future studies."},{"bbox":{"x0":134,"x1":548,"y0":927,"y1":961},"conf":0.9063,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":542,"y0":928,"y1":956},"font_size":0.0,"text":"5.2. Insights of Reinforcement Learning"}],"source":"layout det","text":"5.2. Insights of Reinforcement Learning"},{"bbox":{"x0":135,"x1":513,"y0":971,"y1":1005},"conf":0.8828,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":507,"y0":973,"y1":1002},"font_size":0.0,"text":"5.2.1. Towards to a Unifed Paradigmi"}],"source":"layout det","text":"5.2.1. Towards to a Unifed Paradigmi"},{"bbox":{"x0":134,"x1":1057,"y0":1015,"y1":1128},"conf":0.9652,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1021,"y1":1044},"font_size":0.0,"text":"In this section, we provide a unifed paradigm to analyze different training methods, such asi"},{"bbox":{"x0":140,"x1":1049,"y0":1047,"y1":1072},"font_size":0.0,"text":"SFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the"},{"bbox":{"x0":140,"x1":1049,"y0":1075,"y1":1100},"font_size":0.0,"text":"unifed paradigm. Generally, the gradient with respect to the parameteri $\\theta$ of a training method"},{"bbox":{"x0":136,"x1":314,"y0":1100,"y1":1126},"font_size":0.0,"text":"can be written as:"}],"source":"layout det","text":"In this section, we provide a unifed paradigm to analyze different training methods, such asi SFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the unifed paradigm. Generally, the gradient with respect to the parameteri $\\theta$ of a training method can be written as:"},{"bbox":{"x0":267,"x1":924,"y0":1140,"y1":1251},"conf":0.9509,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{\\mathcal{A}}(\\theta)=\\mathbb{E}[\\underbrace{(q,o) \\sim\\mathcal{D}}_{Data Source}]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\underbrace{\\mathit{GC}_{\\mathcal{A}}(q,o,t,\\pi_{rf})}_{\\mathit{Gradient Coefficient}} \\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$"},{"bbox":{"x0":1020,"x1":1055,"y0":1179,"y1":1211},"conf":0.8243,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1019,"x1":1055,"y0":1181,"y1":1210},"font_size":0.0,"text":"(5)"}],"source":"layout det","text":"(5)"},{"bbox":{"x0":133,"x1":1056,"y0":1264,"y1":1411},"conf":0.9664,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1270,"y1":1295},"font_size":0.0,"text":"There exist three key components: 1) Data Source $\\mathcal{D},$  which determines the training data; 2)"},{"bbox":{"x0":138,"x1":1049,"y0":1298,"y1":1325},"font_size":0.0,"text":"Reward Function $\\pi_{rf},$  which is the source of the training reward signal; 3) Algorithm $\\mathcal{A}\\text{:}$  which"},{"bbox":{"x0":138,"x1":1051,"y0":1324,"y1":1349},"font_size":0.0,"text":"processes the training data and the reward signal to the gradient coeffcienti $G\\mathcal{C}$ that determines"},{"bbox":{"x0":140,"x1":1049,"y0":1351,"y1":1375},"font_size":0.0,"text":"the magnitude of the penalty or reinforcement for the data. We analyze several representative"},{"bbox":{"x0":138,"x1":564,"y0":1377,"y1":1403},"font_size":0.0,"text":"methods based on such a unifed paradigm:i"}],"source":"layout det","text":"There exist three key components: 1) Data Source $\\mathcal{D},$  which determines the training data; 2)Reward Function $\\pi_{rf},$  which is the source of the training reward signal; 3) Algorithm $\\mathcal{A}\\text{:}$  which processes the training data and the reward signal to the gradient coeffcienti $G\\mathcal{C}$ that determines the magnitude of the penalty or reinforcement for the data. We analyze several representative methods based on such a unifed paradigm:i"},{"bbox":{"x0":189,"x1":1056,"y0":1438,"y1":1497},"conf":0.8152,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":1444,"y1":1468},"font_size":0.0,"text":"Supervised Fine-tuning (SFT): SFT fne-tunes pretrained model on human selected SFTi"},{"bbox":{"x0":189,"x1":247,"y0":1466,"y1":1497},"font_size":0.0,"text":"data."}],"source":"layout det","text":"Supervised Fine-tuning (SFT): SFT fne-tunes pretrained model on human selected SFTi data."}],"formula_dets":[{"bbox":{"x0":267,"x1":924,"y0":1140,"y1":1251},"conf":0.9509,"label":"print_isolated","label_id":1},{"bbox":{"x0":864,"x1":892,"y0":1328,"y1":1345},"conf":0.8341,"label":"print_embedding","label_id":0},{"bbox":{"x0":832,"x1":845,"y0":1078,"y1":1095},"conf":0.8263,"label":"print_embedding","label_id":0},{"bbox":{"x0":390,"x1":434,"y0":348,"y1":371},"conf":0.809,"label":"print_embedding","label_id":0},{"bbox":{"x0":302,"x1":338,"y0":1306,"y1":1325},"conf":0.8009,"label":"print_embedding","label_id":0},{"bbox":{"x0":953,"x1":978,"y0":1301,"y1":1318},"conf":0.6946,"label":"print_embedding","label_id":0},{"bbox":{"x0":634,"x1":660,"y0":1273,"y1":1293},"conf":0.6714,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1059,"y0":439,"y1":636},"conf":0.9757,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":169,"y1":312},"conf":0.9703,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":1264,"y1":1411},"conf":0.9664,"label":"Text","label_id":1},{"bbox":{"x0":261,"x1":929,"y0":1137,"y1":1254},"conf":0.9657,"label":"Equation","label_id":8},{"bbox":{"x0":134,"x1":1057,"y0":1015,"y1":1128},"conf":0.9652,"label":"Text","label_id":1},{"bbox":{"x0":189,"x1":1058,"y0":711,"y1":794},"conf":0.9461,"label":"Text","label_id":1},{"bbox":{"x0":189,"x1":1058,"y0":376,"y1":433},"conf":0.9267,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1057,"y0":318,"y1":375},"conf":0.9218,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":548,"y0":927,"y1":961},"conf":0.9063,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":513,"y0":971,"y1":1005},"conf":0.8828,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":1057,"y0":643,"y1":703},"conf":0.8274,"label":"Text","label_id":1},{"bbox":{"x0":1020,"x1":1055,"y0":1179,"y1":1211},"conf":0.8243,"label":"Equation caption","label_id":9},{"bbox":{"x0":189,"x1":1056,"y0":1438,"y1":1497},"conf":0.8152,"label":"Text","label_id":1},{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.7923,"label":"Abandon","label_id":2},{"bbox":{"x0":189,"x1":848,"y0":795,"y1":822},"conf":0.714,"label":"Text","label_id":1},{"bbox":{"x0":267,"x1":1052,"y0":823,"y1":853},"conf":0.6734,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":838,"y0":860,"y1":894},"conf":0.585,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1051,"y0":827,"y1":894},"conf":0.4244,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1577],[580,1577]],"score":0.7478},{"poly":[[191,1466],[247,1471],[245,1497],[189,1492]],"score":0.7916},{"poly":[[172,1444],[1049,1444],[1049,1466],[172,1466]],"score":0.9757},{"poly":[[138,1377],[564,1379],[564,1403],[138,1402]],"score":0.7349},{"poly":[[140,1351],[1049,1351],[1049,1375],[140,1375]],"score":0.7517},{"poly":[[138,1324],[1051,1324],[1051,1349],[138,1349]],"score":0.7564},{"poly":[[138,1298],[1049,1298],[1049,1323],[138,1323]],"score":0.7411},{"poly":[[140,1270],[1051,1270],[1051,1295],[140,1295]],"score":0.7382},{"poly":[[581,1228],[727,1228],[727,1251],[581,1251]],"score":0.8245},{"poly":[[408,1226],[493,1226],[493,1244],[408,1244]],"score":0.9795},{"poly":[[523,1186],[555,1194],[546,1229],[514,1221]],"score":0.6733},{"poly":[[578,1181],[907,1181],[907,1210],[578,1210]],"score":0.7669},{"poly":[[268,1182],[503,1182],[503,1207],[268,1207]],"score":0.8147},{"poly":[[1019,1181],[1055,1181],[1055,1210],[1019,1210]],"score":0.8997},{"poly":[[525,1168],[541,1168],[541,1188],[525,1188]],"score":0.7832},{"poly":[[548,1161],[573,1155],[579,1179],[554,1186]],"score":0.6884},{"poly":[[137,1100],[314,1102],[314,1126],[136,1124]],"score":0.739},{"poly":[[140,1075],[1049,1075],[1049,1100],[140,1100]],"score":0.8075},{"poly":[[140,1047],[1049,1047],[1049,1072],[140,1072]],"score":0.7599},{"poly":[[138,1021],[1049,1021],[1049,1044],[138,1044]],"score":0.8978},{"poly":[[137,973],[507,977],[507,1002],[136,998]],"score":0.7844},{"poly":[[138,928],[542,932],[542,956],[138,952]],"score":0.7247},{"poly":[[138,861],[831,863],[831,888],[138,886]],"score":0.7887},{"poly":[[168,821],[1046,821],[1046,851],[168,851]],"score":0.6654},{"poly":[[172,795],[845,796],[845,821],[172,819]],"score":0.7823},{"poly":[[193,770],[425,770],[425,793],[193,793]],"score":0.8521},{"poly":[[191,744],[1049,744],[1049,767],[191,767]],"score":0.8527},{"poly":[[174,716],[1051,716],[1051,740],[174,740]],"score":0.77},{"poly":[[140,675],[346,675],[346,698],[140,698]],"score":0.8982},{"poly":[[172,647],[1049,647],[1049,672],[172,672]],"score":0.7547},{"poly":[[138,607],[624,605],[624,630],[138,632]],"score":0.7683},{"poly":[[140,579],[1049,579],[1049,603],[140,603]],"score":0.7454},{"poly":[[137,547],[1051,551],[1051,581],[136,577]],"score":0.6687},{"poly":[[136,526],[1051,523],[1051,547],[137,551]],"score":0.747},{"poly":[[138,500],[1051,500],[1051,523],[138,523]],"score":0.8372},{"poly":[[140,472],[1049,472],[1049,496],[140,496]],"score":0.739},{"poly":[[140,444],[1051,444],[1051,468],[140,468]],"score":0.7237},{"poly":[[193,405],[876,405],[876,428],[193,428]],"score":0.9171},{"poly":[[175,375],[1051,375],[1051,400],[175,400]],"score":0.7881},{"poly":[[188,346],[787,347],[787,377],[188,375]],"score":0.7011},{"poly":[[172,317],[1051,321],[1051,351],[172,347]],"score":0.7301},{"poly":[[140,282],[647,282],[647,307],[140,307]],"score":0.7688},{"poly":[[140,254],[1049,254],[1049,279],[140,279]],"score":0.7265},{"poly":[[140,228],[1051,228],[1051,251],[140,251]],"score":0.8536},{"poly":[[138,198],[1051,200],[1051,225],[138,223]],"score":0.772},{"poly":[[140,174],[1049,174],[1049,198],[140,198]],"score":0.7538}],"page_no":17,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.7755,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1575},"font_size":0.0,"text":"19"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":188,"x1":996,"y0":165,"y1":390},"conf":0.9705,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":205,"x1":294,"y0":174,"y1":199},"font_size":0.0,"text":"Methods"},{"bbox":{"x0":401,"x1":519,"y0":175,"y1":199},"font_size":0.0,"text":"Data Source"},{"bbox":{"x0":606,"x1":770,"y0":175,"y1":199},"font_size":0.0,"text":"Reward Function"},{"bbox":{"x0":786,"x1":979,"y0":175,"y1":199},"font_size":0.0,"text":"Gradient Coeffcient"},{"bbox":{"x0":931,"x1":931,"y0":180,"y1":199},"font_size":9.0,"text":"i"},{"bbox":{"x0":202,"x1":249,"y0":206,"y1":237},"font_size":0.0,"text":"SFT"},{"bbox":{"x0":392,"x1":525,"y0":215,"y1":237},"font_size":0.0,"text":" $q,o\\sim P_{sft}(\\mathsf{Q},\\mathsf{O})$ "},{"bbox":{"x0":678,"x1":696,"y0":213,"y1":233},"font_size":0.0,"text":"-"},{"bbox":{"x0":871,"x1":893,"y0":210,"y1":236},"font_size":0.0,"text":"1"},{"bbox":{"x0":204,"x1":252,"y0":242,"y1":268},"font_size":0.0,"text":"RFT"},{"bbox":{"x0":351,"x1":566,"y0":249,"y1":270},"font_size":0.0,"text":" $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{sft}(\\mathsf{O}|q)$ "},{"bbox":{"x0":662,"x1":713,"y0":242,"y1":268},"font_size":0.0,"text":"Rule"},{"bbox":{"x0":824,"x1":940,"y0":241,"y1":272},"font_size":0.0,"text":"Equation 10"},{"bbox":{"x0":204,"x1":258,"y0":267,"y1":294},"font_size":0.0,"text":"DPO"},{"bbox":{"x0":333,"x1":584,"y0":273,"y1":295},"font_size":0.0,"text":" $q \\sim P_{sft}( Q ), o^{+}, o^{-}\\sim\\pi_{sft}(\\mathsf{O}| q )$ "},{"bbox":{"x0":660,"x1":714,"y0":266,"y1":295},"font_size":0.0,"text":"Rule"},{"bbox":{"x0":825,"x1":940,"y0":266,"y1":295},"font_size":0.0,"text":"Equation 14"},{"bbox":{"x0":204,"x1":315,"y0":299,"y1":327},"font_size":0.0,"text":"Online RFT"},{"bbox":{"x0":357,"x1":560,"y0":308,"y1":329},"font_size":0.0,"text":" $q\\sim P_{sft}(\\mathsf{Q}), o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ "},{"bbox":{"x0":663,"x1":712,"y0":303,"y1":326},"font_size":0.0,"text":"Rule"},{"bbox":{"x0":824,"x1":940,"y0":299,"y1":330},"font_size":0.0,"text":"Equation 10"},{"bbox":{"x0":204,"x1":255,"y0":327,"y1":350},"font_size":0.0,"text":"PPO"},{"bbox":{"x0":356,"x1":560,"y0":333,"y1":353},"font_size":0.0,"text":" $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ "},{"bbox":{"x0":654,"x1":720,"y0":326,"y1":351},"font_size":0.0,"text":"Model"},{"bbox":{"x0":824,"x1":941,"y0":325,"y1":355},"font_size":0.0,"text":"Equation 18"},{"bbox":{"x0":204,"x1":271,"y0":350,"y1":375},"font_size":0.0,"text":"GRPO"},{"bbox":{"x0":336,"x1":582,"y0":356,"y1":379},"font_size":0.0,"text":" $\\boldsymbol{q\\sim P_{sft}(Q),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta}(O|q)}$ "},{"bbox":{"x0":655,"x1":719,"y0":352,"y1":374},"font_size":0.0,"text":"Model"},{"bbox":{"x0":825,"x1":938,"y0":349,"y1":379},"font_size":0.0,"text":"Equation 21"}],"source":"layout det","text":"<html><body><table><thead><tr><td>Methods</td><td>Data Source</td><td>Reward Function</td><td>Gradient Coeffcient</td></tr></thead><tbody><tr><td>SFT</td><td> $q,o\\sim P_{sft}(\\mathsf{Q},\\mathsf{O})$ </td><td>-</td><td>1</td></tr><tr><td>RFT</td><td> $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{sft}(\\mathsf{O}|q)$ </td><td>Rule</td><td>Equation 10</td></tr><tr><td>DPO</td><td> $q \\sim P_{sft}( Q ), o^{+}, o^{-}\\sim\\pi_{sft}(\\mathsf{O}| q )$ </td><td>Rule</td><td>Equation 14</td></tr><tr><td>Online RFT</td><td> $q\\sim P_{sft}(\\mathsf{Q}), o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ </td><td>Rule</td><td>Equation 10</td></tr><tr><td>PPO</td><td> $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ </td><td>Model</td><td>Equation 18</td></tr><tr><td>GRPO</td><td> $\\boldsymbol{q\\sim P_{sft}(Q),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta}(O|q)}$ </td><td>Model</td><td>Equation 21</td></tr></tbody></table></body></html>"},{"bbox":{"x0":132,"x1":1057,"y0":398,"y1":492},"conf":0.9156,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":402,"y1":431},"font_size":0.0,"text":"Table 10 | The data source and gradient coeffcient of different methods.i $P_{sft}$ denotes the data"},{"bbox":{"x0":138,"x1":1051,"y0":428,"y1":459},"font_size":0.0,"text":"distribution of supervised fne-tuning datasets.i $\\pi_{\\theta_{sft}}$ and $\\pi_{\\theta}$ denote the supervised fne-tunedi"},{"bbox":{"x0":136,"x1":968,"y0":456,"y1":482},"font_size":0.0,"text":"model and the real-time policy model during the online training process, respectively."}],"source":"layout det","text":"Table 10 | The data source and gradient coeffcient of different methods.i $P_{sft}$ denotes the data distribution of supervised fne-tuning datasets.i $\\pi_{\\theta_{sft}}$ and $\\pi_{\\theta}$ denote the supervised fne-tunedi model and the real-time policy model during the online training process, respectively."},{"bbox":{"x0":184,"x1":1005,"y0":511,"y1":908},"conf":0.9687,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![fcd66559f5e33490d79f3a48343510e4](imgs/fcd66559f5e33490d79f3a48343510e4.jpg)"},{"bbox":{"x0":131,"x1":1058,"y0":912,"y1":974},"conf":0.9558,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":914,"y1":940},"font_size":0.0,"text":"Figure 5 | Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained"},{"bbox":{"x0":140,"x1":567,"y0":944,"y1":968},"font_size":0.0,"text":"using various methods, on two benchmarks."}],"source":"layout det","text":"Figure 5 | Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained using various methods, on two benchmarks."},{"bbox":{"x0":187,"x1":1058,"y0":1009,"y1":1092},"conf":0.9499,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":1012,"y1":1039},"font_size":0.0,"text":"Rejection Sampling Fine-tuning (RFT): RFT further fne-tunes the SFT model on thei"},{"bbox":{"x0":193,"x1":1049,"y0":1037,"y1":1066},"font_size":0.0,"text":"fltered outputs sampled from the SFT model based on SFT questions. RFT flters theii"},{"bbox":{"x0":191,"x1":677,"y0":1068,"y1":1091},"font_size":0.0,"text":"outputs based on the correctness of their answers."}],"source":"layout det","text":"Rejection Sampling Fine-tuning (RFT): RFT further fne-tunes the SFT model on thei fltered outputs sampled from the SFT model based on SFT questions. RFT flters theii outputs based on the correctness of their answers."},{"bbox":{"x0":187,"x1":1056,"y0":1094,"y1":1149},"conf":0.9275,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":181,"x1":1051,"y0":1088,"y1":1123},"font_size":0.0,"text":"Direct Preference Optimization (DPO): DPO further refnes the SFT model by fne-tuningii"},{"bbox":{"x0":191,"x1":978,"y0":1123,"y1":1147},"font_size":0.0,"text":"it on augmented outputs sampled from the SFT model, using pair-wise DPO loss."}],"source":"layout det","text":"Direct Preference Optimization (DPO): DPO further refnes the SFT model by fne-tuningii it on augmented outputs sampled from the SFT model, using pair-wise DPO loss."},{"bbox":{"x0":188,"x1":1056,"y0":1151,"y1":1230},"conf":0.9514,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":1149,"y1":1172},"font_size":0.0,"text":"Online Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT"},{"bbox":{"x0":191,"x1":1051,"y0":1175,"y1":1201},"font_size":0.0,"text":"initiates the policy model using the SFT model and refnes it by fne-tuning with theii"},{"bbox":{"x0":190,"x1":790,"y0":1202,"y1":1228},"font_size":0.0,"text":"augmented outputs sampled from the real-time policy model."}],"source":"layout det","text":"Online Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT initiates the policy model using the SFT model and refnes it by fne-tuning with theii augmented outputs sampled from the real-time policy model."},{"bbox":{"x0":189,"x1":1057,"y0":1231,"y1":1289},"conf":0.9447,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":167,"x1":1051,"y0":1224,"y1":1256},"font_size":0.0,"text":"PPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces"},{"bbox":{"x0":191,"x1":782,"y0":1258,"y1":1281},"font_size":0.0,"text":"it with the outputs sampled from the real-time policy model."}],"source":"layout det","text":"PPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces it with the outputs sampled from the real-time policy model."},{"bbox":{"x0":134,"x1":1057,"y0":1317,"y1":1381},"conf":0.9405,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1321,"y1":1345},"font_size":0.0,"text":"We summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a"},{"bbox":{"x0":138,"x1":464,"y0":1349,"y1":1374},"font_size":0.0,"text":"more detailed derivation process."}],"source":"layout det","text":"We summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a more detailed derivation process."},{"bbox":{"x0":132,"x1":1060,"y0":1416,"y1":1507},"conf":0.9486,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1417,"y1":1444},"font_size":0.0,"text":"Observation about Data SourceWe divide the data source into two categories, online sam-"},{"bbox":{"x0":138,"x1":1051,"y0":1447,"y1":1473},"font_size":0.0,"text":"pling, and offine sampling. Online sampling denotes that the training data is from the explo-l"},{"bbox":{"x0":138,"x1":1051,"y0":1475,"y1":1500},"font_size":0.0,"text":"ration results of the real-time training policy model, while offine sampling denotes that thel"}],"source":"layout det","text":"Observation about Data SourceWe divide the data source into two categories, online sampling, and offine sampling. Online sampling denotes that the training data is from the explo-l ration results of the real-time training policy model, while offine sampling denotes that thel"}],"formula_dets":[{"bbox":{"x0":351,"x1":566,"y0":249,"y1":270},"conf":0.8707,"label":"print_embedding","label_id":0},{"bbox":{"x0":392,"x1":525,"y0":215,"y1":237},"conf":0.8625,"label":"print_embedding","label_id":0},{"bbox":{"x0":333,"x1":584,"y0":273,"y1":295},"conf":0.8574,"label":"print_embedding","label_id":0},{"bbox":{"x0":356,"x1":560,"y0":333,"y1":353},"conf":0.854,"label":"print_embedding","label_id":0},{"bbox":{"x0":357,"x1":560,"y0":308,"y1":329},"conf":0.8532,"label":"print_embedding","label_id":0},{"bbox":{"x0":336,"x1":582,"y0":356,"y1":379},"conf":0.8506,"label":"print_embedding","label_id":0},{"bbox":{"x0":846,"x1":880,"y0":409,"y1":431},"conf":0.8177,"label":"print_embedding","label_id":0},{"bbox":{"x0":606,"x1":645,"y0":440,"y1":459},"conf":0.7632,"label":"print_embedding","label_id":0},{"bbox":{"x0":694,"x1":717,"y0":441,"y1":455},"conf":0.6053,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":188,"x1":996,"y0":165,"y1":390},"conf":0.9705,"label":"Table","label_id":5},{"bbox":{"x0":184,"x1":1005,"y0":511,"y1":908},"conf":0.9687,"label":"Figure","label_id":3},{"bbox":{"x0":131,"x1":1058,"y0":912,"y1":974},"conf":0.9558,"label":"Figure caption","label_id":4},{"bbox":{"x0":188,"x1":1056,"y0":1151,"y1":1230},"conf":0.9514,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1058,"y0":1009,"y1":1092},"conf":0.9499,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":1416,"y1":1507},"conf":0.9486,"label":"Text","label_id":1},{"bbox":{"x0":189,"x1":1057,"y0":1231,"y1":1289},"conf":0.9447,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1057,"y0":1317,"y1":1381},"conf":0.9405,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1056,"y0":1094,"y1":1149},"conf":0.9275,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":398,"y1":492},"conf":0.9156,"label":"Text","label_id":1},{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.7755,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1575],[580,1575]],"score":0.8895},{"poly":[[138,1475],[1051,1475],[1051,1500],[138,1500]],"score":0.7542},{"poly":[[138,1447],[1051,1447],[1051,1472],[138,1472]],"score":0.7812},{"poly":[[138,1417],[1051,1419],[1051,1444],[138,1442]],"score":0.7616},{"poly":[[138,1349],[464,1349],[464,1374],[138,1374]],"score":0.7462},{"poly":[[140,1321],[1049,1321],[1049,1345],[140,1345]],"score":0.7724},{"poly":[[191,1258],[782,1258],[782,1281],[191,1281]],"score":0.9094},{"poly":[[167,1224],[1051,1226],[1051,1256],[167,1254]],"score":0.7061},{"poly":[[190,1203],[790,1202],[790,1226],[190,1228]],"score":0.7793},{"poly":[[191,1175],[1051,1175],[1051,1200],[191,1200]],"score":0.7719},{"poly":[[170,1149],[1049,1149],[1049,1172],[170,1172]],"score":0.8882},{"poly":[[191,1123],[978,1123],[978,1147],[191,1147]],"score":0.7172},{"poly":[[181,1088],[1051,1093],[1051,1123],[181,1117]],"score":0.6213},{"poly":[[191,1068],[677,1068],[677,1091],[191,1091]],"score":0.8359},{"poly":[[193,1037],[1049,1037],[1049,1061],[193,1061]],"score":0.6884},{"poly":[[170,1012],[1049,1012],[1049,1037],[170,1037]],"score":0.7238},{"poly":[[140,944],[567,944],[567,968],[140,968]],"score":0.7771},{"poly":[[138,916],[1049,914],[1049,938],[138,940]],"score":0.8207},{"poly":[[794,872],[858,872],[858,903],[794,903]],"score":0.8183},{"poly":[[385,872],[450,872],[450,903],[385,903]],"score":0.8175},{"poly":[[925,853],[971,853],[971,874],[925,874]],"score":0.8416},{"poly":[[860,853],[906,853],[906,874],[860,874]],"score":0.8452},{"poly":[[792,853],[837,853],[837,874],[792,874]],"score":0.932},{"poly":[[721,853],[766,853],[766,874],[721,874]],"score":0.925},{"poly":[[666,854],[682,854],[682,872],[666,872]],"score":0.9331},{"poly":[[518,853],[564,853],[564,874],[518,874]],"score":0.8735},{"poly":[[452,853],[494,853],[494,874],[452,874]],"score":0.8786},{"poly":[[383,853],[429,853],[429,874],[383,874]],"score":0.8537},{"poly":[[314,853],[358,853],[358,874],[314,874]],"score":0.8829},{"poly":[[259,854],[276,854],[276,872],[259,872]],"score":0.8116},{"poly":[[222,835],[248,835],[248,856],[222,856]],"score":0.8175},{"poly":[[627,791],[656,791],[656,816],[627,816]],"score":0.7502},{"poly":[[220,782],[248,782],[248,805],[220,805]],"score":0.7995},{"poly":[[455,737],[498,737],[498,763],[455,763]],"score":0.6471},{"poly":[[296,737],[305,737],[305,747],[296,747]],"score":0.6866},{"poly":[[481,734],[569,727],[571,754],[483,760]],"score":0.6537},{"poly":[[218,731],[246,731],[246,754],[218,754]],"score":0.7071},{"poly":[[220,682],[246,682],[246,705],[220,705]],"score":0.7441},{"poly":[[624,675],[654,675],[654,698],[624,698]],"score":0.7138},{"poly":[[187,669],[221,668],[226,757],[192,758]],"score":0.7173},{"poly":[[886,667],[918,667],[918,686],[886,686]],"score":0.6479},{"poly":[[220,630],[246,630],[246,653],[220,653]],"score":0.8271},{"poly":[[627,617],[656,617],[656,640],[627,640]],"score":0.7691},{"poly":[[220,579],[253,579],[253,602],[220,602]],"score":0.6396},{"poly":[[376,554],[459,554],[459,579],[376,579]],"score":0.9755},{"poly":[[790,553],[861,553],[861,579],[790,579]],"score":0.8733},{"poly":[[642,523],[732,523],[732,542],[642,542]],"score":0.9644},{"poly":[[461,523],[555,523],[555,542],[461,542]],"score":0.9672},{"poly":[[331,523],[376,523],[376,544],[331,544]],"score":0.9226},{"poly":[[813,517],[907,517],[907,547],[813,547]],"score":0.744},{"poly":[[136,456],[968,458],[968,482],[136,481]],"score":0.8416},{"poly":[[138,428],[1051,428],[1051,458],[138,458]],"score":0.6593},{"poly":[[140,402],[1049,402],[1049,426],[140,426]],"score":0.7436},{"poly":[[826,351],[938,351],[938,375],[826,375]],"score":0.8563},{"poly":[[330,347],[588,347],[588,382],[330,382]],"score":0.6679},{"poly":[[654,349],[720,349],[720,375],[654,375]],"score":0.8516},{"poly":[[204,349],[269,349],[269,375],[204,375]],"score":0.9541},{"poly":[[826,328],[938,328],[938,353],[826,353]],"score":0.8526},{"poly":[[354,328],[562,328],[562,353],[354,353]],"score":0.8269},{"poly":[[652,326],[720,326],[720,351],[652,351]],"score":0.9472},{"poly":[[204,325],[257,325],[257,351],[204,351]],"score":0.8361},{"poly":[[354,305],[562,305],[562,328],[354,328]],"score":0.9302},{"poly":[[826,302],[939,302],[939,326],[826,326]],"score":0.8358},{"poly":[[661,302],[714,302],[714,328],[661,328]],"score":0.831},{"poly":[[204,302],[314,302],[314,326],[204,326]],"score":0.86},{"poly":[[826,268],[939,268],[939,293],[826,293]],"score":0.8277},{"poly":[[330,267],[588,267],[588,296],[330,296]],"score":0.7338},{"poly":[[661,262],[713,267],[711,295],[659,290]],"score":0.8054},{"poly":[[202,267],[257,267],[257,293],[202,293]],"score":0.9005},{"poly":[[826,244],[939,244],[939,268],[826,268]],"score":0.8582},{"poly":[[347,242],[571,242],[571,272],[347,272]],"score":0.7411},{"poly":[[661,242],[712,242],[712,270],[661,270]],"score":0.8808},{"poly":[[202,240],[252,240],[252,268],[202,268]],"score":0.8419},{"poly":[[682,216],[693,216],[693,232],[682,232]],"score":0.7751},{"poly":[[390,210],[530,210],[530,235],[390,235]],"score":0.8498},{"poly":[[874,212],[890,212],[890,232],[874,232]],"score":0.89},{"poly":[[202,207],[246,207],[246,235],[202,235]],"score":0.8727},{"poly":[[606,175],[980,175],[980,198],[606,198]],"score":0.8324},{"poly":[[399,172],[520,176],[519,200],[398,196]],"score":0.8155},{"poly":[[203,170],[295,174],[293,200],[202,196]],"score":0.749}],"page_no":18,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":576,"x1":613,"y0":1550,"y1":1579},"conf":0.7772,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1575},"font_size":0.0,"text":"20"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":185,"x1":1007,"y0":172,"y1":569},"conf":0.9719,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![3e7f2cddcbded9f544f610a264d04d46](imgs/3e7f2cddcbded9f544f610a264d04d46.jpg)"},{"bbox":{"x0":132,"x1":1057,"y0":574,"y1":634},"conf":0.9478,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":579,"y1":602},"font_size":0.0,"text":"Figure 6 | Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on"},{"bbox":{"x0":138,"x1":308,"y0":605,"y1":628},"font_size":0.0,"text":"two benchmarks."}],"source":"layout det","text":"Figure 6 | Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on two benchmarks."},{"bbox":{"x0":133,"x1":1057,"y0":671,"y1":734},"conf":0.9195,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":675,"y1":698},"font_size":0.0,"text":"training data is from the sampling results of the initial SFT model. RFT and DPO follow the"},{"bbox":{"x0":140,"x1":771,"y0":703,"y1":728},"font_size":0.0,"text":"offine style, while Online RFT and GRPO follow the online style.l"}],"source":"layout det","text":"training data is from the sampling results of the initial SFT model. RFT and DPO follow the offine style, while Online RFT and GRPO follow the online style.l"},{"bbox":{"x0":132,"x1":1057,"y0":740,"y1":937},"conf":0.9775,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":744,"y1":768},"font_size":0.0,"text":"As shown in Figure 5, we fnd that the Online RFT signifcantly outperforms RFT on twoii"},{"bbox":{"x0":140,"x1":1051,"y0":770,"y1":795},"font_size":0.0,"text":"benchmarks. Specifcally, Online RFT is comparable to RFT in the early stage of training buti"},{"bbox":{"x0":136,"x1":1053,"y0":795,"y1":826},"font_size":0.0,"text":"gains an absolute advantage in the later stage, demonstrating the superiority of online training."},{"bbox":{"x0":136,"x1":1053,"y0":819,"y1":851},"font_size":0.0,"text":"This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,"},{"bbox":{"x0":138,"x1":1049,"y0":853,"y1":875},"font_size":0.0,"text":"with the sampled data revealing only minor differences. In the later stage, however, the data"},{"bbox":{"x0":136,"x1":1051,"y0":875,"y1":907},"font_size":0.0,"text":"sampled from the actor will exhibit more signifcant differences, and real-time data samplingi"},{"bbox":{"x0":136,"x1":422,"y0":903,"y1":930},"font_size":0.0,"text":"will offer greater advantages."}],"source":"layout det","text":"As shown in Figure 5, we fnd that the Online RFT signifcantly outperforms RFT on twoii benchmarks. Specifcally, Online RFT is comparable to RFT in the early stage of training buti gains an absolute advantage in the later stage, demonstrating the superiority of online training.This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more signifcant differences, and real-time data samplingi will offer greater advantages."},{"bbox":{"x0":133,"x1":1060,"y0":971,"y1":1252},"conf":0.976,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":974,"y1":1002},"font_size":0.0,"text":"Observation about Gradient CoeffcientThe algorithm processes the reward signal to thei"},{"bbox":{"x0":136,"x1":1053,"y0":998,"y1":1031},"font_size":0.0,"text":"gradient coeffcient to update the model parameter. We divide the reward function as â€˜Ruleâ€™i"},{"bbox":{"x0":138,"x1":1051,"y0":1031,"y1":1056},"font_size":0.0,"text":"and â€˜Modelâ€™ in our experiments. Rule refers to judging the quality of a response based on"},{"bbox":{"x0":138,"x1":1049,"y0":1058,"y1":1081},"font_size":0.0,"text":"the correctness of the answer, and Model denotes that we train a reward model to score each"},{"bbox":{"x0":138,"x1":1049,"y0":1086,"y1":1109},"font_size":0.0,"text":"response. The training data of the reward model is based on the rule judgment. Equations 10"},{"bbox":{"x0":136,"x1":1051,"y0":1110,"y1":1137},"font_size":0.0,"text":"and 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its"},{"bbox":{"x0":136,"x1":1053,"y0":1133,"y1":1167},"font_size":0.0,"text":"gradient coeffcient based on the reward value provided by the reward model. This allows fori"},{"bbox":{"x0":140,"x1":1051,"y0":1167,"y1":1191},"font_size":0.0,"text":"differential reinforcement and penalization of responses according to their varying magnitudes."},{"bbox":{"x0":136,"x1":1049,"y0":1191,"y1":1217},"font_size":0.0,"text":"In contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly"},{"bbox":{"x0":138,"x1":852,"y0":1221,"y1":1244},"font_size":0.0,"text":"reinforces all responses with correct answers at the same level of intensity."}],"source":"layout det","text":"Observation about Gradient CoeffcientThe algorithm processes the reward signal to thei gradient coeffcient to update the model parameter. We divide the reward function as â€˜Ruleâ€™i and â€˜Modelâ€™ in our experiments. Rule refers to judging the quality of a response based on the correctness of the answer, and Model denotes that we train a reward model to score each response. The training data of the reward model is based on the rule judgment. Equations 10 and 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its gradient coeffcient based on the reward value provided by the reward model. This allows fori differential reinforcement and penalization of responses according to their varying magnitudes.In contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly reinforces all responses with correct answers at the same level of intensity."},{"bbox":{"x0":134,"x1":1058,"y0":1256,"y1":1429},"conf":0.9723,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":1259,"y1":1287},"font_size":0.0,"text":"As demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the effciencyi"},{"bbox":{"x0":138,"x1":1051,"y0":1286,"y1":1314},"font_size":0.0,"text":"of altering positive and negative gradient coeffcients. In addition, GRPO+PS shows superiori"},{"bbox":{"x0":138,"x1":1049,"y0":1314,"y1":1341},"font_size":0.0,"text":"performance compared to GRPO+OS, indicating the benefts of using fne-grained, step-awareii"},{"bbox":{"x0":135,"x1":1053,"y0":1338,"y1":1370},"font_size":0.0,"text":"gradient coeffcients. Furthermore, we explore the iterative RL, in our experiments, we conducti"},{"bbox":{"x0":136,"x1":1049,"y0":1368,"y1":1395},"font_size":0.0,"text":"two rounds of iteration. As shown in Figure $6,$  we notice that the iterative RL signifcantlyi"},{"bbox":{"x0":136,"x1":698,"y0":1395,"y1":1422},"font_size":0.0,"text":"improves the performance, especially at the frst iteration.i"}],"source":"layout det","text":"As demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the effciencyi of altering positive and negative gradient coeffcients. In addition, GRPO+PS shows superiori performance compared to GRPO+OS, indicating the benefts of using fne-grained, step-awareii gradient coeffcients. Furthermore, we explore the iterative RL, in our experiments, we conducti two rounds of iteration. As shown in Figure $6,$  we notice that the iterative RL signifcantlyi improves the performance, especially at the frst iteration.i"}],"formula_dets":[{"bbox":{"x0":592,"x1":611,"y0":1373,"y1":1394},"conf":0.5141,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1057,"y0":740,"y1":937},"conf":0.9775,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":971,"y1":1252},"conf":0.976,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1058,"y0":1256,"y1":1429},"conf":0.9723,"label":"Text","label_id":1},{"bbox":{"x0":185,"x1":1007,"y0":172,"y1":569},"conf":0.9719,"label":"Figure","label_id":3},{"bbox":{"x0":132,"x1":1057,"y0":574,"y1":634},"conf":0.9478,"label":"Figure caption","label_id":4},{"bbox":{"x0":133,"x1":1057,"y0":671,"y1":734},"conf":0.9195,"label":"Text","label_id":1},{"bbox":{"x0":576,"x1":613,"y0":1550,"y1":1579},"conf":0.7772,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1575],[580,1575]],"score":0.9269},{"poly":[[136,1396],[698,1395],[698,1419],[137,1421]],"score":0.7908},{"poly":[[136,1368],[1049,1370],[1049,1395],[136,1393]],"score":0.8248},{"poly":[[135,1340],[1053,1338],[1053,1368],[135,1370]],"score":0.6681},{"poly":[[138,1314],[1049,1314],[1049,1338],[138,1338]],"score":0.7389},{"poly":[[138,1286],[1051,1288],[1051,1312],[138,1310]],"score":0.7773},{"poly":[[174,1259],[1049,1261],[1049,1286],[174,1284]],"score":0.8435},{"poly":[[138,1221],[852,1221],[852,1244],[138,1244]],"score":0.8257},{"poly":[[137,1191],[1049,1193],[1049,1217],[136,1216]],"score":0.7869},{"poly":[[140,1167],[1051,1167],[1051,1191],[140,1191]],"score":0.7548},{"poly":[[136,1137],[1053,1133],[1053,1163],[137,1167]],"score":0.6315},{"poly":[[136,1112],[1051,1110],[1051,1135],[137,1137]],"score":0.7807},{"poly":[[138,1086],[1049,1086],[1049,1109],[138,1109]],"score":0.8835},{"poly":[[138,1058],[1049,1058],[1049,1081],[138,1081]],"score":0.8515},{"poly":[[138,1031],[1051,1031],[1051,1056],[138,1056]],"score":0.7246},{"poly":[[136,1002],[1053,998],[1053,1028],[137,1031]],"score":0.705},{"poly":[[138,974],[1051,975],[1051,1000],[138,998]],"score":0.7914},{"poly":[[137,903],[422,905],[422,930],[136,928]],"score":0.8001},{"poly":[[137,875],[1051,877],[1051,907],[136,905]],"score":0.7269},{"poly":[[138,853],[1049,853],[1049,875],[138,875]],"score":0.8846},{"poly":[[137,819],[1053,821],[1053,851],[136,849]],"score":0.7202},{"poly":[[137,795],[1053,796],[1053,826],[136,824]],"score":0.7448},{"poly":[[140,770],[1051,770],[1051,795],[140,795]],"score":0.7613},{"poly":[[174,744],[1049,744],[1049,767],[174,767]],"score":0.8635},{"poly":[[140,703],[771,703],[771,726],[140,726]],"score":0.8461},{"poly":[[138,675],[1049,675],[1049,698],[138,698]],"score":0.8631},{"poly":[[138,605],[308,605],[308,628],[138,628]],"score":0.9204},{"poly":[[140,579],[1051,579],[1051,602],[140,602]],"score":0.8633},{"poly":[[796,532],[857,539],[854,567],[793,560]],"score":0.7878},{"poly":[[389,532],[449,539],[446,567],[385,560]],"score":0.7913},{"poly":[[727,512],[992,512],[992,537],[727,537]],"score":0.7269},{"poly":[[666,516],[682,516],[682,533],[666,533]],"score":0.9031},{"poly":[[319,512],[587,512],[587,537],[319,537]],"score":0.7283},{"poly":[[259,516],[275,516],[275,533],[259,533]],"score":0.8861},{"poly":[[222,482],[246,482],[246,505],[222,505]],"score":0.8138},{"poly":[[627,475],[654,475],[654,500],[627,500]],"score":0.7655},{"poly":[[222,444],[248,444],[248,467],[222,467]],"score":0.7957},{"poly":[[629,433],[654,433],[654,454],[629,454]],"score":0.9218},{"poly":[[220,402],[245,402],[245,425],[220,425]],"score":0.7145},{"poly":[[627,388],[654,388],[654,410],[627,410]],"score":0.7161},{"poly":[[385,370],[397,370],[397,379],[385,379]],"score":0.6019},{"poly":[[222,367],[246,367],[246,388],[222,388]],"score":0.8263},{"poly":[[631,347],[650,347],[650,365],[631,365]],"score":0.8279},{"poly":[[603,335],[629,335],[629,419],[603,419]],"score":0.7394},{"poly":[[189,327],[221,326],[226,420],[194,421]],"score":0.6972},{"poly":[[220,326],[245,326],[245,347],[220,347]],"score":0.7943},{"poly":[[631,303],[650,303],[650,321],[631,321]],"score":0.7188},{"poly":[[222,288],[246,288],[246,309],[222,309]],"score":0.8287},{"poly":[[629,256],[654,256],[654,277],[629,277]],"score":0.926},{"poly":[[220,249],[245,249],[245,270],[220,270]],"score":0.9534},{"poly":[[376,216],[459,216],[459,240],[376,240]],"score":0.9679},{"poly":[[790,214],[861,214],[861,240],[790,240]],"score":0.8603},{"poly":[[597,184],[679,184],[679,202],[597,202]],"score":0.9432},{"poly":[[441,184],[521,184],[521,202],[441,202]],"score":0.956},{"poly":[[755,182],[837,182],[837,202],[755,202]],"score":0.9521}],"page_no":19,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":576,"x1":612,"y0":1550,"y1":1581},"conf":0.7448,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":608,"y0":1552,"y1":1575},"font_size":0.0,"text":"21"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":160,"x1":1027,"y0":172,"y1":589},"conf":0.9636,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![deb628a35fe29870d566481c44b7be39](imgs/deb628a35fe29870d566481c44b7be39.jpg)"},{"bbox":{"x0":131,"x1":1057,"y0":595,"y1":660},"conf":0.9641,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":600,"y1":623},"font_size":0.0,"text":"Figure 7 | The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH"},{"bbox":{"x0":140,"x1":844,"y0":628,"y1":653},"font_size":0.0,"text":"(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K."}],"source":"layout det","text":"Figure 7 | The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K."},{"bbox":{"x0":133,"x1":369,"y0":692,"y1":729},"conf":0.8853,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":362,"y0":696,"y1":719},"font_size":0.0,"text":"5.2.2. Why RL Works?"}],"source":"layout det","text":"5.2.2. Why RL Works?"},{"bbox":{"x0":132,"x1":1059,"y0":738,"y1":1046},"conf":0.9784,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":738,"y1":768},"font_size":0.0,"text":"In this paper, we conduct reinforcement learning based on a subset of instruction tuning"},{"bbox":{"x0":140,"x1":1051,"y0":770,"y1":795},"font_size":0.0,"text":"data, and it achieves signifcant performance enhancement upon the instruction tuning model.i"},{"bbox":{"x0":140,"x1":1049,"y0":795,"y1":819},"font_size":0.0,"text":"To further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K"},{"bbox":{"x0":138,"x1":1049,"y0":823,"y1":847},"font_size":0.0,"text":"accuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances"},{"bbox":{"x0":140,"x1":1051,"y0":851,"y1":876},"font_size":0.0,"text":"Maj@Kâ€™s performance but not Pass@K. These fndings indicate that RL enhances the modelâ€™si"},{"bbox":{"x0":138,"x1":1049,"y0":877,"y1":902},"font_size":0.0,"text":"overall performance by rendering the output distribution more robust, in other words, it seems"},{"bbox":{"x0":140,"x1":1051,"y0":905,"y1":928},"font_size":0.0,"text":"that the improvement is attributed to boosting the correct response from TopK rather than"},{"bbox":{"x0":138,"x1":1051,"y0":931,"y1":957},"font_size":0.0,"text":"the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identifed ai"},{"bbox":{"x0":135,"x1":1051,"y0":954,"y1":988},"font_size":0.0,"text":"misalignment problem in reasoning tasks within the SFT model, showing that the reasoning"},{"bbox":{"x0":138,"x1":1049,"y0":986,"y1":1010},"font_size":0.0,"text":"performance of SFT models can be improved through a series of preference alignment strategies"},{"bbox":{"x0":140,"x1":673,"y0":1014,"y1":1037},"font_size":0.0,"text":"(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b)."}],"source":"layout det","text":"In this paper, we conduct reinforcement learning based on a subset of instruction tuning data, and it achieves signifcant performance enhancement upon the instruction tuning model.i To further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K accuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances Maj@Kâ€™s performance but not Pass@K. These fndings indicate that RL enhances the modelâ€™si overall performance by rendering the output distribution more robust, in other words, it seems that the improvement is attributed to boosting the correct response from TopK rather than the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identifed ai misalignment problem in reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b)."},{"bbox":{"x0":132,"x1":555,"y0":1069,"y1":1106},"conf":0.9021,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":548,"y0":1074,"y1":1098},"font_size":0.0,"text":"5.2.3. How to Achieve More Effective RL?"}],"source":"layout det","text":"5.2.3. How to Achieve More Effective RL?"},{"bbox":{"x0":131,"x1":1057,"y0":1114,"y1":1261},"conf":0.9706,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1116,"y1":1144},"font_size":0.0,"text":"We demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unifedi"},{"bbox":{"x0":135,"x1":1051,"y0":1142,"y1":1174},"font_size":0.0,"text":"paradigm to understand different representative training methods. Within this paradigm, all"},{"bbox":{"x0":138,"x1":1049,"y0":1174,"y1":1198},"font_size":0.0,"text":"methods are conceptualized as either direct or simplifed RL techniques. As summarized ini"},{"bbox":{"x0":140,"x1":1051,"y0":1200,"y1":1223},"font_size":0.0,"text":"Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function."},{"bbox":{"x0":136,"x1":845,"y0":1224,"y1":1253},"font_size":0.0,"text":"We provide some potential future directions about the three components."}],"source":"layout det","text":"We demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unifedi paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplifed RL techniques. As summarized ini Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function.We provide some potential future directions about the three components."},{"bbox":{"x0":132,"x1":1059,"y0":1292,"y1":1521},"conf":0.973,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1296,"y1":1323},"font_size":0.0,"text":"Data SourceData source is the raw material of all training methods. In the context of $\\mathrm{RL},$ we"},{"bbox":{"x0":138,"x1":1049,"y0":1324,"y1":1351},"font_size":0.0,"text":"specifcally refer to the data source as the unlabeled questions with the outputs sampled fromi"},{"bbox":{"x0":138,"x1":1049,"y0":1352,"y1":1377},"font_size":0.0,"text":"the policy model. In this paper, we only use the questions from the instruction tuning stage and"},{"bbox":{"x0":138,"x1":1051,"y0":1379,"y1":1403},"font_size":0.0,"text":"a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL"},{"bbox":{"x0":135,"x1":1053,"y0":1403,"y1":1435},"font_size":0.0,"text":"pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline"},{"bbox":{"x0":138,"x1":1049,"y0":1435,"y1":1458},"font_size":0.0,"text":"on out-of-distribution question prompts, in conjunction with advanced sampling (decoding)"},{"bbox":{"x0":136,"x1":1051,"y0":1459,"y1":1486},"font_size":0.0,"text":"strategies, like those based on tree-search methods (Yao et al., 2023). Also, the effcient inferencei"},{"bbox":{"x0":140,"x1":1049,"y0":1489,"y1":1512},"font_size":0.0,"text":"techniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determines"}],"source":"layout det","text":"Data SourceData source is the raw material of all training methods. In the context of $\\mathrm{RL},$ we specifcally refer to the data source as the unlabeled questions with the outputs sampled fromi the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction with advanced sampling (decoding)strategies, like those based on tree-search methods (Yao et al., 2023). Also, the effcient inferencei techniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determines"}],"formula_dets":[{"bbox":{"x0":979,"x1":1015,"y0":1300,"y1":1323},"conf":0.5266,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1059,"y0":738,"y1":1046},"conf":0.9784,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1292,"y1":1521},"conf":0.973,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":1114,"y1":1261},"conf":0.9706,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":595,"y1":660},"conf":0.9641,"label":"Figure caption","label_id":4},{"bbox":{"x0":160,"x1":1027,"y0":172,"y1":589},"conf":0.9636,"label":"Figure","label_id":3},{"bbox":{"x0":132,"x1":555,"y0":1069,"y1":1106},"conf":0.9021,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":369,"y0":692,"y1":729},"conf":0.8853,"label":"Title","label_id":0},{"bbox":{"x0":576,"x1":612,"y0":1550,"y1":1581},"conf":0.7448,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[608,1552],[608,1575],[580,1575]],"score":0.9601},{"poly":[[140,1489],[1049,1489],[1049,1512],[140,1512]],"score":0.8475},{"poly":[[136,1461],[1051,1459],[1051,1484],[137,1486]],"score":0.7798},{"poly":[[138,1435],[1049,1435],[1049,1458],[138,1458]],"score":0.8645},{"poly":[[135,1405],[1053,1403],[1053,1433],[135,1435]],"score":0.6698},{"poly":[[138,1379],[1051,1379],[1051,1403],[138,1403]],"score":0.7437},{"poly":[[138,1352],[1049,1352],[1049,1377],[138,1377]],"score":0.7472},{"poly":[[138,1326],[1049,1324],[1049,1349],[138,1351]],"score":0.7796},{"poly":[[138,1296],[1051,1298],[1051,1323],[138,1321]],"score":0.8131},{"poly":[[137,1224],[845,1228],[845,1253],[136,1249]],"score":0.7574},{"poly":[[140,1200],[1051,1200],[1051,1223],[140,1223]],"score":0.829},{"poly":[[138,1174],[1049,1174],[1049,1196],[138,1196]],"score":0.909},{"poly":[[135,1144],[1051,1142],[1051,1172],[135,1174]],"score":0.6949},{"poly":[[138,1116],[1049,1117],[1049,1142],[138,1140]],"score":0.8176},{"poly":[[140,1074],[548,1074],[548,1098],[140,1098]],"score":0.7639},{"poly":[[140,1014],[673,1014],[673,1037],[140,1037]],"score":0.9362},{"poly":[[138,986],[1049,986],[1049,1010],[138,1010]],"score":0.7803},{"poly":[[135,954],[1051,958],[1051,988],[135,984]],"score":0.679},{"poly":[[138,931],[1051,931],[1051,956],[138,956]],"score":0.7327},{"poly":[[140,905],[1051,905],[1051,928],[140,928]],"score":0.8588},{"poly":[[138,877],[1049,877],[1049,902],[138,902]],"score":0.7232},{"poly":[[140,851],[1051,851],[1051,874],[140,874]],"score":0.9027},{"poly":[[138,823],[1049,823],[1049,847],[138,847]],"score":0.7252},{"poly":[[140,795],[1049,795],[1049,819],[140,819]],"score":0.7764},{"poly":[[140,770],[1051,770],[1051,793],[140,793]],"score":0.9341},{"poly":[[135,738],[1051,740],[1051,768],[135,767]],"score":0.7894},{"poly":[[138,696],[362,696],[362,719],[138,719]],"score":0.8878},{"poly":[[140,628],[844,628],[844,653],[140,653]],"score":0.7524},{"poly":[[140,600],[1049,600],[1049,623],[140,623]],"score":0.9651},{"poly":[[693,560],[984,560],[984,582],[693,582]],"score":0.9532},{"poly":[[262,560],[553,560],[553,582],[262,582]],"score":0.946},{"poly":[[734,537],[751,537],[751,553],[734,553]],"score":0.8062},{"poly":[[303,537],[321,537],[321,553],[303,553]],"score":0.8572},{"poly":[[985,533],[1012,533],[1012,556],[985,556]],"score":0.8757},{"poly":[[858,533],[886,533],[886,556],[858,556]],"score":0.7929},{"poly":[[798,535],[815,535],[815,553],[798,553]],"score":0.7777},{"poly":[[555,533],[583,533],[583,556],[555,556]],"score":0.811},{"poly":[[427,533],[455,533],[455,556],[427,556]],"score":0.8039},{"poly":[[915,530],[950,530],[950,558],[915,558]],"score":0.7631},{"poly":[[486,530],[521,530],[521,558],[486,558]],"score":0.7196},{"poly":[[365,532],[386,532],[386,556],[365,556]],"score":0.6781},{"poly":[[631,500],[656,500],[656,523],[631,523]],"score":0.899},{"poly":[[200,495],[225,495],[225,519],[200,519]],"score":0.8511},{"poly":[[631,472],[658,472],[658,495],[631,495]],"score":0.853},{"poly":[[198,467],[227,467],[227,491],[198,491]],"score":0.7268},{"poly":[[631,442],[656,442],[656,465],[631,465]],"score":0.8074},{"poly":[[200,439],[227,439],[227,461],[200,461]],"score":0.7828},{"poly":[[629,412],[656,412],[656,433],[629,433]],"score":0.8244},{"poly":[[629,381],[656,381],[656,405],[629,405]],"score":0.7413},{"poly":[[205,373],[229,385],[217,409],[193,397]],"score":0.6875},{"poly":[[631,353],[658,353],[658,375],[631,375]],"score":0.7706},{"poly":[[197,351],[223,351],[223,375],[197,375]],"score":0.779},{"poly":[[601,344],[633,344],[633,435],[601,435]],"score":0.6809},{"poly":[[200,325],[225,325],[225,347],[200,347]],"score":0.8063},{"poly":[[633,323],[656,323],[656,346],[633,346]],"score":0.8413},{"poly":[[631,295],[658,295],[658,316],[631,316]],"score":0.9015},{"poly":[[200,295],[225,295],[225,318],[200,318]],"score":0.8265},{"poly":[[200,267],[225,267],[225,289],[200,289]],"score":0.8683},{"poly":[[629,265],[656,265],[656,288],[629,288]],"score":0.8635},{"poly":[[801,218],[877,218],[877,244],[801,244]],"score":0.9345},{"poly":[[363,218],[454,218],[454,246],[363,246]],"score":0.9176},{"poly":[[677,186],[812,186],[812,203],[677,203]],"score":0.9836},{"poly":[[500,182],[599,182],[599,207],[500,207]],"score":0.8078},{"poly":[[289,184],[420,184],[420,203],[289,203]],"score":0.8343},{"poly":[[888,179],[994,179],[994,209],[888,209]],"score":0.7453}],"page_no":20,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":159,"x1":883,"y0":1492,"y1":1581},"conf":0.6019,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"22"},{"bbox":{"x0":168,"x1":877,"y0":1493,"y1":1521},"font_size":0.0,"text":"7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":955,"y0":168,"y1":204},"conf":0.7571,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":948,"y0":174,"y1":198},"font_size":0.0,"text":"the exploration effciency of policy models, also play an exceedingly important role.i"}],"source":"layout det","text":"the exploration effciency of policy models, also play an exceedingly important role.i"},{"bbox":{"x0":133,"x1":1060,"y0":239,"y1":492},"conf":0.9738,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":246,"y1":269},"font_size":0.0,"text":"AlgorithmsAlgorithms process the data and reward signal to the gradient coeffcient to updatei"},{"bbox":{"x0":138,"x1":1049,"y0":272,"y1":295},"font_size":0.0,"text":"the model parameter. Based on Equation $5,$  to some extent, all methods now fully TRUST the"},{"bbox":{"x0":138,"x1":1051,"y0":298,"y1":323},"font_size":0.0,"text":"signal of the reward function to increase or decrease the conditional probability of a certain"},{"bbox":{"x0":140,"x1":1049,"y0":326,"y1":351},"font_size":0.0,"text":"token. However, it is impossible to ensure the reward signal is always reliable, especially in"},{"bbox":{"x0":140,"x1":1051,"y0":354,"y1":379},"font_size":0.0,"text":"extremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023),"},{"bbox":{"x0":136,"x1":1051,"y0":377,"y1":409},"font_size":0.0,"text":"which have been carefully annotated by well-trained annotators, still contain approximately $20\\%$"},{"bbox":{"x0":140,"x1":1047,"y0":409,"y1":432},"font_size":0.0,"text":"of incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm"},{"bbox":{"x0":140,"x1":1053,"y0":435,"y1":460},"font_size":0.0,"text":"that is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,"},{"bbox":{"x0":142,"x1":966,"y0":461,"y1":486},"font_size":0.0,"text":"2023) alignment methods will bring a fundamental change to the learning algorithms."}],"source":"layout det","text":"AlgorithmsAlgorithms process the data and reward signal to the gradient coeffcient to updatei the model parameter. Based on Equation $5,$  to some extent, all methods now fully TRUST the signal of the reward function to increase or decrease the conditional probability of a certain token. However, it is impossible to ensure the reward signal is always reliable, especially in extremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023), which have been carefully annotated by well-trained annotators, still contain approximately $20\\%$ of incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm that is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,2023) alignment methods will bring a fundamental change to the learning algorithms."},{"bbox":{"x0":133,"x1":1059,"y0":526,"y1":806},"conf":0.9782,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":530,"y1":560},"font_size":0.0,"text":"Reward FunctionReward function is the source of the training signal. In RL, the reward"},{"bbox":{"x0":138,"x1":1049,"y0":560,"y1":584},"font_size":0.0,"text":"function is usually the neural reward model. We think there exist three important directions for"},{"bbox":{"x0":138,"x1":1049,"y0":588,"y1":610},"font_size":0.0,"text":"reward models: 1) How to enhance the generalization ability of the reward model. The reward"},{"bbox":{"x0":140,"x1":1049,"y0":614,"y1":639},"font_size":0.0,"text":"model must be effectively generalized to handle out-of-distribution questions and advanced"},{"bbox":{"x0":140,"x1":1051,"y0":642,"y1":667},"font_size":0.0,"text":"decoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of"},{"bbox":{"x0":138,"x1":1049,"y0":667,"y1":693},"font_size":0.0,"text":"LLMs rather than improve their fundamental capabilities; 2) How to refect the uncertaintyl"},{"bbox":{"x0":140,"x1":1049,"y0":696,"y1":719},"font_size":0.0,"text":"of reward model. The uncertainty could potentially act as a linking bridge between the weak"},{"bbox":{"x0":140,"x1":1051,"y0":723,"y1":747},"font_size":0.0,"text":"reward model and the weak-to-strong learning algorithms; 3) How to effciently build high-i"},{"bbox":{"x0":142,"x1":1049,"y0":751,"y1":775},"font_size":0.0,"text":"quality process reward models that can provide fne-grained training signals for the reasoningi"},{"bbox":{"x0":136,"x1":622,"y0":775,"y1":802},"font_size":0.0,"text":"process (Lightman et al., 2023; Wang et al., 2023b)."}],"source":"layout det","text":"Reward FunctionReward function is the source of the training signal. In RL, the reward function is usually the neural reward model. We think there exist three important directions for reward models: 1) How to enhance the generalization ability of the reward model. The reward model must be effectively generalized to handle out-of-distribution questions and advanced decoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of LLMs rather than improve their fundamental capabilities; 2) How to refect the uncertaintyl of reward model. The uncertainty could potentially act as a linking bridge between the weak reward model and the weak-to-strong learning algorithms; 3) How to effciently build high-i quality process reward models that can provide fne-grained training signals for the reasoningi process (Lightman et al., 2023; Wang et al., 2023b)."},{"bbox":{"x0":133,"x1":666,"y0":841,"y1":877},"conf":0.9131,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":659,"y0":847,"y1":872},"font_size":0.0,"text":"6. Conclusion, Limitation, and Future Work"}],"source":"layout det","text":"6.Conclusion, Limitation, and Future Work"},{"bbox":{"x0":132,"x1":1058,"y0":894,"y1":1200},"conf":0.9789,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1053,"y0":900,"y1":924},"font_size":0.0,"text":"We present DeepSeekMath, which outperforms all open-source models on the competition-"},{"bbox":{"x0":138,"x1":1051,"y0":928,"y1":953},"font_size":0.0,"text":"level MATH benchmark and approaches the performance of closed models. DeepSeekMath is"},{"bbox":{"x0":138,"x1":1049,"y0":954,"y1":979},"font_size":0.0,"text":"initialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with"},{"bbox":{"x0":140,"x1":1049,"y0":979,"y1":1005},"font_size":0.0,"text":"a signifcant component of the training data being 120B math tokens sourced from Commoni"},{"bbox":{"x0":140,"x1":1049,"y0":1009,"y1":1033},"font_size":0.0,"text":"Crawl. Our extensive ablation study shows web pages offer signifcant potential for high-qualityi"},{"bbox":{"x0":136,"x1":1049,"y0":1030,"y1":1063},"font_size":0.0,"text":"mathematical data, while arXiv may not as benefcial as we expected. We introduce Groupi"},{"bbox":{"x0":140,"x1":1049,"y0":1063,"y1":1086},"font_size":0.0,"text":"Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which"},{"bbox":{"x0":140,"x1":1051,"y0":1089,"y1":1114},"font_size":0.0,"text":"can notably improve mathematical reasoning capabilities with less memory consumption. The"},{"bbox":{"x0":140,"x1":1051,"y0":1117,"y1":1140},"font_size":0.0,"text":"experiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached"},{"bbox":{"x0":140,"x1":1051,"y0":1144,"y1":1168},"font_size":0.0,"text":"a high score on benchmarks. We also provide a unifed paradigm to understand a series ofi"},{"bbox":{"x0":136,"x1":1053,"y0":1166,"y1":1200},"font_size":0.0,"text":"methods and summarize several potential directions for more effective reinforcement learning."}],"source":"layout det","text":"We present DeepSeekMath, which outperforms all open-source models on the competitionlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is initialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with a signifcant component of the training data being 120B math tokens sourced from Commoni Crawl. Our extensive ablation study shows web pages offer signifcant potential for high-qualityi mathematical data, while arXiv may not as benefcial as we expected. We introduce Groupi Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached a high score on benchmarks. We also provide a unifed paradigm to understand a series ofi methods and summarize several potential directions for more effective reinforcement learning."},{"bbox":{"x0":133,"x1":1058,"y0":1205,"y1":1458},"conf":0.9787,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":1210,"y1":1233},"font_size":0.0,"text":"Although DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,"},{"bbox":{"x0":140,"x1":1051,"y0":1238,"y1":1263},"font_size":0.0,"text":"its capability on geometry and theorem-proof are relatively weaker than closed models. For"},{"bbox":{"x0":140,"x1":1049,"y0":1265,"y1":1288},"font_size":0.0,"text":"instance, in our dry run, the model cannot handle problems related to triangles and ellipses,"},{"bbox":{"x0":140,"x1":1049,"y0":1293,"y1":1317},"font_size":0.0,"text":"which may indicate data selection bias in pre-training and fne-tuning. In addition, restrictedi"},{"bbox":{"x0":138,"x1":1049,"y0":1321,"y1":1344},"font_size":0.0,"text":"by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could"},{"bbox":{"x0":138,"x1":1049,"y0":1347,"y1":1370},"font_size":0.0,"text":"improve its performance with few-shot inputs, while DeepSeekMath shows similar performance"},{"bbox":{"x0":138,"x1":1049,"y0":1374,"y1":1398},"font_size":0.0,"text":"in zero-shot and few-shot evaluation. In the future, we will further improve our engineered"},{"bbox":{"x0":136,"x1":1051,"y0":1396,"y1":1428},"font_size":0.0,"text":"data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will"},{"bbox":{"x0":140,"x1":1053,"y0":1428,"y1":1452},"font_size":0.0,"text":"explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs."}],"source":"layout det","text":"Although DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses,which may indicate data selection bias in pre-training and fne-tuning. In addition, restrictedi by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while DeepSeekMath shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs."}],"formula_dets":[{"bbox":{"x0":1009,"x1":1051,"y0":380,"y1":401},"conf":0.8281,"label":"print_embedding","label_id":0},{"bbox":{"x0":541,"x1":560,"y0":274,"y1":295},"conf":0.6797,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1058,"y0":894,"y1":1200},"conf":0.9789,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":1205,"y1":1458},"conf":0.9787,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":526,"y1":806},"conf":0.9782,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":239,"y1":492},"conf":0.9738,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":666,"y0":841,"y1":877},"conf":0.9131,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":955,"y0":168,"y1":204},"conf":0.7571,"label":"Text","label_id":1},{"bbox":{"x0":165,"x1":885,"y0":1493,"y1":1581},"conf":0.6062,"label":"Abandon","label_id":2},{"bbox":{"x0":159,"x1":883,"y0":1492,"y1":1581},"conf":0.6019,"label":"Abandon","label_id":2},{"bbox":{"x0":577,"x1":612,"y0":1551,"y1":1579},"conf":0.3542,"label":"Abandon","label_id":2},{"bbox":{"x0":167,"x1":882,"y0":1493,"y1":1523},"conf":0.2921,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.9145},{"poly":[[168,1493],[877,1496],[877,1521],[168,1517]],"score":0.6994},{"poly":[[140,1428],[1053,1428],[1053,1452],[140,1452]],"score":0.7576},{"poly":[[136,1398],[1051,1396],[1051,1426],[137,1428]],"score":0.6968},{"poly":[[138,1374],[1049,1374],[1049,1398],[138,1398]],"score":0.735},{"poly":[[138,1347],[1049,1347],[1049,1370],[138,1370]],"score":0.8498},{"poly":[[138,1321],[1049,1321],[1049,1344],[138,1344]],"score":0.8168},{"poly":[[140,1293],[1049,1293],[1049,1317],[140,1317]],"score":0.7181},{"poly":[[140,1265],[1049,1265],[1049,1288],[140,1288]],"score":0.7588},{"poly":[[140,1238],[1051,1238],[1051,1263],[140,1263]],"score":0.7201},{"poly":[[174,1210],[1049,1210],[1049,1233],[174,1233]],"score":0.7985},{"poly":[[137,1166],[1053,1170],[1053,1200],[136,1196]],"score":0.6807},{"poly":[[140,1144],[1051,1144],[1051,1168],[140,1168]],"score":0.7363},{"poly":[[140,1117],[1051,1117],[1051,1140],[140,1140]],"score":0.8394},{"poly":[[140,1089],[1051,1089],[1051,1114],[140,1114]],"score":0.7444},{"poly":[[140,1063],[1049,1063],[1049,1086],[140,1086]],"score":0.8432},{"poly":[[137,1030],[1049,1033],[1049,1063],[136,1059]],"score":0.6572},{"poly":[[140,1009],[1049,1009],[1049,1033],[140,1033]],"score":0.7536},{"poly":[[140,979],[1049,979],[1049,1003],[140,1003]],"score":0.7146},{"poly":[[138,954],[1049,954],[1049,979],[138,979]],"score":0.7223},{"poly":[[138,928],[1051,928],[1051,953],[138,953]],"score":0.7175},{"poly":[[140,900],[1053,900],[1053,924],[140,924]],"score":0.7633},{"poly":[[140,847],[659,847],[659,872],[140,872]],"score":0.7443},{"poly":[[136,777],[622,775],[622,800],[137,802]],"score":0.7829},{"poly":[[142,751],[1049,751],[1049,775],[142,775]],"score":0.7833},{"poly":[[140,723],[1051,723],[1051,747],[140,747]],"score":0.7619},{"poly":[[140,696],[1049,696],[1049,719],[140,719]],"score":0.9029},{"poly":[[138,667],[1049,668],[1049,693],[138,691]],"score":0.7815},{"poly":[[140,642],[1051,642],[1051,667],[140,667]],"score":0.7521},{"poly":[[140,614],[1049,614],[1049,639],[140,639]],"score":0.7514},{"poly":[[138,588],[1049,588],[1049,610],[138,610]],"score":0.8641},{"poly":[[138,560],[1049,560],[1049,584],[138,584]],"score":0.7423},{"poly":[[138,530],[1051,530],[1051,560],[138,560]],"score":0.6407},{"poly":[[142,461],[966,461],[966,486],[142,486]],"score":0.7807},{"poly":[[140,435],[1053,435],[1053,460],[140,460]],"score":0.7549},{"poly":[[140,409],[1047,409],[1047,432],[140,432]],"score":0.865},{"poly":[[136,379],[1053,377],[1053,407],[137,409]],"score":0.6825},{"poly":[[140,354],[1051,354],[1051,379],[140,379]],"score":0.7401},{"poly":[[140,326],[1049,326],[1049,351],[140,351]],"score":0.7537},{"poly":[[138,298],[1051,298],[1051,323],[138,323]],"score":0.7749},{"poly":[[138,272],[1049,272],[1049,295],[138,295]],"score":0.8764},{"poly":[[140,246],[1049,246],[1049,268],[140,268]],"score":0.8913},{"poly":[[140,174],[948,174],[948,198],[140,198]],"score":0.743}],"page_no":21,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8018,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1577},"font_size":0.0,"text":"23"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":277,"y0":165,"y1":200},"conf":0.8811,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":271,"y0":170,"y1":198},"font_size":0.0,"text":"References"}],"source":"layout det","text":"References"},{"bbox":{"x0":141,"x1":1061,"y0":217,"y1":468},"conf":0.9631,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":223,"y1":247},"font_size":0.0,"text":"R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,"},{"bbox":{"x0":158,"x1":1051,"y0":249,"y1":275},"font_size":0.0,"text":"K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,"},{"bbox":{"x0":161,"x1":1051,"y0":279,"y1":303},"font_size":0.0,"text":"E. Pitler, T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan,"},{"bbox":{"x0":158,"x1":1053,"y0":302,"y1":333},"font_size":0.0,"text":"B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,"},{"bbox":{"x0":159,"x1":1051,"y0":330,"y1":358},"font_size":0.0,"text":"K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,"},{"bbox":{"x0":161,"x1":1051,"y0":360,"y1":384},"font_size":0.0,"text":"B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,"},{"bbox":{"x0":158,"x1":1051,"y0":382,"y1":414},"font_size":0.0,"text":"M. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal"},{"bbox":{"x0":159,"x1":1051,"y0":412,"y1":439},"font_size":0.0,"text":"models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https:"},{"bbox":{"x0":161,"x1":608,"y0":442,"y1":465},"font_size":0.0,"text":"//doi.org/10.48550/arXiv.2312.11805."}],"source":"layout det","text":"R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,E. Pitler, T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan,B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,M. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805."},{"bbox":{"x0":130,"x1":1059,"y0":477,"y1":567},"conf":0.9565,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":482,"y1":507},"font_size":0.0,"text":"J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,"},{"bbox":{"x0":160,"x1":1049,"y0":512,"y1":537},"font_size":0.0,"text":"Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,"},{"bbox":{"x0":157,"x1":215,"y0":534,"y1":567},"font_size":0.0,"text":"2021."}],"source":"layout det","text":"J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,2021."},{"bbox":{"x0":131,"x1":1059,"y0":575,"y1":666},"conf":0.9519,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":581,"y1":605},"font_size":0.0,"text":"Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Bider-"},{"bbox":{"x0":158,"x1":1053,"y0":605,"y1":637},"font_size":0.0,"text":"man, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint"},{"bbox":{"x0":159,"x1":385,"y0":633,"y1":661},"font_size":0.0,"text":"arXiv:2310.10631, 2023."}],"source":"layout det","text":"Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023."},{"bbox":{"x0":131,"x1":1057,"y0":672,"y1":737},"conf":0.9398,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1047,"y0":679,"y1":703},"font_size":0.0,"text":"J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen"},{"bbox":{"x0":161,"x1":691,"y0":707,"y1":730},"font_size":0.0,"text":"technical report. arXiv preprint arXiv:2309.16609, 2023."}],"source":"layout det","text":"J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023."},{"bbox":{"x0":131,"x1":1058,"y0":742,"y1":834},"conf":0.9498,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":749,"y1":772},"font_size":0.0,"text":"C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,"},{"bbox":{"x0":161,"x1":1051,"y0":777,"y1":802},"font_size":0.0,"text":"M. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with"},{"bbox":{"x0":161,"x1":711,"y0":805,"y1":828},"font_size":0.0,"text":"weak supervision. arXiv preprint arXiv:2312.09390, 2023."}],"source":"layout det","text":"C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,M. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023."},{"bbox":{"x0":132,"x1":1058,"y0":840,"y1":902},"conf":0.9365,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":847,"y1":870},"font_size":0.0,"text":"ChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c"},{"bbox":{"x0":159,"x1":383,"y0":872,"y1":898},"font_size":0.0,"text":"om/THUDM/ChatGLM3."}],"source":"layout det","text":"ChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c om/THUDM/ChatGLM3."},{"bbox":{"x0":144,"x1":1062,"y0":910,"y1":1162},"conf":0.9727,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":916,"y1":938},"font_size":0.0,"text":"M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,"},{"bbox":{"x0":161,"x1":1049,"y0":942,"y1":967},"font_size":0.0,"text":"N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,"},{"bbox":{"x0":161,"x1":1051,"y0":970,"y1":995},"font_size":0.0,"text":"B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,"},{"bbox":{"x0":161,"x1":1049,"y0":996,"y1":1021},"font_size":0.0,"text":"F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,"},{"bbox":{"x0":161,"x1":1051,"y0":1024,"y1":1049},"font_size":0.0,"text":"A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,"},{"bbox":{"x0":158,"x1":1049,"y0":1049,"y1":1077},"font_size":0.0,"text":"A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,"},{"bbox":{"x0":161,"x1":1049,"y0":1079,"y1":1103},"font_size":0.0,"text":"M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and"},{"bbox":{"x0":158,"x1":1053,"y0":1102,"y1":1133},"font_size":0.0,"text":"W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021."},{"bbox":{"x0":161,"x1":619,"y0":1131,"y1":1156},"font_size":0.0,"text":"URL https://arxiv.org/abs/2107.03374."}],"source":"layout det","text":"M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.URL https://arxiv.org/abs/2107.03374."},{"bbox":{"x0":133,"x1":1059,"y0":1170,"y1":1261},"conf":0.9526,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1172,"y1":1205},"font_size":0.0,"text":"W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling"},{"bbox":{"x0":159,"x1":1051,"y0":1202,"y1":1228},"font_size":0.0,"text":"computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:"},{"bbox":{"x0":163,"x1":1019,"y0":1230,"y1":1254},"font_size":0.0,"text":"10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588."}],"source":"layout det","text":"W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588."},{"bbox":{"x0":131,"x1":1059,"y0":1266,"y1":1357},"conf":0.9552,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1274,"y1":1298},"font_size":0.0,"text":"K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,"},{"bbox":{"x0":158,"x1":1047,"y0":1300,"y1":1325},"font_size":0.0,"text":"J. Hilton, R. Nakano, et al. Training verifers to solve math word problems. arXiv preprinti"},{"bbox":{"x0":159,"x1":388,"y0":1326,"y1":1353},"font_size":0.0,"text":"arXiv:2110.14168, 2021."}],"source":"layout det","text":"K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,J. Hilton, R. Nakano, et al. Training verifers to solve math word problems. arXiv preprinti arXiv:2110.14168, 2021."},{"bbox":{"x0":132,"x1":1058,"y0":1364,"y1":1429},"conf":0.9368,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1366,"y1":1398},"font_size":0.0,"text":"T. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL"},{"bbox":{"x0":161,"x1":798,"y0":1398,"y1":1423},"font_size":0.0,"text":"https://github.com/togethercomputer/RedPajama-Data."}],"source":"layout det","text":"T. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL https://github.com/togethercomputer/RedPajama-Data."},{"bbox":{"x0":133,"x1":1059,"y0":1435,"y1":1524},"conf":0.9553,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1442,"y1":1466},"font_size":0.0,"text":"DeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,"},{"bbox":{"x0":161,"x1":1049,"y0":1468,"y1":1493},"font_size":0.0,"text":"abs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485"},{"bbox":{"x0":161,"x1":411,"y0":1495,"y1":1517},"font_size":0.0,"text":"50/arXiv.2401.02954."}],"source":"layout det","text":"DeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,abs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485 50/arXiv.2401.02954."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":144,"x1":1062,"y0":910,"y1":1162},"conf":0.9727,"label":"Text","label_id":1},{"bbox":{"x0":141,"x1":1061,"y0":217,"y1":468},"conf":0.9631,"label":"Text","label_id":1},{"bbox":{"x0":130,"x1":1059,"y0":477,"y1":567},"conf":0.9565,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":1435,"y1":1524},"conf":0.9553,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1059,"y0":1266,"y1":1357},"conf":0.9552,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":1170,"y1":1261},"conf":0.9526,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1059,"y0":575,"y1":666},"conf":0.9519,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":742,"y1":834},"conf":0.9498,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":672,"y1":737},"conf":0.9398,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1364,"y1":1429},"conf":0.9368,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":840,"y1":902},"conf":0.9365,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":277,"y0":165,"y1":200},"conf":0.8811,"label":"Title","label_id":0},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8018,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1577],[580,1577]],"score":0.7097},{"poly":[[161,1495],[411,1495],[411,1517],[161,1517]],"score":0.8962},{"poly":[[161,1468],[1049,1468],[1049,1493],[161,1493]],"score":0.8187},{"poly":[[140,1442],[1049,1442],[1049,1466],[140,1466]],"score":0.7838},{"poly":[[161,1398],[798,1398],[798,1423],[161,1423]],"score":0.7595},{"poly":[[137,1366],[1051,1368],[1051,1398],[136,1396]],"score":0.734},{"poly":[[159,1328],[388,1326],[388,1351],[160,1353]],"score":0.8334},{"poly":[[158,1300],[1047,1300],[1047,1324],[158,1324]],"score":0.7722},{"poly":[[140,1274],[1049,1274],[1049,1298],[140,1298]],"score":0.7567},{"poly":[[163,1230],[1019,1230],[1019,1254],[163,1254]],"score":0.7401},{"poly":[[159,1203],[1051,1202],[1051,1226],[160,1228]],"score":0.8079},{"poly":[[138,1172],[1051,1175],[1051,1205],[138,1202]],"score":0.7191},{"poly":[[161,1131],[619,1131],[619,1156],[161,1156]],"score":0.7012},{"poly":[[158,1102],[1053,1103],[1053,1133],[158,1131]],"score":0.7196},{"poly":[[161,1079],[1049,1079],[1049,1103],[161,1103]],"score":0.8004},{"poly":[[158,1049],[1049,1053],[1049,1077],[158,1074]],"score":0.7555},{"poly":[[161,1024],[1051,1024],[1051,1049],[161,1049]],"score":0.7518},{"poly":[[161,996],[1049,996],[1049,1021],[161,1021]],"score":0.7742},{"poly":[[161,970],[1051,970],[1051,995],[161,995]],"score":0.756},{"poly":[[161,942],[1049,942],[1049,967],[161,967]],"score":0.7349},{"poly":[[140,916],[1049,916],[1049,938],[140,938]],"score":0.831},{"poly":[[159,874],[383,872],[383,896],[160,898]],"score":0.798},{"poly":[[140,847],[1049,847],[1049,870],[140,870]],"score":0.8968},{"poly":[[161,805],[711,805],[711,828],[161,828]],"score":0.9128},{"poly":[[161,777],[1051,777],[1051,802],[161,802]],"score":0.7551},{"poly":[[140,749],[1051,749],[1051,772],[140,772]],"score":0.8829},{"poly":[[161,707],[691,707],[691,730],[161,730]],"score":0.9302},{"poly":[[136,679],[1047,679],[1047,703],[136,703]],"score":0.7729},{"poly":[[159,637],[384,633],[385,658],[160,661]],"score":0.7866},{"poly":[[158,605],[1053,607],[1053,637],[158,635]],"score":0.7485},{"poly":[[140,581],[1051,581],[1051,605],[140,605]],"score":0.7592},{"poly":[[159,534],[215,539],[213,567],[157,562]],"score":0.7764},{"poly":[[160,512],[1049,512],[1049,537],[160,537]],"score":0.8057},{"poly":[[136,482],[1049,482],[1049,507],[136,507]],"score":0.7528},{"poly":[[161,442],[608,442],[608,465],[161,465]],"score":0.8446},{"poly":[[160,412],[1051,414],[1051,439],[159,437]],"score":0.8113},{"poly":[[158,382],[1051,384],[1051,414],[158,412]],"score":0.7047},{"poly":[[161,360],[1051,360],[1051,384],[161,384]],"score":0.7438},{"poly":[[160,330],[1051,333],[1051,358],[159,354]],"score":0.7494},{"poly":[[158,302],[1053,303],[1053,333],[158,332]],"score":0.6738},{"poly":[[161,279],[1051,279],[1051,303],[161,303]],"score":0.7796},{"poly":[[158,249],[1051,251],[1051,275],[158,274]],"score":0.7993},{"poly":[[138,223],[1049,223],[1049,247],[138,247]],"score":0.7317},{"poly":[[139,170],[271,174],[271,198],[138,194]],"score":0.8777}],"page_no":22,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8057,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"24"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":1059,"y0":165,"y1":282},"conf":0.9601,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":174,"y1":198},"font_size":0.0,"text":"Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model"},{"bbox":{"x0":162,"x1":1048,"y0":203,"y1":225},"font_size":10.0,"text":"pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting"},{"bbox":{"x0":161,"x1":1051,"y0":228,"y1":253},"font_size":0.0,"text":"of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320â€“335,"},{"bbox":{"x0":157,"x1":215,"y0":248,"y1":281},"font_size":0.0,"text":"2022."}],"source":"layout det","text":"Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320â€“335,2022."},{"bbox":{"x0":132,"x1":1061,"y0":292,"y1":464},"conf":0.9735,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1053,"y0":295,"y1":328},"font_size":0.0,"text":"L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. PAL: program-"},{"bbox":{"x0":158,"x1":1051,"y0":323,"y1":354},"font_size":0.0,"text":"aided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and"},{"bbox":{"x0":160,"x1":1047,"y0":354,"y1":379},"font_size":0.0,"text":"J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July"},{"bbox":{"x0":163,"x1":1047,"y0":382,"y1":405},"font_size":0.0,"text":"2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,"},{"bbox":{"x0":159,"x1":1053,"y0":407,"y1":435},"font_size":0.0,"text":"pages 10764â€“10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f."},{"bbox":{"x0":160,"x1":222,"y0":437,"y1":461},"font_size":0.0,"text":"html."}],"source":"layout det","text":"L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. PAL: programaided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,pages 10764â€“10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html."},{"bbox":{"x0":132,"x1":1060,"y0":473,"y1":590},"conf":0.9616,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":481,"y1":505},"font_size":0.0,"text":"Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A tool-"},{"bbox":{"x0":161,"x1":1051,"y0":509,"y1":533},"font_size":0.0,"text":"integrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023."},{"bbox":{"x0":161,"x1":1047,"y0":533,"y1":556},"font_size":0.0,"text":"doi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745"},{"bbox":{"x0":160,"x1":184,"y0":561,"y1":588},"font_size":0.0,"text":"2."}],"source":"layout det","text":"Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A toolintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.doi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745 2."},{"bbox":{"x0":132,"x1":1060,"y0":599,"y1":692},"conf":0.9624,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":607,"y1":632},"font_size":0.0,"text":"D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,"},{"bbox":{"x0":158,"x1":1051,"y0":631,"y1":663},"font_size":0.0,"text":"Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming"},{"bbox":{"x0":158,"x1":498,"y0":661,"y1":684},"font_size":0.0,"text":"â€“ the rise of code intelligence, 2024."}],"source":"layout det","text":"D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programmingâ€“ the rise of code intelligence, 2024."},{"bbox":{"x0":132,"x1":1058,"y0":698,"y1":765},"conf":0.9457,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":700,"y1":735},"font_size":10.0,"text":"D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring"},{"bbox":{"x0":161,"x1":955,"y0":735,"y1":758},"font_size":10.0,"text":"massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020."}],"source":"layout det","text":"D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020."},{"bbox":{"x0":132,"x1":1059,"y0":772,"y1":860},"conf":0.9667,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":779,"y1":803},"font_size":0.0,"text":"D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-"},{"bbox":{"x0":161,"x1":1047,"y0":807,"y1":830},"font_size":0.0,"text":"suring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874"},{"bbox":{"x0":160,"x1":214,"y0":831,"y1":858},"font_size":0.0,"text":"2021."}],"source":"layout det","text":"D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 2021."},{"bbox":{"x0":130,"x1":1058,"y0":869,"y1":934},"conf":0.9489,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":877,"y1":902},"font_size":10.0,"text":"High-fyer. Hai-llm: é«˜æ•ˆä¸”è½»é‡çš„å¤§æ¨¡åž‹è®­ç»ƒå·¥å…·, 2023. URL https://www.high-flyer.cl"},{"bbox":{"x0":161,"x1":385,"y0":905,"y1":930},"font_size":10.0,"text":"n/en/blog/hai-llm."}],"source":"layout det","text":"High-fyer. Hai-llm: é«˜æ•ˆä¸”è½»é‡çš„å¤§æ¨¡åž‹è®­ç»ƒå·¥å…·, 2023. URL https://www.high-flyer.cl n/en/blog/hai-llm."},{"bbox":{"x0":132,"x1":948,"y0":941,"y1":981},"conf":0.9065,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":936,"y0":951,"y1":974},"font_size":10.0,"text":"Infection AI. Infection-2, 2023. URL https://inflection.ai/inflection-2.ll"}],"source":"layout det","text":"Infection AI. Infection-2, 2023. URL https://inflection.ai/inflection-2.ll"},{"bbox":{"x0":133,"x1":1056,"y0":988,"y1":1079},"conf":0.958,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":993,"y1":1019},"font_size":10.0,"text":"A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,"},{"bbox":{"x0":161,"x1":1049,"y0":1023,"y1":1047},"font_size":10.0,"text":"sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint"},{"bbox":{"x0":159,"x1":386,"y0":1047,"y1":1074},"font_size":10.0,"text":"arXiv:2210.12283, 2022."}],"source":"layout det","text":"A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022."},{"bbox":{"x0":132,"x1":1059,"y0":1087,"y1":1152},"conf":0.9469,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1093,"y1":1117},"font_size":10.0,"text":"A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,"},{"bbox":{"x0":160,"x1":1033,"y0":1121,"y1":1145},"font_size":10.0,"text":"G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023."}],"source":"layout det","text":"A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023."},{"bbox":{"x0":132,"x1":1059,"y0":1158,"y1":1225},"conf":0.9511,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1165,"y1":1189},"font_size":10.0,"text":"A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. JÃ©gou, and T. Mikolov. Fasttext. zip: Compress-"},{"bbox":{"x0":161,"x1":819,"y0":1195,"y1":1219},"font_size":10.0,"text":"ing text classifcation models. arXiv preprint arXiv:1612.03651, 2016.i"}],"source":"layout det","text":"A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. JÃ©gou, and T. Mikolov. Fasttext. zip: Compressing text classifcation models. arXiv preprint arXiv:1612.03651, 2016.i"},{"bbox":{"x0":133,"x1":1060,"y0":1230,"y1":1324},"conf":0.9641,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1238,"y1":1261},"font_size":0.0,"text":"W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica."},{"bbox":{"x0":161,"x1":1049,"y0":1267,"y1":1291},"font_size":0.0,"text":"Effcient memory management for large language model serving with pagedattention. Ini"},{"bbox":{"x0":163,"x1":1030,"y0":1293,"y1":1317},"font_size":0.0,"text":"Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023."}],"source":"layout det","text":"W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.Effcient memory management for large language model serving with pagedattention. Ini Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023."},{"bbox":{"x0":132,"x1":1060,"y0":1331,"y1":1420},"conf":0.9596,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1338,"y1":1361},"font_size":10.0,"text":"Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative"},{"bbox":{"x0":163,"x1":1051,"y0":1365,"y1":1389},"font_size":10.0,"text":"decoding. In International Conference on Machine Learning, pages 19274â€“19286. PMLR,"},{"bbox":{"x0":160,"x1":216,"y0":1389,"y1":1416},"font_size":10.0,"text":"2023."}],"source":"layout det","text":"Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274â€“19286. PMLR,2023."},{"bbox":{"x0":133,"x1":1061,"y0":1429,"y1":1524},"conf":0.9558,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1437,"y1":1459},"font_size":10.0,"text":"A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,"},{"bbox":{"x0":158,"x1":1051,"y0":1459,"y1":1491},"font_size":10.0,"text":"C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with"},{"bbox":{"x0":163,"x1":1051,"y0":1491,"y1":1516},"font_size":10.0,"text":"language models. Advances in Neural Information Processing Systems, 35:3843â€“3857, 2022a."}],"source":"layout det","text":"A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843â€“3857, 2022a."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1061,"y0":292,"y1":464},"conf":0.9735,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":772,"y1":860},"conf":0.9667,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":1230,"y1":1324},"conf":0.9641,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":599,"y1":692},"conf":0.9624,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":473,"y1":590},"conf":0.9616,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":165,"y1":282},"conf":0.9601,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":1331,"y1":1420},"conf":0.9596,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":988,"y1":1079},"conf":0.958,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1061,"y0":1429,"y1":1524},"conf":0.9558,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1158,"y1":1225},"conf":0.9511,"label":"Text","label_id":1},{"bbox":{"x0":130,"x1":1058,"y0":869,"y1":934},"conf":0.9489,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1087,"y1":1152},"conf":0.9469,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":698,"y1":765},"conf":0.9457,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":948,"y0":941,"y1":981},"conf":0.9065,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8057,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.9272},{"poly":[[163,1491],[1051,1491],[1051,1516],[163,1516]],"score":0.7701},{"poly":[[158,1459],[1051,1461],[1051,1491],[158,1489]],"score":0.7046},{"poly":[[140,1437],[1051,1437],[1051,1459],[140,1459]],"score":0.907},{"poly":[[160,1389],[216,1389],[216,1416],[160,1416]],"score":0.8064},{"poly":[[163,1365],[1051,1365],[1051,1389],[163,1389]],"score":0.7392},{"poly":[[140,1338],[1049,1338],[1049,1361],[140,1361]],"score":0.8567},{"poly":[[163,1293],[1030,1293],[1030,1317],[163,1317]],"score":0.7593},{"poly":[[161,1267],[1049,1267],[1049,1291],[161,1291]],"score":0.7516},{"poly":[[140,1238],[1051,1238],[1051,1261],[140,1261]],"score":0.8484},{"poly":[[161,1195],[819,1195],[819,1219],[161,1219]],"score":0.7309},{"poly":[[140,1165],[1049,1165],[1049,1189],[140,1189]],"score":0.746},{"poly":[[160,1121],[1033,1121],[1033,1145],[160,1145]],"score":0.7717},{"poly":[[140,1093],[1051,1093],[1051,1117],[140,1117]],"score":0.8002},{"poly":[[159,1049],[386,1047],[386,1072],[160,1074]],"score":0.8116},{"poly":[[161,1023],[1049,1023],[1049,1047],[161,1047]],"score":0.8128},{"poly":[[138,995],[1051,993],[1051,1017],[138,1019]],"score":0.8031},{"poly":[[140,951],[936,951],[936,974],[140,974]],"score":0.8525},{"poly":[[161,905],[385,905],[385,930],[161,930]],"score":0.8343},{"poly":[[140,877],[1049,877],[1049,902],[140,902]],"score":0.7952},{"poly":[[160,831],[214,831],[214,858],[160,858]],"score":0.8445},{"poly":[[161,807],[1047,807],[1047,830],[161,830]],"score":0.9165},{"poly":[[140,779],[1051,779],[1051,803],[140,803]],"score":0.7967},{"poly":[[161,735],[955,735],[955,758],[161,758]],"score":0.9123},{"poly":[[137,700],[1051,705],[1051,735],[136,730]],"score":0.6325},{"poly":[[158,661],[498,661],[498,684],[158,684]],"score":0.8576},{"poly":[[158,631],[1051,633],[1051,663],[158,661]],"score":0.7066},{"poly":[[140,607],[1051,607],[1051,632],[140,632]],"score":0.7682},{"poly":[[160,561],[184,561],[184,588],[160,588]],"score":0.8481},{"poly":[[161,533],[1047,533],[1047,556],[161,556]],"score":0.7813},{"poly":[[161,509],[1051,509],[1051,533],[161,533]],"score":0.7532},{"poly":[[140,481],[1051,481],[1051,505],[140,505]],"score":0.7199},{"poly":[[160,437],[222,437],[222,461],[160,461]],"score":0.8183},{"poly":[[159,410],[1053,407],[1053,432],[160,435]],"score":0.7254},{"poly":[[163,382],[1047,382],[1047,405],[163,405]],"score":0.8611},{"poly":[[160,354],[1047,354],[1047,379],[160,379]],"score":0.7346},{"poly":[[158,325],[1051,323],[1051,353],[158,354]],"score":0.6925},{"poly":[[135,295],[1053,298],[1053,328],[135,324]],"score":0.7309},{"poly":[[159,248],[215,253],[213,281],[157,276]],"score":0.7843},{"poly":[[161,228],[1051,228],[1051,253],[161,253]],"score":0.7917},{"poly":[[138,174],[1049,174],[1049,198],[138,198]],"score":0.7842}],"page_no":23,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8076,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1577},"font_size":0.0,"text":"25"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":1060,"y0":167,"y1":391},"conf":0.9742,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":174,"y1":198},"font_size":0.0,"text":"A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone,"},{"bbox":{"x0":158,"x1":1051,"y0":195,"y1":230},"font_size":0.0,"text":"C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving"},{"bbox":{"x0":161,"x1":1051,"y0":228,"y1":253},"font_size":0.0,"text":"quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,"},{"bbox":{"x0":161,"x1":1049,"y0":256,"y1":279},"font_size":0.0,"text":"D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems"},{"bbox":{"x0":163,"x1":1049,"y0":282,"y1":305},"font_size":0.0,"text":"35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New"},{"bbox":{"x0":161,"x1":1053,"y0":309,"y1":333},"font_size":0.0,"text":"Orleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips."},{"bbox":{"x0":163,"x1":1047,"y0":337,"y1":361},"font_size":0.0,"text":"cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr"},{"bbox":{"x0":161,"x1":409,"y0":365,"y1":388},"font_size":0.0,"text":"act-Conference.html."}],"source":"layout det","text":"A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone,C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr act-Conference.html."},{"bbox":{"x0":134,"x1":1058,"y0":402,"y1":466},"conf":0.8371,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1053,"y0":403,"y1":435},"font_size":0.0,"text":"H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,"},{"bbox":{"x0":160,"x1":1037,"y0":437,"y1":460},"font_size":0.0,"text":"I. Sutskever, and K. Cobbe. Letâ€™s verify step by step. arXiv preprint arXiv:2305.20050, 2023."}],"source":"layout det","text":"H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,I. Sutskever, and K. Cobbe. Letâ€™s verify step by step. arXiv preprint arXiv:2305.20050, 2023."},{"bbox":{"x0":130,"x1":1057,"y0":474,"y1":538},"conf":0.8902,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":481,"y1":507},"font_size":0.0,"text":"I. Loshchilov and F. Hutter.Decoupled weight decay regularization.arXiv preprint"},{"bbox":{"x0":159,"x1":386,"y0":507,"y1":533},"font_size":0.0,"text":"arXiv:1711.05101, 2017."}],"source":"layout det","text":"I. Loshchilov and F. Hutter.Decoupled weight decay regularization.arXiv preprint arXiv:1711.05101, 2017."},{"bbox":{"x0":133,"x1":1059,"y0":546,"y1":637},"conf":0.9627,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":547,"y1":581},"font_size":0.0,"text":"H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang."},{"bbox":{"x0":161,"x1":1049,"y0":581,"y1":605},"font_size":0.0,"text":"Wizardmath: Empowering mathematical reasoning for large language models via reinforced"},{"bbox":{"x0":161,"x1":665,"y0":607,"y1":632},"font_size":0.0,"text":"evol-instruct. arXiv preprint arXiv:2308.09583, 2023."}],"source":"layout det","text":"H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023."},{"bbox":{"x0":134,"x1":1061,"y0":645,"y1":845},"conf":0.9779,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":651,"y1":675},"font_size":0.0,"text":"S. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sab-"},{"bbox":{"x0":158,"x1":1053,"y0":674,"y1":707},"font_size":0.0,"text":"harwal, P. Clark, and A. Kalyan. LILA: A unifed benchmark for mathematical reasoning.i"},{"bbox":{"x0":161,"x1":1049,"y0":707,"y1":730},"font_size":0.0,"text":"In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on"},{"bbox":{"x0":163,"x1":1049,"y0":733,"y1":758},"font_size":0.0,"text":"Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab"},{"bbox":{"x0":159,"x1":1053,"y0":756,"y1":788},"font_size":0.0,"text":"Emirates, December 7-11, 2022, pages 5807â€“5832. Association for Computational Linguistics,"},{"bbox":{"x0":158,"x1":1051,"y0":782,"y1":814},"font_size":0.0,"text":"2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/"},{"bbox":{"x0":160,"x1":406,"y0":816,"y1":838},"font_size":0.0,"text":"2022.emnlp-main.392."}],"source":"layout det","text":"S. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sabharwal, P. Clark, and A. Kalyan. LILA: A unifed benchmark for mathematical reasoning.i In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5807â€“5832. Association for Computational Linguistics,2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/2022.emnlp-main.392."},{"bbox":{"x0":133,"x1":1059,"y0":853,"y1":969},"conf":0.7287,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":854,"y1":888},"font_size":0.0,"text":"X. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,"},{"bbox":{"x0":158,"x1":1051,"y0":882,"y1":914},"font_size":0.0,"text":"C. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,"},{"bbox":{"x0":158,"x1":1051,"y0":909,"y1":940},"font_size":0.0,"text":"abs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485"},{"bbox":{"x0":163,"x1":408,"y0":944,"y1":961},"font_size":0.0,"text":"50/arXiv.2312.00738."}],"source":"layout det","text":"X. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,C. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,abs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485 50/arXiv.2312.00738."},{"bbox":{"x0":132,"x1":828,"y0":979,"y1":1017},"conf":0.8531,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":821,"y0":986,"y1":1010},"font_size":0.0,"text":"OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023."}],"source":"layout det","text":"OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023."},{"bbox":{"x0":132,"x1":1058,"y0":1025,"y1":1115},"conf":0.9209,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":1028,"y1":1058},"font_size":0.0,"text":"L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,"},{"bbox":{"x0":161,"x1":1051,"y0":1058,"y1":1082},"font_size":0.0,"text":"K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback."},{"bbox":{"x0":161,"x1":886,"y0":1082,"y1":1109},"font_size":0.0,"text":"Advances in Neural Information Processing Systems, 35:27730â€“27744, 2022."}],"source":"layout det","text":"L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.Advances in Neural Information Processing Systems, 35:27730â€“27744, 2022."},{"bbox":{"x0":133,"x1":1059,"y0":1123,"y1":1214},"conf":0.962,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1047,"y0":1130,"y1":1154},"font_size":0.0,"text":"K. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality"},{"bbox":{"x0":161,"x1":1049,"y0":1158,"y1":1181},"font_size":0.0,"text":"mathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL"},{"bbox":{"x0":161,"x1":686,"y0":1184,"y1":1209},"font_size":0.0,"text":"https://doi.org/10.48550/arXiv.2310.06786."}],"source":"layout det","text":"K. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL https://doi.org/10.48550/arXiv.2310.06786."},{"bbox":{"x0":134,"x1":1060,"y0":1223,"y1":1367},"conf":0.9726,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1230,"y1":1254},"font_size":0.0,"text":"L. C. Paulson. Three years of experience with sledgehammer, a practical link between auto-"},{"bbox":{"x0":161,"x1":1047,"y0":1256,"y1":1279},"font_size":0.0,"text":"matic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,"},{"bbox":{"x0":161,"x1":1051,"y0":1284,"y1":1309},"font_size":0.0,"text":"Proceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010,"},{"bbox":{"x0":158,"x1":1055,"y0":1307,"y1":1338},"font_size":0.0,"text":"Edinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1â€“10."},{"bbox":{"x0":163,"x1":961,"y0":1337,"y1":1361},"font_size":0.0,"text":"EasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd."}],"source":"layout det","text":"L. C. Paulson. Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,Proceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010,Edinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1â€“10.EasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd."},{"bbox":{"x0":131,"x1":1059,"y0":1376,"y1":1440},"conf":0.9441,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1377,"y1":1410},"font_size":0.0,"text":"S. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,"},{"bbox":{"x0":161,"x1":838,"y0":1410,"y1":1433},"font_size":0.0,"text":"abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393."}],"source":"layout det","text":"S. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393."},{"bbox":{"x0":132,"x1":1056,"y0":1448,"y1":1512},"conf":0.9387,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1454,"y1":1479},"font_size":0.0,"text":"R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference"},{"bbox":{"x0":159,"x1":826,"y0":1475,"y1":1510},"font_size":0.0,"text":"optimization: Your language model is secretly a reward model. 2023."}],"source":"layout det","text":"R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. 2023."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1061,"y0":645,"y1":845},"conf":0.9779,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":167,"y1":391},"conf":0.9742,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":1223,"y1":1367},"conf":0.9726,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":546,"y1":637},"conf":0.9627,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":1123,"y1":1214},"conf":0.962,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1059,"y0":1376,"y1":1440},"conf":0.9441,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1056,"y0":1448,"y1":1512},"conf":0.9387,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1025,"y1":1115},"conf":0.9209,"label":"Text","label_id":1},{"bbox":{"x0":130,"x1":1057,"y0":474,"y1":538},"conf":0.8902,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":828,"y0":979,"y1":1017},"conf":0.8531,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1058,"y0":402,"y1":466},"conf":0.8371,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8076,"label":"Abandon","label_id":2},{"bbox":{"x0":133,"x1":1059,"y0":853,"y1":969},"conf":0.7287,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1577],[580,1577]],"score":0.8015},{"poly":[[159,1481],[826,1475],[826,1505],[160,1510]],"score":0.6576},{"poly":[[140,1454],[1049,1454],[1049,1479],[140,1479]],"score":0.7542},{"poly":[[161,1410],[838,1410],[838,1433],[161,1433]],"score":0.9253},{"poly":[[137,1377],[1051,1381],[1051,1410],[136,1407]],"score":0.6807},{"poly":[[163,1337],[961,1337],[961,1361],[163,1361]],"score":0.7792},{"poly":[[158,1307],[1055,1309],[1054,1338],[158,1337]],"score":0.6844},{"poly":[[161,1284],[1051,1284],[1051,1309],[161,1309]],"score":0.7865},{"poly":[[161,1256],[1047,1256],[1047,1279],[161,1279]],"score":0.817},{"poly":[[140,1230],[1051,1230],[1051,1254],[140,1254]],"score":0.7664},{"poly":[[161,1184],[686,1184],[686,1209],[161,1209]],"score":0.7543},{"poly":[[161,1158],[1049,1158],[1049,1181],[161,1181]],"score":0.946},{"poly":[[140,1130],[1047,1130],[1047,1154],[140,1154]],"score":0.7662},{"poly":[[161,1084],[886,1082],[886,1107],[161,1109]],"score":0.814},{"poly":[[161,1058],[1051,1058],[1051,1082],[161,1082]],"score":0.7557},{"poly":[[138,1028],[1053,1028],[1053,1058],[138,1058]],"score":0.665},{"poly":[[140,986],[821,986],[821,1010],[140,1010]],"score":0.7561},{"poly":[[163,944],[408,944],[408,961],[163,961]],"score":0.9907},{"poly":[[158,910],[1051,909],[1051,938],[158,940]],"score":0.7191},{"poly":[[158,884],[1051,882],[1051,912],[158,914]],"score":0.7092},{"poly":[[137,854],[1051,858],[1051,888],[136,884]],"score":0.6972},{"poly":[[160,816],[406,816],[406,838],[160,838]],"score":0.9023},{"poly":[[158,782],[1051,784],[1051,814],[158,812]],"score":0.6893},{"poly":[[160,756],[1053,758],[1053,788],[159,786]],"score":0.6602},{"poly":[[163,733],[1049,733],[1049,758],[163,758]],"score":0.7326},{"poly":[[161,707],[1049,707],[1049,730],[161,730]],"score":0.8808},{"poly":[[158,674],[1053,677],[1053,707],[158,703]],"score":0.7253},{"poly":[[138,651],[1051,651],[1051,675],[138,675]],"score":0.7391},{"poly":[[161,607],[665,607],[665,632],[161,632]],"score":0.7478},{"poly":[[161,581],[1049,581],[1049,605],[161,605]],"score":0.7695},{"poly":[[135,547],[1051,551],[1051,581],[135,577]],"score":0.7159},{"poly":[[159,509],[386,507],[386,531],[160,533]],"score":0.7847},{"poly":[[897,482],[1049,482],[1049,507],[897,507]],"score":0.7687},{"poly":[[138,481],[883,481],[883,505],[138,505]],"score":0.7828},{"poly":[[160,437],[1037,437],[1037,460],[160,460]],"score":0.9021},{"poly":[[137,403],[1053,405],[1053,435],[136,433]],"score":0.7267},{"poly":[[161,365],[409,365],[409,388],[161,388]],"score":0.8481},{"poly":[[163,337],[1047,337],[1047,361],[163,361]],"score":0.7907},{"poly":[[161,309],[1053,309],[1053,333],[161,333]],"score":0.7452},{"poly":[[163,282],[1049,282],[1049,305],[163,305]],"score":0.8894},{"poly":[[161,256],[1049,256],[1049,279],[161,279]],"score":0.8399},{"poly":[[161,228],[1051,228],[1051,253],[161,253]],"score":0.7464},{"poly":[[158,195],[1051,200],[1051,230],[158,224]],"score":0.638},{"poly":[[140,174],[1051,174],[1051,198],[140,198]],"score":0.8053}],"page_no":24,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7985,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"26"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":1055,"y0":165,"y1":228},"conf":0.9468,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":133,"x1":1049,"y0":170,"y1":204},"font_size":0.0,"text":"J. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app"},{"bbox":{"x0":161,"x1":266,"y0":203,"y1":223},"font_size":0.0,"text":"rox.html."}],"source":"layout det","text":"J. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app rox.html."},{"bbox":{"x0":131,"x1":1056,"y0":239,"y1":303},"conf":0.8285,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":246,"y1":270},"font_size":0.0,"text":"J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous"},{"bbox":{"x0":161,"x1":998,"y0":274,"y1":298},"font_size":0.0,"text":"control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015."}],"source":"layout det","text":"J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015."},{"bbox":{"x0":131,"x1":1056,"y0":311,"y1":375},"conf":0.9447,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":318,"y1":342},"font_size":0.0,"text":"J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization"},{"bbox":{"x0":161,"x1":645,"y0":346,"y1":370},"font_size":0.0,"text":"algorithms. arXiv preprint arXiv:1707.06347, 2017."}],"source":"layout det","text":"J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."},{"bbox":{"x0":133,"x1":1059,"y0":383,"y1":528},"conf":0.9719,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":386,"y1":414},"font_size":0.0,"text":"F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,"},{"bbox":{"x0":161,"x1":1051,"y0":416,"y1":440},"font_size":0.0,"text":"D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners."},{"bbox":{"x0":159,"x1":1049,"y0":442,"y1":468},"font_size":0.0,"text":"In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,"},{"bbox":{"x0":161,"x1":1049,"y0":472,"y1":495},"font_size":0.0,"text":"Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id="},{"bbox":{"x0":159,"x1":309,"y0":494,"y1":523},"font_size":0.0,"text":"fR3wGCk-IXp."}],"source":"layout det","text":"F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=fR3wGCk-IXp."},{"bbox":{"x0":131,"x1":1058,"y0":535,"y1":599},"conf":0.9459,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":537,"y1":568},"font_size":0.0,"text":"F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for"},{"bbox":{"x0":160,"x1":714,"y0":570,"y1":593},"font_size":0.0,"text":"human alignment. arXiv preprint arXiv:2306.17492, 2023."}],"source":"layout det","text":"F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023."},{"bbox":{"x0":132,"x1":1059,"y0":607,"y1":699},"conf":0.9575,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":614,"y1":639},"font_size":0.0,"text":"M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,"},{"bbox":{"x0":161,"x1":1047,"y0":639,"y1":663},"font_size":0.0,"text":"E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve"},{"bbox":{"x0":161,"x1":590,"y0":668,"y1":693},"font_size":0.0,"text":"them. arXiv preprint arXiv:2210.09261, 2022."}],"source":"layout det","text":"M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022."},{"bbox":{"x0":131,"x1":1058,"y0":706,"y1":770},"conf":0.9422,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":710,"y1":737},"font_size":0.0,"text":"T. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro"},{"bbox":{"x0":161,"x1":599,"y0":740,"y1":765},"font_size":0.0,"text":"soft.com/ai-anthology/terence-tao/."}],"source":"layout det","text":"T. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro soft.com/ai-anthology/terence-tao/."},{"bbox":{"x0":146,"x1":1062,"y0":778,"y1":1082},"conf":0.9693,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":782,"y1":809},"font_size":0.0,"text":"H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,"},{"bbox":{"x0":158,"x1":1051,"y0":810,"y1":837},"font_size":0.0,"text":"P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,"},{"bbox":{"x0":160,"x1":1051,"y0":838,"y1":863},"font_size":0.0,"text":"J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,"},{"bbox":{"x0":159,"x1":1051,"y0":863,"y1":891},"font_size":0.0,"text":"R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,"},{"bbox":{"x0":161,"x1":1049,"y0":891,"y1":916},"font_size":0.0,"text":"M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,"},{"bbox":{"x0":161,"x1":1051,"y0":921,"y1":944},"font_size":0.0,"text":"I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M."},{"bbox":{"x0":161,"x1":1051,"y0":947,"y1":972},"font_size":0.0,"text":"Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,"},{"bbox":{"x0":160,"x1":1049,"y0":974,"y1":998},"font_size":0.0,"text":"I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and"},{"bbox":{"x0":161,"x1":1049,"y0":1002,"y1":1025},"font_size":0.0,"text":"T. Scialom. Llama 2: Open foundation and fne-tuned chat models. CoRR, abs/2307.09288,i"},{"bbox":{"x0":159,"x1":1055,"y0":1026,"y1":1053},"font_size":0.0,"text":"2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307."},{"bbox":{"x0":163,"x1":234,"y0":1058,"y1":1077},"font_size":0.0,"text":"09288."}],"source":"layout det","text":"H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fne-tuned chat models. CoRR, abs/2307.09288,i 2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288."},{"bbox":{"x0":133,"x1":1057,"y0":1092,"y1":1156},"conf":0.3506,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1100,"y1":1124},"font_size":0.0,"text":"T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human"},{"bbox":{"x0":161,"x1":638,"y0":1126,"y1":1151},"font_size":0.0,"text":"demonstrations. Nature, 625(7995):476â€“482, 2024."}],"source":"layout det","text":"T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476â€“482, 2024."},{"bbox":{"x0":131,"x1":1056,"y0":1164,"y1":1230},"conf":0.9581,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1172,"y1":1196},"font_size":0.0,"text":"P. Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models"},{"bbox":{"x0":160,"x1":858,"y0":1200,"y1":1223},"font_size":0.0,"text":"better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a."}],"source":"layout det","text":"P. Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a."},{"bbox":{"x0":132,"x1":1057,"y0":1237,"y1":1302},"conf":0.9543,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1047,"y0":1242,"y1":1268},"font_size":0.0,"text":"P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify"},{"bbox":{"x0":161,"x1":1040,"y0":1272,"y1":1296},"font_size":0.0,"text":"and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b."}],"source":"layout det","text":"P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b."},{"bbox":{"x0":132,"x1":1059,"y0":1309,"y1":1399},"conf":0.945,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1316,"y1":1338},"font_size":0.0,"text":"Z. Wang, R. Xia, and P. Liu. Generative AI for math: Part I - mathpile: A billion-token-scale"},{"bbox":{"x0":159,"x1":1051,"y0":1338,"y1":1370},"font_size":0.0,"text":"pretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120."},{"bbox":{"x0":163,"x1":734,"y0":1370,"y1":1395},"font_size":0.0,"text":"URL https://doi.org/10.48550/arXiv.2312.17120."}],"source":"layout det","text":"Z. Wang, R. Xia, and P. Liu. Generative AI for math: Part I - mathpile: A billion-token-scale pretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120.URL https://doi.org/10.48550/arXiv.2312.17120."},{"bbox":{"x0":132,"x1":1060,"y0":1407,"y1":1524},"conf":0.9555,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1414,"y1":1438},"font_size":0.0,"text":"J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou."},{"bbox":{"x0":161,"x1":1049,"y0":1442,"y1":1466},"font_size":0.0,"text":"Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022."},{"bbox":{"x0":163,"x1":1049,"y0":1468,"y1":1493},"font_size":0.0,"text":"URL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf"},{"bbox":{"x0":159,"x1":698,"y0":1493,"y1":1519},"font_size":0.0,"text":"4f15af0f7b31abca4-Abstract-Conference.html."}],"source":"layout det","text":"J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.URL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf 4f15af0f7b31abca4-Abstract-Conference.html."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1059,"y0":383,"y1":528},"conf":0.9719,"label":"Text","label_id":1},{"bbox":{"x0":146,"x1":1062,"y0":778,"y1":1082},"conf":0.9693,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1056,"y0":1164,"y1":1230},"conf":0.9581,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":607,"y1":699},"conf":0.9575,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":1407,"y1":1524},"conf":0.9555,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":1237,"y1":1302},"conf":0.9543,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1055,"y0":165,"y1":228},"conf":0.9468,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":535,"y1":599},"conf":0.9459,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1309,"y1":1399},"conf":0.945,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1056,"y0":311,"y1":375},"conf":0.9447,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":706,"y1":770},"conf":0.9422,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1056,"y0":239,"y1":303},"conf":0.8285,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7985,"label":"Abandon","label_id":2},{"bbox":{"x0":133,"x1":1057,"y0":1093,"y1":1156},"conf":0.7108,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":1092,"y1":1156},"conf":0.3506,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.9341},{"poly":[[160,1493],[698,1495],[698,1519],[159,1517]],"score":0.7889},{"poly":[[163,1468],[1049,1468],[1049,1493],[163,1493]],"score":0.7864},{"poly":[[161,1442],[1049,1442],[1049,1466],[161,1466]],"score":0.8177},{"poly":[[136,1414],[1051,1414],[1051,1438],[136,1438]],"score":0.7818},{"poly":[[163,1370],[734,1370],[734,1395],[163,1395]],"score":0.7439},{"poly":[[159,1342],[1051,1338],[1051,1366],[160,1370]],"score":0.7293},{"poly":[[140,1316],[1049,1316],[1049,1338],[140,1338]],"score":0.9226},{"poly":[[161,1272],[1040,1272],[1040,1296],[161,1296]],"score":0.7823},{"poly":[[136,1242],[1047,1244],[1047,1268],[136,1266]],"score":0.8165},{"poly":[[160,1200],[858,1200],[858,1223],[160,1223]],"score":0.904},{"poly":[[138,1172],[1049,1172],[1049,1196],[138,1196]],"score":0.8132},{"poly":[[161,1126],[638,1126],[638,1151],[161,1151]],"score":0.7595},{"poly":[[140,1100],[1049,1100],[1049,1124],[140,1124]],"score":0.7719},{"poly":[[163,1058],[234,1058],[234,1077],[163,1077]],"score":0.9661},{"poly":[[160,1026],[1055,1028],[1055,1053],[159,1051]],"score":0.813},{"poly":[[161,1002],[1049,1002],[1049,1024],[161,1024]],"score":0.8805},{"poly":[[160,974],[1049,974],[1049,998],[160,998]],"score":0.7653},{"poly":[[161,947],[1051,947],[1051,972],[161,972]],"score":0.7879},{"poly":[[161,921],[1051,921],[1051,944],[161,944]],"score":0.8919},{"poly":[[161,891],[1049,891],[1049,916],[161,916]],"score":0.7435},{"poly":[[160,863],[1051,867],[1051,891],[159,888]],"score":0.7222},{"poly":[[160,838],[1051,838],[1051,863],[160,863]],"score":0.7233},{"poly":[[158,810],[1051,812],[1051,837],[158,835]],"score":0.8208},{"poly":[[136,782],[1051,784],[1051,809],[136,807]],"score":0.7385},{"poly":[[161,740],[599,740],[599,765],[161,765]],"score":0.7355},{"poly":[[138,710],[1049,712],[1049,737],[138,735]],"score":0.8134},{"poly":[[161,668],[590,668],[590,693],[161,693]],"score":0.7791},{"poly":[[161,639],[1047,639],[1047,663],[161,663]],"score":0.7017},{"poly":[[140,614],[1051,614],[1051,639],[140,639]],"score":0.7567},{"poly":[[160,570],[714,570],[714,593],[160,593]],"score":0.9015},{"poly":[[135,537],[1051,539],[1051,568],[135,567]],"score":0.7388},{"poly":[[160,494],[309,498],[308,523],[159,519]],"score":0.787},{"poly":[[161,472],[1049,472],[1049,495],[161,495]],"score":0.9068},{"poly":[[160,442],[1049,444],[1049,468],[159,467]],"score":0.801},{"poly":[[161,416],[1051,416],[1051,440],[161,440]],"score":0.7403},{"poly":[[138,386],[1051,389],[1051,414],[138,410]],"score":0.759},{"poly":[[161,346],[645,346],[645,370],[161,370]],"score":0.7837},{"poly":[[136,318],[1049,318],[1049,342],[136,342]],"score":0.7911},{"poly":[[161,274],[998,274],[998,298],[161,298]],"score":0.7533},{"poly":[[136,246],[1049,246],[1049,270],[136,270]],"score":0.7568},{"poly":[[161,203],[266,203],[266,223],[161,223]],"score":0.9348},{"poly":[[133,170],[1049,174],[1049,204],[133,200]],"score":0.7161}],"page_no":25,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7845,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":581,"x1":608,"y0":1552,"y1":1575},"font_size":0.0,"text":"27"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":1056,"y0":167,"y1":229},"conf":0.9248,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":174,"y1":198},"font_size":0.0,"text":"T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese"},{"bbox":{"x0":159,"x1":502,"y0":198,"y1":225},"font_size":0.0,"text":"elementary school math test?, 2023."}],"source":"layout det","text":"T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023."},{"bbox":{"x0":134,"x1":1060,"y0":239,"y1":386},"conf":0.9663,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":246,"y1":268},"font_size":0.0,"text":"M. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A."},{"bbox":{"x0":161,"x1":1049,"y0":272,"y1":296},"font_size":0.0,"text":"MuÃ±oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International"},{"bbox":{"x0":161,"x1":1049,"y0":300,"y1":325},"font_size":0.0,"text":"Conference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170"},{"bbox":{"x0":158,"x1":1051,"y0":323,"y1":354},"font_size":0.0,"text":"of Lecture Notes in Computer Science, pages 33â€“38. Springer, 2008. doi: 10.1007/978-3-540-7"},{"bbox":{"x0":161,"x1":874,"y0":354,"y1":379},"font_size":0.0,"text":"1067-7\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7."}],"source":"layout det","text":"M. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A.MuÃ±oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International Conference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170 of Lecture Notes in Computer Science, pages 33â€“38. Springer, 2008. doi: 10.1007/978-3-540-7 1067-7\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7."},{"bbox":{"x0":133,"x1":1061,"y0":393,"y1":539},"conf":0.9697,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":400,"y1":425},"font_size":0.0,"text":"H. Xia, T. Ge, P. Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting"},{"bbox":{"x0":158,"x1":1053,"y0":423,"y1":454},"font_size":0.0,"text":"speculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,"},{"bbox":{"x0":161,"x1":1051,"y0":454,"y1":479},"font_size":0.0,"text":"editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909â€“"},{"bbox":{"x0":160,"x1":1051,"y0":479,"y1":509},"font_size":0.0,"text":"3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20"},{"bbox":{"x0":161,"x1":1051,"y0":505,"y1":532},"font_size":0.0,"text":"23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257."}],"source":"layout det","text":"H. Xia, T. Ge, P. Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909â€“3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20 23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257."},{"bbox":{"x0":132,"x1":1059,"y0":546,"y1":637},"conf":0.9622,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":547,"y1":581},"font_size":0.0,"text":"H. Xia, Z. Yang, Q. Dong, P. Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking effciencyi"},{"bbox":{"x0":161,"x1":1049,"y0":581,"y1":605},"font_size":0.0,"text":"in large language model inference: A comprehensive survey of speculative decoding. arXiv"},{"bbox":{"x0":159,"x1":472,"y0":602,"y1":635},"font_size":0.0,"text":"preprint arXiv:2401.07851, 2024."}],"source":"layout det","text":"H. Xia, Z. Yang, Q. Dong, P. Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking effciencyi in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024."},{"bbox":{"x0":132,"x1":1059,"y0":644,"y1":733},"conf":0.9553,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":651,"y1":676},"font_size":0.0,"text":"S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffths, Y. Cao, and K. Narasimhan. Tree of thoughts:i"},{"bbox":{"x0":161,"x1":1051,"y0":679,"y1":703},"font_size":0.0,"text":"Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,"},{"bbox":{"x0":161,"x1":214,"y0":703,"y1":730},"font_size":0.0,"text":"2023."}],"source":"layout det","text":"S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffths, Y. Cao, and K. Narasimhan. Tree of thoughts:i Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,2023."},{"bbox":{"x0":133,"x1":1059,"y0":743,"y1":860},"conf":0.9637,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":751,"y1":775},"font_size":0.0,"text":"L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu."},{"bbox":{"x0":161,"x1":1049,"y0":779,"y1":803},"font_size":0.0,"text":"Metamath: Bootstrap your own mathematical questions for large language models. CoRR,"},{"bbox":{"x0":161,"x1":1047,"y0":805,"y1":830},"font_size":0.0,"text":"abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485"},{"bbox":{"x0":161,"x1":409,"y0":831,"y1":854},"font_size":0.0,"text":"50/arXiv.2309.12284."}],"source":"layout det","text":"L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.Metamath: Bootstrap your own mathematical questions for large language models. CoRR,abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485 50/arXiv.2309.12284."},{"bbox":{"x0":132,"x1":1058,"y0":870,"y1":936},"conf":0.9452,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":874,"y1":905},"font_size":0.0,"text":"Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning"},{"bbox":{"x0":161,"x1":1051,"y0":905,"y1":930},"font_size":0.0,"text":"mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a."}],"source":"layout det","text":"Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a."},{"bbox":{"x0":132,"x1":1058,"y0":944,"y1":1009},"conf":0.945,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":947,"y1":974},"font_size":0.0,"text":"Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align"},{"bbox":{"x0":161,"x1":1051,"y0":977,"y1":1002},"font_size":0.0,"text":"language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b."}],"source":"layout det","text":"Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b."},{"bbox":{"x0":131,"x1":1058,"y0":1015,"y1":1105},"conf":0.9559,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":1016,"y1":1049},"font_size":0.0,"text":"X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building"},{"bbox":{"x0":161,"x1":1051,"y0":1049,"y1":1074},"font_size":0.0,"text":"math generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:"},{"bbox":{"x0":161,"x1":1021,"y0":1074,"y1":1100},"font_size":0.0,"text":"10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653."}],"source":"layout det","text":"X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653."},{"bbox":{"x0":131,"x1":1057,"y0":1114,"y1":1178},"conf":0.944,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1121,"y1":1144},"font_size":0.0,"text":"K. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level"},{"bbox":{"x0":161,"x1":663,"y0":1149,"y1":1172},"font_size":0.0,"text":"mathematics. arXiv preprint arXiv:2109.00110, 2021."}],"source":"layout det","text":"K. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021."},{"bbox":{"x0":132,"x1":1059,"y0":1187,"y1":1279},"conf":0.9412,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1193,"y1":1217},"font_size":0.0,"text":"W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A"},{"bbox":{"x0":161,"x1":1051,"y0":1219,"y1":1244},"font_size":0.0,"text":"human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023."},{"bbox":{"x0":161,"x1":1049,"y0":1247,"y1":1272},"font_size":0.0,"text":"doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364."}],"source":"layout det","text":"W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1061,"y0":393,"y1":539},"conf":0.9697,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":239,"y1":386},"conf":0.9663,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":743,"y1":860},"conf":0.9637,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":546,"y1":637},"conf":0.9622,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":1015,"y1":1105},"conf":0.9559,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":644,"y1":733},"conf":0.9553,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":870,"y1":936},"conf":0.9452,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":944,"y1":1009},"conf":0.945,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":1114,"y1":1178},"conf":0.944,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1187,"y1":1279},"conf":0.9412,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":167,"y1":229},"conf":0.9248,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7845,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[581,1552],[608,1552],[608,1575],[581,1575]],"score":0.8313},{"poly":[[161,1247],[1049,1247],[1049,1272],[161,1272]],"score":0.7648},{"poly":[[161,1219],[1051,1219],[1051,1244],[161,1244]],"score":0.7388},{"poly":[[140,1193],[1049,1193],[1049,1217],[140,1217]],"score":0.816},{"poly":[[161,1149],[663,1149],[663,1172],[161,1172]],"score":0.9484},{"poly":[[140,1121],[1049,1121],[1049,1144],[140,1144]],"score":0.9192},{"poly":[[161,1074],[1021,1075],[1021,1100],[161,1098]],"score":0.8041},{"poly":[[161,1049],[1051,1049],[1051,1074],[161,1074]],"score":0.8121},{"poly":[[135,1016],[1051,1019],[1051,1049],[135,1045]],"score":0.6844},{"poly":[[161,977],[1051,977],[1051,1002],[161,1002]],"score":0.7667},{"poly":[[138,947],[1049,949],[1049,974],[138,972]],"score":0.7875},{"poly":[[161,905],[1051,905],[1051,930],[161,930]],"score":0.7938},{"poly":[[137,874],[1051,875],[1051,905],[136,903]],"score":0.7126},{"poly":[[161,831],[409,831],[409,854],[161,854]],"score":0.8559},{"poly":[[161,805],[1047,805],[1047,830],[161,830]],"score":0.7656},{"poly":[[161,779],[1049,779],[1049,803],[161,803]],"score":0.755},{"poly":[[138,751],[1051,751],[1051,775],[138,775]],"score":0.7853},{"poly":[[161,703],[214,703],[214,730],[161,730]],"score":0.8824},{"poly":[[161,679],[1051,679],[1051,703],[161,703]],"score":0.7678},{"poly":[[140,651],[1051,651],[1051,675],[140,675]],"score":0.7448},{"poly":[[159,605],[471,602],[472,631],[160,635]],"score":0.7143},{"poly":[[161,581],[1049,581],[1049,605],[161,605]],"score":0.7812},{"poly":[[135,547],[1051,551],[1051,581],[135,577]],"score":0.678},{"poly":[[161,505],[1051,507],[1051,532],[161,530]],"score":0.7868},{"poly":[[160,479],[1051,479],[1051,509],[160,509]],"score":0.6451},{"poly":[[161,454],[1051,454],[1051,479],[161,479]],"score":0.7459},{"poly":[[158,425],[1053,423],[1053,453],[158,454]],"score":0.6977},{"poly":[[138,400],[1049,400],[1049,425],[138,425]],"score":0.7886},{"poly":[[161,354],[874,354],[874,379],[161,379]],"score":0.7732},{"poly":[[158,325],[1051,323],[1051,353],[158,354]],"score":0.6885},{"poly":[[161,300],[1049,300],[1049,325],[161,325]],"score":0.7542},{"poly":[[161,272],[1049,272],[1049,296],[161,296]],"score":0.768},{"poly":[[138,246],[1051,246],[1051,268],[138,268]],"score":0.9677},{"poly":[[159,200],[501,198],[502,223],[160,225]],"score":0.8323},{"poly":[[140,174],[1049,174],[1049,198],[140,198]],"score":0.7765}],"page_no":26,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7903,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1577},"font_size":0.0,"text":"28"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":308,"y0":166,"y1":205},"conf":0.8885,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":303,"y0":170,"y1":200},"font_size":0.0,"text":"A. Appendix"}],"source":"layout det","text":"A. Appendix"},{"bbox":{"x0":133,"x1":560,"y0":221,"y1":256},"conf":0.8929,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":555,"y0":224,"y1":251},"font_size":0.0,"text":"A.1. Analysis of Reinforcement Learning"}],"source":"layout det","text":"A.1. Analysis of Reinforcement Learning"},{"bbox":{"x0":132,"x1":1057,"y0":264,"y1":352},"conf":0.9475,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":270,"y1":295},"font_size":0.0,"text":"We provide the detailed derivation of the data source and gradient coeffcient (algorithm andi"},{"bbox":{"x0":138,"x1":1049,"y0":298,"y1":321},"font_size":0.0,"text":"reward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and"},{"bbox":{"x0":138,"x1":211,"y0":325,"y1":349},"font_size":0.0,"text":"GRPO."}],"source":"layout det","text":"We provide the detailed derivation of the data source and gradient coeffcient (algorithm andi reward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and GRPO."},{"bbox":{"x0":133,"x1":438,"y0":381,"y1":415},"conf":0.8632,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":434,"y0":381,"y1":414},"font_size":0.0,"text":"A.1.1. Supervised Fine-tuning"}],"source":"layout det","text":"A.1.1. Supervised Fine-tuning"},{"bbox":{"x0":132,"x1":906,"y0":425,"y1":461},"conf":0.8718,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":899,"y0":430,"y1":454},"font_size":0.0,"text":"The objective of Supervised Fine-tuning is maximizing the following objective:"}],"source":"layout det","text":"The objective of Supervised Fine-tuning is maximizing the following objective:"},{"bbox":{"x0":336,"x1":853,"y0":473,"y1":546},"conf":0.9433,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{S F T}(\\theta)=\\mathbb{E}[q,o\\sim P_{sft}(\\mathtt{Q},\\mathtt{O})]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$"},{"bbox":{"x0":1019,"x1":1056,"y0":494,"y1":528},"conf":0.8422,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1019,"x1":1055,"y0":495,"y1":526},"font_size":0.0,"text":"(6)"}],"source":"layout det","text":"(6)"},{"bbox":{"x0":133,"x1":398,"y0":557,"y1":592},"conf":0.9051,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":392,"y0":560,"y1":587},"font_size":0.0,"text":"The gradient of $\\mathcal{J}_{{ST T}}(\\theta)$  is:"}],"source":"layout det","text":"The gradient of $\\mathcal{J}_{{ST T}}(\\theta)$  is:"},{"bbox":{"x0":326,"x1":864,"y0":605,"y1":679},"conf":0.9442,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{SFT}=\\mathbb{E}\\left[q,o\\sim P_{sft}(Q,O)\\right] \\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$"},{"bbox":{"x0":1019,"x1":1055,"y0":627,"y1":659},"conf":0.8301,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1055,"y0":626,"y1":660},"font_size":0.0,"text":"(7)"}],"source":"layout det","text":"(7)"},{"bbox":{"x0":132,"x1":1057,"y0":688,"y1":754},"conf":0.2944,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":689,"y1":721},"font_size":0.0,"text":"Data Source: The dataset employed for SFT. Reward Function: This can be regarded as human"},{"bbox":{"x0":138,"x1":590,"y0":723,"y1":746},"font_size":0.0,"text":"selection. Gradient Coeffcient: always set to 1.i"}],"source":"layout det","text":"Data Source: The dataset employed for SFT. Reward Function: This can be regarded as human selection. Gradient Coeffcient: always set to 1.i"},{"bbox":{"x0":134,"x1":522,"y0":778,"y1":812},"conf":0.906,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":516,"y0":781,"y1":809},"font_size":0.0,"text":"A.1.2. Rejection Sampling Fine-tuning"}],"source":"layout det","text":"A.1.2. Rejection Sampling Fine-tuning"},{"bbox":{"x0":133,"x1":1058,"y0":821,"y1":913},"conf":0.9563,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":826,"y1":851},"font_size":0.0,"text":"Rejection Sampling Fine-tuning frst samples multiple outputs from the supervised fne-tunedii"},{"bbox":{"x0":138,"x1":1053,"y0":853,"y1":879},"font_size":0.0,"text":"LLMs for each question, and then trains LLMs on the sampled outputs with the correct answer."},{"bbox":{"x0":140,"x1":815,"y0":881,"y1":905},"font_size":0.0,"text":"Formally, the objective of RFT is to maximize the following objectives:"}],"source":"layout det","text":"Rejection Sampling Fine-tuning frst samples multiple outputs from the supervised fne-tunedii LLMs for each question, and then trains LLMs on the sampled outputs with the correct answer.Formally, the objective of RFT is to maximize the following objectives:"},{"bbox":{"x0":271,"x1":916,"y0":924,"y1":997},"conf":0.9195,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\mathit{RFT}}(\\theta)=\\mathbb{E}\\left[q\\sim P_{\\mathit{sft}}( \\mathit{Q}),o\\sim\\pi_{\\mathit{sft}}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|} \\sum_{t=1}^{|o|}\\mathbb{I}\\hskip-1.0pt\\mathbb{I}(o)\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$"},{"bbox":{"x0":1019,"x1":1056,"y0":945,"y1":978},"conf":0.8333,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1055,"y0":945,"y1":977},"font_size":0.0,"text":"(8)"}],"source":"layout det","text":"(8)"},{"bbox":{"x0":133,"x1":399,"y0":1008,"y1":1043},"conf":0.9123,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":395,"y0":1012,"y1":1038},"font_size":0.0,"text":"The gradient of $\\mathcal{J}_{{R} {F} {T}}(\\theta)$  is:"}],"source":"layout det","text":"The gradient of $\\mathcal{J}_{{R} {F} {T}}(\\theta)$  is:"},{"bbox":{"x0":248,"x1":940,"y0":1056,"y1":1130},"conf":0.9194,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{RFT}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}( \\mathit{Q}),o\\sim\\pi_{sft}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\mathbb{I}\\hskip0.5pt(o)\\hskip0.5pt\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$"},{"bbox":{"x0":1019,"x1":1056,"y0":1078,"y1":1110},"conf":0.8371,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1019,"x1":1055,"y0":1077,"y1":1109},"font_size":0.0,"text":"(9)"}],"source":"layout det","text":"(9)"},{"bbox":{"x0":132,"x1":1056,"y0":1141,"y1":1203},"conf":0.931,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1053,"y0":1145,"y1":1170},"font_size":0.0,"text":"Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:"},{"bbox":{"x0":140,"x1":769,"y0":1174,"y1":1197},"font_size":0.0,"text":"Rule (whether the answer is correct or not). Gradient Coeffcient:i"}],"source":"layout det","text":"Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:Rule (whether the answer is correct or not). Gradient Coeffcient:i"},{"bbox":{"x0":338,"x1":852,"y0":1213,"y1":1285},"conf":0.9034,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathit{GC}_{\\mathit{RFT}}(\\mathit{q},\\mathit{o},t)=\\mathbb{I}(\\mathit{o})=\\left\\{\\begin{aligned} & 1&\\mathrm{the answer of o is correct}\\\\ & 0&\\mathrm{the answer of o is incorrect}\\end{aligned}\\right.$$"},{"bbox":{"x0":1008,"x1":1056,"y0":1232,"y1":1266},"conf":0.8526,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1010,"x1":1055,"y0":1233,"y1":1263},"font_size":0.0,"text":"(10)"}],"source":"layout det","text":"(10)"},{"bbox":{"x0":133,"x1":594,"y0":1311,"y1":1347},"conf":0.8243,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":588,"y0":1314,"y1":1342},"font_size":0.0,"text":"A.1.3. Online Rejection Sampling Fine-tuning"}],"source":"layout det","text":"A.1.3. Online Rejection Sampling Fine-tuning"},{"bbox":{"x0":133,"x1":1056,"y0":1355,"y1":1444},"conf":0.9518,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":1358,"y1":1384},"font_size":0.0,"text":"The only difference between RFT and Online RFT is that the outputs of Online RFT are sampled"},{"bbox":{"x0":138,"x1":1051,"y0":1388,"y1":1417},"font_size":0.0,"text":"from the real-time policy model $\\pi_{\\theta},$  rather than from the SFT model $\\pi_{\\theta_{sft}}.$  Therefore, the gradient"},{"bbox":{"x0":138,"x1":301,"y0":1416,"y1":1438},"font_size":0.0,"text":"of online RFT is:"}],"source":"layout det","text":"The only difference between RFT and Online RFT is that the outputs of Online RFT are sampled from the real-time policy model $\\pi_{\\theta},$  rather than from the SFT model $\\pi_{\\theta_{sft}}.$  Therefore, the gradient of online RFT is:"},{"bbox":{"x0":244,"x1":944,"y0":1452,"y1":1527},"conf":0.9154,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{\\texttt{OnRFT}}(\\theta)=\\mathbb{E}\\left[q\\sim P_{\\mathit{sft}}(\\mathit{Q}),o\\sim\\pi_{\\theta}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\mathbb{I}\\hskip0.5pt(o)\\hskip0.5pt\\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}|q,\\mathit{o}_{<t})\\right).$$"},{"bbox":{"x0":1009,"x1":1056,"y0":1473,"y1":1506},"conf":0.8386,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1008,"x1":1056,"y0":1474,"y1":1505},"font_size":0.0,"text":"(11)"}],"source":"layout det","text":"(11)"}],"formula_dets":[{"bbox":{"x0":326,"x1":864,"y0":605,"y1":679},"conf":0.9442,"label":"print_isolated","label_id":1},{"bbox":{"x0":336,"x1":853,"y0":473,"y1":546},"conf":0.9433,"label":"print_isolated","label_id":1},{"bbox":{"x0":271,"x1":916,"y0":924,"y1":997},"conf":0.9195,"label":"print_isolated","label_id":1},{"bbox":{"x0":248,"x1":940,"y0":1056,"y1":1130},"conf":0.9194,"label":"print_isolated","label_id":1},{"bbox":{"x0":244,"x1":944,"y0":1452,"y1":1527},"conf":0.9154,"label":"print_isolated","label_id":1},{"bbox":{"x0":338,"x1":852,"y0":1213,"y1":1285},"conf":0.9034,"label":"print_isolated","label_id":1},{"bbox":{"x0":294,"x1":364,"y0":564,"y1":587},"conf":0.8712,"label":"print_embedding","label_id":0},{"bbox":{"x0":294,"x1":365,"y0":1015,"y1":1038},"conf":0.8558,"label":"print_embedding","label_id":0},{"bbox":{"x0":444,"x1":473,"y0":1396,"y1":1413},"conf":0.7543,"label":"print_embedding","label_id":0},{"bbox":{"x0":779,"x1":824,"y0":1394,"y1":1417},"conf":0.6713,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1058,"y0":821,"y1":913},"conf":0.9563,"label":"Text","label_id":1},{"bbox":{"x0":241,"x1":948,"y0":1053,"y1":1133},"conf":0.9549,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1056,"y0":1355,"y1":1444},"conf":0.9518,"label":"Text","label_id":1},{"bbox":{"x0":320,"x1":871,"y0":602,"y1":683},"conf":0.9514,"label":"Equation","label_id":8},{"bbox":{"x0":132,"x1":1057,"y0":264,"y1":352},"conf":0.9475,"label":"Text","label_id":1},{"bbox":{"x0":331,"x1":860,"y0":470,"y1":550},"conf":0.9413,"label":"Equation","label_id":8},{"bbox":{"x0":238,"x1":952,"y0":1450,"y1":1529},"conf":0.9351,"label":"Equation","label_id":8},{"bbox":{"x0":266,"x1":923,"y0":921,"y1":1002},"conf":0.9321,"label":"Equation","label_id":8},{"bbox":{"x0":331,"x1":858,"y0":1211,"y1":1287},"conf":0.931,"label":"Equation","label_id":8},{"bbox":{"x0":132,"x1":1056,"y0":1141,"y1":1203},"conf":0.931,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":399,"y0":1008,"y1":1043},"conf":0.9123,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":522,"y0":778,"y1":812},"conf":0.906,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":398,"y0":557,"y1":592},"conf":0.9051,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":560,"y0":221,"y1":256},"conf":0.8929,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":308,"y0":166,"y1":205},"conf":0.8885,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":906,"y0":425,"y1":461},"conf":0.8718,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":438,"y0":381,"y1":415},"conf":0.8632,"label":"Title","label_id":0},{"bbox":{"x0":1008,"x1":1056,"y0":1232,"y1":1266},"conf":0.8526,"label":"Equation caption","label_id":9},{"bbox":{"x0":1019,"x1":1056,"y0":494,"y1":528},"conf":0.8422,"label":"Equation caption","label_id":9},{"bbox":{"x0":1009,"x1":1056,"y0":1473,"y1":1506},"conf":0.8386,"label":"Equation caption","label_id":9},{"bbox":{"x0":1019,"x1":1056,"y0":1078,"y1":1110},"conf":0.8371,"label":"Equation caption","label_id":9},{"bbox":{"x0":1019,"x1":1056,"y0":945,"y1":978},"conf":0.8333,"label":"Equation caption","label_id":9},{"bbox":{"x0":1019,"x1":1055,"y0":627,"y1":659},"conf":0.8301,"label":"Equation caption","label_id":9},{"bbox":{"x0":133,"x1":594,"y0":1311,"y1":1347},"conf":0.8243,"label":"Title","label_id":0},{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7903,"label":"Abandon","label_id":2},{"bbox":{"x0":133,"x1":1056,"y0":688,"y1":754},"conf":0.785,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":688,"y1":754},"conf":0.2944,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1577],[580,1577]],"score":0.9271},{"poly":[[645,1481],[682,1481],[682,1523],[645,1523]],"score":0.7202},{"poly":[[243,1475],[638,1475],[638,1505],[243,1505]],"score":0.784},{"poly":[[709,1472],[932,1472],[932,1507],[709,1507]],"score":0.837},{"poly":[[1008,1474],[1056,1474],[1056,1505],[1008,1505]],"score":0.8985},{"poly":[[673,1449],[714,1447],[718,1530],[678,1532]],"score":0.6982},{"poly":[[138,1416],[301,1416],[301,1438],[138,1438]],"score":0.9814},{"poly":[[138,1388],[1051,1388],[1051,1412],[138,1412]],"score":0.7702},{"poly":[[137,1358],[1049,1359],[1049,1384],[136,1382]],"score":0.8254},{"poly":[[138,1314],[588,1317],[588,1342],[138,1338]],"score":0.8026},{"poly":[[583,1251],[851,1251],[851,1275],[583,1275]],"score":0.8096},{"poly":[[335,1233],[544,1233],[544,1263],[335,1263]],"score":0.7623},{"poly":[[1010,1233],[1055,1233],[1055,1263],[1010,1263]],"score":0.8828},{"poly":[[604,1217],[852,1217],[852,1242],[604,1242]],"score":0.8502},{"poly":[[548,1217],[574,1217],[574,1244],[548,1244]],"score":0.7254},{"poly":[[140,1174],[769,1174],[769,1196],[140,1196]],"score":0.9607},{"poly":[[140,1145],[1053,1145],[1053,1170],[140,1170]],"score":0.7706},{"poly":[[640,1086],[681,1086],[681,1128],[640,1128]],"score":0.7341},{"poly":[[245,1077],[638,1079],[638,1109],[245,1107]],"score":0.7744},{"poly":[[706,1075],[929,1079],[928,1111],[705,1107]],"score":0.8517},{"poly":[[1019,1077],[1055,1077],[1055,1109],[1019,1109]],"score":0.8761},{"poly":[[643,1063],[679,1063],[679,1093],[643,1093]],"score":0.8028},{"poly":[[674,1052],[717,1054],[712,1136],[669,1133]],"score":0.7306},{"poly":[[140,1012],[395,1012],[395,1037],[140,1037]],"score":0.8553},{"poly":[[677,975],[707,975],[707,996],[677,996]],"score":0.8255},{"poly":[[640,956],[681,956],[681,995],[640,995]],"score":0.7502},{"poly":[[268,945],[638,947],[638,977],[268,975]],"score":0.8261},{"poly":[[705,944],[907,944],[907,981],[705,981]],"score":0.7974},{"poly":[[1021,945],[1055,945],[1055,977],[1021,977]],"score":0.8899},{"poly":[[647,931],[670,931],[670,956],[647,956]],"score":0.7711},{"poly":[[677,924],[705,924],[705,947],[677,947]],"score":0.7562},{"poly":[[140,881],[815,881],[815,905],[140,905]],"score":0.7768},{"poly":[[138,853],[1053,854],[1053,879],[138,877]],"score":0.8507},{"poly":[[138,826],[1049,826],[1049,849],[138,849]],"score":0.9744},{"poly":[[138,781],[516,784],[516,809],[138,805]],"score":0.8229},{"poly":[[138,723],[590,723],[590,746],[138,746]],"score":0.9078},{"poly":[[137,689],[1051,691],[1051,721],[136,719]],"score":0.7119},{"poly":[[604,639],[636,639],[636,672],[604,672]],"score":0.8526},{"poly":[[323,628],[596,628],[596,658],[323,658]],"score":0.824},{"poly":[[1021,626],[1055,626],[1055,660],[1021,660]],"score":0.8721},{"poly":[[666,623],[856,623],[856,660],[666,660]],"score":0.8077},{"poly":[[611,614],[634,614],[634,642],[611,642]],"score":0.8518},{"poly":[[637,598],[676,600],[671,683],[632,681]],"score":0.6758},{"poly":[[138,560],[392,560],[392,584],[138,584]],"score":0.7302},{"poly":[[620,498],[659,498],[659,540],[620,540]],"score":0.7508},{"poly":[[333,495],[613,495],[613,524],[333,524]],"score":0.8379},{"poly":[[1019,495],[1055,495],[1055,526],[1019,526]],"score":0.8064},{"poly":[[680,495],[846,489],[848,528],[682,534]],"score":0.7684},{"poly":[[608,466],[695,458],[704,554],[617,562]],"score":0.6009},{"poly":[[140,430],[899,430],[899,454],[140,454]],"score":0.8217},{"poly":[[137,381],[434,384],[434,414],[136,410]],"score":0.7639},{"poly":[[138,325],[211,325],[211,349],[138,349]],"score":0.8732},{"poly":[[138,298],[1049,298],[1049,321],[138,321]],"score":0.8816},{"poly":[[138,270],[1049,270],[1049,293],[138,293]],"score":0.8801},{"poly":[[138,224],[555,226],[555,251],[138,249]],"score":0.8039},{"poly":[[138,170],[303,170],[303,200],[138,200]],"score":0.8684}],"page_no":27,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":612,"y0":1551,"y1":1579},"conf":0.7719,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1575},"font_size":0.0,"text":"29"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":588,"y0":168,"y1":203},"conf":0.8568,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":581,"y0":172,"y1":196},"font_size":0.0,"text":"A.1.4. Direct Preference Optimization (DPO)"}],"source":"layout det","text":"A.1.4. Direct Preference Optimization (DPO)"},{"bbox":{"x0":133,"x1":378,"y0":214,"y1":247},"conf":0.8859,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":374,"y0":218,"y1":240},"font_size":0.0,"text":"The objective of DPO is:"}],"source":"layout det","text":"The objective of DPO is:"},{"bbox":{"x0":153,"x1":1008,"y0":256,"y1":325},"conf":0.8601,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{DPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(Q),o^{+},o^{-} \\sim\\pi_{sft}(O|q)]\\\\ \\log\\sigma\\left(\\beta\\frac{1}{|o^{+}|}\\sum_{t=1}^{|o^{+}|}\\log\\frac{\\pi_{\\theta}(o_{t}^{+}|q,o_{<t}^{+})}{\\pi_{\\mathrm{ref}}(o_{t}^{+}|q,o_{<t}^{+})}-\\beta\\frac{1}{|o^{-}|}\\sum_{t=1}^{|o^{-}|}\\log\\frac{\\pi_{\\theta}(o_{<t}^{-}|q,o_{<t}^{-})}{\\pi_{\\mathrm{ref}}(o_{<t}^{-}|q,o_{<t}^{-})}\\right)$$"},{"bbox":{"x0":1016,"x1":1055,"y0":276,"y1":304},"conf":0.8379,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1016,"x1":1056,"y0":275,"y1":303},"font_size":0.0,"text":"(12)"}],"source":"layout det","text":"(12)"},{"bbox":{"x0":133,"x1":403,"y0":332,"y1":366},"conf":0.9127,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":397,"y0":333,"y1":362},"font_size":0.0,"text":"The gradient of $\\mathcal{J}_{DPO}(\\theta)$  is:"}],"source":"layout det","text":"The gradient of $\\mathcal{J}_{DPO}(\\theta)$  is:"},{"bbox":{"x0":243,"x1":937,"y0":375,"y1":441},"conf":0.924,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{DPO}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}( \\mathit{Q}),o^{+},o^{-}\\sim\\pi_{sft}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o^{+} |}\\sum_{t=1}^{|o^{+}|}\\mathit{GC}_{DPO}(\\mathit{q},\\mathit{o},t)\\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}^{+}|\\mathit{q},\\mathit{o}_{<t}^{+})\\right.$$"},{"bbox":{"x0":1015,"x1":1055,"y0":430,"y1":458},"conf":0.8455,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1014,"x1":1055,"y0":430,"y1":458},"font_size":0.0,"text":"(13)"}],"source":"layout det","text":"(13)"},{"bbox":{"x0":590,"x1":948,"y0":445,"y1":513},"conf":0.9164,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$-\\left.\\frac{1}{|o^{-}|}\\sum_{t=1}^{|o^{-}|}GC_{DPO}(q,o,t)\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}^{-}|q,o_{<t}^{-})\\right)$$"},{"bbox":{"x0":135,"x1":1056,"y0":519,"y1":603},"conf":0.9488,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":523,"y1":549},"font_size":0.0,"text":"Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:"},{"bbox":{"x0":138,"x1":1051,"y0":553,"y1":575},"font_size":0.0,"text":"human preference in the general domain (can be â€˜Ruleâ€™ in mathematical tasks). Gradient"},{"bbox":{"x0":142,"x1":252,"y0":581,"y1":603},"font_size":0.0,"text":"Coeffcient:i"},{"bbox":{"x0":585,"x1":805,"y0":597,"y1":609},"font_size":0.0,"text":"âˆ’âˆ’++"}],"source":"layout det","text":"Data Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:human preference in the general domain (can be â€˜Ruleâ€™ in mathematical tasks). Gradient Coeffcient:iâˆ’âˆ’++"},{"bbox":{"x0":354,"x1":836,"y0":596,"y1":648},"conf":0.9311,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathit{GC}_{DPO}(q,o,t)=\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(o_{t}^{-}|q,o_{<t}^{-})}{\\pi_{ref}(o_{t}^{-}|q,o_{<t}^{-})}-\\beta\\log\\frac{\\pi_{\\theta}(o_{t}^{+}|q,o_{<t}^{+})}{\\pi_{ref}(o_{t}^{+}|q,o_{<t}^{+})}\\right)$$"},{"bbox":{"x0":1015,"x1":1056,"y0":608,"y1":636},"conf":0.8314,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1014,"x1":1056,"y0":607,"y1":635},"font_size":0.0,"text":"(14)"}],"source":"layout det","text":"(14)"},{"bbox":{"x0":133,"x1":580,"y0":676,"y1":711},"conf":0.8908,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":574,"y0":681,"y1":705},"font_size":0.0,"text":"A.1.5. Proximal Policy Optimization (PPO)"}],"source":"layout det","text":"A.1.5. Proximal Policy Optimization (PPO)"},{"bbox":{"x0":133,"x1":374,"y0":721,"y1":756},"conf":0.889,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":369,"y0":724,"y1":749},"font_size":0.0,"text":"The objective of PPO is:"}],"source":"layout det","text":"The objective of PPO is:"},{"bbox":{"x0":169,"x1":994,"y0":799,"y1":863},"conf":0.8703,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}(Q),o\\sim\\pi_{\\theta_{old}}(O|q)\\right]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\min\\left[\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t} )}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\right)A_{t}\\right].$$"},{"bbox":{"x0":1014,"x1":1055,"y0":817,"y1":846},"conf":0.8335,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1014,"x1":1055,"y0":816,"y1":844},"font_size":0.0,"text":"(15)"}],"source":"layout det","text":"(15)"},{"bbox":{"x0":133,"x1":1058,"y0":870,"y1":956},"conf":0.9512,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1053,"y0":870,"y1":902},"font_size":0.0,"text":"To simplify the analysis, it is assumed that the model only has a single update following each"},{"bbox":{"x0":136,"x1":1051,"y0":900,"y1":930},"font_size":0.0,"text":"exploration stage, thereby ensuring that $\\pi_{\\theta_{old}}=\\pi_{\\theta}.$  In this case, we can remove the min and clip"},{"bbox":{"x0":136,"x1":243,"y0":928,"y1":956},"font_size":0.0,"text":"operation:"}],"source":"layout det","text":"To simplify the analysis, it is assumed that the model only has a single update following each exploration stage, thereby ensuring that $\\pi_{\\theta_{old}}=\\pi_{\\theta}.$  In this case, we can remove the min and clip operation:"},{"bbox":{"x0":345,"x1":844,"y0":954,"y1":1017},"conf":0.945,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\textit{Q}),o\\sim\\pi_{\\theta_{old}}(\\textit{O}|q)]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t}.$$"},{"bbox":{"x0":1015,"x1":1055,"y0":972,"y1":1001},"conf":0.8343,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1014,"x1":1056,"y0":972,"y1":1000},"font_size":0.0,"text":"(16)"}],"source":"layout det","text":"(16)"},{"bbox":{"x0":133,"x1":401,"y0":1018,"y1":1052},"conf":0.9044,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":397,"y0":1019,"y1":1048},"font_size":0.0,"text":"The gradient of $\\mathcal{J}_{PPO}(\\theta)$  is:"}],"source":"layout det","text":"The gradient of $\\mathcal{J}_{PPO}(\\theta)$  is:"},{"bbox":{"x0":321,"x1":868,"y0":1061,"y1":1125},"conf":0.9423,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\mathit{Q}),o\\sim\\pi_{\\theta_{old}}(\\mathit{O}|q)]\\frac{1}{|o|}\\sum_{t=1}^{|o|}A_{t} \\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}|q,o_{<t})$$"},{"bbox":{"x0":1015,"x1":1055,"y0":1079,"y1":1108},"conf":0.8411,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1016,"x1":1056,"y0":1079,"y1":1109},"font_size":0.0,"text":"(17)"}],"source":"layout det","text":"(17)"},{"bbox":{"x0":133,"x1":1057,"y0":1132,"y1":1191},"conf":0.9103,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1053,"y0":1137,"y1":1161},"font_size":0.0,"text":"Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function:"},{"bbox":{"x0":140,"x1":494,"y0":1165,"y1":1189},"font_size":0.0,"text":"reward model. Gradient Coeffcient:i"}],"source":"layout det","text":"Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function:reward model. Gradient Coeffcient:i"},{"bbox":{"x0":484,"x1":705,"y0":1212,"y1":1238},"conf":0.8987,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$GC_{PPO}(q,o,t,\\pi_{\\theta_{rm}})=A_{t},$$"},{"bbox":{"x0":1009,"x1":1055,"y0":1209,"y1":1240},"conf":0.8448,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1008,"x1":1056,"y0":1209,"y1":1240},"font_size":0.0,"text":"(18)"}],"source":"layout det","text":"(18)"},{"bbox":{"x0":133,"x1":1056,"y0":1254,"y1":1317},"conf":0.9469,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1259,"y1":1284},"font_size":0.0,"text":"where $A_{t}$ is the advantage, which is computed by applying Generalized Advantage Estimation"},{"bbox":{"x0":136,"x1":1006,"y0":1280,"y1":1314},"font_size":0.0,"text":"(GAE) (Schulman et al., 2015), based on the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$"}],"source":"layout det","text":"where $A_{t}$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$"},{"bbox":{"x0":134,"x1":656,"y0":1342,"y1":1377},"conf":0.8787,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":650,"y0":1347,"y1":1370},"font_size":0.0,"text":"A.1.6. Group Relative Policy Optimization (GRPO)"}],"source":"layout det","text":"A.1.6. Group Relative Policy Optimization (GRPO)"},{"bbox":{"x0":132,"x1":797,"y0":1386,"y1":1423},"conf":0.8712,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":794,"y0":1389,"y1":1419},"font_size":0.0,"text":"The objective of GRPO is (assume $\\pi_{{{\\theta}_{old}}}=\\pi_{{{\\theta}}}$ for simplifed analysis):i"}],"source":"layout det","text":"The objective of GRPO is (assume $\\pi_{{{\\theta}_{old}}}=\\pi_{{{\\theta}}}$ for simplifed analysis):i"},{"bbox":{"x0":232,"x1":601,"y0":1430,"y1":1459},"conf":0.7813,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\textit{Q}),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\textit{O}|q)]$$"},{"bbox":{"x0":298,"x1":957,"y0":1464,"y1":1529},"conf":0.8368,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\left[\\frac{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta_{std}}(o_{i,t}|q,o_{i,<t})}\\hat{A}_{i,t}-\\beta(\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-\\log\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-1) \\right].$$"},{"bbox":{"x0":1014,"x1":1056,"y0":1464,"y1":1494},"conf":0.8177,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1014,"x1":1055,"y0":1465,"y1":1493},"font_size":0.0,"text":"(19)"}],"source":"layout det","text":"(19)"}],"formula_dets":[{"bbox":{"x0":345,"x1":844,"y0":954,"y1":1017},"conf":0.945,"label":"print_isolated","label_id":1},{"bbox":{"x0":321,"x1":868,"y0":1061,"y1":1125},"conf":0.9423,"label":"print_isolated","label_id":1},{"bbox":{"x0":354,"x1":836,"y0":596,"y1":648},"conf":0.9311,"label":"print_isolated","label_id":1},{"bbox":{"x0":243,"x1":937,"y0":375,"y1":441},"conf":0.924,"label":"print_isolated","label_id":1},{"bbox":{"x0":590,"x1":948,"y0":445,"y1":513},"conf":0.9164,"label":"print_isolated","label_id":1},{"bbox":{"x0":484,"x1":705,"y0":1212,"y1":1238},"conf":0.8987,"label":"print_isolated","label_id":1},{"bbox":{"x0":646,"x1":692,"y0":1289,"y1":1312},"conf":0.886,"label":"print_embedding","label_id":0},{"bbox":{"x0":169,"x1":994,"y0":799,"y1":863},"conf":0.8703,"label":"print_isolated","label_id":1},{"bbox":{"x0":294,"x1":367,"y0":1024,"y1":1048},"conf":0.87,"label":"print_embedding","label_id":0},{"bbox":{"x0":153,"x1":1008,"y0":256,"y1":325},"conf":0.8601,"label":"print_isolated","label_id":1},{"bbox":{"x0":293,"x1":368,"y0":338,"y1":362},"conf":0.8579,"label":"print_embedding","label_id":0},{"bbox":{"x0":525,"x1":619,"y0":906,"y1":929},"conf":0.8492,"label":"print_embedding","label_id":0},{"bbox":{"x0":470,"x1":558,"y0":1395,"y1":1419},"conf":0.8383,"label":"print_embedding","label_id":0},{"bbox":{"x0":298,"x1":957,"y0":1464,"y1":1529},"conf":0.8368,"label":"print_isolated","label_id":1},{"bbox":{"x0":204,"x1":226,"y0":1264,"y1":1284},"conf":0.79,"label":"print_embedding","label_id":0},{"bbox":{"x0":232,"x1":601,"y0":1430,"y1":1459},"conf":0.7813,"label":"print_isolated","label_id":1},{"bbox":{"x0":978,"x1":1006,"y0":1290,"y1":1314},"conf":0.693,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":241,"x1":951,"y0":372,"y1":515},"conf":0.9704,"label":"Equation","label_id":8},{"bbox":{"x0":316,"x1":874,"y0":1059,"y1":1127},"conf":0.9552,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1058,"y0":870,"y1":956},"conf":0.9512,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1056,"y0":519,"y1":603},"conf":0.9488,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":1254,"y1":1317},"conf":0.9469,"label":"Text","label_id":1},{"bbox":{"x0":340,"x1":849,"y0":955,"y1":1020},"conf":0.9353,"label":"Equation","label_id":8},{"bbox":{"x0":349,"x1":840,"y0":597,"y1":653},"conf":0.9254,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":403,"y0":332,"y1":366},"conf":0.9127,"label":"Text","label_id":1},{"bbox":{"x0":161,"x1":1000,"y0":796,"y1":866},"conf":0.9125,"label":"Equation","label_id":8},{"bbox":{"x0":143,"x1":1014,"y0":253,"y1":327},"conf":0.9112,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1057,"y0":1132,"y1":1191},"conf":0.9103,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":401,"y0":1018,"y1":1052},"conf":0.9044,"label":"Text","label_id":1},{"bbox":{"x0":480,"x1":710,"y0":1208,"y1":1243},"conf":0.9038,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":580,"y0":676,"y1":711},"conf":0.8908,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":374,"y0":721,"y1":756},"conf":0.889,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":378,"y0":214,"y1":247},"conf":0.8859,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":656,"y0":1342,"y1":1377},"conf":0.8787,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":797,"y0":1386,"y1":1423},"conf":0.8712,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":588,"y0":168,"y1":203},"conf":0.8568,"label":"Title","label_id":0},{"bbox":{"x0":1015,"x1":1055,"y0":430,"y1":458},"conf":0.8455,"label":"Equation caption","label_id":9},{"bbox":{"x0":1009,"x1":1055,"y0":1209,"y1":1240},"conf":0.8448,"label":"Equation caption","label_id":9},{"bbox":{"x0":1015,"x1":1055,"y0":1079,"y1":1108},"conf":0.8411,"label":"Equation caption","label_id":9},{"bbox":{"x0":1016,"x1":1055,"y0":276,"y1":304},"conf":0.8379,"label":"Equation caption","label_id":9},{"bbox":{"x0":1015,"x1":1055,"y0":972,"y1":1001},"conf":0.8343,"label":"Equation caption","label_id":9},{"bbox":{"x0":1014,"x1":1055,"y0":817,"y1":846},"conf":0.8335,"label":"Equation caption","label_id":9},{"bbox":{"x0":1015,"x1":1056,"y0":608,"y1":636},"conf":0.8314,"label":"Equation caption","label_id":9},{"bbox":{"x0":1014,"x1":1056,"y0":1464,"y1":1494},"conf":0.8177,"label":"Equation caption","label_id":9},{"bbox":{"x0":577,"x1":612,"y0":1551,"y1":1579},"conf":0.7719,"label":"Abandon","label_id":2},{"bbox":{"x0":302,"x1":964,"y0":1430,"y1":1529},"conf":0.5548,"label":"Equation","label_id":8}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1575],[580,1575]],"score":0.8713},{"poly":[[611,1496],[730,1496],[730,1521],[611,1521]],"score":0.751},{"poly":[[777,1489],[906,1495],[905,1527],[776,1520]],"score":0.6619},{"poly":[[747,1479],[787,1489],[782,1510],[742,1500]],"score":0.6835},{"poly":[[546,1482],[615,1482],[615,1509],[546,1509]],"score":0.8058},{"poly":[[918,1481],[946,1481],[946,1505],[918,1505]],"score":0.7167},{"poly":[[778,1470],[907,1470],[907,1495],[778,1495]],"score":0.8097},{"poly":[[604,1472],[735,1468],[736,1493],[605,1497]],"score":0.8044},{"poly":[[415,1464],[543,1468],[542,1498],[414,1494]],"score":0.7669},{"poly":[[1014,1465],[1055,1465],[1055,1493],[1014,1493]],"score":0.8954},{"poly":[[382,1460],[411,1466],[406,1487],[378,1481]],"score":0.7629},{"poly":[[232,1433],[596,1433],[596,1458],[232,1458]],"score":0.8275},{"poly":[[138,1389],[794,1389],[794,1419],[138,1419]],"score":0.6673},{"poly":[[138,1347],[650,1347],[650,1370],[138,1370]],"score":0.9587},{"poly":[[137,1280],[1008,1284],[1008,1314],[136,1310]],"score":0.6694},{"poly":[[138,1259],[1051,1259],[1051,1284],[138,1284]],"score":0.8133},{"poly":[[482,1212],[704,1212],[704,1235],[482,1235]],"score":0.756},{"poly":[[1008,1209],[1056,1209],[1056,1240],[1008,1240]],"score":0.8488},{"poly":[[140,1165],[494,1165],[494,1188],[140,1188]],"score":0.8361},{"poly":[[140,1137],[1053,1137],[1053,1161],[140,1161]],"score":0.7747},{"poly":[[654,1086],[679,1094],[670,1121],[645,1113]],"score":0.6352},{"poly":[[702,1082],[870,1082],[870,1105],[702,1105]],"score":0.8629},{"poly":[[317,1081],[656,1082],[656,1107],[317,1105]],"score":0.7684},{"poly":[[1016,1079],[1056,1079],[1056,1109],[1016,1109]],"score":0.8299},{"poly":[[656,1070],[672,1070],[672,1089],[656,1089]],"score":0.7804},{"poly":[[674,1055],[708,1058],[703,1130],[669,1128]],"score":0.7126},{"poly":[[138,1019],[397,1021],[397,1046],[138,1044]],"score":0.8339},{"poly":[[817,977],[845,977],[845,998],[817,998]],"score":0.7487},{"poly":[[344,975],[658,975],[658,1000],[344,1000]],"score":0.7925},{"poly":[[1014,972],[1056,972],[1056,1000],[1014,1000]],"score":0.8738},{"poly":[[700,963],[817,963],[817,988],[700,988]],"score":0.7783},{"poly":[[656,963],[681,963],[681,986],[656,986]],"score":0.7056},{"poly":[[675,951],[712,951],[712,993],[675,993]],"score":0.7104},{"poly":[[136,932],[242,928],[243,952],[137,956]],"score":0.786},{"poly":[[136,900],[1051,900],[1051,930],[136,930]],"score":0.667},{"poly":[[135,872],[1053,870],[1053,900],[135,902]],"score":0.7112},{"poly":[[746,833],[868,833],[868,858],[746,858]],"score":0.8485},{"poly":[[564,833],[688,833],[688,858],[564,858]],"score":0.841},{"poly":[[477,831],[502,831],[502,856],[477,856]],"score":0.7121},{"poly":[[684,819],[743,819],[743,844],[684,844]],"score":0.7717},{"poly":[[161,816],[482,817],[482,847],[161,845]],"score":0.706},{"poly":[[875,818],[961,814],[963,840],[876,844]],"score":0.6959},{"poly":[[526,819],[565,819],[565,840],[526,840]],"score":0.8151},{"poly":[[1014,816],[1055,816],[1055,844],[1014,844]],"score":0.7928},{"poly":[[950,815],[981,809],[987,843],[957,849]],"score":0.6425},{"poly":[[494,812],[532,812],[532,867],[494,867]],"score":0.6988},{"poly":[[755,809],[860,809],[860,833],[755,833]],"score":0.8903},{"poly":[[572,809],[679,809],[679,833],[572,833]],"score":0.8293},{"poly":[[503,798],[526,798],[526,821],[503,821]],"score":0.8627},{"poly":[[140,724],[369,724],[369,749],[140,749]],"score":0.8328},{"poly":[[138,681],[574,681],[574,705],[138,705]],"score":0.802},{"poly":[[539,623],[654,623],[654,647],[539,647]],"score":0.881},{"poly":[[709,619],[835,619],[835,649],[709,649]],"score":0.8202},{"poly":[[662,604],[719,611],[715,638],[658,631]],"score":0.6704},{"poly":[[1014,607],[1056,607],[1056,635],[1014,635]],"score":0.892},{"poly":[[348,600],[544,604],[544,639],[347,635]],"score":0.6267},{"poly":[[544,600],[649,600],[649,619],[544,619]],"score":0.9772},{"poly":[[718,596],[828,596],[828,621],[718,621]],"score":0.9162},{"poly":[[142,581],[252,581],[252,600],[142,600]],"score":0.956},{"poly":[[138,553],[1051,553],[1051,575],[138,575]],"score":0.9109},{"poly":[[138,523],[1053,525],[1053,549],[138,547]],"score":0.7696},{"poly":[[615,479],[649,479],[649,502],[615,502]],"score":0.8721},{"poly":[[677,467],[934,467],[934,491],[677,491]],"score":0.7894},{"poly":[[619,451],[643,451],[643,475],[619,475]],"score":0.6887},{"poly":[[647,440],[686,440],[686,514],[647,514]],"score":0.7929},{"poly":[[1014,430],[1055,430],[1055,458],[1014,458]],"score":0.8492},{"poly":[[619,403],[654,403],[654,433],[619,433]],"score":0.7631},{"poly":[[681,396],[936,396],[936,419],[681,419]],"score":0.979},{"poly":[[246,396],[608,396],[608,421],[246,421]],"score":0.7911},{"poly":[[626,381],[649,381],[649,405],[626,405]],"score":0.7427},{"poly":[[650,370],[684,370],[684,442],[650,442]],"score":0.7117},{"poly":[[138,333],[397,335],[397,360],[138,358]],"score":0.8419},{"poly":[[831,296],[847,296],[847,310],[831,310]],"score":0.7144},{"poly":[[644,284],[762,288],[761,320],[643,315]],"score":0.728},{"poly":[[152,279],[542,279],[542,303],[152,303]],"score":0.845},{"poly":[[620,273],[657,287],[648,310],[611,296]],"score":0.6978},{"poly":[[1016,275],[1056,275],[1056,303],[1016,303]],"score":0.9032},{"poly":[[883,261],[1001,261],[1001,291],[883,291]],"score":0.8176},{"poly":[[649,261],[760,261],[760,291],[649,291]],"score":0.7692},{"poly":[[796,263],[819,263],[819,286],[796,286]],"score":0.9032},{"poly":[[562,263],[585,263],[585,288],[562,288]],"score":0.6834},{"poly":[[819,251],[860,251],[860,326],[819,326]],"score":0.6739},{"poly":[[589,248],[628,251],[623,327],[584,325]],"score":0.6528},{"poly":[[140,218],[374,218],[374,240],[140,240]],"score":0.9037},{"poly":[[138,172],[581,172],[581,196],[138,196]],"score":0.7293}],"page_no":28,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1579},"conf":0.7846,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1552,"y1":1577},"font_size":0.0,"text":"30"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":411,"y0":168,"y1":203},"conf":0.8969,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":406,"y0":172,"y1":198},"font_size":0.0,"text":"The gradient of $\\mathcal{J}_{GRPO}(\\theta)$  is:"}],"source":"layout det","text":"The gradient of $\\mathcal{J}_{GRPO}(\\theta)$  is:"},{"bbox":{"x0":297,"x1":684,"y0":212,"y1":242},"conf":0.8348,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\mathsf{Q} ),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\mathsf{O}|q)]$$"},{"bbox":{"x0":389,"x1":893,"y0":247,"y1":309},"conf":0.8977,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\left[\\hat{A}_{i,t}+\\beta\\left(\\frac{\\pi_{ref}(o_{i,t}|o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|o_{i,<t})}-1\\right)\\right]\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{i,t}|q,o_{i,<t}).$$"},{"bbox":{"x0":1015,"x1":1055,"y0":247,"y1":273},"conf":0.851,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1014,"x1":1056,"y0":246,"y1":275},"font_size":0.0,"text":"(20)"}],"source":"layout det","text":"(20)"},{"bbox":{"x0":133,"x1":1057,"y0":317,"y1":375},"conf":0.4985,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":319,"y1":344},"font_size":0.0,"text":"Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function:"},{"bbox":{"x0":140,"x1":493,"y0":349,"y1":374},"font_size":0.0,"text":"reward model. Gradient Coeffcient:i"}],"source":"layout det","text":"Data Source: question in SFT dataset with outputs sampled from policy model. Reward Function:reward model. Gradient Coeffcient:i"},{"bbox":{"x0":398,"x1":791,"y0":382,"y1":433},"conf":0.9524,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\textit{GC}_{GRPO}(\\textit{q},\\textit{o},\\textit{t},\\pi_{\\theta_{rm}})=\\hat{A}_{i,t}+\\beta\\left(\\frac{\\pi_{ref}(o_{i,t}|o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|o_{i,<t})}-1\\right),$$"},{"bbox":{"x0":1015,"x1":1055,"y0":394,"y1":421},"conf":0.8379,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1016,"x1":1058,"y0":391,"y1":425},"font_size":0.0,"text":"(21)"}],"source":"layout det","text":"(21)"},{"bbox":{"x0":134,"x1":700,"y0":443,"y1":479},"conf":0.8911,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":695,"y0":447,"y1":474},"font_size":0.0,"text":"where $\\hat{A}_{i,t}$ is computed based on the group reward scores."}],"source":"layout det","text":"where $\\hat{A}_{i,t}$ is computed based on the group reward scores."}],"formula_dets":[{"bbox":{"x0":398,"x1":791,"y0":382,"y1":433},"conf":0.9524,"label":"print_isolated","label_id":1},{"bbox":{"x0":389,"x1":893,"y0":247,"y1":309},"conf":0.8977,"label":"print_isolated","label_id":1},{"bbox":{"x0":294,"x1":377,"y0":175,"y1":198},"conf":0.8657,"label":"print_embedding","label_id":0},{"bbox":{"x0":204,"x1":234,"y0":450,"y1":474},"conf":0.8474,"label":"print_embedding","label_id":0},{"bbox":{"x0":297,"x1":684,"y0":212,"y1":242},"conf":0.8348,"label":"print_isolated","label_id":1}],"height":841,"layout_dets":[{"bbox":{"x0":395,"x1":797,"y0":379,"y1":438},"conf":0.9397,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":411,"y0":168,"y1":203},"conf":0.8969,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":700,"y0":443,"y1":479},"conf":0.8911,"label":"Text","label_id":1},{"bbox":{"x0":1015,"x1":1055,"y0":247,"y1":273},"conf":0.851,"label":"Equation caption","label_id":9},{"bbox":{"x0":1015,"x1":1055,"y0":394,"y1":421},"conf":0.8379,"label":"Equation caption","label_id":9},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1579},"conf":0.7846,"label":"Abandon","label_id":2},{"bbox":{"x0":322,"x1":899,"y0":209,"y1":310},"conf":0.5796,"label":"Equation","label_id":8},{"bbox":{"x0":132,"x1":1055,"y0":317,"y1":375},"conf":0.5096,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":317,"y1":375},"conf":0.4985,"label":"Text","label_id":1},{"bbox":{"x0":293,"x1":687,"y0":209,"y1":244},"conf":0.4486,"label":"Equation","label_id":8},{"bbox":{"x0":262,"x1":906,"y0":209,"y1":311},"conf":0.4275,"label":"Equation","label_id":8}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[610,1552],[610,1577],[580,1577]],"score":0.8196},{"poly":[[140,447],[695,447],[695,472],[140,472]],"score":0.9369},{"poly":[[630,405],[743,411],[742,437],[629,431]],"score":0.7888},{"poly":[[1016,391],[1058,391],[1058,425],[1016,425]],"score":0.8478},{"poly":[[391,391],[634,386],[635,422],[392,428]],"score":0.6406},{"poly":[[757,389],[789,389],[789,425],[757,425]],"score":0.6975},{"poly":[[629,381],[750,381],[750,410],[629,410]],"score":0.7834},{"poly":[[140,349],[493,349],[493,374],[140,374]],"score":0.8367},{"poly":[[140,319],[1049,319],[1049,344],[140,344]],"score":0.8248},{"poly":[[560,277],[675,277],[675,302],[560,302]],"score":0.8567},{"poly":[[718,259],[893,263],[893,295],[718,291]],"score":0.8264},{"poly":[[695,265],[711,265],[711,284],[695,284]],"score":0.7746},{"poly":[[498,256],[569,256],[569,295],[498,295]],"score":0.7644},{"poly":[[560,249],[682,249],[682,279],[560,279]],"score":0.844},{"poly":[[386,244],[473,244],[473,284],[386,284]],"score":0.6143},{"poly":[[1014,246],[1056,246],[1056,275],[1014,275]],"score":0.8377},{"poly":[[461,242],[502,242],[502,312],[461,312]],"score":0.7368},{"poly":[[294,212],[684,212],[684,242],[294,242]],"score":0.7633},{"poly":[[140,172],[406,172],[406,196],[140,196]],"score":0.9137}],"page_no":29,"scale":2.0,"width":595}],"pages_success_ratio":0.0,"src_path":"oss://glm-data-ocr-data/services/maas/docs/ad3c0df6-78b4-46dd-b563-53286a11b2cb","text":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\nZhihong Shao1,2âˆ—â€ , Peiyi Wang1,3âˆ—â€ , Qihao Zhu1,3âˆ—â€ , Runxin Xu1, Junxiao Song1 Xiao $\\mathbf{Bi^{1}},$  Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1âˆ—\n1DeepSeek-AI, 2Tsinghua University, 3Peking University\nzhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com https://github.com/deepseek-ai/DeepSeek-Math\nAbstract\nMathematical reasoning poses a signifcant challenge for language models due to its complexi and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pretraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of $51.7\\%$ on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves $60.9\\%$ on MATH.The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,we harness the signifcant potential of publicly available web data through a meticulouslyi engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.\nFigure 1 | Top1 accuracy of open-source models on the competition-level MATH benchmark(Hendrycks et al., 2021) without the use of external toolkits and voting techniques.\n\n1.Introduction\nLarge language models (LLM) have revolutionized the approach to mathematical reasoning in artifcial intelligence, spurring signifcant advancements in both the quantitative reasoningii benchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024).Moreover, these models have proven instrumental in assisting humans in solving complex mathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible open-source models considerably trail behind in performance.\nIn this study, we introduce DeepSeekMath, a domain-specifc language model that signif-ii cantly outperforms the mathematical capabilities of open-source models and approaches the performance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeekMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This dataset is extracted from the Common Crawl (CC) using a fastText-based classifer (Joulin et al.,i 2016). In the initial iteration, the classifer is trained using instances from OpenWebMath (Pasteri et al., 2023) as positive examples, while incorporating a diverse selection of other web pages to serve as negative examples. Subsequently, we employ the classifer to mine additional positivei instances from the CC, which are further refned through human annotation. The classifer isii then updated with this enhanced dataset to improve its performance. The evaluation results indicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base 7B achieves $64.2\\%$ on GSM8K (Cobbe et al., 2021) and $36.2\\%$ on the competition-level MATH dataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In addition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese mathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience in mathematical data processing is a starting point for the research community, and there is signifcant room for improvement in the future.i\nDeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as we notice that starting from a code training model is a better choice compared to a general LLM. Furthermore, we observe the math training also improves model capability on MMLU(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only enhance the modelâ€™s mathematical abilities but also amplifes general reasoning capabilities.i\nAfter pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with chain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and tool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct 7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models.\nFurthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant reinforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).GRPO foregoes the critic model, instead estimating the baseline from group scores, signifcantlyi reducing training resources. By solely using a subset of English instruction tuning data, GRPO obtains a substantial improvement over the strong DeepSeekMath-Instruct, including both in-domain (GSM8K: $82.9\\%\\to88.2\\%,$ MATH: $46.8\\%\\to51.7\\%\\big)$  and out-of-domain mathematical tasks (e.g., CMATH: $84.6\\%\\to88.8\\%)$  during the reinforcement learning phase. We also provide a unifed paradigm to understand different methods, such as Rejection Sampling Fine-Tuningi(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and GRPO. Based on such a unifed paradigm, we fnd that all these methods are conceptualized asii either direct or simplifed RL techniques. We also conduct extensive experiments, e.g., onlinei v.s. offine training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,lto deeply investigate the essential elements of this paradigm. At last, we explain why our RL boosts the performance of instruction-tuned models, and further summarize potential directions to achieve more effective RL based on this unifed paradigm.i\n1.1. Contributions\nOur contribution includes scalable math pre-training, along with the exploration and analysis of reinforcement learning.\nMath Pre-Training at Scale\nOur research provides compelling evidence that the publicly accessible Common Crawl data contains valuable information for mathematical purposes. By implementing a meticulously designed data selection pipeline, we successfully construct the DeepSeekMath Corpus, a high-quality dataset of 120B tokens from web pages fltered for mathemati-i cal content, which is almost 7 times the size of the math web pages used by Minerva(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath(Paster et al., 2023).\nOur pre-trained base model DeepSeekMath-Base 7B achieves comparable performance with Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not the only key factor in mathematical reasoning capability. A smaller model pre-trained on high-quality data could achieve strong performance as well.\nWe share our fndings from math training experiments. Code training prior to mathi training improves modelsâ€™ ability to solve mathematical problems both with and without tool use. This offers a partial answer to the long-standing question: does code training improve reasoning abilities? We believe it does, at least for mathematical reasoning.\nAlthough training on arXiv papers is common, especially in many math-related papers, it brings no notable improvements on all mathematical benchmarks adopted in this paper.\nExploration and Analysis of Reinforcement Learning\nWe introduce Group Relative Policy Optimization (GRPO), an effcient and effectivei reinforcement learning algorithm. GRPO foregoes the critic model, instead estimating the baseline from group scores, signifcantly reducing training resources compared toi Proximal Policy Optimization (PPO).\nWe demonstrate that GRPO signifcantly enhances the performance of our instruction-i tuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Furthermore, we observe enhancements in the out-of-domain performance during the reinforcement learning process.\nWe provide a unifed paradigm to understand different methods, such as RFT, DPO,i PPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offine training,l outcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so on to deeply investigate the essential elements of this paradigm.\nBased on our unifed paradigm, we explore the reasons behind the effectiveness of rein-i forcement learning, and summarize several potential directions to achieve more effective reinforcement learning of LLMs.\n1.2. Summary of Evaluations and Metrics\nEnglish and Chinese Mathematical Reasoning: We conduct comprehensive assessments of our models on English and Chinese benchmarks, covering mathematical problemsfrom grade-school level to college level. English benchmarks include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks include MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong et al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate modelsâ€™ ability to generate self-contained text solutions without tool use, and also the ability to solve problems using Python.\nOn English benchmarks, DeepSeekMath-Base is competitive with the closed-source Minerva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mistral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether theyâ€™ve undergone math pre-training or not, often by a signifcant margin. Notably,i DeepSeekMath-Base is superior on Chinese benchmarks, likely because we donâ€™t follow previous works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only math pre-training data, and also include high-quality non-English ones. With mathematical instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct and DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over $50\\%$ on the competition-level MATH dataset for the frst time within the open-sourcei community.\nFormal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal theorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates strong few-shot autoformalization performance.\nNatural Language Understanding, Reasoning, and Code: To build a comprehensive profle of modelsâ€™ general understanding, reasoning, and coding capabilities, we eval-i uate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)benchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering diverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 challenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code language models. Math pre-training benefts both language understanding and reasoningi performance.\n2.Math Pre-Training\n2.1. Data Collection and Decontamination\nIn this section, we will outline the process of constructing the DeepSeekMath Corpus from Common Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates how to systematically gather a large-scale mathematical corpus from Common Crawl, starting with a seed corpus (e.g., a small but high-quality collection of math-related dataset). Itâ€™s worth noting that this approach is also applicable to other domains, such as coding.\nFirst, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical web texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,2016) to recall more OpenWebMath-like mathematical web pages. Specifcally, we randomlyi select 500,000 data points from the seed corpus as positive training examples and another 500,000 web pages from Common Crawl as negative ones. We employ an open-source library1 for training, confguring the vector dimension to 256, learning rate to 0.1, the maximum lengthi\n\n\nFigure 2 | An iterative pipeline that collects mathematical web pages from Common Crawl.\nof word n-gram to $3,$ the minimum number of word occurrences to 3, and the number of training epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based deduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then recall mathematical web pages from deduplicated Common Crawl with the fastText model.To flter out low-quality mathematical content, we rank the collected pages according to theiri scores predicted by the fastText model, and only preserve the top-ranking ones. The volume of data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and 160B tokens. In the frst iteration, we choose to keep the top 40B tokens.i\nAfter the frst iteration of data collection, numerous mathematical web pages remain un-i collected, mainly because the fastText model is trained on a set of positive examples that lacks suffcient diversity. We therefore identify additional mathematical web sources to enrich the seedi corpus, so that we can optimize the fastText model. Specifcally, we frst organize the entire Com-ii mon Crawl into disjoint domains; a domain is defned as web pages sharing the same base URL.i For each domain, we calculate the percentage of web pages that are collected in the frst iteration.i Domains where over $10\\%$ of the web pages have been collected are classifed as math-relatedi(e.g., mathoverflow.net). Subsequently, we manually annotate the URLs associated with mathematical content within these identifed domains (e.g., mathoverflow.net/questions).i Web pages linked to these URLs, yet uncollected, will be added to the seed corpus. This approach enables us to gather more positive examples, thereby training an improved fastText model capable of recalling more mathematical data in the subsequent iteration. After four iterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B tokens. In the fourth iteration, we notice that nearly $98\\%$ of the data has already been collected in the third iteration, so we decide to cease data collection.\nTo avoid benchmark contamination, we follow Guo et al. (2024) to flter out web pagesi containing questions or answers from English mathematical benchmarks such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The fltering criteria are as follows: anyi text segment containing a 10-gram string that matches exactly with any sub-string from the evaluation benchmarks is removed from our math training corpus. For benchmark texts that are shorter than 10 grams but have at least 3 grams, we employ exact matching to flter outi contaminated web pages.\n\n2.2. Validating the Quality of the DeepSeekMath Corpus\nWe run pre-training experiments to investigate how the DeepSeekMath Corpus is compared with the recently released math-training corpora:\nMathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from textbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the majority (over $85\\%$ ) sourced from arXiv;\nOpenWebMath (Paster et al., 2023): CommonCrawl data fltered for mathematical content,i totaling 13.6B tokens;\nProof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWebMath, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B tokens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an arXiv:Web:Code ratio of 2:4:1.\n2.2.1. Training Setting\nWe apply math training to a general pre-trained language model with 1.3B parameters, which shares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeekLLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All experiments are conducted using the effcient and light-weight HAI-LLM (High-fyer, 2023)il training framework. Following the training practice of DeepSeek LLMs, we use the AdamW optimizer (Loshchilov and Hutter, 2017) with $\\beta_{1}=0.9,\\beta_{2}=0.95,$ and weight_decay $=0.1,$  along with a multi-step learning rate schedule where the learning rate reaches the peak after 2,000 warmup steps, decreases to its $31.6\\%$ after $80\\%$ of the training process, and further decreases to $10.0\\%$ of the peak after $90\\%$ of the training process. We set the maximum value of learning rate to 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Math Corpus</td><td rowspan=\"2\">Size</td><td colspan=\"5\">English Benchmarks</td><td colspan=\"3\">Chinese Benchmarks</td></tr><tr><td>GSM8K MATH </td><td></td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathClozeMathQA </td><td>Gaokao</td></tr></thead><tbody><tr><td>No Math Training</td><td>N/A</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $2.9\\%$ </td><td> $15.6\\%$ </td><td> $19.5\\%$ </td><td> $12.3\\%$ </td><td> $0.8\\%$ </td><td> $17.9\\%$ </td></tr><tr><td>MathPile</td><td>8.9B</td><td> $2.7\\%$ </td><td> $3.3\\%$ </td><td> $2.2\\%$ </td><td> $12.5\\%$ </td><td> $15.7\\%$ </td><td> $1.2\\%$ </td><td> $0.0\\%$ </td><td> $2.8\\%$ </td></tr><tr><td>OpenWebMath</td><td>13.6B</td><td> $11.5\\%$ </td><td> $8.9\\%$ </td><td> $3.7\\%$ </td><td> $31.3\\%$ </td><td> $29.6\\%$ </td><td> $16.8\\%$ </td><td> $0.0\\%$ </td><td> $14.2\\%$ </td></tr><tr><td>Proof-Pile-2</td><td>51.9B</td><td> $14.3\\%$ </td><td> $11.2\\%$ </td><td> $3.7\\%$ </td><td> $43.8\\%$ </td><td> $29.2\\%$ </td><td> $19.9\\%$ </td><td> $5.1\\%$ </td><td> $11.7\\%$ </td></tr><tr><td>DeepSeekMath Corpus </td><td>120.2B</td><td> $\\mathbf{23.8\\%}$ </td><td> $\\mathbf{13.6\\%}$ </td><td> $4.8\\%$ </td><td> $\\textbf{56.3\\%}$ </td><td> $\\mathbf{33.1\\%}$ </td><td> $\\textbf{41.5\\%}$ </td><td> $5.9\\%$ </td><td> $\\mathbf{23.6\\%}$ </td></tr></tbody></table></body></html>\n\nTable 1 | Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evaluated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer with a vocabulary size of 100K.\n2.2.2. Evaluation Results\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and is the largest in size.\nHigh-quality: We evaluate downstream performance on 8 mathematical benchmarks using few-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear performance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that the model trained on the DeepSeekMath Corpus demonstrates better performance than\n\nFigure 3 | Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.\nProof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of DeepSeekMath Corpus is higher.\nMultilingual: The DeepSeekMath Corpus encompasses data in multiple languages, predominantly featuring English and Chinese as the two most represented languages. As shown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning performance in both English and Chinese. In contrast, existing mathematical corpora,which are primarily English-centric, show limited improvement and may even hinder performance in Chinese mathematical reasoning.\nLarge-scale: The DeepSeekMath Corpus is several times larger than existing mathematical corpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeekMath Corpus, shows a steeper learning curve along with more lasting improvements. In contrast, the baseline corpora are much smaller, and have already been repeated multiple rounds during training, with the resulting model performance quickly reaching a plateau.\n2.3. Training and Evaluating DeepSeekMath-Base 7B\nIn this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning abilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: $56\\%$ is from the DeepSeekMath Corpus, $4\\%$ from AlgebraicStack, $10\\%$ from arXiv, $20\\%$ is Github code, and the remaining $10\\%$ is natural language data from Common Crawl in both English and Chinese. We mainly adopt the training setting specifed in Section 2.2.1, except that we set thei maximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens.\nWe conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMathBase 7B, focusing on its ability to produce self-contained mathematical solutions without relying on external tools, solve mathematical problems using tools, and conduct formal theorem proving.Beyond mathematics, we also provide a more general profle of the base model, including itsi performance of natural language understanding, reasoning, and programming skills.\nMathematical Problem Solving with Step-by-Step ReasoningWe evaluate DeepSeekMathBaseâ€™s performance of solving mathematical problems using few-shot chain-of-thought prompting (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encompass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),and CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks et al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse felds of mathematicsi from elementary to college-level complexity.\nAs shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight benchmarks among the open-source base models (including the widely-used general model Mistral 7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which underwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competitionlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over $10\\%$ absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base model 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained on mathematical texts.\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"4\">English Benchmarks</td><td colspan=\"3\">Chinese Benchmarks</td></tr><tr><td>GSM8K MATH </td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathClozeMathQA</td><td>Gaokao</td></tr></thead><tbody><tr><td colspan=\"10\">Closed-Source Base Model</td></tr><tr><td>Minerva</td><td>7B</td><td> $16.2\\%$ </td><td> $14.1\\%$ </td><td> $7.7\\%$ </td><td>-  $35.6\\%$ </td><td>-</td><td></td><td>-</td><td>-</td></tr><tr><td>Minerva</td><td>62B</td><td> $52.4\\%$ </td><td> $27.6\\%$ </td><td> $12.0\\%$ </td><td>-</td><td> $53.9\\%$ </td><td>-</td><td>-</td><td>-</td></tr><tr><td>Minerva</td><td>540B</td><td> $58.8\\%$ </td><td> $33.6\\%$ </td><td> $17.6\\%$ </td><td>-</td><td> $63.9\\%$ </td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan=\"10\">Open-Source Base Model</td></tr><tr><td>Mistral</td><td>7B</td><td> $40.3\\%$ </td><td> $14.3\\%$ </td><td> $9.2\\%$ </td><td> $71.9\\%$ </td><td> $51.1\\%$ </td><td> $44.9\\%$ </td><td> $5.1\\%$ </td><td> $23.4\\%$ </td></tr><tr><td>Llemma</td><td>7B</td><td> $37.4\\%$ </td><td> $18.1\\%$ </td><td> $6.3\\%$ </td><td> $59.4\\%$ </td><td> $43.1\\%$ </td><td> $43.4\\%$ </td><td> $11.9\\%$ </td><td> $23.6\\%$ </td></tr><tr><td>Llemma</td><td>34B</td><td> $54.0\\%$ </td><td> $25.3\\%$ </td><td> $10.3\\%$ </td><td> $71.9\\%$ </td><td> $52.9\\%$ </td><td> $56.1\\%$ </td><td> $11.9\\%$ </td><td> $26.2\\%$ </td></tr><tr><td>DeepSeekMath-Base 7B</td><td></td><td> $64.2\\%$ </td><td> $\\mathbf{36.2\\%}$ </td><td> $\\mathbf{15.4\\%}$ </td><td> $\\textbf{84.4\\%}$ </td><td> $\\mathbf{56.5\\%}$ </td><td> $\\textbf{71.7\\%}$ </td><td> $\\mathbf{20.3\\%}$ </td><td> $\\mathbf{35.3\\%}$ </td></tr></tbody></table></body></html>\n\nTable 2 | Comparisons between DeepSeekMath-Base 7B and strong base models on English and Chinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting.Minerva results are quoted from Lewkowycz et al. (2022a).\n\nMathematical Problem Solving with Tool UseWe evaluate program-aided mathematical reasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program where libraries such as math and sympy can be utilized for intricate computations. The execution result of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B outperforms the prior state-of-the-art Llemma 34B.\n\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"2\">Problem Solving w/ Tools</td><td colspan=\"2\">Informal-to-Formal Proving</td></tr><tr><td>GSM8K+Python</td><td> MATH+Python</td><td> miniF2F-valid miniF2F-test</td><td></td></tr><tr><td>Mistral</td><td>7B</td><td> $48.5\\%$ </td><td> $18.2\\%$ </td><td> $18.9\\%$ </td><td> $18.0\\%$ </td></tr><tr><td>CodeLlama</td><td>7B</td><td> $27.1\\%$ </td><td> $17.2\\%$ </td><td> $16.3\\%$ </td><td> $17.6\\%$ </td></tr><tr><td>CodeLlama</td><td>34B</td><td> $52.7\\%$ </td><td> $23.5\\%$ </td><td> $18.5\\%$ </td><td> $18.0\\%$ </td></tr><tr><td>Llemma</td><td>7B</td><td> $41.0\\%$ </td><td> $18.6\\%$ </td><td> $20.6\\%$ </td><td> $22.1\\%$ </td></tr><tr><td>Llemma</td><td>34B</td><td> $64.6\\%$ </td><td> $26.3\\%$ </td><td> $21.0\\%$ </td><td> $21.3\\%$ </td></tr><tr><td>DeepSeekMath-Base </td><td>7B</td><td> $66.9\\%$ </td><td> $\\textbf{31.4\\%}$ </td><td> $\\mathbf{25.8\\%}$ </td><td> $\\mathbf{24.6\\%}$ </td></tr></table></body></html>\n\nTable 3 | Few-shot evaluation of base modelsâ€™ ability to solve mathematical problems using tools and the ability to conduct informal-to-formal theorem proving in Isabelle.\nFormal MathematicsFormal proof automation is benefcial to ensure the accuracy and relia-i bility of mathematical proofs and enhance effciency, with increasing attention in recent years.i We evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,2022) which is to generate a formal proof based on an informal statement, a formal counterpart of the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a benchmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each problem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate proof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)to fll in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strongi performance in proof autoformalization.\n\n\n<html><body><table><thead><tr><td>Model</td><td>Size</td><td> MMLU</td><td>BBH</td><td>HumanEval (Pass@1) MBPP (Pass@1)</td><td></td></tr></thead><tbody><tr><td>Mistral</td><td>7B</td><td> $\\mathbf{62.4\\%}$ </td><td> $55.7\\%$ </td><td> $28.0\\%$ </td><td> $41.4\\%$ </td></tr><tr><td>DeepSeek-Coder-Base-v1.5â€ </td><td> 7B</td><td> $42.9\\%$ </td><td> $42.9\\%$ </td><td> $40.2\\%$ </td><td> $52.6\\%$ </td></tr><tr><td>DeepSeek-Coder-Base-v1.5</td><td>7B</td><td> $49.1\\%$ </td><td> $55.2\\%$ </td><td> $\\mathbf{43.2\\%}$ </td><td> $\\mathbf{60.4\\%}$ </td></tr><tr><td>DeepSeekMath-Base</td><td>7B</td><td> $54.9\\%$ </td><td> $\\textbf{59.5\\%}$ </td><td> $40.9\\%$ </td><td> $52.6\\%$ </td></tr></tbody></table></body></html>\n\nTable 4 | Evaluation on natural language understanding, reasoning, and code benchmarks.DeepSeek-Coder-Base-v1.5â€  is the checkpoint right before learning rate decay, which is used to train DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.On HumanEval and MBPP, we evaluate model performance under the zero-shot setting and a few-shot setting, respectively.\nNatural Language Understanding, Reasoning, and CodeWe evaluate model performance of natural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun et al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits signifcant enhancements in per-i formance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),illustrating the positive impact of math training on language understanding and reasoning.Additionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively maintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Overall, DeepSeekMath-Base 7B signifcantly outperforms the general model Mistral 7B (Jiang et al.,i 2023) on the three reasoning and coding benchmarks.\n3.Supervised Fine-Tuning\n3.1. SFT Data Curation\nWe construct a mathematical instruction-tuning dataset covering English and Chinese problems from different mathematical felds and of varying complexity levels: problems are paired withi solutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number of training examples is 776K.\nEnglish mathematical datasets: We annotate GSM8K and MATH problems with toolintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the training set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or PoT. Our English collection covers diverse felds of mathematics, e.g., algebra, probability,i number theory, calculus, and geometry.\nChinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning 76 sub-topics such as linear equations, with solutions annotated in both CoT and toolintegrated reasoning format.\n3.2. Training and Evaluating DeepSeekMath-Instruct 7B\nIn this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruction tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until reaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch size of 256 and a constant learning rate of 5e-5.\nWe evaluate modelsâ€™ mathematical performance both without and with tool use, on 4 quantitative reasoning benchmarks in English and Chinese. We benchmark our model against the leading models of the time:\nClosed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)and GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil et al., 2023), (3) Infection-2 (Infection AI, 2023), (4) Grok-1 3, as well as models recentlyll released by Chinese companies including (5) Baichuan $\\cdot3^{4},$  (6) the latest GLM-4 5 from the GLM family (Du et al., 2022). These models are for general purposes, most of which have undergone a series of alignment procedures.\nOpen-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeekAI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathematics including (5) InternLM2-Math $20\\mathrm{B}^{6}$ which builds on InternLM2 and underwent math training followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO training (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised reward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical reasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,a version of instruction tuning that uses AI-evolved instructions) and PPO training with training problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,2023) which is Llama-2 70B fne-tuned on an augmented version of GSM8K and MATH,i(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fne-tuned to do tool-integratedi mathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B instruction-tuned on MathInstruct.\nAs shown in Table $5,$  under the evaluation setting where tool use is disallowed, DeepSeekMathInstruct 7B demonstrates strong performance of step-by-step reasoning.Notably, on the competition-level MATH dataset, our model surpasses all open-source models and the majority of proprietary models (e.g., Infection-2 and Gemini Pro) by at leastl $9\\%$ absolute. This is true even for models that are substantially larger (e.g., Qwen 72B) or have been specif-i cally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While DeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,it still underperforms GPT-4 and Gemini Ultra.\nUnder the evaluation setting where models are allowed to integrate natural language reasoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches an accuracy of $60\\%$ on MATH, surpassing all existing open-source models. On the other benchmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is 10 times larger.\n4.Reinforcement Learning\n4.1. Group Relative Policy Optimization\nReinforcement learning (RL) has been proven to be effective in further improving the mathematical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;Wang et al., 2023b). In this section, we introduce our effcient and effective RL algorithm, Groupi Relative Policy Optimization (GRPO).\n4.1.1. From PPO to GRPO\nProximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is widely used in the RL fne-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizesi LLMs by maximizing the following surrogate objective:\n$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}\\left[q\\sim P(Q),o\\sim\\pi_{\\theta_{old}}( O|q)\\right]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\min\\left[\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta} (o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\right)A_{t}\\right],$$\n(1)\nwhere $\\pi_{\\theta}$ and $\\pi_{\\theta_{old}}$ are the current and old policy models, and $q,o$ are questions and outputs sampled from the question dataset and the old policy $\\pi_{\\theta_{old}},$  respectively. ðœ€is a clipping-related hyper-parameter introduced in PPO for stabilizing training. $A_{t}$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Size</td><td colspan=\"2\">English Benchmarks</td><td colspan=\"2\"> Chinese Benchmarks</td></tr><tr><td>GSM8K</td><td>MATH</td><td>MGSM-zh </td><td>CMATH</td></tr></thead><tbody><tr><td colspan=\"7\">Chain-of-Thought Reasoning</td></tr><tr><td></td><td></td><td></td><td>Closed-Source Model</td><td></td><td></td><td></td></tr><tr><td>Gemini Ultra</td><td>-</td><td> $94.4\\%$ </td><td></td><td> $53.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>GPT-4</td><td>-</td><td></td><td> $92.0\\%$ </td><td> $52.9\\%$ </td><td>-</td><td> $86.0\\%$ </td></tr><tr><td>Infection-2</td><td>-</td><td></td><td> $81.4\\%$ </td><td> $34.8\\%$ </td><td>-</td><td>-</td></tr><tr><td>GPT-3.5</td><td>-</td><td></td><td> $80.8\\%$ </td><td> $34.1\\%$ </td><td>-</td><td> $73.8\\%$ </td></tr><tr><td>Gemini Pro</td><td>-</td><td></td><td> $86.5\\%$ </td><td> $32.6\\%$ </td><td>-</td><td>-</td></tr><tr><td>Grok-1</td><td>-</td><td></td><td> $62.9\\%$ </td><td> $23.9\\%$ </td><td>-</td><td>-</td></tr><tr><td>Baichuan-3</td><td>-</td><td></td><td> $88.2\\%$ </td><td> $49.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>GLM-4</td><td>-</td><td></td><td> $87.6\\%$ </td><td> $47.9\\%$ </td><td>-</td><td>-</td></tr><tr><td colspan=\"7\">Open-Source Model</td></tr><tr><td>InternLM2-Math</td><td>20B</td><td> $82.6\\%$ </td><td></td><td> $37.7\\%$ </td><td>-</td><td>-</td></tr><tr><td>Qwen</td><td></td><td>72B</td><td> $78.9\\%$ </td><td> $35.2\\%$ </td><td>-</td><td>-</td></tr><tr><td>Math-Shepherd-Mistral</td><td>7B</td><td></td><td> $84.1\\%$ </td><td> $33.0\\%$ </td><td>-</td><td>-</td></tr><tr><td>WizardMath-v1.1</td><td>7B</td><td></td><td> $83.2\\%$ </td><td> $33.0\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeek-LLM-Chat</td><td>67B</td><td></td><td> $84.1\\%$ </td><td> $32.6\\%$ </td><td> $74.0\\%$ </td><td> $80.3\\%$ </td></tr><tr><td>MetaMath</td><td></td><td>70B</td><td> $82.3\\%$ </td><td> $26.6\\%$ </td><td> $66.4\\%$ </td><td> $70.9\\%$ </td></tr><tr><td>SeaLLM-v2</td><td>7B</td><td></td><td> $78.2\\%$ </td><td> $27.5\\%$ </td><td> $64.8\\%$ </td><td>-</td></tr><tr><td>ChatGLM3</td><td>6B</td><td></td><td> $72.3\\%$ </td><td> $25.7\\%$ </td><td>-</td><td>-</td></tr><tr><td>WizardMath-v1.0</td><td>70B</td><td></td><td> $81.6\\%$ </td><td> $22.7\\%$ </td><td> $64.8\\%$ </td><td> $65.4\\%$ </td></tr><tr><td>DeepSeekMath-Instruct</td><td> 7B</td><td></td><td> $82.9\\%$ </td><td> $46.8\\%$ </td><td> $73.2\\%$ </td><td> $84.6\\%$ </td></tr><tr><td>DeepSeekMath-RL</td><td>7B</td><td></td><td> $\\mathbf{88.2\\%}$ </td><td> $\\textbf{51.7\\%}$ </td><td> $79.6\\%$ </td><td> $\\mathbf{88.8\\%}$ </td></tr><tr><td colspan=\"7\">Tool-Integrated Reasoning</td></tr><tr><td></td><td></td><td></td><td>Closed-Source Model</td><td></td><td></td><td></td></tr><tr><td>GPT-4 Code Interpreter</td><td>-</td><td></td><td> $97.0\\%$ </td><td> $69.7\\%$ </td><td>-</td><td>-</td></tr><tr><td colspan=\"7\">Open-Source Model</td></tr><tr><td>InternLM2-Math</td><td></td><td>20B  $80.7\\%$ </td><td></td><td> $54.3\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeek-LLM-Chat</td><td></td><td>67B</td><td> $86.7\\%$ </td><td> $51.1\\%$ </td><td> $76.4\\%$ </td><td> $85.4\\%$ </td></tr><tr><td>ToRA</td><td></td><td>34B</td><td> $80.7\\%$ </td><td> $50.8\\%$ </td><td> $41.2\\%$ </td><td> $53.4\\%$ </td></tr><tr><td>MAmmoTH</td><td>70B</td><td></td><td> $76.9\\%$ </td><td> $41.8\\%$ </td><td>-</td><td>-</td></tr><tr><td>DeepSeekMath-Instruct </td><td>7B</td><td></td><td> $83.7\\%$ </td><td> $57.4\\%$ </td><td> $72.0\\%$ </td><td> $84.3\\%$ </td></tr><tr><td>DeepSeekMath-RL</td><td>7B</td><td></td><td> $\\mathbf{86.7\\%}$ </td><td> $\\textbf{58.8\\%}$ </td><td> $78.4\\%$ </td><td> $\\textbf{87.6\\%}$ </td></tr></tbody></table></body></html>\n\nTable 5 | Performance of Open- and Closed-Source models with both Chain-of-Thought and Tool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority votes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all opensource models from 7B to 70B, as well as the majority of closed-source models. Although DeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks.\n\nFigure 4 | Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead estimating the baseline from group scores, signifcantly reducing training resources.i\non the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$  Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model,the standard approach is to add a per-token KL penalty from a reference model in the reward at each token (Ouyang et al., 2022), i.e.,\n$$r_{t}=r_{\\varphi}(\\mathit{q},\\mathit{o}_{\\leq t})-\\beta\\log\\frac{\\pi_{\\theta} (\\mathit{o}_{t}|\\mathit{q},\\mathit{o}_{<t})}{\\pi_{\\mathit{ref}}(\\mathit{o}_{t}|\\mathit{q},\\mathit{o}_{<t})},$$\n(2)\nwhere $r_{\\varphi}$ is the reward model, $\\pi_{ref}$ is the reference model, which is usually the initial SFT model,and $\\beta$ is the coeffcient of the KL penalty.i\nAs the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. Additionally,during RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token. To address this, as shown in Figure $4,$  we propose Group Relative Policy Optimization (GRPO), which obviates the need for additional value function approximation as in PPO, and instead uses the average reward of multiple sampled outputs, produced in response to the same question, as the baseline. More specifcally, for each questioni $q,$  GRPO samples a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model by maximizing the following objective:\n$$\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P( \\small{\\it Q}),\\{o_{i} \\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}( \\small{\\it O}|q)]$$\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|} \\left\\{\\min\\left[\\frac{\\pi_{\\theta}(o_{i,t}|\\mathsf{q},o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|\\mathsf{q},o_{i,<t})}\\hat{A}_{i,t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{i,t}|\\mathsf{q},o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|\\mathsf{q}, o_{i,<t})},1-\\varepsilon,1+\\varepsilon\\right)\\hat{A}_{i,t}\\right]-\\beta\\mathbb{D}_{KL}\\left[\\pi_{\\theta}||\\pi_{ref}\\right]\\right\\},$$\n(3)\nwhere ðœ€and $\\beta$ are hyper-parameters, and $\\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only, which will be detailed in the following subsections. The group relative way that GRPO leverages to calculate the advantages, aligns well with the comparative nature of rewards models, as reward models are typically trained on datasets of comparisons between outputs on the same question. Also note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}.$\n\nAlgorithm 1 Iterative Group Relative Policy Optimization\nput initial policy model $\\pi_{\\theta_{\\rm init}};$  reward models $r_{\\varphi};$  task prompts $\\mathcal{D}$  hyperparameters $\\varepsilon, \\beta, \\mu$\n1: policy model $\\pi_{\\theta}\\leftrightarrow\\pi_{\\theta_{\\text{init}}}$\n2: for iteration $=1,\\ldots,\\mathrm{I}$  do\n3:reference model $\\pi_{ref}\\leftrightarrow\\pi_{\\theta}$\n4:for step = 1, ..., M do\n5:Sample a batch $\\mathcal{D}_{b}$ from $\\mathcal{D}$\n6:Update the old policy model $\\pi_{\\theta_{old}}\\leftrightarrow\\pi_{\\theta}$\n7:Sample $G$ outputs $\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\cdot\\mid q)$  for each question $q\\in\\mathcal{D}_{b}$\n8:Compute rewards $\\{r_{i}\\}_{i=1}^{G}$  for each sampled output $o_{i}$ by running $r_{\\varphi}$\n9:Compute $\\widehat{A}_{i,t}$ for the $t$ th token of $o_{i}$ through group relative advantage estimation.\n10:for GRPO iteration $=1,\\ldots,\\mu$ do\n11:Update the policy model $\\pi_{\\theta}$ by maximizing the GRPO objective (Equation 21)\n12:Update $r_{\\varphi}$ through continuous training using a replay mechanism.\nOutput $\\pi_{\\theta}$\nAnd different from the KL penalty term used in (2), we estimate the KL divergence with the following unbiased estimator (Schulman, 2020):\n$$\\text{I D}_{KL}\\left[\\pi_{\\theta}||\\pi_{ref}\\right]=\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-\\log\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-1,$$\n(4)\nwhich is guaranteed to be positive.\n4.1.2. Outcome Supervision RL with GRPO\nFormally, for each question $q,$  a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  are sampled from the old policy model $\\pi_{\\theta_{old}}.$  A reward model is then used to score the outputs, yielding $G$ rewards $\\mathbf{r}=\\{r_{1},r_{2},\\cdots,r_{G}\\}$  correspondingly. Subsequently, these rewards are normalized by subtracting the group average and dividing by the group standard deviation. Outcome supervision provides the normalized reward at the end of each output $o_{i}$ and sets the advantages $\\hat{A}_{i,t}$ of all tokens in the output as the normalized reward, i.e., $\\hat{A}_{i,t}=\\widetilde{r}_{i}=\\tfrac{r_{i}-\\mathrm{mean}(\\mathbf{r})}{\\mathrm{std}(\\mathbf{r})},$ and then optimizes the policy by maximizing the objective defned in equation (3).i\n4.1.3. Process Supervision RL with GRPO\nOutcome supervision only provides a reward at the end of each output, which may not be suffcient and effcient to supervise the policy in complex mathematical tasks. Following Wangii et al. (2023b), we also explore process supervision, which provides a reward at the end of each reasoning step. Formally, given the question $q$ and $G$ sampled outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\},$  a process reward model is used to score each step of the outputs, yielding corresponding rewards: $\\mathbf{R}=\\{\\{r_{1}^{index}{}^{(1)},\\cdots,r_{1}^{index}{}^{(K_{1})}\\}, \\cdots,\\{r_{G}^{index}{}^{(1)},\\cdots,r_{G}^{index}{}^{(K_{G})}\\}\\},$  where $index(j)$  is the end token index of the $j$ th step, and $K_{i}$ is the total number of steps in the $i$ th output. We also normalize these rewards with the average and the standard deviation, i.e. $\\underline{\\widetilde{r}_{i}^{index(j)}}=\\underline{\\tfrac{r_{i}^{index(j)}-\\mathbf{mean(R)}}{\\mathbf{std(R)}}}.$  Subsequently,the process supervision calculates the advantage of each token as the sum of the normalized rewards from the following steps, i.e., $\\begin{array}{l}\\hat{A}_{i,t}=\\sum_{index(j)\\geq t}\\widetilde{r}_{i}^{index (j)},\\end{array}$  and then optimizes the policy by maximizing the objective defned in equation (3).i\n\n4.1.4. Iterative RL with GRPO\nAs the reinforcement learning training process progresses, the old reward model may not be suffcient to supervise the current policy model. Therefore, we also explore the iterative RLi with GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the reward model based on the sampling results from the policy model and continually train the old reward model using a replay mechanism that incorporates $10\\%$ of historical data. Then, we set the reference model as the policy model, and continually train the policy model with the new reward model.\n4.2. Training and Evaluating DeepSeekMath-RL\nWe conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-ofthought-format questions related to GSM8K and MATH from the SFT data, which consists of around 144K questions. We exclude other SFT questions to investigate the impact of RL on benchmarks that lack data throughout the RL phase. We construct the training set of reward models following (Wang et al., 2023b). We train our initial reward model based on the DeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the policy model as 1e-6. The KL coeffcient is 0.04. For each question, we sample 64 outputs. Thei max length is set to 1024, and the training batch size is 1024. The policy model only has a single update following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks following DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with chain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks can be regarded as out-of-domain tasks.\nTable 5 demonstrates the performance of open- and closed-source models with both chainof-thought and tool-integrated reasoning on English and Chinese benchmarks. We fnd that:i 1) DeepSeekMath-RL 7B attains accuracies of $88.2\\%$ and $51.7\\%$ on GSM8K and MATH, respectively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source models in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,DeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of GSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope of its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,showcasing the effectiveness of reinforcement learning.\n5.Discussion\nIn this section, we will share our fndings in pre-training and RL experiments.i\n5.1. Lessons Learnt in Pre-Training\nWe frst share our experience in pre-training. Unless otherwise specifed, we will adhere toii the training settings outlined in Section 2.2.1. It is worth noting that, when referring to the DeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of the data collection process.\n5.1.1. Code Training Benefts Mathematical Reasoningi\nA popular yet unverifed hypothesis suggests that code training improves reasoning. We attempti to offer a partial response to this, particularly within the mathematical domain: code training\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Training Setting</td><td colspan=\"3\">Training Tokens</td><td colspan=\"3\">w/o Tool Use</td><td colspan=\"2\">w/ Tool Use</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>General Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python</td><td></td></tr></thead><tbody><tr><td>No Continual Training</td><td>â€“</td><td>â€“</td><td>â€“</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $12.3\\%$ </td><td> $2.7\\%$ </td><td> $2.3\\%$ </td></tr><tr><td colspan=\"9\">Two-Stage Training</td></tr><tr><td>Stage 1: General Training</td><td>400B</td><td>â€“</td><td>â€“</td><td> $2.9\\%$ </td><td> $3.2\\%$ </td><td> $14.8\\%$ </td><td> $3.3\\%$ </td><td> $2.3\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $19.1\\%$ </td><td> $14.4\\%$ </td><td> $37.2\\%$ </td><td> $14.3\\%$ </td><td> $6.7\\%$ </td></tr><tr><td>Stage 1: Code Training</td><td>â€“</td><td>400B </td><td>â€“</td><td> $5.9\\%$ </td><td> $3.6\\%$ </td><td> $19.9\\%$ </td><td> $12.4\\%$ </td><td> $10.0\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>-</td><td>150B</td><td> $21.9\\%$ </td><td> $\\mathbf{15.3\\%}$ </td><td> $39.7 \\%$ </td><td> $17.4\\%$ </td><td> $9.4\\%$ </td></tr><tr><td colspan=\"9\">One-Stage Training</td></tr><tr><td>Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $20.5\\%$ </td><td> $13.1\\%$ </td><td> $37.6\\%$ </td><td> $11.4\\%$ </td><td> $6.5\\%$ </td></tr><tr><td>Code & Math Mixed Training â€“</td><td></td><td>400B</td><td> 150B</td><td> $17.6\\%$ </td><td> $12.1\\%$ </td><td> $36.3\\%$ </td><td> $\\mathbf{19.7\\%}$ </td><td> $\\mathbf{13.5\\%}$ </td></tr></tbody></table></body></html>\n\nTable 6 | Investigation of how code affects mathematical reasoning under different training settings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning performance without and with tool use via few-shot chain-of-thought prompting and few-shot program-of-thought prompting, respectively.\noves modelsâ€™ ability to do mathematical reasoning both with and without tool use\nTo study how code training affects mathematical reasoning, we experimented with the following two-stage training and one-stage training settings:\nTwo-Stage Training\nCode Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: We train DeepSeekLLM 1.3B for 400B code tokens followed by 150B math tokens;\nGeneral Training for 400B Tokens $\\rightarrow$ Math Training for 150B Tokens: As a control experiment, we also experiment with general tokens (sampled from a large-scale general corpus created by DeepSeek-AI) instead of code tokens in the frst stage of training, in ani attempt to investigate the advantages of code tokens over general tokens in improving mathematical reasoning.\nOne-Stage Training\nMath Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\nTraining on a mixture of 400B Code Tokens and 150B Math Tokens: Math training following code training degrades coding performance. We investigate whether code tokens,when mixed with math tokens for one-stage training, would still improve mathematical reasoning and also alleviate the problem of catastrophic forgetting.\nResultsTable 6 and Table 7 demonstrate the downstream performance under different training settings.\nCode training benefts program-aided mathematical reasoning, both under the two-stagei training and one-stage training settings. As shown in Table 6, under the two-stage training setting, code training alone already signifcantly enhances the ability to solve GSM8K andi MATH problems using Python. Math training in the second stage yields further improvements.Interestingly, under the one-stage training setting, mixing code tokens and math tokens effectively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also synergizes coding (Table 7) and program-aided mathematical reasoning (Table 6).\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Training Setting</td><td colspan=\"3\">Training Tokens</td><td rowspan=\"2\">MMLU</td><td rowspan=\"2\">BBHHumanEval (Pass@1) MBPP (Pass@1)</td><td rowspan=\"2\"></td></tr><tr><td>General Code Math</td><td></td></tr></thead><tbody><tr><td>No Continual Training</td><td>â€“</td><td>â€“ â€“</td><td> $24.5\\%$ </td><td> $28.1\\%$ </td><td> $12.2\\%$ </td><td> $13.0\\%$ </td></tr><tr><td colspan=\"7\">Two-Stage Training</td></tr><tr><td>Stage 1: General Training</td><td>400B</td><td>â€“</td><td>-</td><td> $25.9\\%$ </td><td> $27.7\\%$ </td><td> $15.2\\%$  $13.6\\%$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $33.1\\%$ </td><td> $32.7\\%$  $12.8\\%$ </td><td> $13.2\\%$ </td></tr><tr><td>Stage 1: Code Training</td><td>â€“</td><td>400B -</td><td></td><td> $25.0\\%$ </td><td> $31.5\\%$  $25.0\\%$ </td><td> $\\mathbf{40.0\\%}$ </td></tr><tr><td>Stage 2: Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $\\mathbf{36.2\\%}$ </td><td> $35.3\\%$  $12.2\\%$ </td><td> $17.0\\%$ </td></tr><tr><td colspan=\"7\">One-Stage Training</td></tr><tr><td>Math Training</td><td>â€“</td><td>â€“</td><td>150B</td><td> $32.3\\%$  $32.5\\%$ </td><td> $11.6\\%$  $\\mathbf{29.3\\%}$ </td><td> $13.2\\%$ </td></tr><tr><td>Code & Math Mixed Training </td><td>-</td><td>400B 150B</td><td></td><td> $33.5\\%$ </td><td> $\\mathbf{35.6\\%}$ </td><td> $39.4\\%$ </td></tr></tbody></table></body></html>\n\nTable 7 | Investigation of how different settings of code and math training affect model performance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM 1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting.On HumanEval and MBPP, we conduct zero-shot and few-shot evaluations, respectively.\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\"></td><td rowspan=\"2\">Size ArXiv Corpus</td><td colspan=\"4\">English Benchmarks</td><td colspan=\"4\">Chinese Benchmarks</td></tr><tr><td></td><td>GSM8K MATH </td><td>OCW</td><td>SAT</td><td>MMLU STEM</td><td>CMATH</td><td>Gaokao MathCloze</td><td>Gaokao MathQA</td></tr></thead><tbody><tr><td rowspan=\"3\">DeepSeek-LLM</td><td rowspan=\"3\">1.3B</td><td>No Math Training</td><td> $2.9\\%$ </td><td> $3.0\\%$ </td><td> $2.9\\%$ </td><td> $15.6\\%$ </td><td> $19.5\\%$ </td><td> $12.3\\%$ </td><td> $0.8\\%$ </td><td> $17.9\\%$ </td></tr><tr><td>MathPile</td><td> $2.7\\%$ </td><td> $3.3\\%$ </td><td> $2.2\\%$ </td><td> $12.5\\%$ </td><td> $15.7\\%$ </td><td> $1.2\\%$ </td><td> $0.0\\%$ </td><td> $2.8\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $3.3\\%$ </td><td> $3.4\\%$ </td><td> $4.0\\%$ </td><td> $9.4\\%$ </td><td> $9.0\\%$ </td><td> $7.4\\%$ </td><td> $0.8\\%$ </td><td> $2.3\\%$ </td></tr><tr><td rowspan=\"3\">DeepSeek-Coder-Base-v1.5 7B</td><td rowspan=\"3\"></td><td>No Math Training</td><td> $29.0\\%$ </td><td> $12.5\\%$ </td><td> $6.6\\%$ </td><td> $40.6\\%$ </td><td> $38.1\\%$ </td><td> $45.9\\%$ </td><td> $5.9\\%$ </td><td> $21.1\\%$ </td></tr><tr><td>MathPile</td><td> $23.6\\%$ </td><td> $11.5\\%$ </td><td> $7.0\\%$ </td><td> $46.9\\%$ </td><td> $35.8\\%$ </td><td> $37.9\\%$ </td><td> $4.2\\%$ </td><td> $25.6\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $28.1\\%$ </td><td>11.1%</td><td> $7.7\\%$ </td><td> $50.0\\%$ </td><td> $35.2\\%$ </td><td> $42.6\\%$ </td><td> $7.6\\%$ </td><td> $24.8\\%$ </td></tr></tbody></table></body></html>\n\nTable 8 | Effect of math training on different arXiv datasets. Model performance is evaluated with few-shot chain-of-thought prompting.\n\n\n<html><body><table><tr><td>ArXiv Corpus</td><td>miniF2F-valid</td><td> miniF2F-test</td></tr><tr><td>No Math Training</td><td> $20.1\\%$ </td><td> $21.7\\%$ </td></tr><tr><td>MathPile</td><td> $16.8\\%$ </td><td> $16.4\\%$ </td></tr><tr><td>ArXiv-RedPajama</td><td> $14.8\\%$ </td><td> $11.9\\%$ </td></tr></table></body></html>\n\nTable 9 | Effect of math training on different arXiv corpora, the base model being DeepSeekCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.\nCode training also improves mathematical reasoning without tool use. Under the two-stage training setting, the initial stage of code training already results in moderate enhancements.It also boosts the effciency of the subsequent math training, eventually leading to the besti performance. However, combining code tokens and math tokens for one-stage training compromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,due to its limited scale, lacks the capacity to fully assimilate both code and mathematical data simultaneously.\n5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev et al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,detailed analysis regarding their impact on mathematical reasoning has not been extensively conducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem ineffective in improving mathematical reasoning. We experiment with models of different sizes,including DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv corpora that underwent varied processing pipelines:\nMathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and flteringi heuristic rules, over $85\\%$ of which are scientifc arXiv papers;i\nArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX fles with preambles,i comments, macros, and bibliographies removed, totaling 28.0B tokens.\nIn our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeekCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective in improving mathematical reasoning. When trained on a arXiv-only corpus, both models display no notable improvements or even deterioration across various mathematical benchmarks of different complexities employed in this study. These benchmarks include quantitative reasoning datasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table 8), and formal mathematics like miniF2F (Table 9).\nHowever, this conclusion has its limitations and should be taken with a grain of salt. We have not yet studied:\nThe impact of arXiv tokens on specifc math-related tasks not included in this research,i such as informalization of theorems which is to convert formal statements or proofs to their informal versions;\nThe effect of arXiv tokens when combined with other types of data;\nâ€¢ Whether the benefts of arXiv papers would manifest themselves at a larger model scale.i Thus, further exploration is required, which we leave for future studies.\n5.2. Insights of Reinforcement Learning\n5.2.1. Towards to a Unifed Paradigmi\nIn this section, we provide a unifed paradigm to analyze different training methods, such asi SFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the unifed paradigm. Generally, the gradient with respect to the parameteri $\\theta$ of a training method can be written as:\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathcal{A}}(\\theta)=\\mathbb{E}[\\underbrace{(q,o) \\sim\\mathcal{D}}_{Data Source}]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\underbrace{\\mathit{GC}_{\\mathcal{A}}(q,o,t,\\pi_{rf})}_{\\mathit{Gradient Coefficient}} \\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n(5)\nThere exist three key components: 1) Data Source $\\mathcal{D},$  which determines the training data; 2)Reward Function $\\pi_{rf},$  which is the source of the training reward signal; 3) Algorithm $\\mathcal{A}\\text{:}$  which processes the training data and the reward signal to the gradient coeffcienti $G\\mathcal{C}$ that determines the magnitude of the penalty or reinforcement for the data. We analyze several representative methods based on such a unifed paradigm:i\nSupervised Fine-tuning (SFT): SFT fne-tunes pretrained model on human selected SFTi data.\n\n\n\n<html><body><table><thead><tr><td>Methods</td><td>Data Source</td><td>Reward Function</td><td>Gradient Coeffcient</td></tr></thead><tbody><tr><td>SFT</td><td> $q,o\\sim P_{sft}(\\mathsf{Q},\\mathsf{O})$ </td><td>-</td><td>1</td></tr><tr><td>RFT</td><td> $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{sft}(\\mathsf{O}|q)$ </td><td>Rule</td><td>Equation 10</td></tr><tr><td>DPO</td><td> $q \\sim P_{sft}( Q ), o^{+}, o^{-}\\sim\\pi_{sft}(\\mathsf{O}| q )$ </td><td>Rule</td><td>Equation 14</td></tr><tr><td>Online RFT</td><td> $q\\sim P_{sft}(\\mathsf{Q}), o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ </td><td>Rule</td><td>Equation 10</td></tr><tr><td>PPO</td><td> $q\\sim P_{sft}(\\mathsf{Q})\\text{,}o\\sim\\pi_{\\theta}(\\mathsf{O}|q)$ </td><td>Model</td><td>Equation 18</td></tr><tr><td>GRPO</td><td> $\\boldsymbol{q\\sim P_{sft}(Q),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta}(O|q)}$ </td><td>Model</td><td>Equation 21</td></tr></tbody></table></body></html>\n\nTable 10 | The data source and gradient coeffcient of different methods.i $P_{sft}$ denotes the data distribution of supervised fne-tuning datasets.i $\\pi_{\\theta_{sft}}$ and $\\pi_{\\theta}$ denote the supervised fne-tunedi model and the real-time policy model during the online training process, respectively.\nFigure 5 | Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained using various methods, on two benchmarks.\nRejection Sampling Fine-tuning (RFT): RFT further fne-tunes the SFT model on thei fltered outputs sampled from the SFT model based on SFT questions. RFT flters theii outputs based on the correctness of their answers.\nDirect Preference Optimization (DPO): DPO further refnes the SFT model by fne-tuningii it on augmented outputs sampled from the SFT model, using pair-wise DPO loss.\nOnline Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT initiates the policy model using the SFT model and refnes it by fne-tuning with theii augmented outputs sampled from the real-time policy model.\nPPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces it with the outputs sampled from the real-time policy model.\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a more detailed derivation process.\nObservation about Data SourceWe divide the data source into two categories, online sampling, and offine sampling. Online sampling denotes that the training data is from the explo-l ration results of the real-time training policy model, while offine sampling denotes that thel\n\nFigure 6 | Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on two benchmarks.\ntraining data is from the sampling results of the initial SFT model. RFT and DPO follow the offine style, while Online RFT and GRPO follow the online style.l\nAs shown in Figure 5, we fnd that the Online RFT signifcantly outperforms RFT on twoii benchmarks. Specifcally, Online RFT is comparable to RFT in the early stage of training buti gains an absolute advantage in the later stage, demonstrating the superiority of online training.This is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,with the sampled data revealing only minor differences. In the later stage, however, the data sampled from the actor will exhibit more signifcant differences, and real-time data samplingi will offer greater advantages.\nObservation about Gradient CoeffcientThe algorithm processes the reward signal to thei gradient coeffcient to update the model parameter. We divide the reward function as â€˜Ruleâ€™i and â€˜Modelâ€™ in our experiments. Rule refers to judging the quality of a response based on the correctness of the answer, and Model denotes that we train a reward model to score each response. The training data of the reward model is based on the rule judgment. Equations 10 and 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its gradient coeffcient based on the reward value provided by the reward model. This allows fori differential reinforcement and penalization of responses according to their varying magnitudes.In contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly reinforces all responses with correct answers at the same level of intensity.\nAs demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the effciencyi of altering positive and negative gradient coeffcients. In addition, GRPO+PS shows superiori performance compared to GRPO+OS, indicating the benefts of using fne-grained, step-awareii gradient coeffcients. Furthermore, we explore the iterative RL, in our experiments, we conducti two rounds of iteration. As shown in Figure $6,$  we notice that the iterative RL signifcantlyi improves the performance, especially at the frst iteration.i\n\nFigure 7 | The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K.\n5.2.2. Why RL Works?\nIn this paper, we conduct reinforcement learning based on a subset of instruction tuning data, and it achieves signifcant performance enhancement upon the instruction tuning model.i To further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K accuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances Maj@Kâ€™s performance but not Pass@K. These fndings indicate that RL enhances the modelâ€™si overall performance by rendering the output distribution more robust, in other words, it seems that the improvement is attributed to boosting the correct response from TopK rather than the enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identifed ai misalignment problem in reasoning tasks within the SFT model, showing that the reasoning performance of SFT models can be improved through a series of preference alignment strategies(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b).\n5.2.3. How to Achieve More Effective RL?\nWe demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unifedi paradigm to understand different representative training methods. Within this paradigm, all methods are conceptualized as either direct or simplifed RL techniques. As summarized ini Equation 5, there exist three key components: Data Source, Algorithm, and Reward Function.We provide some potential future directions about the three components.\nData SourceData source is the raw material of all training methods. In the context of $\\mathrm{RL},$ we specifcally refer to the data source as the unlabeled questions with the outputs sampled fromi the policy model. In this paper, we only use the questions from the instruction tuning stage and a naive nucleus sampling to sample outputs. We think this is a potential reason that our RL pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline on out-of-distribution question prompts, in conjunction with advanced sampling (decoding)strategies, like those based on tree-search methods (Yao et al., 2023). Also, the effcient inferencei techniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determinesthe exploration effciency of policy models, also play an exceedingly important role.i\nAlgorithmsAlgorithms process the data and reward signal to the gradient coeffcient to updatei the model parameter. Based on Equation $5,$  to some extent, all methods now fully TRUST the signal of the reward function to increase or decrease the conditional probability of a certain token. However, it is impossible to ensure the reward signal is always reliable, especially in extremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023), which have been carefully annotated by well-trained annotators, still contain approximately $20\\%$ of incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm that is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,2023) alignment methods will bring a fundamental change to the learning algorithms.\nReward FunctionReward function is the source of the training signal. In RL, the reward function is usually the neural reward model. We think there exist three important directions for reward models: 1) How to enhance the generalization ability of the reward model. The reward model must be effectively generalized to handle out-of-distribution questions and advanced decoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of LLMs rather than improve their fundamental capabilities; 2) How to refect the uncertaintyl of reward model. The uncertainty could potentially act as a linking bridge between the weak reward model and the weak-to-strong learning algorithms; 3) How to effciently build high-i quality process reward models that can provide fne-grained training signals for the reasoningi process (Lightman et al., 2023; Wang et al., 2023b).\n6.Conclusion, Limitation, and Future Work\nWe present DeepSeekMath, which outperforms all open-source models on the competitionlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is initialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with a signifcant component of the training data being 120B math tokens sourced from Commoni Crawl. Our extensive ablation study shows web pages offer signifcant potential for high-qualityi mathematical data, while arXiv may not as benefcial as we expected. We introduce Groupi Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which can notably improve mathematical reasoning capabilities with less memory consumption. The experiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached a high score on benchmarks. We also provide a unifed paradigm to understand a series ofi methods and summarize several potential directions for more effective reinforcement learning.\nAlthough DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,its capability on geometry and theorem-proof are relatively weaker than closed models. For instance, in our dry run, the model cannot handle problems related to triangles and ellipses,which may indicate data selection bias in pre-training and fne-tuning. In addition, restrictedi by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while DeepSeekMath shows similar performance in zero-shot and few-shot evaluation. In the future, we will further improve our engineered data selection pipeline to construct more high-quality pre-trained corpus. In addition, we will explore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs.\n\nReferences\nR. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,E. Pitler, T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan,B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,M. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,2021.\nZ. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\nC. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,M. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.\nChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c om/THUDM/ChatGLM3.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.URL https://arxiv.org/abs/2107.03374.\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,J. Hilton, R. Nakano, et al. Training verifers to solve math word problems. arXiv preprinti arXiv:2110.14168, 2021.\nT. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL https://github.com/togethercomputer/RedPajama-Data.\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,abs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485 50/arXiv.2401.02954.\n\nZ. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320â€“335,2022.\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. PAL: programaided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,pages 10764â€“10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html.\nZ. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A toolintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.doi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745 2.\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programmingâ€“ the rise of code intelligence, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 2021.\nHigh-fyer. Hai-llm: é«˜æ•ˆä¸”è½»é‡çš„å¤§æ¨¡åž‹è®­ç»ƒå·¥å…·, 2023. URL https://www.high-flyer.cl n/en/blog/hai-llm.\nInfection AI. Infection-2, 2023. URL https://inflection.ai/inflection-2.ll\nA. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\nA. Joulin, E. Grave, P. Bojanowski, M. Douze, H. JÃ©gou, and T. Mikolov. Fasttext. zip: Compressing text classifcation models. arXiv preprint arXiv:1612.03651, 2016.i\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.Effcient memory management for large language model serving with pagedattention. Ini Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274â€“19286. PMLR,2023.\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843â€“3857, 2022a.\n\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone,C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr act-Conference.html.\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,I. Sutskever, and K. Cobbe. Letâ€™s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nI. Loshchilov and F. Hutter.Decoupled weight decay regularization.arXiv preprint arXiv:1711.05101, 2017.\nH. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\nS. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sabharwal, P. Clark, and A. Kalyan. LILA: A unifed benchmark for mathematical reasoning.i In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5807â€“5832. Association for Computational Linguistics,2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/2022.emnlp-main.392.\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,C. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,abs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485 50/arXiv.2312.00738.\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.Advances in Neural Information Processing Systems, 35:27730â€“27744, 2022.\nK. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL https://doi.org/10.48550/arXiv.2310.06786.\nL. C. Paulson. Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,Proceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning, PAAR-2010,Edinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1â€“10.EasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd.\nS. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. 2023.\n\nJ. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app rox.html.\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=fR3wGCk-IXp.\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.\nM. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nT. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro soft.com/ai-anthology/terence-tao/.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fne-tuned chat models. CoRR, abs/2307.09288,i 2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.\nT. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476â€“482, 2024.\nP. Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b.\nZ. Wang, R. Xia, and P. Liu. Generative AI for math: Part I - mathpile: A billion-token-scale pretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120.URL https://doi.org/10.48550/arXiv.2312.17120.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.URL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf 4f15af0f7b31abca4-Abstract-Conference.html.\n\nT. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.\nM. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A.MuÃ±oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International Conference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170 of Lecture Notes in Computer Science, pages 33â€“38. Springer, 2008. doi: 10.1007/978-3-540-7 1067-7\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7.\nH. Xia, T. Ge, P. Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909â€“3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20 23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.\nH. Xia, Z. Yang, Q. Dong, P. Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking effciencyi in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffths, Y. Cao, and K. Narasimhan. Tree of thoughts:i Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,2023.\nL. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.Metamath: Bootstrap your own mathematical questions for large language models. CoRR,abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485 50/arXiv.2309.12284.\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a.\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b.\nX. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653.\nK. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021.\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.\n\nA. Appendix\nA.1. Analysis of Reinforcement Learning\nWe provide the detailed derivation of the data source and gradient coeffcient (algorithm andi reward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and GRPO.\nA.1.1. Supervised Fine-tuning\nThe objective of Supervised Fine-tuning is maximizing the following objective:\n$$\\mathcal{J}_{S F T}(\\theta)=\\mathbb{E}[q,o\\sim P_{sft}(\\mathtt{Q},\\mathtt{O})]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n(6)\nThe gradient of $\\mathcal{J}_{{ST T}}(\\theta)$  is:\n$$\\nabla_{\\theta}\\mathcal{J}_{SFT}=\\mathbb{E}\\left[q,o\\sim P_{sft}(Q,O)\\right] \\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n(7)\nData Source: The dataset employed for SFT. Reward Function: This can be regarded as human selection. Gradient Coeffcient: always set to 1.i\nA.1.2. Rejection Sampling Fine-tuning\nRejection Sampling Fine-tuning frst samples multiple outputs from the supervised fne-tunedii LLMs for each question, and then trains LLMs on the sampled outputs with the correct answer.Formally, the objective of RFT is to maximize the following objectives:\n$$\\mathcal{J}_{\\mathit{RFT}}(\\theta)=\\mathbb{E}\\left[q\\sim P_{\\mathit{sft}}( \\mathit{Q}),o\\sim\\pi_{\\mathit{sft}}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|} \\sum_{t=1}^{|o|}\\mathbb{I}\\hskip-1.0pt\\mathbb{I}(o)\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n(8)\nThe gradient of $\\mathcal{J}_{{R} {F} {T}}(\\theta)$  is:\n$$\\nabla_{\\theta}\\mathcal{J}_{RFT}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}( \\mathit{Q}),o\\sim\\pi_{sft}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\mathbb{I}\\hskip0.5pt(o)\\hskip0.5pt\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}|q,o_{<t})\\right).$$\n(9)\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:Rule (whether the answer is correct or not). Gradient Coeffcient:i\n$$\\mathit{GC}_{\\mathit{RFT}}(\\mathit{q},\\mathit{o},t)=\\mathbb{I}(\\mathit{o})=\\left\\{\\begin{aligned} & 1&\\mathrm{the answer of o is correct}\\\\ & 0&\\mathrm{the answer of o is incorrect}\\end{aligned}\\right.$$\n(10)\nA.1.3. Online Rejection Sampling Fine-tuning\nThe only difference between RFT and Online RFT is that the outputs of Online RFT are sampled from the real-time policy model $\\pi_{\\theta},$  rather than from the SFT model $\\pi_{\\theta_{sft}}.$  Therefore, the gradient of online RFT is:\n$$\\nabla_{\\theta}\\mathcal{J}_{\\texttt{OnRFT}}(\\theta)=\\mathbb{E}\\left[q\\sim P_{\\mathit{sft}}(\\mathit{Q}),o\\sim\\pi_{\\theta}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\mathbb{I}\\hskip0.5pt(o)\\hskip0.5pt\\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}|q,\\mathit{o}_{<t})\\right).$$\n(11)\n\nA.1.4. Direct Preference Optimization (DPO)\nThe objective of DPO is:\n$$\\mathcal{J}_{DPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(Q),o^{+},o^{-} \\sim\\pi_{sft}(O|q)]\\\\ \\log\\sigma\\left(\\beta\\frac{1}{|o^{+}|}\\sum_{t=1}^{|o^{+}|}\\log\\frac{\\pi_{\\theta}(o_{t}^{+}|q,o_{<t}^{+})}{\\pi_{\\mathrm{ref}}(o_{t}^{+}|q,o_{<t}^{+})}-\\beta\\frac{1}{|o^{-}|}\\sum_{t=1}^{|o^{-}|}\\log\\frac{\\pi_{\\theta}(o_{<t}^{-}|q,o_{<t}^{-})}{\\pi_{\\mathrm{ref}}(o_{<t}^{-}|q,o_{<t}^{-})}\\right)$$\n(12)\nThe gradient of $\\mathcal{J}_{DPO}(\\theta)$  is:\n$$\\nabla_{\\theta}\\mathcal{J}_{DPO}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}( \\mathit{Q}),o^{+},o^{-}\\sim\\pi_{sft}(\\mathit{O}|q)\\right]\\left(\\frac{1}{|o^{+} |}\\sum_{t=1}^{|o^{+}|}\\mathit{GC}_{DPO}(\\mathit{q},\\mathit{o},t)\\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}^{+}|\\mathit{q},\\mathit{o}_{<t}^{+})\\right.$$\n(13)\n$$-\\left.\\frac{1}{|o^{-}|}\\sum_{t=1}^{|o^{-}|}GC_{DPO}(q,o,t)\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{t}^{-}|q,o_{<t}^{-})\\right)$$\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:human preference in the general domain (can be â€˜Ruleâ€™ in mathematical tasks). Gradient Coeffcient:iâˆ’âˆ’++\n$$\\mathit{GC}_{DPO}(q,o,t)=\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(o_{t}^{-}|q,o_{<t}^{-})}{\\pi_{ref}(o_{t}^{-}|q,o_{<t}^{-})}-\\beta\\log\\frac{\\pi_{\\theta}(o_{t}^{+}|q,o_{<t}^{+})}{\\pi_{ref}(o_{t}^{+}|q,o_{<t}^{+})}\\right)$$\n(14)\nA.1.5. Proximal Policy Optimization (PPO)\nThe objective of PPO is:\n$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}\\left[q\\sim P_{sft}(Q),o\\sim\\pi_{\\theta_{old}}(O|q)\\right]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\min\\left[\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t} )}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})},1-\\varepsilon,1+\\varepsilon\\right)A_{t}\\right].$$\n(15)\nTo simplify the analysis, it is assumed that the model only has a single update following each exploration stage, thereby ensuring that $\\pi_{\\theta_{old}}=\\pi_{\\theta}.$  In this case, we can remove the min and clip operation:\n$$\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\textit{Q}),o\\sim\\pi_{\\theta_{old}}(\\textit{O}|q)]\\frac{1}{|o|}\\sum_{t=1}^{|o|}\\frac{\\pi_{\\theta}(o_{t}|q,o_{<t})}{\\pi_{\\theta_{old}}(o_{t}|q,o_{<t})}A_{t}.$$\n(16)\nThe gradient of $\\mathcal{J}_{PPO}(\\theta)$  is:\n$$\\nabla_{\\theta}\\mathcal{J}_{PPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\mathit{Q}),o\\sim\\pi_{\\theta_{old}}(\\mathit{O}|q)]\\frac{1}{|o|}\\sum_{t=1}^{|o|}A_{t} \\nabla_{\\theta}\\log\\pi_{\\theta}(\\mathit{o}_{t}|q,o_{<t})$$\n(17)\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:reward model. Gradient Coeffcient:i\n$$GC_{PPO}(q,o,t,\\pi_{\\theta_{rm}})=A_{t},$$\n(18)\nwhere $A_{t}$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on the rewards $\\{r_{\\geq t}\\}$  and a learned value function $V_{\\psi}.$\nA.1.6. Group Relative Policy Optimization (GRPO)\nThe objective of GRPO is (assume $\\pi_{{{\\theta}_{old}}}=\\pi_{{{\\theta}}}$ for simplifed analysis):i\n$$\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\textit{Q}),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\textit{O}|q)]$$\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\left[\\frac{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta_{std}}(o_{i,t}|q,o_{i,<t})}\\hat{A}_{i,t}-\\beta(\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-\\log\\frac{\\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q,o_{i,<t})}-1) \\right].$$\n(19)\n\nThe gradient of $\\mathcal{J}_{GRPO}(\\theta)$  is:\n$$\\nabla_{\\theta}\\mathcal{J}_{GRPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(\\mathsf{Q} ),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\mathsf{O}|q)]$$\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\left[\\hat{A}_{i,t}+\\beta\\left(\\frac{\\pi_{ref}(o_{i,t}|o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|o_{i,<t})}-1\\right)\\right]\\nabla_{\\theta}\\log\\pi_{\\theta}(o_{i,t}|q,o_{i,<t}).$$\n(20)\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:reward model. Gradient Coeffcient:i\n$$\\textit{GC}_{GRPO}(\\textit{q},\\textit{o},\\textit{t},\\pi_{\\theta_{rm}})=\\hat{A}_{i,t}+\\beta\\left(\\frac{\\pi_{ref}(o_{i,t}|o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|o_{i,<t})}-1\\right),$$\n(21)\nwhere $\\hat{A}_{i,t}$ is computed based on the group reward scores.\n"}