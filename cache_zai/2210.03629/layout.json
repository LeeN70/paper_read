{"code":0,"doc_size":633805,"doc_type":"pdf","dst_path":"oss://glm-data-ocr-data/services/maas/pa/47e1b393-0965-444c-92ba-1d158022748e.tar","markdown":"## REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\n\nShunyu $\\mathrm{Yao}^{*,1},$  Jeffrey Zhao2, Dian $\\mathrm{Yu}^{2},$  Nan $\\mathrm{Du}^{2},$  Izhak Shafran2, Karthik Narasimhan1, Yuan Cao\n\n1Department of Computer Science, Princeton University 2Google Research, Brain team\n\n1{shunyuy,karthikn}@princeton.edu\n\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\n\n## ABSTRACT\n\nWhile large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specifc actionsi in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verifcation (Fever), ReAct overcomes prevalenti issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces.Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of $34\\%$  and $10\\%$ respectively, while being prompted with only one or two in-context examples.\n\n## 1INTRODUCTION\n\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Consider the example of cooking up a dish in the kitchen. Between any two specifc actions, we mayi reason in language in order to track progress (“now that everything is cut, I should heat up the pot of water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let me use soy sauce and pepper instead”), and to realize when external information is needed (“how do I prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties.\n\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLMs)have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\n\n\n\n![7ffea04fde729d63b580c8b63747bd25](imgs/7ffea04fde729d63b580c8b63747bd25.jpg)\n\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task solving trajectories generated by the model (Act, Thought) and the environment (Obs).\n\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors. These approaches usually convert multi-modal observations into text, use a language model to generate domain-specifc actions or plans, and theni use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring Huang et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been studies on how reasoning and acting can be combined in a synergistic manner for general task solving,and if such a combination can bring systematic benefts compared to reasoning or acting alone.i\n\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\n\n\n\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:question answering (HotPotQA, Yang et al., 2018), fact verifcation (Fever, Thorne et al., 2018),i text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact with, ReAct outperforms vanilla action generation models while being competitive with chain-ofthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct and CoT that allows for the use of both internal knowledge and externally obtained information during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able to outperform imitation or reinforcement learning methods trained with $10^{3}\\sim\\hat{10}^{5}$ task instances,with an absolute improvement of $34\\%$  and $10\\%$  in success rates respectively. We also demonstrate the importance of sparse, versatile reasoning in decision making by showing consistent advantages over controlled baselines with actions only. Besides general applicability and performance boost,the combination of reasoning and acting also contributes to model interpretability, trustworthiness,and diagnosability across all domains, as humans can readily distinguish information from model’s internal knowledge versus external environments, as well as inspect reasoning traces to understand the decision basis of model actions.\n\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel promptbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a few-shot learning setup over prior approaches that perform either reasoning or action generation in isolation; (3) we present systematic ablations and analysis to understand the importance of acting in reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial fnetuningi experiments showing the potential of ReAct to improve with additional training data. Scaling up ReAct to train and operate on more tasks and combining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models.\n\n## 2REAC T: SYNERGIZING REASONING + ACTING\n\nConsider a general setup of an agent interacting with an environment for task solving. At time step $t,$  an agent receives an observation $o_{t} \\in\\mathcal{O}$  from the environment and takes an action $a_{t}\\in\\mathcal{A}$ following some policy $\\pi(a_{t}|c_{t}),$  where $c_{t}=(o_{1},a_{1},\\cdots,o_{t-1},a_{t-1},o_{t})$  is the context to the agent.Learning a policy is challenging when the mapping $c_{t}\\mapsto a_{t}$ � is highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct fnali action (Act 4) to fnish the QA task as it requires complex reasoning over the trajectory contexti(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\n\nThe idea of ReAct is simple: we augment the agent’s action space to $\\hat{\\mathcal{A}}=\\mathcal{A}\\cup\\mathcal{L},$  where $\\mathcal{L}$  is the space of language. An action $\\hat{a}_{t}\\in\\mathcal{L}$  in the language space, which we will refer to as a thought or a reasoning trace, does not affect the external environment, thus leading to no observation feedback. Instead, a thought $\\widehat{a}_{t}$  aims to compose useful information by reasoning over the current context $c_{t}$ and update the context $c_{t+1}=(c_{t},\\hat{a}_{t})$  to support future reasoning or acting. As shown in Figure 1,there could be various types of useful thoughts, e.g. decomposing task goals and create action plans(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\n\nHowever, as the language space $\\mathcal{L}$  is unlimited, learning in this augmented action space is diffculti and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both domain-specifc actions and free-form language thoughts for task solvingi(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need toappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself.\n\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators just type down their thoughts in language on top of their actions taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail prompt design for each task in Sections 3 and 4. B) General and fexible: Due to the fexible thoughtll space and thought-action occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but not limited to QA, fact verifcation, text game, and webi navigation. C) Performant and robust: ReAct shows strong generalization to new task instances while learning solely from one to six in-context examples, consistently outperforming baselines with only reasoning or acting across different domains. We also show in Section 3 additional beneftsi when fnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.i D) Human aligned and controllable: ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,humans can also control or correct the agent behavior on the go by thought editing, as shown in Figure 5 in Section 4.\n\n## 3KNOWLEDGE-INTENSIVE REASONING TASKS\n\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact verifcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able toi retrieve information to support reasoning, while also use reasoning to target what to retrieve next,demonstrating a synergy of reasoning and acting.\n\n## 3.1SETUP\n\nDomainsWe consider two datasets challenging knowledge retrieval and reasoning: (1) HotPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact verifcationi benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only setup for both tasks, where models only receive the question/claim as input without access to support paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with an external environment to support reasoning.\n\nAction SpaceWe design a simple Wikipedia web API with three types of actions to support interactive information retrieval: (1) search[entity], which returns the frst 5 sentences fromi the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page containing string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which would fnish the current task with answer. We note that this action space mostly can only retrieve ai small part of a passage based on exact passage name, which is signifcantly weaker than state-of-the-i art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,and force models to retrieve via explicit reasoning in language.\n\n## 3.2METHODS\n\nReAct PromptingFor HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training set and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense thought), where free-form thoughts are used for various purposes. Specifcally, we use a combinationi of thoughts that decompose questions (“I need to search $\\mathbf{X},$  fndi $y,$  then fndi $z^{3}$ ), extract information from Wikipedia observations ( $``{x}$ was started in $1844\",$  “The paragraph does not tell $\\mathbf{x}\"$ ), perform commonsense ( $``x$  is not y, so z must instead be...”) or arithmetic reasoning $\\left(``1844<1989\"\\right) ,$  guide\n\n\n\n\n\n<html><body><table><tr><td>Prompt Methoda</td><td>HotpotQA (EM)</td><td>Fever (Acc)</td></tr><tr><td>Standard</td><td>28.7</td><td>57.1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>29.4</td><td>56.3</td></tr><tr><td>CoT-SC (Wang et al., 2022a)</td><td>33.4</td><td>60.4</td></tr><tr><td>Act</td><td>25.7</td><td>58.9</td></tr><tr><td>ReAct</td><td>27.4</td><td>60.9</td></tr><tr><td>CoT-SC ￫ReAct ReAct￫CoT-SC</td><td>34.2</td><td>64.6</td></tr><tr><td></td><td>35.1</td><td>62.0</td></tr><tr><td>Supervised SoTAb</td><td>67.5</td><td>89.5</td></tr></table></body></html>\n\n![0fd5cdf2b99b418ece20d72b856688bd](imgs/0fd5cdf2b99b418ece20d72b856688bd.jpg)\n\nTable 1: PaLM-540B prompting results on HotpotQA and Fever.\n\nFigure 2: PaLM-540B prompting results with respect to number of CoT-SC samples used.\n\na CoT-SC in Wang et al. (2022b).28.9, 33.8 for Standard, CoT,\n\nb(Zhu et al., 2021; Lewis et al., 2020)\n\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the fnal answer (“...soi the answer is $\\mathbf{x}^{\\prime\\prime}$ ). See Appendix C for more details.\n\nBaselinesWe systematically ablate ReAct trajectories to build prompts for multiple baselines (with formats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,2022), which removes actions and observations and serve as a reasoning-only baseline. We also build a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories with decoding temperature 0.7 during inference and adopting the majority answer, which is found to consistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the Internet to answer questions, though it operates on a different task and action space, and uses imitation and reinforcement learning instead of prompting.\n\nCombining Internal and External KnowledgeAs will be detail in Section 3.3, we observe that the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts or thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide when to switch to the other method based on the following heuristics: A) ReAct $\\rightarrow$ CoT-SC: when ReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for HotpotQA and FEVER respectively as we fnd more steps will not improve ReAct performance3.i B) CoT-SC ￫ReAct: when the majority answer among n CoT-SC samples occurs less than $n/2$ times (i.e. internal knowledge might not support the task confdently), back off to ReAct.i\n\nFinetuningDue to the challenge of manually annotating reasoning traces and actions at scale,we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories with correct answers generated by ReAct (also for other baselines) to fnetune smaller languagei models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on input questions/claims. More details are in Appendix B.1.\n\n## 3.3RESULTS AND OBSERVATIONS\n\nReAct outperforms Act consistentlyTable 1 shows HotpotQA and Fever results using PaLM540B as the base model with different prompting methods. We note that ReAct is better than Act on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the fnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also confrm the beneft of reasoningiii traces for more informed acting.\n\n\n\n\n\n<html><body><table><thead><tr><td></td><td>Type</td><td>Defnition</td><td>ReAct</td><td>CoT</td></tr></thead><tbody><tr><td rowspan=\"2\">Success</td><td>True positive</td><td>Correct reasoning trace and facts</td><td> $94\\%$ </td><td>86%</td></tr><tr><td>False positive</td><td>Hallucinated reasoning trace or facts</td><td>6%</td><td>14%</td></tr><tr><td rowspan=\"4\">Failure</td><td>Reasoning error</td><td>Wrong reasoning trace (including failing to recover from repetitive steps)</td><td>47%</td><td>16%</td></tr><tr><td>Search result error</td><td>Search return empty or does not contain useful information</td><td> $23\\%$ </td><td>-</td></tr><tr><td>Hallucination</td><td>Hallucinated reasoning trace or facts</td><td> $0\\%$ </td><td>56%</td></tr><tr><td>Label ambiguity</td><td>Right prediction but did not match the label precisely</td><td> $29\\%$ </td><td> $28\\%$ </td></tr></tbody></table></body></html>\n\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their percentages in randomly selected examples studied by human.\n\nReAct vs. CoTOn the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct and CoT respectively (thus 200 examples in total), and manually labeled their success and failure modes in Table 2. Some key observations are as follows:\n\nA) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than ReAct ( $14\\%$  vs. $6\\%$  in success mode, and make up its major failure mode $(56\\%)$ . In contrast, the problem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the access of an external knowledge base.\n\nB) While interleaving reasoning, action and observation steps improves ReAct’s groundedness and trustworthiness, such a structural constraint also reduces its fexibility in formulatingl reasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent error pattern specifc to ReAct, in which the model repetitively generates the previous thoughts andi actions, and we categorize it as part of “reasoning error” as the model fails to reason about what the proper next action to take and jump out of the loop4.\n\nC) For ReAct, successfully retrieving informative knowledge via search is critical. Noninformative search, which counts for $23\\%$  of the error cases, derails the model reasoning and gives it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between factuality and fexibility, which motivates our proposed strategies of combining two methods.l\n\nWe provide examples for each success and failure modes in Appendix E.1. We also fnd somei HotpotQA questions may contain outdated answer labels, see Figure 4 for example.\n\nReAct + CoT-SC perform best for prompting LLMsAlso shown in Table 1, the best prompting method on HotpotQA and Fever are ReAct $\\rightarrow$ CoT-SC and $\\mathtt{CoT-SC}\\to\\mathtt{ReAct}$  respectively.Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both signifcantly and consistently outperform CoT-SC across different number of samples, reachingi CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of properly combining model internal knowledge and external knowledge for reasoning tasks.\n\nReAct performs best for fne-tuningFigure 3 shows the scaling effect of prompting/fnetuningii four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct performs worst among four methods due to the diffculty to learn both reasoning and acting fromi in-context examples. However, when fnetuned with just 3,000 examples, ReAct becomes the besti method among the four, with PaLM-8B fnetuned ReAct outperforming all PaLM-62B promptingi methods, and PaLM-62B fnetuned ReAct outperforming all 540B prompting methods. In contrast,i fnetuning Standard or CoT is signifcantly worse than fnetuning ReAct or Act for both PaLM-iii 8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a more generalizable skill for knowledge reasoning. As all prompting methods are still signifcantlyi far from domain-specifc state-of-the-art approaches (Table 1), we believe fnetuning with moreii human-written data might be a better way to unleash the power of ReAct.\n\n\n\n![2ae384b9b60a913387de123ec05332f5](imgs/2ae384b9b60a913387de123ec05332f5.jpg)\n\nre 3: Scaling results for prompting and fnetuning on HotPotQA with ReAct (ours) and baselinei\n\n## 4DECISION MAKING TASKS\n\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and WebShop, both of which feature complex environments that require agents to act over long horizons with sparse rewards, warranting the need for reasoning to act and explore effectively.\n\nALFWorldALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of tasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by navigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take paper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy more than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore systematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into ALFWorld is the need to determine likely locations for common household items (e.g. desklamps will likely be on desks, shelfs, or dressers), making this environment a good ft for LLMs to exploit theiri pretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories from the training set for each task type, where each trajectory includes sparse thoughts that (1)decompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via commonsense where to fnd an object and what to do with it. We show prompts used for ALFWorldi in Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games in a task-specifc setup. For robustness, we construct 6 prompts for each task type through eachi permutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using the same trajectories, but without thoughts — since task instances are randomly chosen from the training set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the importance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation learning agent trained on $10^{5}$ expert trajectories for each task type5.\n\nWebShopCan ReAct also interact with noisy real-world language environments for practical applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping website environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,and options crawled from Amazon), and requires an agent to purchase a product based on a user instruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel fnish, andi priced lower than $\\$140\")$  through web interactions (e.g. search “nightstand drawers”, choose buttons such as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score(percentage of desired attributes covered by the chosen product averaged across all episodes) and success rate (percentage of episodes where the chosen product satisfes all requirements) on 500 testi instructions. We formulate Act prompts with actions to search, choose product, choose options,and buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,and what products options are relevant to the instruction. See Table 6 for an example prompt, and Table 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\n\n\n\n\n\n<html><body><table><thead><tr><td>Method</td><td>Pick</td><td>Clean</td><td>Heat</td><td>Cool</td><td>Look</td><td>Pick 2</td><td>All</td></tr></thead><tbody><tr><td>Act (best of 6)</td><td>88</td><td>42</td><td>74</td><td>67</td><td>72</td><td>41</td><td>45</td></tr><tr><td>ReAct (avg)</td><td>65</td><td>39</td><td>83</td><td>76</td><td>55</td><td>24</td><td>57</td></tr><tr><td>ReAct (best of 6)</td><td>92</td><td>58</td><td>96</td><td>86</td><td>78</td><td>41</td><td>71</td></tr><tr><td>ReAct-IM (avg)</td><td>55</td><td>59</td><td>60</td><td>55</td><td>23</td><td>24</td><td>48</td></tr><tr><td>ReAct-IM (best of 6)</td><td>62</td><td>68</td><td>87</td><td>57</td><td>39</td><td>33</td><td>53</td></tr><tr><td>BUTLERg (best of 8)</td><td>33</td><td>26</td><td>70</td><td>76</td><td>17</td><td>12</td><td>2327</td></tr><tr><td>BUTLER (best of 8)</td><td>46</td><td>39</td><td>74</td><td>100</td><td>22</td><td>24</td><td></td></tr></tbody></table></body></html>\n\nTable 3: AlfWorld task-specifc success rates (%). BUTLER andi BUTLERg results are from Table 4 of Shridhar et al. (2020b). All methods use greedy decoding, except that BUTLER uses beam search.\n\n\n\n<html><body><table><tr><td>Method</td><td>Score</td><td>SR</td></tr><tr><td>Act ReAct</td><td>62.3 66.6</td><td>30.1 40.0</td></tr><tr><td>IL</td><td>59.9</td><td>29.1</td></tr><tr><td>IL+RL</td><td>62.4</td><td>28.7</td></tr><tr><td>Human Expert</td><td>82.1</td><td>59.6</td></tr></table></body></html>\n\nTable 4: Score and success rate (SR) on Webshop. IL/IL+RL taken from Yao et al. (2022).\n\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)method additionally trained with 10,587 training instructions.\n\nResultsReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On ALFWorld, the best ReAct trial achieves an average success rate of $71\\%,$  signifcantly outperformingi the best Act $(45\\%)$  and BUTLER $(37\\%)$  trials. In fact, even the worse ReAct trial $$  beats the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across six controlled trials, with relative performance gain ranging from $33\\%$ to $90\\%$  and averaging $62\\%.$ Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals into smaller subgoals, or loses track of the current state of the environment. Example trajectories comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\n\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With additional sparse reasoning, ReAct achieves signifcantly better performance, with an absolutei $10\\%$ improvement over the previous best success rate. By checking examples, we fnd that ReAct is morei likely to identify instruction-relevant products and options by reasoning to bridge the gap between noisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item has options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are still far from the performance of expert humans (Table 4), who perform signifcantly more producti explorations and query re-formulations that are still challenging for prompting-based methods.\n\nOn the value of internal reasoning vs. external feedbackTo our knowledge, ReAct is the frsti demonstration of combined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”. However, IM’s “inner monologue” is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisfed. In contrast, thei reasoning traces in ReAct for decision making is fexible and sparse, allowing diverse reasoningl types (see Section 2) to be induced for different tasks.\n\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on fve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakesi in identifying when subgoals were fnished, or what the next subgoal should be, due to a lack of high-i level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example trajectory in Appendix D.2.3.\n\n\n\n## 5RELATED WORK\n\nLanguage model for reasoningPerhaps the most well-known work of using LLMs for reasoning is Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their own “thinking procedure” for problem solving. Several follow-up works have since been performed,including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shotCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,(Madaan $\\&$  Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other work has also been extended to more sophisticated reasoning architecture beyond simple prompting.For example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps of “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by fnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswelli& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated LM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which fnetunes a LM oni intermediate computation steps, also demonstrate improvement on multi-step computation problems.In contrast to these methods, ReAct performs more than just isolated, fxed reasoning, and integratesi model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\n\nLanguage model for decision makingThe strong capability of LLMs has enabled them to perform tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a policy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for reinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)and Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl et al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly consider the reasoning procedure either, and also relies on expensive datasets and human feedback collections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the decision making process only requires language description of the reasoning procedure.6\n\nLLMS have also been increasingly employed in interactive and embodied environments for planning and decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which is then reranked by an affordance model grounded on the visual environments for fnal prediction.i Inner Monologue made further improvements by adding the eponymous “inner monologue\", which is implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the frst work that demonstrates such a closed-loop system, which ReAct builds on. However, we arguei that Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We also note that leveraging language as semantically-rich inputs in the process of interactive decision making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti et al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and decision making. What is more, progress in LLMs has also inspired the development of versatile and generalist agents like Reed et al. (2022).\n\n## 6CONCLUSION\n\nWe have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experiments on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. Despite the simplicity of our method, complex tasks with large action spaces require more demonstrations to learn well, which unfortunately can easily go beyond the input length limit of in-context learning. We explore the fne-tuning approach on HotpotQAiwith initial promising results, but learning from more high-quality human annotations will be the desiderata to further improve the performance. Scaling up ReAct with multi-task training and combining it with complementary paradigms like reinforcement learning could result in stronger agents that further unlock the potential of LLMs for more applications.\n\n## ACKNOWLEDGMENTS\n\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP Group. This work was supported in part by the National Science Foundation under Grant No.2107048. Any opinions, fndings, and conclusions or recommendations expressed in this material arei those of the author(s) and do not necessarily refect the views of the National Science Foundation.l\n\n## REPRODUCIBILITY STATEMENT\n\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible model yet. To increase reproducibility, we have included all used prompts in Appendix C, additional experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct prompting code at https://anonymous.4open.science/r/ReAct-2268/.\n\n## ETHICS STATEMENT\n\nReAct prompts large language models to generate more human interpretable, diagnosable, and controllable task-solving trajectories than previous methods. However, hooking up a large language model with an action space to interact with external environments (e.g. the web, physical environments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful actions in an environment. Our experiments minimize such risks by limiting the interactions to specifc websites (Wikipedia or WebShop) that are free of private information, without any dangerousi actions in the action space design (i.e. models cannot really buy products on WebShop the research benchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing more extensive experiments in the future.\n\n## REFERENCES\n\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew son, Sonˇa Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,2020. URL https://arxiv.org/abs/2012.05672.\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/2204.01691.\n\nBen Alderson-Day and Charles Fernyhough.Inner speech: development, cognitive functions,phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.\n\nAlan Baddeley. Working memory. Science, 255(5044):556–559, 1992.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL https://arxiv.org/abs/2208.14271.\n\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/2205.09712.\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/P19-1346.\n\nCharles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social interaction and the development of social understanding and executive functions, pp. 56–79, 2010.\n\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,Sonˇa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving.Improving alignment of dialogue agents via targeted human judgements,2022.URL https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf.\n\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems,33:20179–20191, 2020.\n\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,2022a.\n\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b.\n\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed latent actions. In CoRL, pp. 1379–1390, 2021. URL https://proceedings.mlr.press/v164/karamcheti22a.html.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022\n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering.arXiv preprint arXiv:2203.05115, 2022.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\n\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https://arxiv.org/abs/2202.01771.\n\n\n\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsychologia, 3(4):387–392, 1965.\n\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022. URL https://arxiv.org/abs/2209.07686.\n\nVincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint arXiv:2104.07972, 2021.\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:Browser-assisted question-answering with human feedback, 2021. URL https://arxiv.org/abs/2112.09332.\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https://arxiv.org/abs/2205.06175.\n\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740–10749, 2020a.\n\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b.\n\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.Language models that seek for knowledge: Modular search $\\&$  generation for dialogue and prompt completion. arXiv preprint arXiv:2203.13224, 2022a.\n\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:a deployed conversational agent that continually learns to responsibly engage, 2022b. URL https://arxiv.org/abs/2208.03188.\n\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verifcation. arXiv preprint arXiv:1803.05355, 2018.i\n\nLev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39–285, 1987.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,2022a. URL https://arxiv.org/abs/2203.11171.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.\n\n\n\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.URL https://aclanthology.org/2020.emnlp-main.704.\n\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,2022.\n\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.\n\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2022. URL https://arxiv.org/abs/2205.10625.\n\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information seeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.\n\n\n\n## AADDITIONAL RESULTS\n\n## A.1GPT-3 EXPERIMENTS\n\n\n\n<html><body><table><tr><td></td><td>PaLM-540B</td><td>GPT-3</td></tr><tr><td>HotpotQA (exact match)</td><td>29.4</td><td>30.8</td></tr><tr><td>ALFWorld (success rate %)</td><td>70.9</td><td>78.4</td></tr></table></body></html>\n\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).On HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all 134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\n\nWe run additional GPT-3 (Brown et al., 2020) experiments to confrm ReAct prompting performancei is general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly because it is fnetuned with human instruction following. This indicates ReAct prompting is effectivei across different large language models on different tasks. The code for these experiments are at https://react-lm.github.io/.\n\n## A.2REACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA\n\n\n\n![8d596dbf2460c78be09b1773e5db7206](imgs/8d596dbf2460c78be09b1773e5db7206.jpg)\n\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is able to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\n\nDuring trajectory inspection, we also fnd that sometimes ReAct does not agree with dataset labels asi the labels themselves could be outdated. For example, as shown in Figure 4, the question asks about the size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT give wrong answers due to hallucination, Act fails despite the access of real-world web interaction,due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to retrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better incorporation of reasoning abilities might beneft recent Internet-augmented language models (Nakanoi et al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\n\n## A.3HUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD\n\nWe also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit ReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act 17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align with these human thought edits and succeed in the task. From a human perspective, solving such a task becomes signifcantly easier, from typing tens of actions to only editing a couple of thoughts,i which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\n\n\n\n![84680182d56688a02452e83527d1856e](imgs/84680182d56688a02452e83527d1856e.jpg)\n\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct trajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\n\nis diffcult for Act and previous RL methods, as a human cannot change the model parameters, andi changing a few actions might not edit the rest of the model behavior. This paradigm is also more than human dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct thoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the fexible thought space supports, for better task solving. We believe this is an exciting direction forl human alignment and leave more systematic study as future work.\n\n## BEXPERIMENT DETAILS\n\n## B.1HOTPOTQA FINETUNING DETAILS\n\nFor all fnetuning we use a batch size of 64. On PaLM-8B, we fnetune ReAct and Act methodsii for 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we fnetunei ReAct and Act methods for 4, 000 steps and Standard and CoT methods for 1, 000 steps. We fnd ReAct and Act methods generally beneft from more training steps (and more training data),ii while Standard and CoT methods degrade soon after fnetuning.i\n\n## B.2ALFWORLD IM-STYLE DETAILS\n\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense external feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)decomposing the current goal and (2) the current subgoal that needs to be completed. In particular,ReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the next subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify where items can be within the environment.\n\n\n\n## CPROMPTS\n\n## C.1HOTPOTQA\n\nHotpot QA Prompts\n\n\n\n<html><body><table><thead><tr><td></td><td></td></tr></thead><tbody><tr><td>Original Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Answer</td><td>1,800 to 7,000 ft</td></tr><tr><td>Question</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"</td></tr><tr><td>Answer</td><td>character Milhouse, who Matt Groening named after who? Richard Nixon</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The</td></tr><tr><td></td><td>Saimaa Gesture? The Saimaa Gesture</td></tr><tr><td>Answer</td><td></td></tr><tr><td>Question Answer</td><td>What profession does Nicholas Ray and Elia Kazan have in common? director, screenwriter, actor</td></tr><tr><td></td><td></td></tr><tr><td>Question Answer</td><td>Which magazine was started first Arthur’s Magazine or First for Women? Arthur’s Magazine</td></tr><tr><td></td><td></td></tr><tr><td>Question Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Yes</td></tr><tr><td>Act</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the</td></tr><tr><td>Action 1</td><td>Colorado orogeny extends into? Search[Colorado orogeny]</td></tr><tr><td>Observation 1</td><td>The Colorado orogeny was an episode of mountain building (an orogeny) in</td></tr><tr><td></td><td>Colorado and surrounding areas.</td></tr><tr><td>Action 2</td><td>Lookup[eastern sector] (Result 1 / 1) The eastern sector extends into the High Plains and is called</td></tr><tr><td>Observation 2</td><td>the Central Plains orogeny.</td></tr><tr><td>Action 3</td><td>Search[High Plains]</td></tr><tr><td>Observation 3</td><td>High Plains refers to one of two distinct land regions: Search[High Plains (United States)]</td></tr><tr><td>Action 4 Observation 4</td><td>The High Plains are a subregion of the Great Plains.From east to west, the</td></tr><tr><td></td><td>High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]</td></tr><tr><td>Action 5</td><td>Finish[1,800 to 7,000 ft]</td></tr><tr><td>Question</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?</td></tr><tr><td>Action 1</td><td>Search[Milhouse] Milhouse Mussolini Van Houten is a recurring character in the Fox animated</td></tr><tr><td>Observation 1</td><td>television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.</td></tr><tr><td>Action 2</td><td>Lookup[named after]</td></tr><tr><td>Observation 2</td><td>(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.</td></tr><tr><td>Action 3</td><td>Finish[Richard Nixon]</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The</td></tr><tr><td></td><td>Saimaa Gesture? Search[Adam Clayton Powell]</td></tr><tr><td>Action 1 Observation 1</td><td>Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office</td></tr><tr><td></td><td>Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell</td></tr><tr><td></td><td>(film)’, ’Giancarlo Esposito’]. Search[Adam Clayton Powell (film)]</td></tr><tr><td>Action 2 Observation 2</td><td>Adam Clayton Powell is a 1989 American documentary film directed by</td></tr><tr><td></td><td>Richard Kilberg.The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired</td></tr><tr><td></td><td>as part of the PBS series The American Experience.</td></tr><tr><td>Action 3</td><td>Finish[The Saimaa Gesture]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Question</td><td>What profession does Nicholas Ray and Elia Kazan have in common?</td></tr><tr><td></td><td></td></tr><tr><td>Action 1</td><td>Search[Nicholas Ray]</td></tr><tr><td></td><td></td></tr><tr><td>Observation 1</td><td></td></tr><tr><td></td><td>Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>1979) was an American film director, screenwriter, and actor best known for</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 2</td><td>the 1955 film Rebel Without a Cause.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Search[Elia Kazan]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Observation 2</td><td>Elia Kazan was an American film and theatre director, producer, screenwriter</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>and actor. C o n t i n u e d  o n  n e x t  p a g e</td></tr></table></body></html>\n\n\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Hotpot QA Prompts – Continued from previous page</td></tr></thead><tbody><tr><td colspan=\"2\"></td></tr><tr><td>Action 3 Question</td><td>Finish[director, screenwriter, actor] Which magazine was started first Arthur’s Magazine or First for Women?</td></tr><tr><td>Action 1</td><td>Search[Arthur’s Magazine]</td></tr><tr><td>Observation 1</td><td>Arthur’s Magazine (1844-1846) was an American literary periodical published</td></tr><tr><td></td><td>in Philadelphia in the 19th century. Search[First for Women]</td></tr><tr><td>Action 2 Observation 2</td><td>First for Women is a woman’s magazine published by Bauer Media Group in the</td></tr><tr><td></td><td>USA.[1] The magazine was started in 1989. Finish[Arthur’s Magazine]</td></tr><tr><td>Action 3</td><td></td></tr><tr><td>Question</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Search[Pavel Urysohn]</td></tr><tr><td>Action 1 Observation 1</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet</td></tr><tr><td></td><td>mathematician who is best known for his contributions in dimension theory.</td></tr><tr><td>Action 2</td><td>Search[Leonid Levin]</td></tr><tr><td>Observation 2</td><td>Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.</td></tr><tr><td>Action 3</td><td>Finish[yes]</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Thought</td><td>Let’s think step by step.The eastern sector of Colorado orogeny extends into the High Plains. High Plains rise in elevation from around 1,800 to</td></tr><tr><td></td><td>7,000 ft, so the answer is 1,800 to 7,000 ft. 1,800 to 7,000 ft</td></tr><tr><td>Answer</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"</td></tr><tr><td>Question</td><td>character Milhouse, who Matt Groening named after who?</td></tr><tr><td>Thought</td><td>Let’s think step by step.Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.</td></tr><tr><td>Answer</td><td>Richard Nixon</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td></tr><tr><td>Thought</td><td>Let’s think step by step. Adam Clayton Powell (film) is a documentary about</td></tr><tr><td></td><td>an African-American politician, not Finnish rock groups.So the documentary about Finnish rock groups must instead be The Saimaa Gesture.</td></tr><tr><td>Answer</td><td>The Saimaa Gesture</td></tr><tr><td>Question Thought</td><td>What profession does Nicholas Ray and Elia Kazan have in common? Let’s think step by step.Professions of Nicholas Ray are director, Professions of Elia Kazan are director, producer,</td></tr><tr><td></td><td>screenwriter, and actor. screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.</td></tr><tr><td>Answer</td><td>director, screenwriter, actor</td></tr><tr><td>Question</td><td>Which magazine was started first Arthur’s Magazine or First for Women? Let’s think step by step.Arthur’s Magazine was started in 1844.First</td></tr><tr><td>Thought</td><td>for Women was started in 1989.1844 (Arthur’s Magazine) < 1989 (First for</td></tr><tr><td></td><td>Women), so Arthur’s Magazine was started first. Arthur’s Magazine</td></tr><tr><td>Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work?</td></tr><tr><td>Question Thought</td><td>Let’s think step by step.Pavel Urysohn is a mathematician.Leonid Levin</td></tr><tr><td></td><td>is a mathematician and computer scientist.So Pavel Urysohn and Leonid Levin have the same type of work.</td></tr><tr><td>Answer</td><td>Yes</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Thought 1</td><td>I need to search Colorado orogeny, find the area that the eastern sector</td></tr><tr><td></td><td>of the Colorado orogeny extends into, then find the elevation range of the</td></tr><tr><td></td><td>area. Search[Colorado orogeny]</td></tr><tr><td>Action 1 Observation 1</td><td>The Colorado orogeny was an episode of mountain building (an orogeny) in</td></tr><tr><td></td><td>Colorado and surrounding areas. It does not mention the eastern sector.</td></tr><tr><td>Thought 2</td><td>So I need to look up eastern sector.</td></tr><tr><td>Action 2</td><td>Lookup[eastern sector]</td></tr><tr><td>Observation 2</td><td>(Result 1 / 1) The eastern sector extends into the High Plains and is called</td></tr><tr><td></td><td>the Central Plains orogeny.</td></tr><tr><td></td><td></td></tr><tr><td>Thought 3</td><td>The eastern sector of Colorado orogeny extends into the High Plains.So I</td></tr><tr><td></td><td></td></tr><tr><td></td><td>need to search High Plains and find its elevation range.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 3</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Search[High Plains]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Observation 3 Thought 4 Action 4</td><td>High Plains refers to one of two distinct land regions I need to instead search High Plains (United States). Search[High Plains (United States)]</td></tr></table></body></html>\n\n\n\n## \n\nObservation 4The High Plains are a subregion of the Great Plains.From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n\nThought 5High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5Finish[1,800 to 7,000 ft]\n\nQuestionMusician and satirist Allie Goertz wrote a song about the \"The Simps character Milhouse, who Matt Groening named after who?\n\nThought 1The question simplifies to \"The Simpsons\" character Milhouse is named after who.I only need to search Milhouse and find who it is named after.\n\nObservation 1Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2The paragraph does not tell who Milhouse is named after, maybe I can look up\"named after\".\n\nObservation 2(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nstionWhich documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\n\nught 1I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.\n\nObservation 1Could not find [Adam Clayton Powell].Similar:[’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell(film)’, ’Giancarlo Esposito’].\n\nAction 2Search[Adam Clayton Powell (film)]\n\nObservation 2Adam Clayton Powell is a 1989 American documentary film directed by\n\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\n\nThought 3Adam Clayton Powell (film) is a documentary about an African-American\n\npolitician, not Finnish rock groups.So the documentary about Finnish rock groups must instead be The Saimaa Gesture.Action 3Finish[The Saimaa Gesture]\n\nQuestionWhat profession does Nicholas Ray and Elia Kazan have in common?\n\nught 1I need to search Nicholas Ray and Elia Kazan, find their professions, then\n\nAction 1Search[Nicholas Ray]\n\nObservation 1Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16 1979) was an American film director, screenwriter, and actor best known f the 1955 film Rebel Without a Cause.\n\nThought 2Professions of Nicholas Ray are director, screenwriter, and actor.I nee to search Elia Kazan next and find his professions.\n\nThought 3Professions of Elia Kazan are director, producer, screenwriter, and actor\n\nAction 3Finish[director, screenwriter, actor]\n\nQuestionWere Pavel Urysohn and Leonid Levin known for the same type of work?\n\n\n\nHotpot QA Prompts – Continued from previous page\n\n\n\n<html><body><table><thead><tr><td>Observation 1</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet</td><td></td><td></td></tr></thead><tbody><tr><td>Thought 2</td><td>mathematician who is best known for his contributions in dimension theory. Pavel Urysohn is a mathematician.I need to search Leonid Levin next and</td><td></td><td></td></tr><tr><td></td><td>find its type of work.</td><td></td><td></td></tr><tr><td>Action 2 Observation 2</td><td>Search[Leonid Levin] Leonid Anatolievich Levin is a Soviet-American mathematician and computer</td><td></td><td></td></tr><tr><td></td><td>scientist.</td><td></td><td></td></tr><tr><td>Thought 3</td><td>and Leonid Levin have the same type of work.</td><td></td><td>Leonid Levin is a mathematician and computer scientist.So Pavel Urysohn</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Action 3</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Finish[yes]</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr></table></body></html>\n\n\n\n## C.2FEVER\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">FEVER Prompts</td></tr></thead><tbody><tr><td>Original</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. SUPPORTS</td></tr><tr><td>Answer</td><td></td></tr><tr><td>Claim Answer</td><td>Stranger Things is set in Bloomington, Indiana. REFUTES</td></tr><tr><td>Claim</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Answer Act</td><td>NOT ENOUGH INFO Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if</td></tr><tr><td>Claim Action2 Observation 1</td><td>there is NOT ENOUGH INFORMATION. Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer.He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and</td></tr><tr><td></td><td>Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot. Finish[SUPPORTS]</td></tr><tr><td>Action 2 Claim</td><td>Stranger Things is set in Bloomington, Indiana. Search[Stranger Things]</td></tr><tr><td>Action 1 Observation 1</td><td>Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers.Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters.</td></tr><tr><td>Action 2</td><td>Finish[REFUTES]</td></tr><tr><td>Claim Action 1</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. Search[Beautiful] Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’,</td></tr><tr><td>Observation 1</td><td>’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is Beautiful’]. Search[Beautiful (Christina Aguilera song)</td></tr><tr><td>Action 2 Observation 2</td><td>\"Beautiful\" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).</td></tr><tr><td>Action 3</td><td>Lookup[Billboard Hot 100] (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the</td></tr><tr><td>Observation 3</td><td>United States, where it was certified Gold for 500,000 units shipped. Finish[NOT ENOUGH INFO]</td></tr><tr><td>Action 4</td><td></td></tr><tr><td>CoT</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.</td></tr><tr><td>Claim</td><td></td></tr><tr><td></td><td>Nikolaj William Coster-Waldau appeared in the 2009 Fox television film</td></tr><tr><td>Thought</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Virtuality, so he has worked with the Fox Broadcasting Company.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>SUPPORTS</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Answer</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Claim</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Stranger Things is set in Bloomington, Indiana.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Thought</td><td>Stranger Things is in the fictional town of Hawkins, Indiana, not in</td></tr></tbody></table></body></html>\n\n\n\nFever Prompts – Continued from previous page\n\n\n\n<html><body><table><thead><tr><td></td><td></td></tr></thead><tbody><tr><td>Claim Thought</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. The song peaked at number two on the Billboard Hot 100 in the United States, but not sure if it was in 2003.</td></tr><tr><td>Answer</td><td>NOT ENOUGH INFO</td></tr><tr><td>ReAct</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. I need to search Nikolaj Coster-Waldau and find if he has worked with the</td></tr><tr><td>Thought 1 Action 1 Observation 1</td><td>Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy</td></tr><tr><td></td><td>Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013).In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as</td></tr><tr><td>Thought 2</td><td>a pilot. Because he \"appeared in the 2009 Fox television film Virtuality\", he should</td></tr><tr><td>Action 2</td><td>have worked with the Fox Broadcasting Company. Finish[SUPPORTS]</td></tr><tr><td>Claim</td><td>Stranger Things is set in Bloomington, Indiana. I should search for Stranger Things, and see if it is set in Bloomington,</td></tr><tr><td>Thought 1 Action 1</td><td>Indiana. Search[Stranger Things] Stranger Things is an American science fiction horror drama television</td></tr><tr><td>Observation 1</td><td>series created by the Duffer Brothers.Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact</td></tr><tr><td></td><td>on an ensemble of child and adult characters. The observation says that it is set in a \"fictional town of Hawkins,</td></tr><tr><td>Thought 2</td><td>Indiana\", so it is not set in Bloomington. Finish[REFUTES]</td></tr><tr><td>Action 2</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Claim</td><td>I need to search the song Beautiful and find if it reached number two on the</td></tr><tr><td>Thought 1</td><td>Billboard Hot 100 in 2003. Search[Beautiful]</td></tr><tr><td>Action 1 Observation 1</td><td>Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’, ’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is</td></tr><tr><td></td><td>Beautiful’].</td></tr><tr><td>Thought 2</td><td>From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to find the song.</td></tr><tr><td>Action 2</td><td>Search[Beautiful (Christina Aguilera song)] \"Beautiful\" is a song recorded by American singer Christina Aguilera for her</td></tr><tr><td>Observation 2</td><td>fourth studio album, Stripped (2002).</td></tr><tr><td>Thought 3</td><td>It does not mention Billboard, so I need to look up \"Billboard Hot  ${}^{100}\"$ to</td></tr><tr><td></td><td>find if it reached number two on it in 2003.</td></tr><tr><td>Action 3</td><td>Lookup[Billboard Hot 100]</td></tr><tr><td>Observation 3</td><td>(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the</td></tr><tr><td></td><td>United States, where it was certified Gold for 500,000 units shipped.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>It only says the song peaked at number two on the Billboard Hot 100, but not</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Thought 4</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>if it was in 2003.I am not sure if this claim is true or not.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 4</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td></table></body></html>\n\n\n\n## C.3WEBSHOP\n\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for Act, and use the prompt on the right (add reasoning) for ReAct.\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Instruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars</td></tr><tr><td>Act</td><td>ReAct</td></tr></thead><tbody><tr><td>Aetion: search [3 ounce bright citrus deodorant sensitive skin]</td><td>Action: search [3 ounce bright citrus deodorant sensitive skin</td></tr><tr><td>Observation:</td><td>Observation:</td></tr><tr><td>[ Back to Search Page 1 (Total results: 50)</td><td>Back to Search Page 1 (Total results: 50)</td></tr><tr><td>[Next]</td><td>[Next]</td></tr><tr><td>[B078GWRC1J]</td><td>[B078GWRC1J]</td></tr><tr><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td></tr><tr><td>Calendula 3-Ounce $10.99</td><td>Calendula 3-Ounce $10.99</td></tr><tr><td>B078GTKVXY Ginger Fresh Deodorant by Earth Mama | Natural and Safe for</td><td>[B078GTKVXY] Ginger Fresh Deodorant by Earth Mama | Natural and Safe for</td></tr><tr><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td></tr><tr><td>$10.99 [B08KBVJ4XN]</td><td>$10.99 [B08KBVJ4XN]</td></tr><tr><td>Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar</td><td>Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar</td></tr><tr><td>& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)</td><td>& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)</td></tr><tr><td>$15.95</td><td>$15.95</td></tr><tr><td>Action: click[B078GWRC1J]</td><td>Action: think[B078GWRC1J and B078GTKVXY are bright citrus deodorant less then 50 dollars. I can check B078GWRC1J frst.</td></tr><tr><td>Observation: [Back to Search]</td><td>Observation: OK.</td></tr><tr><td>[Prev]</td><td></td></tr><tr><td>scent [assorted scents][bright citrus][calming lavender][ginger</td><td>Action: click[B078GWRC1J]</td></tr><tr><td>[fresh] simply non-scents]</td><td>Observation:</td></tr><tr><td>size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)]</td><td>[Back to Search] [Prev]</td></tr><tr><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for</td><td>scent [assorted scents][bright citrus][calming lavender][ginger</td></tr><tr><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td><td>[fresh] simply non-scents]</td></tr><tr><td>Calendula 3-Ounce Price: $10.99</td><td>size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)]</td></tr><tr><td>Rating: N.A.</td><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for</td></tr><tr><td>[Description]</td><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td></tr><tr><td>[Features]</td><td>Calendula 3-Ounce</td></tr><tr><td>[Reviews]</td><td>Price: $10.99</td></tr><tr><td>[Buy Now]</td><td>Rating: N.A.</td></tr><tr><td></td><td>[Description]</td></tr><tr><td>Action: click[bright citrus]</td><td>[Features]</td></tr><tr><td>Observation: You have clicked bright citrus.</td><td>[Reviews]</td></tr><tr><td>Action: click[3 ounce (pack of 1)]</td><td>[Buy Now]</td></tr><tr><td>Observation: You have clicked 3 ounce (pack of 1).</td><td>Action: think[For 3 ounce bottle of bright citrus deodorant</td></tr><tr><td></td><td>for sensitive skin, the item has options ’bright citrus’</td></tr><tr><td>Action: click[Buy Now]</td><td>and ’3 ounce (pack of  $11'$  and seems good to buy.]</td></tr><tr><td></td><td>Observation: OK.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[bright citrus]</td></tr><tr><td></td><td>Observation: You have clicked bright citrus.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[3 ounce (pack of 1)]</td></tr><tr><td></td><td>Observation: You have clicked 3 ounce (pack of 1).</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[Buy Now]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></body></html>\n\n\n\nC.4ALFWORLD\n\n## Table 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\n\n> go to fridge 1\n\nThe fridge 1 is closed.\n\n$>$ open fridge 1\n\nYou open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2.\n\n> go to diningtable 1\n\npeppershaker 1, a spatula 3, a tomato\n\n> take lettuce 1 from diningtable 1\n\nYou pick up the lettuce 1 from the diningtable 1.\n\n> go to sinkbasin 1\n\n> clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n> go to diningtable 1\n\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n> put lettuce 1 in/on diningtable 1\n\nYou put the lettuce 1 in/on the diningtable 1.\n\n\n\n## Table 8: An ReAct prompt on the ALFWorld clean task.\n\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to:put a clean lettuce in diningtable.\n\n> think:To solve the task, I need to find and take a lettuce, then clean it with sinkbasin, then put it in diningtable.OK.\n\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13).I> think:First I need to find a lettuce.A lettuce is more likely to appear in can check one by one, starting with fridge 1.\n\nOK.\n\n> go to fridge 1\n\nThe fridge 1 is closed.\n\n> open fridge 1\n\nYou open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2.\n\n> go to diningtable 1\n\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\n\n> take lettuce 1 from diningtable 1\n\nYou pick up the lettuce 1 from the diningtable 1.\n\n> think:Now I take a lettuce (1).Next, I need to go to sinkbasin (1) and clean it.OK.\n\n> go to sinkbasin 1\n\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n> clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n> think:Now I clean a lettuce (1).Next, I need to put it in/on diningtable 1.\n\nOK.\n\n> go to diningtable 1\n\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n> put lettuce 1 in/on diningtable 1\n\nYou put the lettuce 1 in/on the diningtable 1.\n\n\n\n## Table 9: An ReAct-IM prompt on the ALFWorld clean task.\n\nYou are in the middle of a room.Looking quickly around you, you see a cabinet\n\nYou are in the middle of a room.13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to:put a clean lettuce in diningtable.\n\n> think:To solve the task, I need to find and take a lettuce, then clean it with sinkbasin, then put it in diningtable.\n\n> think:First I need to find a lettuce.\n\n> go to fridge 1\n\nThe fridge 1 is closed.\n\n> think:First I need to find a lettuce.\n\nOK.\n\n> open fridge 1\n\nYou open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2.\n\n> think:First I need to find a lettuce.\n\n> go to diningtable 1\n\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\n\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n> think:First I need to find a lettuce.\n\n> take lettuce 1 from diningtable 1\n\nYou pick up the lettuce 1 from the diningtable 1\n\n> think:I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n> go to sinkbasin 1\n\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n> think:I need to clean this lettuce (1) using sinkbasin 1.\n\n> clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n> think:I need to put this cleaned lettuce (1) in/on diningtable 1.\n\n> go to diningtable 1\n\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork 2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n> think:I need to put this cleaned lettuce (1) in/on diningtable 1.\n\n> put lettuce 1 in/on diningtable 1\n\nYou put the lettuce 1 in/on the diningtable 1.\n\n## DTRAJECTORIES\n\n## D.1FEVER TRAJECTORIES\n\nWe provide some trajectories produced by ReAct, CoTand Acton several randomly selected examples from the FEVER dev set below. Search results are omitted to space.\n\n\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Example 2491 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Thought 1:</td><td>I need to search Bermuda Triangle and find if it is in the Pacific Ocean.</td></tr><tr><td>Action 1:</td><td>Search[Bermuda Triangle]</td></tr><tr><td>Observation 1:</td><td>Reportedly haunted locations...</td></tr><tr><td>Thought 2:</td><td>The observation says that it is in the western part of the North Atlantic</td></tr><tr><td>Action 2:</td><td>Ocean, so it is not in the Pacific Ocean.</td></tr><tr><td>Observation 2:</td><td>Finish[REFUTES] Episode finished</td></tr><tr><td colspan=\"2\">Act</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Action 1</td><td>:</td></tr><tr><td>Observation 1</td><td>Search[Bermuda Triangle] : Reportedly haunted locations:...</td></tr><tr><td>Action 2:</td><td></td></tr><tr><td>Observation 2:</td><td>Finish[REFUTES] Episode finished</td></tr><tr><td colspan=\"2\">CoT</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Thought:</td><td>Let’s think step by step. The Bermuda Triangle is a loosely-defined region</td></tr><tr><td>Answer:</td><td>in the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean. REFUTES</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Example 5908 (gt: SUPPORTS)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Princess Mononoke is a film.</td></tr><tr><td>Thought 1:</td><td>I need to search Princess Mononoke and find if it is a film.</td></tr><tr><td>Action 1:</td><td>Search[Princess Mononoke]</td></tr><tr><td>Observation 1:</td><td>Princess Mononoke . . .</td></tr><tr><td>Thought 2:</td><td>From the observation, it says that Princess Mononoke is a film.</td></tr><tr><td>Action 2:</td><td>Finish[SUPPORTS] Episode finished</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td colspan=\"2\">Act Claim:</td></tr><tr><td></td><td>Princess Mononoke is a film.</td></tr><tr><td>Action 1:</td><td>Search[Princess Mononoke]</td></tr><tr><td>Observation 1: Action 2:</td><td>Princess Mononoke  ..</td></tr><tr><td>Observation 2:</td><td>Finish[SUPPORTS]</td></tr><tr><td></td><td>Episode finished</td></tr><tr><td colspan=\"2\">CoT Claim:</td></tr><tr><td>Thought:</td><td>Princess Mononoke is a film. Let’s think step by step.Princess Mononoke is a movie, and movies are</td></tr><tr><td>Answer:</td><td>films.</td></tr><tr><td></td><td>SUPPORTS</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Example 1951 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Soyuz was part of the American space program.</td></tr><tr><td>Thought 1:</td><td>I need to search Soyuz and find if it was part of the American space</td></tr><tr><td>Action 1:</td><td>program. Search[Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Thought 2:</td><td>It does not mention the American space program, so I need to search \"American space program\" to find if it was part of it.</td></tr><tr><td>Action 2:</td><td>Search[American space program]</td></tr><tr><td>Observation 2:</td><td>The space policy...</td></tr><tr><td>Thought 3:</td><td>It does not mention Soyuz, so I am not sure if this claim is true or not.</td></tr><tr><td>Action 3: Observation 3:</td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td></td><td>Episode finished</td></tr><tr><td colspan=\"2\">Act Claim:</td></tr><tr><td></td><td>Soyuz was part of the American space program.</td></tr><tr><td>Action 1:</td><td>Search[Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Action 2:</td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td>Observation 2:</td><td>Episode finished</td></tr><tr><td colspan=\"2\">CoT</td></tr><tr><td>Claim: Thought:</td><td>Soyuz was part of the American space program. Let’s think step by step. Soyuz is a Russian spacecraft. The American</td></tr><tr><td></td><td>space program is NASA. NASA and Russia have worked together on the</td></tr><tr><td>Answer:</td><td>International Space Station. SUPPORTS</td></tr></tbody></table></body></html>\n\n\n\n\n\n<html><body><table><thead><tr><td></td><td>Example 3208 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Thought 1:</td><td>I need to search for Reign Over Me and find if it is an American film made</td></tr><tr><td>Action 1:</td><td>in 2010. Search[Reign Over Me]</td></tr><tr><td>Observation 1:</td><td>Reign Over Me ...</td></tr><tr><td>Thought 2:</td><td>The observation says that it is an American film made in 2007, so it is not</td></tr><tr><td>Action 2:</td><td>made in 2010. Finish[REFUTES]</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td>Act</td><td>Episode finished</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Action 1:</td><td>Search[Reign Over Me]</td></tr><tr><td>Action 2:</td><td>Finish[SUPPORTS] Episode finished</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td>CoT Claim:</td><td></td></tr><tr><td>Thought:</td><td>Reign Over Me is an American film made in 2010. Let’s think step by step.First, is Reign Over Me an American film? Yes,</td></tr><tr><td>Answer:</td><td>it is. Second, was it made in 2010?Yes, it was. SUPPORTS</td></tr></tbody></table></body></html>\n\nD.2ALFWORLD TRAJECTORIES\n\nWe provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,pick_clean_then_place_in_recep-Knife-None-CounterTop-10/trial_T20190909_110445_675754.\n\n## D.2.1REACT TRAJECTORY\n\nReAct is able to correctly fnd the knife, clean it, and then place it on a countertop.i\n\n\n\n## ReAct ALFWorld trajectory\n\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\n\nYour task is to:put a clean knife in countertop.\n\nsinkbasin, then put it in countertop.> think:To solve the task, I need to find and take a knife, then clean it with\n\n> think:First I need to find a knife.A knife is more likely to appear in cabinet(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\n\nsinkbasin (1), stoveburner (1-4), toaster (1).I can check one by one, starting with cabinet 1.\n\nOK.\n\n> go to cabinet 1\n\nOn the cabinet 1, you see a bowl 1.\n\n> go to cabinet 2\n\nThe cabinet 2 is closed.\n\n> go to cabinet 3\n\nOn the cabinet 3, you see a glassbottle 1.\n\n> go to cabinet 4\n\nOn the cabinet 4, you see a mug 1.\n\n> go to cabinet 5\n\nThe cabinet 5 is closed.\n\n> go to cabinet 6\n\nThe cabinet 6 is closed.\n\n> go to drawer 1\n\nThe drawer 1 is closed.\n\n> go to drawer 2\n\nThe drawer 2 is closed\n\n> go to drawer 3\n\nThe drawer 3 is closed.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> go to countertop 2\n\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1.\n\n> think:Now I find a knife (1).Next, I need to take it.\n\n> take knife 1 from countertop 2\n\nYou pick up the knife 1 from the countertop 2.\n\n> think:Now I take a knife (1).Next, I need to go to sinkbasin (1) and clean it.\n\n> go to sinkbasin 1\n\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\n\n> clean knife 1 with sinkbasin 1\n\nYou clean the knife 1 using the sinkbasin 1.\n\n> think:Now I clean a knife (1).Next, I need to put it in/on countertop 1.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> put knife 1 in/on countertop 1\n\nYou put the knife 1 in/on the countertop 1\n\n## D.2.2ACT TRAJECTORY\n\nAct is able to correctly fnd the knife, but tries to clean it using a sinkbasin before actually goingi to the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same sequence of commands. With ReAct, an existing thought would have concretely identifed that thei knife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\n\n\n\n## Act ALFWorld trajectory\n\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\n\nYour task is to:put a clean knife in countertop.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> go to countertop 2\n\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1.\n\n> take knife 1 from countertop 2\n\nYou pick up the knife 1 from the countertop 2.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n> go to countertop 3\n\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2.\n\n> take knife 2 from countertop 3\n\nNothing happens.\n\n> go to countertop 2\n\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1.\n\n> take knife 1 from countertop 2\n\nNothing happens.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n> clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n> go to countertop 3\n\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2.\n\n> take knife 2 from countertop 3\n\nNothing happens.\n\n> go to countertop 2\n\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1.\n\n> take knife 1 from countertop 2\n\nNothing happens.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n> go to countertop 3\n\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard 1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2.\n\n> take knife 2 from countertop 3\n\nNothing happens.\n\n> go to countertop 2\n\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1.\n\n> take knife 1 from countertop 2\n\nNothing happens.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1 Nothing happens.\n\n## D.2.3REACT-IM TRAJECTORY\n\nReAct-IM is able to correctly fnd the knife, but is unable to clean it as well. The LLM theni gets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought think:I need to find a clean knife., which seems to trick the LLM in believing that the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured by each thought.\n\n\n\n## ReAct-IM ALFWorld trajectory\n\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\n\nYour task is to:put a clean knife in countertop.\n\nTo solve the task, I need to find and take a clean knife, then put it in countertop.OK.\n\n> think:I need to find a clean knife.\n\nOK.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> think:I need to find a clean knife.\n\nOK.\n\n> go to countertop 2\n\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1.\n\n> think:I need to find a clean knife.\n\nOK.\n\n> take knife 1 from countertop 2\n\nu pick up the knife 1 from the countertop 2.\n\n> think:I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n> go to countertop 1\n\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> think:I need to put this knife (1) in/on countertop 1.\n\n> put knife 1 in/on countertop 1\n\nYou put the knife 1 in/on the countertop 1.\n\n> think:I need to put this knife (1) in/\n\n> put knife 1 in/on countertop 1\n\nNothing happens.\n\n> think:I need to put this knife (1) in/on countertop 1.\n\n> put knife 1 in/on countertop 1\n\nNothing happens.\n\n> think:I need to put this knife (1) in/on countertop 1.\n\n> put knife 1 in/on countertop 1\n\nNothing happens.\n\n> think:I need to put this knife (1) in/on countertop 1.\n\n> put knife 1 in/on countertop 1\n\nNothing happens.\n\n> think:I need to put this knife (1) in/on countertop 1.\n\n> put knife 1 in/on countertop 1\n\nNothing happens.\n\n> think:I need to put this knife (1) in/on countertop 1.OK.\n\n\n\n## D.3WEBSHOP TRAJECTORIES\n\nTable 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,ReAct uses reasoning to fnd products that satisfy all target attributes.i\n\n\n\n<html><body><table><thead><tr><td>Instruction: get me a sixteen pack of apple cinnamon freeze dried ba Act</td></tr></thead><tbody><tr><td>Action: search [sixteen pack apple cinnamon freeze dried banana</td></tr><tr><td>chips]</td></tr><tr><td>Observation: [Back to Search ]</td></tr><tr><td>Page 1 (Total results: 50)</td></tr><tr><td>[Next]</td></tr><tr><td>[B0061IVFZE]</td></tr><tr><td>Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -</td></tr><tr><td>Ounce (Pack of 100) $85.0</td></tr><tr><td>[B096H2P6G2]</td></tr><tr><td>Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -</td></tr><tr><td>(Variety Pack) $18.99</td></tr><tr><td>[B092JLLYK6]</td></tr><tr><td>Nature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect</td></tr><tr><td>For School Lunches or an On-The-Go Snack - No Sugar Added, Non GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Pack</td></tr><tr><td>$12.99</td></tr><tr><td></td></tr><tr><td>Action: click [B0061IVFZE] Observation:</td></tr><tr><td>[Back to Search]</td></tr><tr><td>[Prev]</td></tr><tr><td>favor name [asian pear][banana][fuji apple & cinnamon][strawberry</td></tr><tr><td>banana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,</td></tr><tr><td>0.42 Ounce (Pack of 100)</td></tr><tr><td>Price: $85.0</td></tr><tr><td>Rating: N.A. [Description]</td></tr><tr><td>[Features]</td></tr><tr><td>[Reviews]</td></tr><tr><td>[Buy Now]</td></tr><tr><td>Action: click[Buy Now]</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td rowspan=\"3\">Score: 0.125</td></tr></tr></tr><tr><td></td></tr></table></body></html>\n\ni  \nl\n\nana chips, and price lower than 50.00 dollars  \nReAct  \nAction: seareh [sixteen pack apple cinnamon freeze dried banana  \nchips]  \nObservation:  \n[Back to Search]  \nPage 1 (Total results: 50)  \n[Next]  \n[B0061IVFZE]  \nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -  \nOunce (Pack of 100)  \n $85.0 [B096H2P6G2]  \nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids  \n(Variety Pack)  \n $18.99 [B092JLLYK6]  \nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect  \nFor School Lunches or an On-The-Go Snack - No Sugar Added,  \nNon GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi  \n $12.99 Action: think[B0061IVFZE is strawberry banana, not apple cinnamon.  \nB096H2P6G2 is fruit snacks, not freeze dried banana chips.  \nB092JLLYK6 is banana crisps, not apple cinnamon. I can check  \nB092JLLYK6 frst.i  \nObservation: OK.  \nAction: click[B092JLLYK6]  \nObservation:  \n[Back to Search]  \n[Prev]  \nfavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]l  \n[strawberry][strawberry banana][sampler variety pack][mega variety  \npack][orchard variety pack]  \nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack  \nof 16)]  \nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect  \nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non  \nGMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi  \nPrice: $12.99  \nRating: N.A.  \n[Description]  \n[Features]  \n[Reviews]  \n[Buy Now]  \nAction: think[For sixteen pack of apple cinnamon freeze dried banana  \nchips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of  \n16)’ and seems good to buy.]  \nObservation: OK.  \nAction: click[apple cinnamon]  \nObservation: You have clicked apple cinnamon.  \nAction: click[0.53 ounce (pack of 16)]  \nObservation: You have clicked 0.53 ounce (pack of 16).  \nAction: click[Buy Now]  \nScore: 1.0\n\n\n\n## EMORE ANALYSIS\n\n## E.1SUCCESS AND FAILURE MODES ANALYSIS\n\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.Search results and non-representative steps are omitted to reduce space.\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Success: True positive</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Question:</td><td>Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?</td></tr><tr><td>Thought 1:</td><td>I need to search David Chanoff and find the U.S. Navy admiral he</td></tr><tr><td>Action 1:</td><td>collaborated with. Search[David Chanoff]</td></tr><tr><td>Observation 1:</td><td>David Chanoff is a noted author...</td></tr><tr><td>Action 2:</td><td>Search[William J. Crowe]</td></tr><tr><td>Observation 2:</td><td>William J. Crowe is...</td></tr><tr><td>Thought 3:</td><td>William J. Crowe served as the ambassador to the United Kingdom under</td></tr><tr><td>Action 3:</td><td>President Bill Clinton. So the answer is Bill Clinton.</td></tr><tr><td>Observation 3:</td><td>Finish[Bill Clinton] Episode finished</td></tr><tr><td colspan=\"2\"></td></tr><tr><td>CoT Question:</td><td>What was the name of the team that represented an Arizona university in</td></tr><tr><td>Thought:</td><td>the conference that covers a broad expanse of the western United States, including Arizona, California, and New Mexico? Let’s think step by step. The name of the team that represented an Arizona university in the conference that covers a broad expanse of the western</td></tr><tr><td>Answer:</td><td>United States, including Arizona, California, and New Mexico is Arizona State Sun Devils.</td></tr><tr><td></td><td>Arizona State Sun Devils</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Success: False positive</td></tr></thead><tbody><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>Did the board game San Marco or About Time come out first?</td></tr><tr><td>Thought 1:</td><td>I need to search San Marco and About Time, and find which came out first.</td></tr><tr><td>Action 1:</td><td>Search[San Marco]</td></tr><tr><td>Observation 1:</td><td>San Marco is one of the ... The was built in venice.</td></tr><tr><td>Thought 2:</td><td>San Marco is a sestiere of Venice. I need to search About Time next.</td></tr><tr><td>Action 2: Observation 2:</td><td>Search[About Time]</td></tr><tr><td>Thought 3:</td><td>Could not find [About Time]. Similar: [...].</td></tr><tr><td>Action 3:</td><td>About Time is a 2013 film. So San Marco came out first. Finish[San Marco]</td></tr><tr><td>Observation 3:</td><td>Episode finished</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Question:</td><td>Emory (Emory</td></tr><tr><td></td><td>Was Vanderbilt University or Emory University founded first?  $\\tt\\char62 \\tt1836$ </td></tr><tr><td>Thought:</td><td>Let’s think step by step.Vanderbilt University was founded in 1873.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>University was founded in 1836. 1873 (Vanderbilt University)</td></tr><tr><td></td><td></td></tr><tr><td>Answer:</td><td>University), so Vanderbilt University was founded first. Emory University</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Failure: Reasoning error</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Question:</td><td>What was the middle name of the actress who was cast alongside Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in</td></tr><tr><td>Thought 1:</td><td>Hairspray? I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur, find the actress that was cast alongside them in</td></tr><tr><td>Action 1:</td><td>Hairspray, then find its middle name. Search[Harvey Fierstein]</td></tr><tr><td>Observation 1: Thought 2:</td><td>Harvey Forbes Fierstein .. . Harvey Fierstein is an actor, playwright, and screenwriter.I need to</td></tr><tr><td></td><td>search Linda Hart next...</td></tr><tr><td>Action 2:</td><td>Search[Linda Hart]...</td></tr><tr><td>CoT</td><td colspan=\"2\"></td></tr><tr><td>Question: Thought:</td><td>Who is older Danny Green or James Worthy? Let’s think step by step.</td><td></td></tr><tr><td></td><td>born in 1961.</td><td>Danny Green was born in 1987.James Worthy was</td></tr><tr><td>Answer:</td><td>older. Danny Green</td><td>1987 (Danny Green) > 1961 (James Worthy), so Danny Green is</td></tr></tbody></table></body></html>\n\n\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Failure: Search error</td></tr></thead><tbody><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>What does the goddess associated with the goddess frigg consists of what tales?</td></tr><tr><td>Thought 1:</td><td>I need to search goddess frigg and find its associated goddess.</td></tr><tr><td>Action 1:</td><td>Search[goddess frigg] Could not find goddess frigg. Similar: [’Frigg’, ’Frigg and Freyja common</td></tr><tr><td>Observation 1:</td><td>origin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses and figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’].</td></tr><tr><td>Thought 2:</td><td>I need to search Frigg instead...</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Failure: Hallucination</td></tr></thead><tbody><tr><td>CoT Question:</td><td>What was the first year a scientific journal published by an organization</td></tr><tr><td></td><td>located in the Bronx was published?</td></tr><tr><td>Thought:</td><td>Let’s think step by step.The first year a scientific journal published by an organization located in the Bronx was published was 1916.</td></tr><tr><td>Answer:</td><td>1916 (Label:1909)</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\"></td><td>Failure: Label ambiguity</td></tr></thead><tbody><tr><td>ReAct</td><td></td><td></td></tr><tr><td>Question:</td><td></td><td>What nationality is the man who developed the dictator game?</td></tr><tr><td>Thought 1:</td><td>nationality.</td><td>I need to search dictator game, find the man who developed it, then find his</td></tr><tr><td>Action 1:</td><td>Search[dictator game]</td><td></td></tr><tr><td>Observation 1:</td><td>and economics...</td><td>The dictator game is a popular experimental instrument in social psychology</td></tr><tr><td>Action 3:</td><td>Search[Amos Tversky]</td><td></td></tr><tr><td>Observation 3:</td><td>Amos Nathan Tversky...</td><td></td></tr><tr><td>Thought 4:</td><td>man who developed the dictator game is Israeli.</td><td>Amos Tversky is an Israeli cognitive and mathematical psychologist.So the</td></tr><tr><td>Action 4:</td><td>Finish[Israeli] (label:Israel-American)</td><td></td></tr><tr><td>CoT</td><td></td><td></td></tr><tr><td>Question:</td><td></td><td>Kurt Fuller from Stockton, CA appeared in which American Detective Comedy</td></tr><tr><td>Thought:</td><td></td><td>Drama television series created by Steve Franks? Let’s think step by step.Kurt Fuller from Stockton, CA appeared in Psych.</td></tr><tr><td>Answer:</td><td>Steve Franks. Psych (label:</td><td>Psych is an American Detective Comedy Drama television series created by Psych is an American detective comedy-drama)</td></tr></tbody></table></body></html>\n\n","msg":"","ocr_all":false,"page_count":33,"pages":[{"abandon_blocks":[{"bbox":{"x0":232,"x1":955,"y0":1441,"y1":1528},"conf":0.4949,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1503,"y1":1520},"font_size":0.0,"text":"1"},{"bbox":{"x0":238,"x1":945,"y0":1440,"y1":1465},"font_size":0.0,"text":"∗Work during Google internship. Projet page with code: https://react-lm.github.io/."}],"source":"layout det","text":""},{"bbox":{"x0":20,"x1":73,"y0":410,"y1":1132},"conf":0.4063,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":32,"x1":72,"y0":416,"y1":1122},"font_size":0.0,"text":"0cs.CL]  1[ Mar 2023.rXiv:2210a03629v3  "}],"source":"layout det","text":""},{"bbox":{"x0":207,"x1":1016,"y0":46,"y1":85},"conf":0.214,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":208,"x1":1016,"y0":155,"y1":236},"conf":0.9414,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":1008,"y0":162,"y1":190},"font_size":3.191e-05,"text":"REACT: SYNERGIZING REASONING AND ACTING IN"},{"bbox":{"x0":213,"x1":522,"y0":200,"y1":231},"font_size":3.191e-05,"text":"LANGUAGE MODELS"}],"source":"layout det","text":"REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS"},{"bbox":{"x0":253,"x1":995,"y0":275,"y1":309},"conf":0.8104,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1006,"y0":279,"y1":304},"font_size":3.191e-05,"text":"Shunyu $\\mathrm{Yao}^{*,1},$  Jeffrey Zhao2, Dian $\\mathrm{Yu}^{2},$  Nan $\\mathrm{Du}^{2},$  Izhak Shafran2, Karthik Narasimhan1, Yuan Cao"}],"source":"layout det","text":"Shunyu $\\mathrm{Yao}^{*,1},$  Jeffrey Zhao2, Dian $\\mathrm{Yu}^{2},$  Nan $\\mathrm{Du}^{2},$  Izhak Shafran2, Karthik Narasimhan1, Yuan Cao"},{"bbox":{"x0":381,"x1":842,"y0":319,"y1":367},"conf":0.8456,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":384,"x1":838,"y0":318,"y1":350},"font_size":3.191e-05,"text":"1Department of Computer Science, Princeton University"},{"bbox":{"x0":489,"x1":735,"y0":343,"y1":368},"font_size":3.191e-05,"text":"2Google Research, Brain team"}],"source":"layout det","text":"1Department of Computer Science, Princeton University 2Google Research, Brain team"},{"bbox":{"x0":411,"x1":813,"y0":368,"y1":390},"conf":0.7578,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":411,"x1":810,"y0":361,"y1":393},"font_size":3.191e-05,"text":"1{shunyuy,karthikn}@princeton.edu"}],"source":"layout det","text":"1{shunyuy,karthikn}@princeton.edu"},{"bbox":{"x0":296,"x1":928,"y0":391,"y1":414},"conf":0.777,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":298,"x1":925,"y0":383,"y1":416},"font_size":3.191e-05,"text":"2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com"}],"source":"layout det","text":"2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com"},{"bbox":{"x0":550,"x1":674,"y0":473,"y1":502},"conf":0.8413,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":557,"x1":667,"y0":478,"y1":497},"font_size":3.191e-05,"text":"ABSTRACT"}],"source":"layout det","text":"ABSTRACT"},{"bbox":{"x0":279,"x1":944,"y0":522,"y1":972},"conf":0.9822,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":286,"x1":938,"y0":528,"y1":549},"font_size":3.191e-05,"text":"While large language models (LLMs) have demonstrated impressive performance"},{"bbox":{"x0":284,"x1":938,"y0":551,"y1":573},"font_size":3.191e-05,"text":"across tasks in language understanding and interactive decision making, their"},{"bbox":{"x0":286,"x1":938,"y0":573,"y1":596},"font_size":3.191e-05,"text":"abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action"},{"bbox":{"x0":284,"x1":938,"y0":592,"y1":617},"font_size":3.191e-05,"text":"plan generation) have primarily been studied as separate topics. In this paper, we"},{"bbox":{"x0":283,"x1":936,"y0":614,"y1":637},"font_size":3.191e-05,"text":"explore the use of LLMs to generate both reasoning traces and task-specifc actionsi"},{"bbox":{"x0":284,"x1":938,"y0":639,"y1":660},"font_size":3.191e-05,"text":"in an interleaved manner, allowing for greater synergy between the two: reasoning"},{"bbox":{"x0":284,"x1":938,"y0":658,"y1":681},"font_size":3.191e-05,"text":"traces help the model induce, track, and update action plans as well as handle"},{"bbox":{"x0":286,"x1":938,"y0":681,"y1":703},"font_size":3.191e-05,"text":"exceptions, while actions allow it to interface with and gather additional information"},{"bbox":{"x0":284,"x1":938,"y0":701,"y1":726},"font_size":3.191e-05,"text":"from external sources such as knowledge bases or environments. We apply our"},{"bbox":{"x0":286,"x1":938,"y0":726,"y1":747},"font_size":3.191e-05,"text":"approach, named ReAct, to a diverse set of language and decision making tasks"},{"bbox":{"x0":284,"x1":938,"y0":747,"y1":769},"font_size":3.191e-05,"text":"and demonstrate its effectiveness over state-of-the-art baselines in addition to"},{"bbox":{"x0":286,"x1":938,"y0":769,"y1":792},"font_size":3.191e-05,"text":"improved human interpretability and trustworthiness. Concretely, on question"},{"bbox":{"x0":286,"x1":938,"y0":792,"y1":813},"font_size":3.191e-05,"text":"answering (HotpotQA) and fact verifcation (Fever), ReAct overcomes prevalenti"},{"bbox":{"x0":284,"x1":936,"y0":810,"y1":837},"font_size":3.191e-05,"text":"issues of hallucination and error propagation in chain-of-thought reasoning by"},{"bbox":{"x0":286,"x1":938,"y0":835,"y1":858},"font_size":3.191e-05,"text":"interacting with a simple Wikipedia API, and generating human-like task-solving"},{"bbox":{"x0":286,"x1":938,"y0":858,"y1":879},"font_size":3.191e-05,"text":"trajectories that are more interpretable than baselines without reasoning traces."},{"bbox":{"x0":286,"x1":938,"y0":879,"y1":901},"font_size":3.191e-05,"text":"Furthermore, on two interactive decision making benchmarks (ALFWorld and"},{"bbox":{"x0":284,"x1":936,"y0":898,"y1":922},"font_size":3.191e-05,"text":"WebShop), ReAct outperforms imitation and reinforcement learning methods by"},{"bbox":{"x0":284,"x1":938,"y0":921,"y1":944},"font_size":3.191e-05,"text":"an absolute success rate of $34\\%$  and $10\\%$ respectively, while being prompted with"},{"bbox":{"x0":284,"x1":584,"y0":945,"y1":967},"font_size":3.191e-05,"text":"only one or two in-context examples."}],"source":"layout det","text":"While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specifc actionsi in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verifcation (Fever), ReAct overcomes prevalenti issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces.Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of $34\\%$  and $10\\%$ respectively, while being prompted with only one or two in-context examples."},{"bbox":{"x0":209,"x1":419,"y0":1008,"y1":1040},"conf":0.8945,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":216,"x1":414,"y0":1013,"y1":1035},"font_size":3.191e-05,"text":"1INTRODUCTION"}],"source":"layout det","text":"1INTRODUCTION"},{"bbox":{"x0":206,"x1":1018,"y0":1058,"y1":1352},"conf":0.9824,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":1063,"y1":1084},"font_size":3.191e-05,"text":"A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with"},{"bbox":{"x0":215,"x1":1008,"y0":1087,"y1":1104},"font_size":3.191e-05,"text":"verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to"},{"bbox":{"x0":211,"x1":1011,"y0":1105,"y1":1130},"font_size":3.191e-05,"text":"play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,"},{"bbox":{"x0":215,"x1":1009,"y0":1129,"y1":1152},"font_size":3.191e-05,"text":"1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-"},{"bbox":{"x0":213,"x1":1009,"y0":1152,"y1":1173},"font_size":3.191e-05,"text":"sider the example of cooking up a dish in the kitchen. Between any two specifc actions, we mayi"},{"bbox":{"x0":213,"x1":1009,"y0":1173,"y1":1195},"font_size":3.191e-05,"text":"reason in language in order to track progress (“now that everything is cut, I should heat up the pot of"},{"bbox":{"x0":213,"x1":1009,"y0":1196,"y1":1218},"font_size":3.191e-05,"text":"water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let"},{"bbox":{"x0":213,"x1":1011,"y0":1218,"y1":1241},"font_size":3.191e-05,"text":"me use soy sauce and pepper instead”), and to realize when external information is needed (“how do"},{"bbox":{"x0":211,"x1":1009,"y0":1237,"y1":1262},"font_size":3.191e-05,"text":"I prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the"},{"bbox":{"x0":211,"x1":1008,"y0":1262,"y1":1284},"font_size":3.191e-05,"text":"recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What"},{"bbox":{"x0":213,"x1":1009,"y0":1284,"y1":1305},"font_size":3.191e-05,"text":"dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans"},{"bbox":{"x0":213,"x1":1008,"y0":1305,"y1":1327},"font_size":3.191e-05,"text":"to learn new tasks quickly and perform robust decision making or reasoning, even under previously"},{"bbox":{"x0":215,"x1":675,"y0":1330,"y1":1346},"font_size":3.191e-05,"text":"unseen circumstances or facing information uncertainties."}],"source":"layout det","text":"A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Consider the example of cooking up a dish in the kitchen. Between any two specifc actions, we mayi reason in language in order to track progress (“now that everything is cut, I should heat up the pot of water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let me use soy sauce and pepper instead”), and to realize when external information is needed (“how do I prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties."},{"bbox":{"x0":208,"x1":1017,"y0":1356,"y1":1432},"conf":0.9408,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1008,"y0":1363,"y1":1379},"font_size":3.191e-05,"text":"Recent results have hinted at the possibility of combining verbal reasoning with interactive decision"},{"bbox":{"x0":213,"x1":1009,"y0":1383,"y1":1404},"font_size":3.191e-05,"text":"making in autonomous systems. On one hand, properly prompted large language models (LLMs)"},{"bbox":{"x0":211,"x1":1009,"y0":1402,"y1":1427},"font_size":3.191e-05,"text":"have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive"}],"source":"layout det","text":"Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLMs)have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive"}],"formula_dets":[{"bbox":{"x0":577,"x1":614,"y0":921,"y1":941},"conf":0.829,"label":"print_embedding","label_id":0},{"bbox":{"x0":501,"x1":539,"y0":921,"y1":941},"conf":0.8134,"label":"print_embedding","label_id":0},{"bbox":{"x0":497,"x1":531,"y0":279,"y1":302},"conf":0.69,"label":"print_embedding","label_id":0},{"bbox":{"x0":279,"x1":333,"y0":279,"y1":302},"conf":0.6139,"label":"print_embedding","label_id":0},{"bbox":{"x0":574,"x1":612,"y0":279,"y1":302},"conf":0.5207,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":206,"x1":1018,"y0":1058,"y1":1352},"conf":0.9824,"label":"Text","label_id":1},{"bbox":{"x0":279,"x1":944,"y0":522,"y1":972},"conf":0.9822,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":155,"y1":236},"conf":0.9414,"label":"Title","label_id":0},{"bbox":{"x0":208,"x1":1017,"y0":1356,"y1":1432},"conf":0.9408,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":419,"y0":1008,"y1":1040},"conf":0.8945,"label":"Title","label_id":0},{"bbox":{"x0":381,"x1":842,"y0":319,"y1":367},"conf":0.8456,"label":"Text","label_id":1},{"bbox":{"x0":550,"x1":674,"y0":473,"y1":502},"conf":0.8413,"label":"Title","label_id":0},{"bbox":{"x0":253,"x1":995,"y0":275,"y1":309},"conf":0.8104,"label":"Text","label_id":1},{"bbox":{"x0":296,"x1":928,"y0":391,"y1":414},"conf":0.777,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":592,"y0":49,"y1":81},"conf":0.7601,"label":"Abandon","label_id":2},{"bbox":{"x0":411,"x1":813,"y0":368,"y1":390},"conf":0.7578,"label":"Text","label_id":1},{"bbox":{"x0":232,"x1":955,"y0":1441,"y1":1528},"conf":0.4949,"label":"Abandon","label_id":2},{"bbox":{"x0":20,"x1":73,"y0":410,"y1":1132},"conf":0.4063,"label":"Abandon","label_id":2},{"bbox":{"x0":235,"x1":953,"y0":1440,"y1":1527},"conf":0.3336,"label":"Abandon","label_id":2},{"bbox":{"x0":207,"x1":1016,"y0":46,"y1":85},"conf":0.214,"label":"Abandon","label_id":2},{"bbox":{"x0":236,"x1":953,"y0":1440,"y1":1470},"conf":0.2135,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1503],[619,1503],[619,1520],[604,1520]],"score":0.7013},{"poly":[[238,1440],[945,1442],[945,1465],[238,1464]],"score":0.7338},{"poly":[[211,1402],[1009,1404],[1009,1427],[211,1426]],"score":0.7616},{"poly":[[213,1383],[1009,1383],[1009,1404],[213,1404]],"score":0.7934},{"poly":[[215,1363],[1008,1363],[1008,1379],[215,1379]],"score":0.9923},{"poly":[[215,1330],[675,1330],[675,1346],[215,1346]],"score":0.9545},{"poly":[[213,1305],[1008,1305],[1008,1327],[213,1327]],"score":0.8295},{"poly":[[213,1284],[1009,1284],[1009,1305],[213,1305]],"score":0.8063},{"poly":[[211,1262],[1008,1262],[1008,1284],[211,1284]],"score":0.8113},{"poly":[[211,1239],[1009,1237],[1009,1261],[211,1262]],"score":0.7191},{"poly":[[213,1218],[1011,1218],[1011,1241],[213,1241]],"score":0.6811},{"poly":[[213,1196],[1009,1196],[1009,1218],[213,1218]],"score":0.826},{"poly":[[213,1173],[1009,1173],[1009,1195],[213,1195]],"score":0.8106},{"poly":[[213,1152],[1009,1152],[1009,1173],[213,1173]],"score":0.7972},{"poly":[[215,1129],[1009,1129],[1009,1152],[215,1152]],"score":0.6973},{"poly":[[211,1105],[1011,1107],[1011,1130],[211,1129]],"score":0.7282},{"poly":[[215,1087],[1008,1087],[1008,1104],[215,1104]],"score":0.9911},{"poly":[[213,1063],[1008,1063],[1008,1084],[213,1084]],"score":0.8204},{"poly":[[248,1013],[414,1013],[414,1035],[248,1035]],"score":0.8308},{"poly":[[216,1015],[249,1015],[249,1033],[216,1033]],"score":0.6596},{"poly":[[284,945],[584,945],[584,967],[284,967]],"score":0.8259},{"poly":[[284,922],[938,922],[938,944],[284,944]],"score":0.8319},{"poly":[[284,898],[936,899],[936,922],[284,921]],"score":0.7244},{"poly":[[286,879],[938,879],[938,901],[286,901]],"score":0.7691},{"poly":[[286,858],[938,858],[938,879],[286,879]],"score":0.8056},{"poly":[[286,835],[938,835],[938,858],[286,858]],"score":0.6935},{"poly":[[284,810],[936,813],[936,837],[284,833]],"score":0.698},{"poly":[[286,792],[938,792],[938,813],[286,813]],"score":0.8033},{"poly":[[286,769],[938,769],[938,792],[286,792]],"score":0.6738},{"poly":[[284,747],[938,747],[938,769],[284,769]],"score":0.7976},{"poly":[[286,726],[938,726],[938,747],[286,747]],"score":0.8011},{"poly":[[284,701],[938,703],[938,726],[284,724]],"score":0.7071},{"poly":[[286,681],[938,681],[938,703],[286,703]],"score":0.8062},{"poly":[[284,660],[938,658],[938,680],[284,681]],"score":0.8269},{"poly":[[284,639],[938,639],[938,660],[284,660]],"score":0.8156},{"poly":[[283,614],[936,614],[936,637],[283,637]],"score":0.6994},{"poly":[[284,592],[938,594],[938,617],[284,615]],"score":0.6959},{"poly":[[286,573],[938,573],[938,596],[286,596]],"score":0.7034},{"poly":[[284,551],[938,551],[938,573],[284,573]],"score":0.8141},{"poly":[[286,528],[938,528],[938,549],[286,549]],"score":0.8185},{"poly":[[557,478],[667,478],[667,497],[557,497]],"score":0.9177},{"poly":[[32,416],[72,416],[72,1122],[32,1122]],"score":0.8206},{"poly":[[298,383],[925,388],[925,416],[298,411]],"score":0.6558},{"poly":[[411,361],[810,365],[810,393],[411,389]],"score":0.6302},{"poly":[[489,343],[735,345],[735,368],[489,366]],"score":0.7267},{"poly":[[384,318],[838,322],[838,350],[384,346]],"score":0.6493},{"poly":[[213,281],[1006,279],[1006,302],[213,304]],"score":0.7759},{"poly":[[213,200],[522,203],[522,231],[213,228]],"score":0.7743},{"poly":[[215,162],[1008,162],[1008,190],[215,190]],"score":0.796},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7586}],"page_no":0,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":209,"x1":1015,"y0":46,"y1":85},"conf":0.5152,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":598,"x1":624,"y0":1499,"y1":1528},"conf":0.4506,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1502,"y1":1521},"font_size":0.0,"text":"2"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":215,"x1":1009,"y0":155,"y1":811},"conf":0.9649,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![7ffea04fde729d63b580c8b63747bd25](imgs/7ffea04fde729d63b580c8b63747bd25.jpg)"},{"bbox":{"x0":206,"x1":1018,"y0":824,"y1":945},"conf":0.928,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":830,"y1":851},"font_size":3.166e-05,"text":"Figure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,"},{"bbox":{"x0":213,"x1":1009,"y0":850,"y1":871},"font_size":3.166e-05,"text":"Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)"},{"bbox":{"x0":211,"x1":1008,"y0":871,"y1":893},"font_size":3.166e-05,"text":"question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar"},{"bbox":{"x0":213,"x1":1009,"y0":894,"y1":916},"font_size":3.166e-05,"text":"et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task"},{"bbox":{"x0":213,"x1":910,"y0":916,"y1":937},"font_size":3.166e-05,"text":"solving trajectories generated by the model (Act, Thought) and the environment (Obs)."}],"source":"layout det","text":"Figure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task solving trajectories generated by the model (Act, Thought) and the environment (Obs)."},{"bbox":{"x0":208,"x1":1017,"y0":987,"y1":1326},"conf":0.9813,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":992,"y1":1013},"font_size":3.166e-05,"text":"answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,"},{"bbox":{"x0":215,"x1":1011,"y0":1013,"y1":1035},"font_size":3.166e-05,"text":"2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses"},{"bbox":{"x0":213,"x1":1006,"y0":1036,"y1":1053},"font_size":3.166e-05,"text":"its own internal representations to generate thoughts and is not grounded in the external world"},{"bbox":{"x0":210,"x1":1009,"y0":1056,"y1":1077},"font_size":3.166e-05,"text":"which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact"},{"bbox":{"x0":213,"x1":1011,"y0":1079,"y1":1101},"font_size":3.166e-05,"text":"hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,"},{"bbox":{"x0":215,"x1":1008,"y0":1102,"y1":1119},"font_size":3.166e-05,"text":"recent work has explored the use of pre-trained language models for planning and acting in interactive"},{"bbox":{"x0":211,"x1":1009,"y0":1120,"y1":1145},"font_size":3.166e-05,"text":"environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with"},{"bbox":{"x0":213,"x1":1009,"y0":1145,"y1":1168},"font_size":3.166e-05,"text":"a focus on predicting actions via language priors. These approaches usually convert multi-modal"},{"bbox":{"x0":215,"x1":1009,"y0":1170,"y1":1189},"font_size":3.166e-05,"text":"observations into text, use a language model to generate domain-specifc actions or plans, and theni"},{"bbox":{"x0":211,"x1":1011,"y0":1190,"y1":1211},"font_size":3.166e-05,"text":"use a controller to choose or execute them. However, they do not employ language models to reason"},{"bbox":{"x0":211,"x1":1010,"y0":1208,"y1":1234},"font_size":3.166e-05,"text":"abstractly about high-level goals or maintain a working memory to support acting, barring Huang"},{"bbox":{"x0":213,"x1":1011,"y0":1233,"y1":1254},"font_size":3.166e-05,"text":"et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the"},{"bbox":{"x0":213,"x1":1011,"y0":1254,"y1":1275},"font_size":3.166e-05,"text":"current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been"},{"bbox":{"x0":211,"x1":1011,"y0":1274,"y1":1300},"font_size":3.166e-05,"text":"studies on how reasoning and acting can be combined in a synergistic manner for general task solving,"},{"bbox":{"x0":213,"x1":973,"y0":1299,"y1":1320},"font_size":3.166e-05,"text":"and if such a combination can bring systematic benefts compared to reasoning or acting alone.i"}],"source":"layout det","text":"answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors. These approaches usually convert multi-modal observations into text, use a language model to generate domain-specifc actions or plans, and theni use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring Huang et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been studies on how reasoning and acting can be combined in a synergistic manner for general task solving,and if such a combination can bring systematic benefts compared to reasoning or acting alone.i"},{"bbox":{"x0":207,"x1":1017,"y0":1329,"y1":1471},"conf":0.9687,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":1330,"y1":1355},"font_size":3.166e-05,"text":"In this work, we present ReAct, a general paradigm to combine reasoning and acting with language"},{"bbox":{"x0":213,"x1":1009,"y0":1355,"y1":1376},"font_size":3.166e-05,"text":"models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct"},{"bbox":{"x0":215,"x1":1011,"y0":1376,"y1":1399},"font_size":3.166e-05,"text":"prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an"},{"bbox":{"x0":215,"x1":1009,"y0":1401,"y1":1417},"font_size":3.166e-05,"text":"interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and"},{"bbox":{"x0":213,"x1":1011,"y0":1421,"y1":1442},"font_size":3.166e-05,"text":"adjust high-level plans for acting (reason to act), while also interact with the external environments"},{"bbox":{"x0":211,"x1":886,"y0":1440,"y1":1465},"font_size":3.166e-05,"text":"(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason)."}],"source":"layout det","text":"In this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason)."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":208,"x1":1017,"y0":987,"y1":1326},"conf":0.9813,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1017,"y0":1329,"y1":1471},"conf":0.9687,"label":"Text","label_id":1},{"bbox":{"x0":215,"x1":1009,"y0":155,"y1":811},"conf":0.9649,"label":"Figure","label_id":3},{"bbox":{"x0":206,"x1":1018,"y0":824,"y1":945},"conf":0.928,"label":"Figure caption","label_id":4},{"bbox":{"x0":208,"x1":593,"y0":49,"y1":81},"conf":0.621,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":1015,"y0":46,"y1":85},"conf":0.5152,"label":"Abandon","label_id":2},{"bbox":{"x0":598,"x1":624,"y0":1499,"y1":1528},"conf":0.4506,"label":"Abandon","label_id":2},{"bbox":{"x0":601,"x1":622,"y0":1500,"y1":1526},"conf":0.3849,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1502],[619,1502],[619,1521],[604,1521]],"score":0.7659},{"poly":[[211,1442],[886,1440],[886,1464],[211,1465]],"score":0.7146},{"poly":[[213,1421],[1011,1421],[1011,1442],[213,1442]],"score":0.7655},{"poly":[[215,1401],[1009,1401],[1009,1417],[215,1417]],"score":0.963},{"poly":[[215,1376],[1011,1376],[1011,1399],[215,1399]],"score":0.6631},{"poly":[[213,1355],[1009,1355],[1009,1376],[213,1376]],"score":0.8049},{"poly":[[211,1330],[1009,1332],[1009,1355],[211,1353]],"score":0.7315},{"poly":[[213,1299],[973,1299],[973,1320],[213,1320]],"score":0.7974},{"poly":[[211,1274],[1011,1277],[1011,1300],[211,1297]],"score":0.679},{"poly":[[213,1254],[1011,1254],[1011,1275],[213,1275]],"score":0.7151},{"poly":[[213,1233],[1011,1233],[1011,1254],[213,1254]],"score":0.7783},{"poly":[[211,1208],[1010,1211],[1009,1234],[211,1231]],"score":0.6576},{"poly":[[211,1190],[1011,1190],[1011,1211],[211,1211]],"score":0.7465},{"poly":[[215,1170],[1009,1170],[1009,1186],[215,1186]],"score":0.9466},{"poly":[[213,1145],[1009,1145],[1009,1168],[213,1168]],"score":0.6554},{"poly":[[211,1122],[1009,1120],[1009,1143],[211,1145]],"score":0.7336},{"poly":[[215,1102],[1008,1102],[1008,1119],[215,1119]],"score":0.7862},{"poly":[[213,1079],[1011,1079],[1011,1101],[213,1101]],"score":0.7618},{"poly":[[210,1056],[1009,1056],[1009,1077],[210,1077]],"score":0.78},{"poly":[[213,1036],[1006,1036],[1006,1053],[213,1053]],"score":0.788},{"poly":[[215,1013],[1011,1013],[1011,1035],[215,1035]],"score":0.7745},{"poly":[[213,992],[1009,992],[1009,1013],[213,1013]],"score":0.7801},{"poly":[[213,916],[910,916],[910,937],[213,937]],"score":0.8309},{"poly":[[213,894],[1009,894],[1009,916],[213,916]],"score":0.7738},{"poly":[[211,871],[1008,871],[1008,893],[211,893]],"score":0.7687},{"poly":[[213,850],[1009,850],[1009,871],[213,871]],"score":0.789},{"poly":[[213,830],[1011,830],[1011,851],[213,851]],"score":0.8255},{"poly":[[674,785],[935,785],[935,797],[674,797]],"score":0.8772},{"poly":[[624,784],[677,784],[677,800],[624,800]],"score":0.8607},{"poly":[[276,784],[378,784],[378,800],[276,800]],"score":0.7741},{"poly":[[231,782],[276,782],[276,799],[231,799]],"score":0.8437},{"poly":[[584,777],[604,777],[604,800],[584,800]],"score":0.8725},{"poly":[[276,769],[499,769],[499,785],[276,785]],"score":0.736},{"poly":[[234,771],[276,771],[276,785],[234,785]],"score":0.9074},{"poly":[[629,751],[780,751],[780,762],[629,762]],"score":0.9048},{"poly":[[276,746],[376,746],[376,757],[276,757]],"score":0.9711},{"poly":[[234,744],[276,744],[276,759],[234,759]],"score":0.8174},{"poly":[[627,738],[752,738],[752,749],[627,749]],"score":0.8934},{"poly":[[379,731],[494,731],[494,747],[379,747]],"score":0.7649},{"poly":[[233,731],[382,731],[382,747],[233,747]],"score":0.6745},{"poly":[[625,711],[783,711],[783,728],[625,728]],"score":0.7566},{"poly":[[627,703],[752,703],[752,714],[627,714]],"score":0.7001},{"poly":[[234,696],[547,696],[547,713],[234,713]],"score":0.7303},{"poly":[[627,691],[770,691],[770,703],[627,703]],"score":0.924},{"poly":[[236,686],[378,686],[378,698],[236,698]],"score":0.9616},{"poly":[[625,676],[981,676],[981,693],[625,693]],"score":0.7622},{"poly":[[233,662],[462,660],[462,676],[233,678]],"score":0.6614},{"poly":[[624,652],[986,652],[986,668],[624,668]],"score":0.7227},{"poly":[[234,648],[580,648],[580,665],[234,665]],"score":0.7031},{"poly":[[625,640],[898,640],[898,657],[625,657]],"score":0.6369},{"poly":[[233,635],[356,637],[356,653],[233,652]],"score":0.7162},{"poly":[[234,614],[414,614],[414,630],[234,630]],"score":0.7623},{"poly":[[231,601],[356,601],[356,617],[231,617]],"score":0.7428},{"poly":[[381,592],[472,592],[472,610],[381,610]],"score":0.7298},{"poly":[[627,579],[968,579],[968,596],[627,596]],"score":0.6938},{"poly":[[629,566],[873,566],[873,578],[629,578]],"score":0.8328},{"poly":[[235,563],[564,564],[564,582],[234,581]],"score":0.6692},{"poly":[[236,554],[557,554],[557,566],[236,566]],"score":0.7681},{"poly":[[629,553],[757,553],[757,564],[629,564]],"score":0.8561},{"poly":[[594,540],[602,548],[594,556],[585,548]],"score":0.8542},{"poly":[[494,540],[590,540],[590,556],[494,556]],"score":0.7113},{"poly":[[236,540],[496,540],[496,556],[236,556]],"score":0.6459},{"poly":[[625,525],[966,526],[966,545],[625,543]],"score":0.6668},{"poly":[[437,526],[585,525],[585,543],[438,545]],"score":0.66},{"poly":[[234,525],[434,525],[434,541],[234,541]],"score":0.6062},{"poly":[[737,511],[885,511],[885,528],[737,528]],"score":0.7932},{"poly":[[384,512],[461,512],[461,528],[384,528]],"score":0.8311},{"poly":[[230,475],[266,475],[266,488],[230,488]],"score":0.9001},{"poly":[[381,470],[396,470],[396,488],[381,488]],"score":0.8052},{"poly":[[595,467],[610,467],[610,487],[595,487]],"score":0.9193},{"poly":[[412,465],[522,465],[522,482],[412,482]],"score":0.8771},{"poly":[[627,462],[712,462],[712,478],[627,478]],"score":0.6998},{"poly":[[705,460],[848,460],[848,477],[705,477]],"score":0.6808},{"poly":[[231,459],[391,459],[391,475],[231,475]],"score":0.7576},{"poly":[[629,452],[760,452],[760,464],[629,464]],"score":0.7266},{"poly":[[855,441],[958,441],[958,452],[855,452]],"score":0.953},{"poly":[[629,441],[851,441],[851,452],[629,452]],"score":0.6415},{"poly":[[416,439],[472,439],[472,450],[416,450]],"score":0.8707},{"poly":[[234,436],[268,436],[268,447],[234,447]],"score":0.9562},{"poly":[[688,426],[976,426],[976,442],[688,442]],"score":0.7258},{"poly":[[625,426],[695,426],[695,442],[625,442]],"score":0.749},{"poly":[[414,422],[565,422],[565,439],[414,439]],"score":0.809},{"poly":[[231,419],[369,419],[369,436],[231,436]],"score":0.686},{"poly":[[414,409],[544,409],[544,426],[414,426]],"score":0.7927},{"poly":[[234,406],[374,406],[374,422],[234,422]],"score":0.7153},{"poly":[[625,403],[966,403],[966,419],[625,419]],"score":0.7159},{"poly":[[414,394],[481,394],[481,412],[414,412]],"score":0.7232},{"poly":[[233,393],[387,393],[387,409],[233,409]],"score":0.6429},{"poly":[[669,393],[708,393],[708,404],[669,404]],"score":0.758},{"poly":[[627,393],[667,393],[667,404],[627,404]],"score":0.8438},{"poly":[[758,389],[837,389],[837,406],[758,406]],"score":0.6998},{"poly":[[416,384],[456,384],[456,396],[416,396]],"score":0.923},{"poly":[[452,381],[555,381],[555,398],[452,398]],"score":0.7917},{"poly":[[629,381],[713,381],[713,393],[629,393]],"score":0.8991},{"poly":[[236,381],[392,381],[392,393],[236,393]],"score":0.9896},{"poly":[[943,368],[976,368],[976,381],[943,381]],"score":0.9057},{"poly":[[625,366],[933,366],[933,383],[625,383]],"score":0.6682},{"poly":[[236,368],[374,368],[374,380],[236,380]],"score":0.9806},{"poly":[[416,355],[506,355],[506,366],[416,366]],"score":0.8109},{"poly":[[236,355],[369,355],[369,366],[236,366]],"score":0.9693},{"poly":[[416,342],[582,342],[582,353],[416,353]],"score":0.9305},{"poly":[[234,342],[283,342],[283,353],[234,353]],"score":0.9462},{"poly":[[288,338],[387,338],[387,355],[288,355]],"score":0.7404},{"poly":[[723,335],[873,335],[873,346],[723,346]],"score":0.9383},{"poly":[[878,333],[933,333],[933,345],[878,345]],"score":0.7734},{"poly":[[452,325],[562,325],[562,342],[452,342]],"score":0.7265},{"poly":[[412,325],[457,325],[457,342],[412,342]],"score":0.7942},{"poly":[[230,322],[386,322],[386,343],[230,343]],"score":0.6893},{"poly":[[935,322],[955,322],[955,333],[935,333]],"score":0.9227},{"poly":[[885,322],[933,322],[933,333],[885,333]],"score":0.6759},{"poly":[[627,322],[881,322],[881,333],[627,333]],"score":0.9214},{"poly":[[625,307],[775,307],[775,323],[625,323]],"score":0.6317},{"poly":[[244,307],[379,307],[379,323],[244,323]],"score":0.8123},{"poly":[[417,299],[499,299],[499,310],[417,310]],"score":0.8772},{"poly":[[414,284],[589,284],[589,300],[414,300]],"score":0.7416},{"poly":[[950,282],[985,282],[985,300],[950,300]],"score":0.8796},{"poly":[[381,277],[394,277],[394,297],[381,297]],"score":0.9229},{"poly":[[236,272],[313,272],[313,289],[236,289]],"score":0.9392},{"poly":[[412,269],[575,269],[575,285],[412,285]],"score":0.8334},{"poly":[[469,251],[554,251],[554,267],[469,267]],"score":0.9777},{"poly":[[269,251],[356,251],[356,267],[269,267]],"score":0.9792},{"poly":[[767,251],[813,251],[813,262],[767,262]],"score":0.7924},{"poly":[[629,249],[765,249],[765,261],[629,261]],"score":0.7422},{"poly":[[629,239],[752,239],[752,251],[629,251]],"score":0.6774},{"poly":[[627,224],[970,224],[970,241],[627,241]],"score":0.6741},{"poly":[[234,219],[402,219],[402,236],[234,236]],"score":0.7269},{"poly":[[627,213],[788,215],[788,228],[627,226]],"score":0.6908},{"poly":[[234,206],[560,206],[560,223],[234,223]],"score":0.7625},{"poly":[[629,205],[936,205],[936,216],[629,216]],"score":0.8299},{"poly":[[627,190],[945,190],[945,206],[627,206]],"score":0.7865},{"poly":[[234,190],[590,190],[590,206],[234,206]],"score":0.809},{"poly":[[732,168],[890,168],[890,185],[732,185]],"score":0.7907},{"poly":[[374,168],[462,168],[462,186],[374,186]],"score":0.6821},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7115}],"page_no":1,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":209,"x1":592,"y0":50,"y1":80},"conf":0.8004,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":235,"x1":984,"y0":1441,"y1":1527},"conf":0.5825,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1503,"y1":1521},"font_size":0.0,"text":"3"},{"bbox":{"x0":239,"x1":973,"y0":1444,"y1":1465},"font_size":0.0,"text":"1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B."}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":206,"x1":1018,"y0":163,"y1":519},"conf":0.9832,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":167,"y1":188},"font_size":-35090000.0,"text":"We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:"},{"bbox":{"x0":211,"x1":1009,"y0":188,"y1":213},"font_size":-35090000.0,"text":"question answering (HotPotQA, Yang et al., 2018), fact verifcation (Fever, Thorne et al., 2018),i"},{"bbox":{"x0":213,"x1":1009,"y0":211,"y1":233},"font_size":-35090000.0,"text":"text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao"},{"bbox":{"x0":213,"x1":1009,"y0":233,"y1":254},"font_size":-35090000.0,"text":"et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact"},{"bbox":{"x0":213,"x1":1011,"y0":254,"y1":277},"font_size":-35090000.0,"text":"with, ReAct outperforms vanilla action generation models while being competitive with chain-of-"},{"bbox":{"x0":213,"x1":1009,"y0":276,"y1":299},"font_size":-35090000.0,"text":"thought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct"},{"bbox":{"x0":213,"x1":1009,"y0":299,"y1":320},"font_size":-35090000.0,"text":"and CoT that allows for the use of both internal knowledge and externally obtained information"},{"bbox":{"x0":213,"x1":1009,"y0":320,"y1":343},"font_size":-35090000.0,"text":"during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able"},{"bbox":{"x0":213,"x1":1009,"y0":340,"y1":365},"font_size":-35090000.0,"text":"to outperform imitation or reinforcement learning methods trained with $10^{3}\\sim\\hat{10}^{5}$ task instances,"},{"bbox":{"x0":213,"x1":1009,"y0":363,"y1":386},"font_size":-35090000.0,"text":"with an absolute improvement of $34\\%$  and $10\\%$  in success rates respectively. We also demonstrate"},{"bbox":{"x0":213,"x1":1009,"y0":388,"y1":409},"font_size":-35090000.0,"text":"the importance of sparse, versatile reasoning in decision making by showing consistent advantages"},{"bbox":{"x0":213,"x1":1009,"y0":409,"y1":431},"font_size":-35090000.0,"text":"over controlled baselines with actions only. Besides general applicability and performance boost,"},{"bbox":{"x0":213,"x1":1011,"y0":431,"y1":452},"font_size":-35090000.0,"text":"the combination of reasoning and acting also contributes to model interpretability, trustworthiness,"},{"bbox":{"x0":213,"x1":1009,"y0":452,"y1":474},"font_size":-35090000.0,"text":"and diagnosability across all domains, as humans can readily distinguish information from model’s"},{"bbox":{"x0":213,"x1":1009,"y0":474,"y1":495},"font_size":-35090000.0,"text":"internal knowledge versus external environments, as well as inspect reasoning traces to understand"},{"bbox":{"x0":213,"x1":501,"y0":495,"y1":516},"font_size":-35090000.0,"text":"the decision basis of model actions."}],"source":"layout det","text":"We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:question answering (HotPotQA, Yang et al., 2018), fact verifcation (Fever, Thorne et al., 2018),i text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact with, ReAct outperforms vanilla action generation models while being competitive with chain-ofthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct and CoT that allows for the use of both internal knowledge and externally obtained information during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able to outperform imitation or reinforcement learning methods trained with $10^{3}\\sim\\hat{10}^{5}$ task instances,with an absolute improvement of $34\\%$  and $10\\%$  in success rates respectively. We also demonstrate the importance of sparse, versatile reasoning in decision making by showing consistent advantages over controlled baselines with actions only. Besides general applicability and performance boost,the combination of reasoning and acting also contributes to model interpretability, trustworthiness,and diagnosability across all domains, as humans can readily distinguish information from model’s internal knowledge versus external environments, as well as inspect reasoning traces to understand the decision basis of model actions."},{"bbox":{"x0":207,"x1":1016,"y0":525,"y1":754},"conf":0.9763,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":530,"y1":551},"font_size":-35090000.0,"text":"To summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-"},{"bbox":{"x0":213,"x1":1009,"y0":551,"y1":574},"font_size":-35090000.0,"text":"based paradigm to synergize reasoning and acting in language models for general task solving; (2) we"},{"bbox":{"x0":211,"x1":1008,"y0":573,"y1":594},"font_size":-35090000.0,"text":"perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a"},{"bbox":{"x0":213,"x1":1009,"y0":596,"y1":617},"font_size":-35090000.0,"text":"few-shot learning setup over prior approaches that perform either reasoning or action generation in"},{"bbox":{"x0":211,"x1":1008,"y0":615,"y1":639},"font_size":-35090000.0,"text":"isolation; (3) we present systematic ablations and analysis to understand the importance of acting in"},{"bbox":{"x0":213,"x1":1009,"y0":640,"y1":662},"font_size":-35090000.0,"text":"reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the"},{"bbox":{"x0":213,"x1":1009,"y0":662,"y1":685},"font_size":-35090000.0,"text":"prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial fnetuningi"},{"bbox":{"x0":210,"x1":1011,"y0":680,"y1":710},"font_size":-35090000.0,"text":"experiments showing the potential of ReAct to improve with additional training data. Scaling up"},{"bbox":{"x0":213,"x1":1009,"y0":706,"y1":728},"font_size":-35090000.0,"text":"ReAct to train and operate on more tasks and combining it with complementary paradigms like"},{"bbox":{"x0":213,"x1":880,"y0":728,"y1":749},"font_size":-35090000.0,"text":"reinforcement learning could further unlock the potential of large language models."}],"source":"layout det","text":"To summarize, our key contributions are the following: (1) we introduce ReAct, a novel promptbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a few-shot learning setup over prior approaches that perform either reasoning or action generation in isolation; (3) we present systematic ablations and analysis to understand the importance of acting in reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial fnetuningi experiments showing the potential of ReAct to improve with additional training data. Scaling up ReAct to train and operate on more tasks and combining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models."},{"bbox":{"x0":208,"x1":726,"y0":778,"y1":809},"conf":0.8356,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":722,"y0":784,"y1":805},"font_size":-35090000.0,"text":"2REAC T: SYNERGIZING REASONING + ACTING"}],"source":"layout det","text":"2REAC T: SYNERGIZING REASONING + ACTING"},{"bbox":{"x0":207,"x1":1017,"y0":826,"y1":1011},"conf":0.9757,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":832,"y1":853},"font_size":-35090000.0,"text":"Consider a general setup of an agent interacting with an environment for task solving. At time"},{"bbox":{"x0":213,"x1":1008,"y0":851,"y1":874},"font_size":-35090000.0,"text":"step $t,$  an agent receives an observation $o_{t} \\in\\mathcal{O}$  from the environment and takes an action $a_{t}\\in\\mathcal{A}$"},{"bbox":{"x0":213,"x1":1011,"y0":875,"y1":899},"font_size":-35090000.0,"text":"following some policy $\\pi(a_{t}|c_{t}),$  where $c_{t}=(o_{1},a_{1},\\cdots,o_{t-1},a_{t-1},o_{t})$  is the context to the agent."},{"bbox":{"x0":213,"x1":1009,"y0":898,"y1":919},"font_size":-35090000.0,"text":"Learning a policy is challenging when the mapping $c_{t}\\mapsto a_{t}$ � is highly implicit and requires extensive"},{"bbox":{"x0":213,"x1":1009,"y0":919,"y1":940},"font_size":-35090000.0,"text":"computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct fnali"},{"bbox":{"x0":213,"x1":1009,"y0":940,"y1":962},"font_size":-35090000.0,"text":"action (Act 4) to fnish the QA task as it requires complex reasoning over the trajectory contexti"},{"bbox":{"x0":213,"x1":1009,"y0":962,"y1":985},"font_size":-35090000.0,"text":"(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the"},{"bbox":{"x0":213,"x1":1011,"y0":985,"y1":1008},"font_size":-35090000.0,"text":"context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions."}],"source":"layout det","text":"Consider a general setup of an agent interacting with an environment for task solving. At time step $t,$  an agent receives an observation $o_{t} \\in\\mathcal{O}$  from the environment and takes an action $a_{t}\\in\\mathcal{A}$ following some policy $\\pi(a_{t}|c_{t}),$  where $c_{t}=(o_{1},a_{1},\\cdots,o_{t-1},a_{t-1},o_{t})$  is the context to the agent.Learning a policy is challenging when the mapping $c_{t}\\mapsto a_{t}$ � is highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct fnali action (Act 4) to fnish the QA task as it requires complex reasoning over the trajectory contexti(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions."},{"bbox":{"x0":207,"x1":1017,"y0":1015,"y1":1222},"conf":0.9733,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":1017,"y1":1043},"font_size":-35090000.0,"text":"The idea of ReAct is simple: we augment the agent’s action space to $\\hat{\\mathcal{A}}=\\mathcal{A}\\cup\\mathcal{L},$  where $\\mathcal{L}$  is the"},{"bbox":{"x0":213,"x1":1011,"y0":1041,"y1":1068},"font_size":-35090000.0,"text":"space of language. An action $\\hat{a}_{t}\\in\\mathcal{L}$  in the language space, which we will refer to as a thought or a"},{"bbox":{"x0":213,"x1":1011,"y0":1066,"y1":1087},"font_size":-35090000.0,"text":"reasoning trace, does not affect the external environment, thus leading to no observation feedback."},{"bbox":{"x0":211,"x1":1008,"y0":1086,"y1":1110},"font_size":-35090000.0,"text":"Instead, a thought $\\widehat{a}_{t}$  aims to compose useful information by reasoning over the current context $c_{t}$"},{"bbox":{"x0":213,"x1":1011,"y0":1110,"y1":1134},"font_size":-35090000.0,"text":"and update the context $c_{t+1}=(c_{t},\\hat{a}_{t})$  to support future reasoning or acting. As shown in Figure 1,"},{"bbox":{"x0":213,"x1":1009,"y0":1132,"y1":1153},"font_size":-35090000.0,"text":"there could be various types of useful thoughts, e.g. decomposing task goals and create action plans"},{"bbox":{"x0":213,"x1":1009,"y0":1153,"y1":1176},"font_size":-35090000.0,"text":"(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),"},{"bbox":{"x0":211,"x1":1008,"y0":1175,"y1":1196},"font_size":-35090000.0,"text":"extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans"},{"bbox":{"x0":215,"x1":865,"y0":1198,"y1":1219},"font_size":-35090000.0,"text":"(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on."}],"source":"layout det","text":"The idea of ReAct is simple: we augment the agent’s action space to $\\hat{\\mathcal{A}}=\\mathcal{A}\\cup\\mathcal{L},$  where $\\mathcal{L}$  is the space of language. An action $\\hat{a}_{t}\\in\\mathcal{L}$  in the language space, which we will refer to as a thought or a reasoning trace, does not affect the external environment, thus leading to no observation feedback. Instead, a thought $\\widehat{a}_{t}$  aims to compose useful information by reasoning over the current context $c_{t}$ and update the context $c_{t+1}=(c_{t},\\hat{a}_{t})$  to support future reasoning or acting. As shown in Figure 1,there could be various types of useful thoughts, e.g. decomposing task goals and create action plans(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on."},{"bbox":{"x0":207,"x1":1016,"y0":1226,"y1":1433},"conf":0.9743,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":1229,"y1":1251},"font_size":-35090000.0,"text":"However, as the language space $\\mathcal{L}$  is unlimited, learning in this augmented action space is diffculti"},{"bbox":{"x0":213,"x1":1009,"y0":1252,"y1":1275},"font_size":-35090000.0,"text":"and requires strong language priors. In this paper, we mainly focus on the setup where a frozen"},{"bbox":{"x0":215,"x1":1009,"y0":1274,"y1":1297},"font_size":-35090000.0,"text":"large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context"},{"bbox":{"x0":215,"x1":1009,"y0":1297,"y1":1318},"font_size":-35090000.0,"text":"examples to generate both domain-specifc actions and free-form language thoughts for task solvingi"},{"bbox":{"x0":213,"x1":1009,"y0":1318,"y1":1341},"font_size":-35090000.0,"text":"(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and"},{"bbox":{"x0":211,"x1":1008,"y0":1341,"y1":1363},"font_size":-35090000.0,"text":"environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is"},{"bbox":{"x0":213,"x1":1009,"y0":1363,"y1":1384},"font_size":-35090000.0,"text":"of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the"},{"bbox":{"x0":213,"x1":1009,"y0":1384,"y1":1406},"font_size":-35090000.0,"text":"task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision"},{"bbox":{"x0":215,"x1":1009,"y0":1407,"y1":1429},"font_size":-35090000.0,"text":"making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to"}],"source":"layout det","text":"However, as the language space $\\mathcal{L}$  is unlimited, learning in this augmented action space is diffculti and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both domain-specifc actions and free-form language thoughts for task solvingi(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to"}],"formula_dets":[{"bbox":{"x0":401,"x1":521,"y0":1110,"y1":1132},"conf":0.8942,"label":"print_embedding","label_id":0},{"bbox":{"x0":401,"x1":472,"y0":875,"y1":898},"conf":0.8866,"label":"print_embedding","label_id":0},{"bbox":{"x0":452,"x1":508,"y0":1044,"y1":1064},"conf":0.8789,"label":"print_embedding","label_id":0},{"bbox":{"x0":784,"x1":884,"y0":1017,"y1":1041},"conf":0.8776,"label":"print_embedding","label_id":0},{"bbox":{"x0":801,"x1":887,"y0":340,"y1":361},"conf":0.8728,"label":"print_embedding","label_id":0},{"bbox":{"x0":947,"x1":1008,"y0":856,"y1":874},"conf":0.8633,"label":"print_embedding","label_id":0},{"bbox":{"x0":540,"x1":599,"y0":855,"y1":874},"conf":0.8419,"label":"print_embedding","label_id":0},{"bbox":{"x0":364,"x1":381,"y0":1090,"y1":1108},"conf":0.8416,"label":"print_embedding","label_id":0},{"bbox":{"x0":623,"x1":687,"y0":901,"y1":917},"conf":0.838,"label":"print_embedding","label_id":0},{"bbox":{"x0":534,"x1":794,"y0":875,"y1":898},"conf":0.8368,"label":"print_embedding","label_id":0},{"bbox":{"x0":484,"x1":523,"y0":363,"y1":383},"conf":0.8238,"label":"print_embedding","label_id":0},{"bbox":{"x0":561,"x1":599,"y0":363,"y1":383},"conf":0.821,"label":"print_embedding","label_id":0},{"bbox":{"x0":944,"x1":958,"y0":1024,"y1":1039},"conf":0.7916,"label":"print_embedding","label_id":0},{"bbox":{"x0":476,"x1":490,"y0":1233,"y1":1249},"conf":0.7848,"label":"print_embedding","label_id":0},{"bbox":{"x0":252,"x1":264,"y0":858,"y1":873},"conf":0.5434,"label":"print_embedding","label_id":0},{"bbox":{"x0":988,"x1":1008,"y0":1094,"y1":1108},"conf":0.5117,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":206,"x1":1018,"y0":163,"y1":519},"conf":0.9832,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":525,"y1":754},"conf":0.9763,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1017,"y0":826,"y1":1011},"conf":0.9757,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1226,"y1":1433},"conf":0.9743,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1017,"y0":1015,"y1":1222},"conf":0.9733,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":726,"y0":778,"y1":809},"conf":0.8356,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":592,"y0":50,"y1":80},"conf":0.8004,"label":"Abandon","label_id":2},{"bbox":{"x0":235,"x1":984,"y0":1441,"y1":1527},"conf":0.5825,"label":"Abandon","label_id":2},{"bbox":{"x0":602,"x1":620,"y0":1501,"y1":1525},"conf":0.2231,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1503],[619,1503],[619,1521],[604,1521]],"score":0.8463},{"poly":[[239,1444],[973,1444],[973,1465],[239,1465]],"score":0.8305},{"poly":[[215,1407],[1009,1407],[1009,1429],[215,1429]],"score":0.8579},{"poly":[[213,1384],[1009,1384],[1009,1406],[213,1406]],"score":0.824},{"poly":[[213,1363],[1009,1363],[1009,1384],[213,1384]],"score":0.7816},{"poly":[[211,1341],[1008,1341],[1008,1363],[211,1363]],"score":0.8284},{"poly":[[213,1318],[1009,1318],[1009,1341],[213,1341]],"score":0.7013},{"poly":[[215,1297],[1009,1297],[1009,1318],[215,1318]],"score":0.7761},{"poly":[[215,1274],[1009,1274],[1009,1297],[215,1297]],"score":0.701},{"poly":[[213,1252],[1009,1252],[1009,1275],[213,1275]],"score":0.699},{"poly":[[211,1229],[1008,1229],[1008,1251],[211,1251]],"score":0.7467},{"poly":[[215,1198],[865,1198],[865,1219],[215,1219]],"score":0.8198},{"poly":[[211,1175],[1008,1175],[1008,1196],[211,1196]],"score":0.7758},{"poly":[[213,1153],[1009,1153],[1009,1176],[213,1176]],"score":0.7135},{"poly":[[213,1132],[1009,1132],[1009,1153],[213,1153]],"score":0.7883},{"poly":[[213,1110],[1011,1110],[1011,1134],[213,1134]],"score":0.6863},{"poly":[[211,1086],[1011,1087],[1011,1110],[211,1109]],"score":0.7552},{"poly":[[213,1066],[1011,1066],[1011,1087],[213,1087]],"score":0.8009},{"poly":[[213,1044],[1011,1041],[1011,1064],[213,1068]],"score":0.6983},{"poly":[[211,1020],[1009,1018],[1009,1041],[211,1043]],"score":0.7439},{"poly":[[213,985],[1011,985],[1011,1008],[213,1008]],"score":0.7121},{"poly":[[213,962],[1009,962],[1009,985],[213,985]],"score":0.7068},{"poly":[[213,940],[1009,940],[1009,962],[213,962]],"score":0.7818},{"poly":[[213,919],[1009,919],[1009,940],[213,940]],"score":0.8181},{"poly":[[213,898],[1009,898],[1009,919],[213,919]],"score":0.8204},{"poly":[[213,876],[1011,876],[1011,899],[213,899]],"score":0.699},{"poly":[[213,851],[1008,851],[1008,874],[213,874]],"score":0.6964},{"poly":[[213,832],[1009,832],[1009,853],[213,853]],"score":0.8365},{"poly":[[248,784],[722,784],[722,805],[248,805]],"score":0.8182},{"poly":[[215,784],[249,784],[249,804],[215,804]],"score":0.6377},{"poly":[[213,728],[880,728],[880,749],[213,749]],"score":0.7842},{"poly":[[213,706],[1009,706],[1009,728],[213,728]],"score":0.7977},{"poly":[[210,680],[1011,681],[1011,710],[210,708]],"score":0.6281},{"poly":[[213,662],[1009,662],[1009,685],[213,685]],"score":0.6489},{"poly":[[213,640],[1009,640],[1009,662],[213,662]],"score":0.7484},{"poly":[[211,615],[1008,615],[1008,639],[211,639]],"score":0.6633},{"poly":[[213,596],[1009,596],[1009,617],[213,617]],"score":0.7773},{"poly":[[211,573],[1008,573],[1008,594],[211,594]],"score":0.7049},{"poly":[[213,551],[1009,551],[1009,574],[213,574]],"score":0.6815},{"poly":[[213,530],[1011,530],[1011,551],[213,551]],"score":0.7836},{"poly":[[213,495],[501,495],[501,516],[213,516]],"score":0.833},{"poly":[[213,474],[1009,474],[1009,495],[213,495]],"score":0.8121},{"poly":[[213,452],[1009,452],[1009,474],[213,474]],"score":0.7999},{"poly":[[213,431],[1011,431],[1011,452],[213,452]],"score":0.7777},{"poly":[[213,409],[1009,409],[1009,431],[213,431]],"score":0.8157},{"poly":[[213,388],[1009,388],[1009,409],[213,409]],"score":0.7949},{"poly":[[213,365],[1009,365],[1009,386],[213,386]],"score":0.7621},{"poly":[[213,342],[1009,342],[1009,365],[213,365]],"score":0.6859},{"poly":[[213,320],[1009,320],[1009,343],[213,343]],"score":0.6733},{"poly":[[213,299],[1009,299],[1009,320],[213,320]],"score":0.8051},{"poly":[[213,276],[1009,276],[1009,299],[213,299]],"score":0.7007},{"poly":[[213,254],[1011,254],[1011,277],[213,277]],"score":0.6662},{"poly":[[213,233],[1009,233],[1009,254],[213,254]],"score":0.8118},{"poly":[[213,211],[1009,211],[1009,233],[213,233]],"score":0.8012},{"poly":[[211,190],[1009,188],[1009,211],[211,213]],"score":0.7129},{"poly":[[211,167],[1009,167],[1009,188],[211,188]],"score":0.8358},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7535}],"page_no":2,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":235,"x1":636,"y0":1440,"y1":1469},"conf":0.5765,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":236,"x1":629,"y0":1440,"y1":1467},"font_size":0.0,"text":"2We fnd more examples do not improve performance."},{"bbox":{"x0":286,"x1":286,"y0":1447,"y1":1463},"font_size":8.0,"text":"i"}],"source":"layout det","text":""},{"bbox":{"x0":208,"x1":1015,"y0":47,"y1":85},"conf":0.2163,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":207,"x1":1015,"y0":164,"y1":215},"conf":0.9359,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":210,"x1":1011,"y0":163,"y1":193},"font_size":-5.032e-22,"text":"appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the"},{"bbox":{"x0":211,"x1":687,"y0":186,"y1":213},"font_size":-5.032e-22,"text":"asynchronous occurrence of thoughts and actions for itself."}],"source":"layout det","text":"appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself."},{"bbox":{"x0":207,"x1":1016,"y0":219,"y1":555},"conf":0.9816,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":223,"y1":244},"font_size":-5.032e-22,"text":"Since decision making and reasoning capabilities are integrated into a large language model, ReAct"},{"bbox":{"x0":211,"x1":1009,"y0":243,"y1":267},"font_size":-5.032e-22,"text":"enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is"},{"bbox":{"x0":213,"x1":1009,"y0":267,"y1":289},"font_size":-5.032e-22,"text":"straightforward as human annotators just type down their thoughts in language on top of their actions"},{"bbox":{"x0":213,"x1":1008,"y0":287,"y1":309},"font_size":-5.032e-22,"text":"taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail"},{"bbox":{"x0":211,"x1":1009,"y0":309,"y1":333},"font_size":-5.032e-22,"text":"prompt design for each task in Sections 3 and 4. B) General and fexible: Due to the fexible thoughtll"},{"bbox":{"x0":211,"x1":1009,"y0":332,"y1":355},"font_size":-5.032e-22,"text":"space and thought-action occurrence format, ReAct works for diverse tasks with distinct action"},{"bbox":{"x0":211,"x1":1009,"y0":353,"y1":378},"font_size":-5.032e-22,"text":"spaces and reasoning needs, including but not limited to QA, fact verifcation, text game, and webi"},{"bbox":{"x0":213,"x1":1009,"y0":376,"y1":399},"font_size":-5.032e-22,"text":"navigation. C) Performant and robust: ReAct shows strong generalization to new task instances"},{"bbox":{"x0":215,"x1":1008,"y0":401,"y1":417},"font_size":-5.032e-22,"text":"while learning solely from one to six in-context examples, consistently outperforming baselines with"},{"bbox":{"x0":215,"x1":1009,"y0":421,"y1":442},"font_size":-5.032e-22,"text":"only reasoning or acting across different domains. We also show in Section 3 additional beneftsi"},{"bbox":{"x0":215,"x1":1011,"y0":442,"y1":464},"font_size":-5.032e-22,"text":"when fnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.i"},{"bbox":{"x0":211,"x1":1009,"y0":462,"y1":487},"font_size":-5.032e-22,"text":"D) Human aligned and controllable: ReAct promises an interpretable sequential decision making"},{"bbox":{"x0":211,"x1":1011,"y0":483,"y1":508},"font_size":-5.032e-22,"text":"and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,"},{"bbox":{"x0":215,"x1":1009,"y0":508,"y1":530},"font_size":-5.032e-22,"text":"humans can also control or correct the agent behavior on the go by thought editing, as shown in"},{"bbox":{"x0":211,"x1":389,"y0":528,"y1":549},"font_size":-5.032e-22,"text":"Figure 5 in Section 4."}],"source":"layout det","text":"Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators just type down their thoughts in language on top of their actions taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail prompt design for each task in Sections 3 and 4. B) General and fexible: Due to the fexible thoughtll space and thought-action occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but not limited to QA, fact verifcation, text game, and webi navigation. C) Performant and robust: ReAct shows strong generalization to new task instances while learning solely from one to six in-context examples, consistently outperforming baselines with only reasoning or acting across different domains. We also show in Section 3 additional beneftsi when fnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.i D) Human aligned and controllable: ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,humans can also control or correct the agent behavior on the go by thought editing, as shown in Figure 5 in Section 4."},{"bbox":{"x0":208,"x1":719,"y0":582,"y1":614},"conf":0.8921,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":713,"y0":587,"y1":609},"font_size":-5.032e-22,"text":"3KNOWLEDGE-INTENSIVE REASONING TASKS"}],"source":"layout det","text":"3KNOWLEDGE-INTENSIVE REASONING TASKS"},{"bbox":{"x0":207,"x1":1016,"y0":633,"y1":730},"conf":0.955,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":639,"y1":660},"font_size":-5.032e-22,"text":"We begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact"},{"bbox":{"x0":213,"x1":1009,"y0":660,"y1":681},"font_size":-5.032e-22,"text":"verifcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able toi"},{"bbox":{"x0":213,"x1":1011,"y0":683,"y1":705},"font_size":-5.032e-22,"text":"retrieve information to support reasoning, while also use reasoning to target what to retrieve next,"},{"bbox":{"x0":211,"x1":607,"y0":701,"y1":728},"font_size":-5.032e-22,"text":"demonstrating a synergy of reasoning and acting."}],"source":"layout det","text":"We begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact verifcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able toi retrieve information to support reasoning, while also use reasoning to target what to retrieve next,demonstrating a synergy of reasoning and acting."},{"bbox":{"x0":209,"x1":325,"y0":752,"y1":780},"conf":0.8733,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":321,"y0":757,"y1":776},"font_size":-5.032e-22,"text":"3.1SETUP"}],"source":"layout det","text":"3.1SETUP"},{"bbox":{"x0":208,"x1":1016,"y0":793,"y1":979},"conf":0.9782,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":797,"y1":820},"font_size":-5.032e-22,"text":"DomainsWe consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-"},{"bbox":{"x0":213,"x1":1009,"y0":820,"y1":842},"font_size":-5.032e-22,"text":"PotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning"},{"bbox":{"x0":213,"x1":1009,"y0":842,"y1":863},"font_size":-5.032e-22,"text":"over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact verifcationi"},{"bbox":{"x0":213,"x1":1009,"y0":863,"y1":884},"font_size":-5.032e-22,"text":"benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based"},{"bbox":{"x0":213,"x1":1008,"y0":886,"y1":907},"font_size":-5.032e-22,"text":"on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only"},{"bbox":{"x0":213,"x1":1009,"y0":908,"y1":929},"font_size":-5.032e-22,"text":"setup for both tasks, where models only receive the question/claim as input without access to support"},{"bbox":{"x0":213,"x1":1009,"y0":931,"y1":952},"font_size":-5.032e-22,"text":"paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with"},{"bbox":{"x0":213,"x1":580,"y0":954,"y1":975},"font_size":-5.032e-22,"text":"an external environment to support reasoning."}],"source":"layout det","text":"DomainsWe consider two datasets challenging knowledge retrieval and reasoning: (1) HotPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact verifcationi benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only setup for both tasks, where models only receive the question/claim as input without access to support paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with an external environment to support reasoning."},{"bbox":{"x0":209,"x1":1017,"y0":993,"y1":1203},"conf":0.9793,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":998,"y1":1023},"font_size":-5.032e-22,"text":"Action SpaceWe design a simple Wikipedia web API with three types of actions to support"},{"bbox":{"x0":213,"x1":1009,"y0":1023,"y1":1044},"font_size":-5.032e-22,"text":"interactive information retrieval: (1) search[entity], which returns the frst 5 sentences fromi"},{"bbox":{"x0":213,"x1":1009,"y0":1044,"y1":1068},"font_size":-5.032e-22,"text":"the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the"},{"bbox":{"x0":215,"x1":1009,"y0":1066,"y1":1089},"font_size":-5.032e-22,"text":"Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page"},{"bbox":{"x0":213,"x1":1009,"y0":1089,"y1":1110},"font_size":-5.032e-22,"text":"containing string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which"},{"bbox":{"x0":213,"x1":1011,"y0":1110,"y1":1132},"font_size":-5.032e-22,"text":"would fnish the current task with answer. We note that this action space mostly can only retrieve ai"},{"bbox":{"x0":213,"x1":1011,"y0":1134,"y1":1155},"font_size":-5.032e-22,"text":"small part of a passage based on exact passage name, which is signifcantly weaker than state-of-the-i"},{"bbox":{"x0":213,"x1":1009,"y0":1155,"y1":1176},"font_size":-5.032e-22,"text":"art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,"},{"bbox":{"x0":211,"x1":715,"y0":1175,"y1":1200},"font_size":-5.032e-22,"text":"and force models to retrieve via explicit reasoning in language."}],"source":"layout det","text":"Action SpaceWe design a simple Wikipedia web API with three types of actions to support interactive information retrieval: (1) search[entity], which returns the frst 5 sentences fromi the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page containing string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which would fnish the current task with answer. We note that this action space mostly can only retrieve ai small part of a passage based on exact passage name, which is signifcantly weaker than state-of-the-i art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,and force models to retrieve via explicit reasoning in language."},{"bbox":{"x0":210,"x1":356,"y0":1224,"y1":1252},"conf":0.8817,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":211,"x1":354,"y0":1226,"y1":1251},"font_size":-5.032e-22,"text":"3.2METHODS"}],"source":"layout det","text":"3.2METHODS"},{"bbox":{"x0":208,"x1":1017,"y0":1264,"y1":1428},"conf":0.9714,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":1269,"y1":1292},"font_size":-5.032e-22,"text":"ReAct PromptingFor HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training"},{"bbox":{"x0":213,"x1":1009,"y0":1292,"y1":1313},"font_size":-5.032e-22,"text":"set and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts."},{"bbox":{"x0":213,"x1":1008,"y0":1313,"y1":1335},"font_size":-5.032e-22,"text":"Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense"},{"bbox":{"x0":213,"x1":1009,"y0":1335,"y1":1358},"font_size":-5.032e-22,"text":"thought), where free-form thoughts are used for various purposes. Specifcally, we use a combinationi"},{"bbox":{"x0":215,"x1":1009,"y0":1358,"y1":1380},"font_size":-5.032e-22,"text":"of thoughts that decompose questions (“I need to search $\\mathbf{X},$  fndi $y,$  then fndi $z^{3}$ ), extract information"},{"bbox":{"x0":213,"x1":1008,"y0":1377,"y1":1401},"font_size":-5.032e-22,"text":"from Wikipedia observations ( $``{x}$ was started in $1844\",$  “The paragraph does not tell $\\mathbf{x}\"$ ), perform"},{"bbox":{"x0":213,"x1":1009,"y0":1400,"y1":1424},"font_size":-5.032e-22,"text":"commonsense ( $``x$  is not y, so z must instead be...”) or arithmetic reasoning $\\left(``1844<1989\"\\right) ,$  guide"}],"source":"layout det","text":"ReAct PromptingFor HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training set and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense thought), where free-form thoughts are used for various purposes. Specifcally, we use a combinationi of thoughts that decompose questions (“I need to search $\\mathbf{X},$  fndi $y,$  then fndi $z^{3}$ ), extract information from Wikipedia observations ( $``{x}$ was started in $1844\",$  “The paragraph does not tell $\\mathbf{x}\"$ ), perform commonsense ( $``x$  is not y, so z must instead be...”) or arithmetic reasoning $\\left(``1844<1989\"\\right) ,$  guide"}],"formula_dets":[{"bbox":{"x0":820,"x1":957,"y0":1400,"y1":1422},"conf":0.8168,"label":"print_embedding","label_id":0},{"bbox":{"x0":344,"x1":360,"y0":1405,"y1":1420},"conf":0.6972,"label":"print_embedding","label_id":0},{"bbox":{"x0":817,"x1":834,"y0":1360,"y1":1376},"conf":0.664,"label":"print_embedding","label_id":0},{"bbox":{"x0":466,"x1":484,"y0":1382,"y1":1398},"conf":0.6552,"label":"print_embedding","label_id":0},{"bbox":{"x0":903,"x1":925,"y0":1381,"y1":1398},"conf":0.5926,"label":"print_embedding","label_id":0},{"bbox":{"x0":723,"x1":737,"y0":1363,"y1":1380},"conf":0.5882,"label":"print_embedding","label_id":0},{"bbox":{"x0":606,"x1":661,"y0":1377,"y1":1399},"conf":0.5567,"label":"print_embedding","label_id":0},{"bbox":{"x0":667,"x1":684,"y0":1361,"y1":1376},"conf":0.5295,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":207,"x1":1016,"y0":219,"y1":555},"conf":0.9816,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1017,"y0":993,"y1":1203},"conf":0.9793,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":793,"y1":979},"conf":0.9782,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":1264,"y1":1428},"conf":0.9714,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":633,"y1":730},"conf":0.955,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1015,"y0":164,"y1":215},"conf":0.9359,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":719,"y0":582,"y1":614},"conf":0.8921,"label":"Title","label_id":0},{"bbox":{"x0":210,"x1":356,"y0":1224,"y1":1252},"conf":0.8817,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":325,"y0":752,"y1":780},"conf":0.8733,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":592,"y0":50,"y1":80},"conf":0.7833,"label":"Abandon","label_id":2},{"bbox":{"x0":235,"x1":636,"y0":1440,"y1":1469},"conf":0.5765,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1015,"y0":47,"y1":85},"conf":0.2163,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1503],[619,1503],[619,1521],[604,1521]],"score":0.7258},{"poly":[[236,1440],[629,1444],[629,1467],[236,1463]],"score":0.7402},{"poly":[[213,1401],[1009,1401],[1009,1424],[213,1424]],"score":0.6899},{"poly":[[213,1379],[1008,1379],[1008,1401],[213,1401]],"score":0.7791},{"poly":[[215,1358],[1009,1358],[1009,1379],[215,1379]],"score":0.8271},{"poly":[[213,1335],[1009,1335],[1009,1358],[213,1358]],"score":0.6986},{"poly":[[213,1313],[1008,1313],[1008,1335],[213,1335]],"score":0.7688},{"poly":[[213,1292],[1009,1292],[1009,1313],[213,1313]],"score":0.7917},{"poly":[[213,1269],[1008,1269],[1008,1292],[213,1292]],"score":0.7007},{"poly":[[211,1226],[354,1228],[354,1251],[211,1249]],"score":0.728},{"poly":[[211,1175],[715,1176],[715,1200],[211,1198]],"score":0.6881},{"poly":[[213,1155],[1009,1155],[1009,1176],[213,1176]],"score":0.797},{"poly":[[213,1134],[1011,1134],[1011,1155],[213,1155]],"score":0.8245},{"poly":[[213,1110],[1011,1110],[1011,1132],[213,1132]],"score":0.7575},{"poly":[[213,1089],[1009,1089],[1009,1110],[213,1110]],"score":0.7882},{"poly":[[215,1066],[1009,1066],[1009,1089],[215,1089]],"score":0.7069},{"poly":[[213,1044],[1009,1044],[1009,1068],[213,1068]],"score":0.6724},{"poly":[[213,1023],[1009,1023],[1009,1044],[213,1044]],"score":0.7787},{"poly":[[211,998],[1009,1000],[1009,1023],[211,1021]],"score":0.7259},{"poly":[[213,954],[580,954],[580,975],[213,975]],"score":0.7917},{"poly":[[213,931],[1009,931],[1009,952],[213,952]],"score":0.8306},{"poly":[[213,908],[1009,908],[1009,929],[213,929]],"score":0.7834},{"poly":[[213,886],[1008,886],[1008,907],[213,907]],"score":0.7953},{"poly":[[213,863],[1009,863],[1009,884],[213,884]],"score":0.8114},{"poly":[[213,842],[1009,842],[1009,863],[213,863]],"score":0.7621},{"poly":[[213,820],[1009,820],[1009,842],[213,842]],"score":0.789},{"poly":[[213,797],[1011,797],[1011,820],[213,820]],"score":0.7051},{"poly":[[215,757],[321,757],[321,776],[215,776]],"score":0.8679},{"poly":[[211,701],[607,705],[607,728],[211,724]],"score":0.6964},{"poly":[[213,683],[1011,683],[1011,705],[213,705]],"score":0.7938},{"poly":[[213,660],[1009,660],[1009,681],[213,681]],"score":0.823},{"poly":[[213,639],[1009,639],[1009,660],[213,660]],"score":0.8005},{"poly":[[213,587],[713,587],[713,609],[213,609]],"score":0.8415},{"poly":[[211,528],[389,528],[389,549],[211,549]],"score":0.7781},{"poly":[[215,508],[1009,508],[1009,530],[215,530]],"score":0.8233},{"poly":[[211,485],[1011,483],[1011,507],[211,508]],"score":0.7244},{"poly":[[211,462],[1009,464],[1009,487],[211,485]],"score":0.7341},{"poly":[[215,442],[1011,442],[1011,464],[215,464]],"score":0.78},{"poly":[[215,421],[1009,421],[1009,442],[215,442]],"score":0.8334},{"poly":[[215,401],[1008,401],[1008,417],[215,417]],"score":0.9749},{"poly":[[213,376],[1009,376],[1009,399],[213,399]],"score":0.6264},{"poly":[[211,355],[1009,353],[1009,376],[211,378]],"score":0.7342},{"poly":[[211,333],[1009,332],[1009,353],[211,355]],"score":0.8416},{"poly":[[211,310],[1009,309],[1009,332],[211,333]],"score":0.7382},{"poly":[[213,287],[1008,287],[1008,309],[213,309]],"score":0.7988},{"poly":[[213,267],[1009,267],[1009,289],[213,289]],"score":0.8118},{"poly":[[211,244],[1009,243],[1009,266],[211,267]],"score":0.707},{"poly":[[213,223],[1008,223],[1008,244],[213,244]],"score":0.8297},{"poly":[[211,190],[687,186],[687,210],[211,213]],"score":0.6953},{"poly":[[210,165],[1011,163],[1011,191],[210,193]],"score":0.6594},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7459}],"page_no":3,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":208,"x1":1015,"y0":46,"y1":85},"conf":0.2937,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":208,"x1":1016,"y0":1419,"y1":1528},"conf":0.2014,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1502,"y1":1521},"font_size":0.0,"text":"5"},{"bbox":{"x0":238,"x1":1009,"y0":1419,"y1":1444},"font_size":0.0,"text":"3Of all trajectories with correct fnal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only"},{"bbox":{"x0":479,"x1":479,"y0":1427,"y1":1443},"font_size":8.0,"text":"i"},{"bbox":{"x0":211,"x1":271,"y0":1444,"y1":1465},"font_size":0.0,"text":"take up"},{"bbox":{"x0":271,"x1":319,"y0":1443,"y1":1463},"font_size":0.0,"text":" $0.84\\%$ "},{"bbox":{"x0":319,"x1":352,"y0":1444,"y1":1465},"font_size":0.0,"text":"and"},{"bbox":{"x0":400,"x1":494,"y0":1444,"y1":1465},"font_size":8.0,"text":" respectively."},{"bbox":{"x0":352,"x1":400,"y0":1443,"y1":1463},"font_size":0.0,"text":" $1.33\\%$ "}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":211,"x1":554,"y0":158,"y1":394},"conf":0.9774,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":223,"x1":347,"y0":174,"y1":196},"font_size":0.0,"text":"Prompt Methoda"},{"bbox":{"x0":399,"x1":481,"y0":165,"y1":188},"font_size":0.0,"text":"HotpotQA"},{"bbox":{"x0":492,"x1":543,"y0":162,"y1":189},"font_size":0.0,"text":"Fever"},{"bbox":{"x0":418,"x1":464,"y0":183,"y1":208},"font_size":0.0,"text":"(EM)"},{"bbox":{"x0":492,"x1":543,"y0":181,"y1":208},"font_size":0.0,"text":"(Acc)"},{"bbox":{"x0":223,"x1":307,"y0":213,"y1":232},"font_size":0.0,"text":"Standard"},{"bbox":{"x0":421,"x1":460,"y0":210,"y1":233},"font_size":0.0,"text":"28.7"},{"bbox":{"x0":498,"x1":539,"y0":210,"y1":234},"font_size":0.0,"text":"57.1"},{"bbox":{"x0":222,"x1":343,"y0":229,"y1":250},"font_size":0.0,"text":"CoT (Wei et al., 2022)"},{"bbox":{"x0":421,"x1":460,"y0":228,"y1":251},"font_size":0.0,"text":"29.4"},{"bbox":{"x0":498,"x1":539,"y0":228,"y1":252},"font_size":0.0,"text":"56.3"},{"bbox":{"x0":222,"x1":387,"y0":248,"y1":269},"font_size":0.0,"text":"CoT-SC (Wang et al., 2022a)"},{"bbox":{"x0":421,"x1":459,"y0":248,"y1":268},"font_size":0.0,"text":"33.4"},{"bbox":{"x0":499,"x1":537,"y0":248,"y1":268},"font_size":0.0,"text":"60.4"},{"bbox":{"x0":221,"x1":259,"y0":275,"y1":295},"font_size":0.0,"text":"Act"},{"bbox":{"x0":421,"x1":460,"y0":272,"y1":297},"font_size":0.0,"text":"25.7"},{"bbox":{"x0":497,"x1":539,"y0":269,"y1":298},"font_size":0.0,"text":"58.9"},{"bbox":{"x0":221,"x1":279,"y0":293,"y1":315},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":421,"x1":460,"y0":291,"y1":315},"font_size":0.0,"text":"27.4"},{"bbox":{"x0":498,"x1":538,"y0":291,"y1":315},"font_size":0.0,"text":"60.9"},{"bbox":{"x0":221,"x1":362,"y0":309,"y1":333},"font_size":0.0,"text":"CoT-SC ￫ReAct"},{"bbox":{"x0":421,"x1":460,"y0":310,"y1":334},"font_size":0.0,"text":"34.2"},{"bbox":{"x0":498,"x1":538,"y0":310,"y1":333},"font_size":0.0,"text":"64.6"},{"bbox":{"x0":222,"x1":359,"y0":329,"y1":350},"font_size":0.0,"text":"ReAct￫CoT-SC"},{"bbox":{"x0":421,"x1":460,"y0":328,"y1":352},"font_size":0.0,"text":"35.1"},{"bbox":{"x0":498,"x1":539,"y0":328,"y1":352},"font_size":0.0,"text":"62.0"},{"bbox":{"x0":221,"x1":357,"y0":358,"y1":384},"font_size":0.0,"text":"Supervised SoTAb"},{"bbox":{"x0":421,"x1":460,"y0":359,"y1":383},"font_size":0.0,"text":"67.5"},{"bbox":{"x0":498,"x1":538,"y0":359,"y1":383},"font_size":0.0,"text":"89.5"}],"source":"layout det","text":"<html><body><table><tr><td>Prompt Methoda</td><td>HotpotQA (EM)</td><td>Fever (Acc)</td></tr><tr><td>Standard</td><td>28.7</td><td>57.1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>29.4</td><td>56.3</td></tr><tr><td>CoT-SC (Wang et al., 2022a)</td><td>33.4</td><td>60.4</td></tr><tr><td>Act</td><td>25.7</td><td>58.9</td></tr><tr><td>ReAct</td><td>27.4</td><td>60.9</td></tr><tr><td>CoT-SC ￫ReAct ReAct￫CoT-SC</td><td>34.2</td><td>64.6</td></tr><tr><td></td><td>35.1</td><td>62.0</td></tr><tr><td>Supervised SoTAb</td><td>67.5</td><td>89.5</td></tr></table></body></html>"},{"bbox":{"x0":566,"x1":1009,"y0":203,"y1":421},"conf":0.9591,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![0fd5cdf2b99b418ece20d72b856688bd](imgs/0fd5cdf2b99b418ece20d72b856688bd.jpg)"},{"bbox":{"x0":209,"x1":554,"y0":403,"y1":454},"conf":0.7839,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":551,"y0":404,"y1":429},"font_size":5.0,"text":"Table 1: PaLM-540B prompting results on"},{"bbox":{"x0":213,"x1":389,"y0":427,"y1":449},"font_size":5.0,"text":"HotpotQA and Fever."}],"source":"layout det","text":"Table 1: PaLM-540B prompting results on HotpotQA and Fever."},{"bbox":{"x0":556,"x1":1019,"y0":435,"y1":488},"conf":0.3532,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":559,"x1":1016,"y0":437,"y1":460},"font_size":5.0,"text":"Figure 2: PaLM-540B prompting results with respect to"},{"bbox":{"x0":562,"x1":837,"y0":464,"y1":482},"font_size":9.0,"text":"number of CoT-SC samples used."}],"source":"layout det","text":"Figure 2: PaLM-540B prompting results with respect to number of CoT-SC samples used."},{"bbox":{"x0":209,"x1":556,"y0":467,"y1":502},"conf":0.8239,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":241,"x1":247,"y0":467,"y1":477},"font_size":155300.0,"text":"a"},{"bbox":{"x0":215,"x1":373,"y0":483,"y1":500},"font_size":155300.0,"text":"CoT-SC in Wang et al. (2022b)."},{"bbox":{"x0":377,"x1":550,"y0":475,"y1":485},"font_size":5.0,"text":"28.9, 33.8 for Standard, CoT,"}],"source":"layout det","text":"a CoT-SC in Wang et al. (2022b).28.9, 33.8 for Standard, CoT,"},{"bbox":{"x0":237,"x1":429,"y0":502,"y1":524},"conf":0.8766,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":236,"x1":428,"y0":496,"y1":523},"font_size":155300.0,"text":"b(Zhu et al., 2021; Lewis et al., 2020)"}],"source":"layout det","text":"b(Zhu et al., 2021; Lewis et al., 2020)"},{"bbox":{"x0":207,"x1":1016,"y0":545,"y1":598},"conf":0.9251,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":548,"y1":570},"font_size":155300.0,"text":"search reformulation (“maybe I can search/look up x instead”), and synthesize the fnal answer (“...soi"},{"bbox":{"x0":213,"x1":630,"y0":571,"y1":592},"font_size":155300.0,"text":"the answer is $\\mathbf{x}^{\\prime\\prime}$ ). See Appendix C for more details."}],"source":"layout det","text":"search reformulation (“maybe I can search/look up x instead”), and synthesize the fnal answer (“...soi the answer is $\\mathbf{x}^{\\prime\\prime}$ ). See Appendix C for more details."},{"bbox":{"x0":208,"x1":1017,"y0":618,"y1":849},"conf":0.9746,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":622,"y1":644},"font_size":155300.0,"text":"BaselinesWe systematically ablate ReAct trajectories to build prompts for multiple baselines (with"},{"bbox":{"x0":211,"x1":1011,"y0":642,"y1":667},"font_size":155300.0,"text":"formats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,"},{"bbox":{"x0":213,"x1":1011,"y0":667,"y1":688},"font_size":155300.0,"text":"actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,"},{"bbox":{"x0":213,"x1":1011,"y0":688,"y1":710},"font_size":155300.0,"text":"2022), which removes actions and observations and serve as a reasoning-only baseline. We also"},{"bbox":{"x0":211,"x1":1011,"y0":708,"y1":733},"font_size":155300.0,"text":"build a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories"},{"bbox":{"x0":213,"x1":1011,"y0":733,"y1":754},"font_size":155300.0,"text":"with decoding temperature 0.7 during inference and adopting the majority answer, which is found to"},{"bbox":{"x0":213,"x1":1011,"y0":754,"y1":776},"font_size":155300.0,"text":"consistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts"},{"bbox":{"x0":211,"x1":1011,"y0":774,"y1":799},"font_size":155300.0,"text":"in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the"},{"bbox":{"x0":213,"x1":1009,"y0":799,"y1":820},"font_size":155300.0,"text":"Internet to answer questions, though it operates on a different task and action space, and uses imitation"},{"bbox":{"x0":211,"x1":607,"y0":818,"y1":843},"font_size":155300.0,"text":"and reinforcement learning instead of prompting."}],"source":"layout det","text":"BaselinesWe systematically ablate ReAct trajectories to build prompts for multiple baselines (with formats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,2022), which removes actions and observations and serve as a reasoning-only baseline. We also build a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories with decoding temperature 0.7 during inference and adopting the majority answer, which is found to consistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the Internet to answer questions, though it operates on a different task and action space, and uses imitation and reinforcement learning instead of prompting."},{"bbox":{"x0":207,"x1":1016,"y0":866,"y1":1077},"conf":0.974,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":871,"y1":893},"font_size":155300.0,"text":"Combining Internal and External KnowledgeAs will be detail in Section 3.3, we observe that"},{"bbox":{"x0":213,"x1":1009,"y0":894,"y1":916},"font_size":155300.0,"text":"the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT"},{"bbox":{"x0":211,"x1":1008,"y0":914,"y1":936},"font_size":155300.0,"text":"is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts"},{"bbox":{"x0":213,"x1":1011,"y0":939,"y1":960},"font_size":155300.0,"text":"or thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide"},{"bbox":{"x0":213,"x1":1011,"y0":960,"y1":982},"font_size":155300.0,"text":"when to switch to the other method based on the following heuristics: A) ReAct $\\rightarrow$ CoT-SC: when"},{"bbox":{"x0":213,"x1":1011,"y0":982,"y1":1003},"font_size":155300.0,"text":"ReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for"},{"bbox":{"x0":209,"x1":1011,"y0":998,"y1":1030},"font_size":155300.0,"text":"HotpotQA and FEVER respectively as we fnd more steps will not improve ReAct performance3.i"},{"bbox":{"x0":213,"x1":1009,"y0":1026,"y1":1050},"font_size":155300.0,"text":"B) CoT-SC ￫ReAct: when the majority answer among n CoT-SC samples occurs less than $n/2$"},{"bbox":{"x0":215,"x1":928,"y0":1051,"y1":1070},"font_size":155300.0,"text":"times (i.e. internal knowledge might not support the task confdently), back off to ReAct.i"}],"source":"layout det","text":"Combining Internal and External KnowledgeAs will be detail in Section 3.3, we observe that the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts or thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide when to switch to the other method based on the following heuristics: A) ReAct $\\rightarrow$ CoT-SC: when ReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for HotpotQA and FEVER respectively as we fnd more steps will not improve ReAct performance3.i B) CoT-SC ￫ReAct: when the majority answer among n CoT-SC samples occurs less than $n/2$ times (i.e. internal knowledge might not support the task confdently), back off to ReAct.i"},{"bbox":{"x0":208,"x1":1017,"y0":1096,"y1":1217},"conf":0.9651,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":1099,"y1":1120},"font_size":155300.0,"text":"FinetuningDue to the challenge of manually annotating reasoning traces and actions at scale,"},{"bbox":{"x0":211,"x1":1009,"y0":1120,"y1":1145},"font_size":155300.0,"text":"we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories"},{"bbox":{"x0":213,"x1":1009,"y0":1143,"y1":1167},"font_size":155300.0,"text":"with correct answers generated by ReAct (also for other baselines) to fnetune smaller languagei"},{"bbox":{"x0":213,"x1":1011,"y0":1167,"y1":1188},"font_size":155300.0,"text":"models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on"},{"bbox":{"x0":211,"x1":679,"y0":1186,"y1":1211},"font_size":155300.0,"text":"input questions/claims. More details are in Appendix B.1."}],"source":"layout det","text":"FinetuningDue to the challenge of manually annotating reasoning traces and actions at scale,we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories with correct answers generated by ReAct (also for other baselines) to fnetune smaller languagei models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on input questions/claims. More details are in Appendix B.1."},{"bbox":{"x0":209,"x1":526,"y0":1240,"y1":1269},"conf":0.8735,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":522,"y0":1242,"y1":1264},"font_size":155300.0,"text":"3.3RESULTS AND OBSERVATIONS"}],"source":"layout det","text":"3.3RESULTS AND OBSERVATIONS"},{"bbox":{"x0":208,"x1":1016,"y0":1283,"y1":1403},"conf":0.9557,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1287,"y1":1308},"font_size":155300.0,"text":"ReAct outperforms Act consistentlyTable 1 shows HotpotQA and Fever results using PaLM-"},{"bbox":{"x0":213,"x1":1009,"y0":1308,"y1":1332},"font_size":155300.0,"text":"540B as the base model with different prompting methods. We note that ReAct is better than Act"},{"bbox":{"x0":213,"x1":1009,"y0":1330,"y1":1353},"font_size":155300.0,"text":"on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the"},{"bbox":{"x0":211,"x1":1011,"y0":1350,"y1":1376},"font_size":155300.0,"text":"fnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also confrm the beneft of reasoningiii"},{"bbox":{"x0":211,"x1":474,"y0":1373,"y1":1398},"font_size":155300.0,"text":"traces for more informed acting."}],"source":"layout det","text":"ReAct outperforms Act consistentlyTable 1 shows HotpotQA and Fever results using PaLM540B as the base model with different prompting methods. We note that ReAct is better than Act on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the fnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also confrm the beneft of reasoningiii traces for more informed acting."}],"formula_dets":[{"bbox":{"x0":974,"x1":1009,"y0":1029,"y1":1050},"conf":0.853,"label":"print_embedding","label_id":0},{"bbox":{"x0":271,"x1":319,"y0":1443,"y1":1463},"conf":0.7778,"label":"print_embedding","label_id":0},{"bbox":{"x0":352,"x1":400,"y0":1443,"y1":1463},"conf":0.7372,"label":"print_embedding","label_id":0},{"bbox":{"x0":322,"x1":342,"y0":573,"y1":590},"conf":0.6523,"label":"print_embedding","label_id":0},{"bbox":{"x0":856,"x1":877,"y0":965,"y1":979},"conf":0.6033,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":211,"x1":554,"y0":158,"y1":394},"conf":0.9774,"label":"Table","label_id":5},{"bbox":{"x0":208,"x1":1017,"y0":618,"y1":849},"conf":0.9746,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":866,"y1":1077},"conf":0.974,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":1096,"y1":1217},"conf":0.9651,"label":"Text","label_id":1},{"bbox":{"x0":566,"x1":1009,"y0":203,"y1":421},"conf":0.9591,"label":"Figure","label_id":3},{"bbox":{"x0":208,"x1":1016,"y0":1283,"y1":1403},"conf":0.9557,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":545,"y1":598},"conf":0.9251,"label":"Text","label_id":1},{"bbox":{"x0":557,"x1":1019,"y0":435,"y1":488},"conf":0.909,"label":"Figure caption","label_id":4},{"bbox":{"x0":237,"x1":429,"y0":502,"y1":524},"conf":0.8766,"label":"Table footnote","label_id":7},{"bbox":{"x0":209,"x1":526,"y0":1240,"y1":1269},"conf":0.8735,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":556,"y0":467,"y1":502},"conf":0.8239,"label":"Table footnote","label_id":7},{"bbox":{"x0":209,"x1":554,"y0":403,"y1":454},"conf":0.7839,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":50,"y1":80},"conf":0.7497,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1016,"y0":1419,"y1":1470},"conf":0.4555,"label":"Abandon","label_id":2},{"bbox":{"x0":556,"x1":1019,"y0":435,"y1":488},"conf":0.3532,"label":"Figure caption","label_id":4},{"bbox":{"x0":208,"x1":1015,"y0":46,"y1":85},"conf":0.2937,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1016,"y0":1419,"y1":1528},"conf":0.2503,"label":"Abandon","label_id":2},{"bbox":{"x0":601,"x1":621,"y0":1501,"y1":1525},"conf":0.2415,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1016,"y0":1419,"y1":1528},"conf":0.2014,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1502],[619,1502],[619,1521],[604,1521]],"score":0.7394},{"poly":[[211,1444],[494,1444],[494,1465],[211,1465]],"score":0.8247},{"poly":[[238,1419],[1009,1421],[1009,1444],[238,1442]],"score":0.7215},{"poly":[[211,1373],[474,1374],[474,1398],[211,1396]],"score":0.7012},{"poly":[[211,1350],[1011,1353],[1011,1376],[211,1373]],"score":0.7046},{"poly":[[213,1330],[1009,1330],[1009,1353],[213,1353]],"score":0.7025},{"poly":[[213,1308],[1009,1308],[1009,1332],[213,1332]],"score":0.6881},{"poly":[[213,1287],[1009,1287],[1009,1308],[213,1308]],"score":0.8081},{"poly":[[213,1242],[522,1242],[522,1264],[213,1264]],"score":0.7655},{"poly":[[211,1190],[678,1186],[679,1208],[211,1211]],"score":0.7816},{"poly":[[213,1167],[1011,1167],[1011,1188],[213,1188]],"score":0.8089},{"poly":[[213,1143],[1009,1143],[1009,1167],[213,1167]],"score":0.7001},{"poly":[[211,1122],[1009,1120],[1009,1143],[211,1145]],"score":0.6947},{"poly":[[211,1099],[1008,1099],[1008,1120],[211,1120]],"score":0.7911},{"poly":[[215,1051],[928,1051],[928,1068],[215,1068]],"score":0.9911},{"poly":[[213,1026],[1011,1026],[1011,1048],[213,1048]],"score":0.7905},{"poly":[[209,1002],[1011,998],[1011,1026],[210,1030]],"score":0.6242},{"poly":[[213,982],[1011,982],[1011,1003],[213,1003]],"score":0.7877},{"poly":[[213,960],[1011,960],[1011,982],[213,982]],"score":0.755},{"poly":[[213,939],[1011,939],[1011,960],[213,960]],"score":0.8073},{"poly":[[211,914],[1008,914],[1008,936],[211,936]],"score":0.6663},{"poly":[[213,894],[1009,894],[1009,916],[213,916]],"score":0.7615},{"poly":[[213,871],[1009,871],[1009,893],[213,893]],"score":0.8259},{"poly":[[211,818],[607,820],[607,843],[211,841]],"score":0.6948},{"poly":[[213,799],[1009,799],[1009,820],[213,820]],"score":0.763},{"poly":[[211,776],[1011,774],[1011,797],[211,799]],"score":0.7282},{"poly":[[213,754],[1011,754],[1011,776],[213,776]],"score":0.7929},{"poly":[[213,733],[1011,733],[1011,754],[213,754]],"score":0.8017},{"poly":[[211,708],[1011,710],[1011,733],[211,731]],"score":0.7532},{"poly":[[213,688],[1011,688],[1011,710],[213,710]],"score":0.7486},{"poly":[[213,667],[1011,667],[1011,688],[213,688]],"score":0.7903},{"poly":[[211,642],[1011,644],[1011,667],[211,665]],"score":0.7307},{"poly":[[213,622],[1011,622],[1011,644],[213,644]],"score":0.7956},{"poly":[[213,571],[630,571],[630,592],[213,592]],"score":0.8298},{"poly":[[213,548],[1011,548],[1011,569],[213,569]],"score":0.805},{"poly":[[236,496],[428,500],[427,523],[236,520]],"score":0.715},{"poly":[[215,483],[373,483],[373,500],[215,500]],"score":0.8058},{"poly":[[559,437],[1016,437],[1016,460],[559,460]],"score":0.7374},{"poly":[[213,427],[389,427],[389,449],[213,449]],"score":0.8358},{"poly":[[211,404],[551,406],[550,429],[211,427]],"score":0.7709},{"poly":[[654,396],[725,396],[725,414],[654,414]],"score":0.7389},{"poly":[[883,394],[961,394],[961,411],[883,411]],"score":0.8726},{"poly":[[717,389],[725,389],[725,398],[717,398]],"score":0.8234},{"poly":[[752,386],[767,386],[767,401],[752,401]],"score":0.8881},{"poly":[[940,383],[960,383],[960,396],[940,396]],"score":0.7881},{"poly":[[905,383],[926,383],[926,396],[905,396]],"score":0.6778},{"poly":[[878,381],[887,390],[880,398],[871,389]],"score":0.8012},{"poly":[[835,383],[850,383],[850,396],[835,396]],"score":0.6268},{"poly":[[975,380],[998,380],[998,399],[975,399]],"score":0.7899},{"poly":[[807,367],[837,362],[841,382],[811,387]],"score":0.7444},{"poly":[[499,361],[536,361],[536,381],[499,381]],"score":0.9749},{"poly":[[421,361],[459,361],[459,381],[421,381]],"score":0.9591},{"poly":[[218,360],[356,358],[356,381],[218,383]],"score":0.7963},{"poly":[[910,355],[940,355],[940,375],[910,375]],"score":0.747},{"poly":[[582,353],[599,353],[599,368],[582,368]],"score":0.731},{"poly":[[810,343],[838,343],[838,363],[810,363]],"score":0.8416},{"poly":[[911,342],[948,342],[948,361],[911,361]],"score":0.8001},{"poly":[[223,332],[358,332],[358,348],[223,348]],"score":0.968},{"poly":[[501,330],[536,330],[536,350],[501,350]],"score":0.9564},{"poly":[[913,328],[951,328],[951,346],[913,346]],"score":0.8196},{"poly":[[421,327],[461,327],[461,351],[421,351]],"score":0.8469},{"poly":[[810,320],[838,320],[838,340],[810,340]],"score":0.7996},{"poly":[[584,320],[597,320],[597,338],[584,338]],"score":0.7266},{"poly":[[913,315],[994,315],[994,332],[913,332]],"score":0.8133},{"poly":[[223,314],[359,314],[359,330],[223,330]],"score":0.9856},{"poly":[[501,312],[537,312],[537,332],[501,332]],"score":0.9214},{"poly":[[421,309],[461,309],[461,333],[421,333]],"score":0.8495},{"poly":[[913,302],[996,302],[996,318],[913,318]],"score":0.7896},{"poly":[[217,294],[277,290],[278,313],[219,317]],"score":0.8048},{"poly":[[497,290],[537,290],[537,315],[497,315]],"score":0.8039},{"poly":[[421,289],[461,289],[461,315],[421,315]],"score":0.7959},{"poly":[[918,287],[963,287],[963,305],[918,305]],"score":0.8026},{"poly":[[220,276],[256,276],[256,294],[220,294]],"score":0.9423},{"poly":[[501,274],[536,274],[536,294],[501,294]],"score":0.965},{"poly":[[421,271],[461,271],[461,297],[421,297]],"score":0.7847},{"poly":[[795,262],[840,262],[840,295],[795,295]],"score":0.6097},{"poly":[[813,252],[838,252],[838,269],[813,269]],"score":0.9139},{"poly":[[221,248],[386,248],[386,269],[221,269]],"score":0.8009},{"poly":[[497,244],[539,244],[539,269],[497,269]],"score":0.7885},{"poly":[[421,244],[461,244],[461,269],[421,269]],"score":0.8876},{"poly":[[223,231],[341,231],[341,249],[223,249]],"score":0.7905},{"poly":[[812,229],[837,229],[837,246],[812,246]],"score":0.9337},{"poly":[[497,226],[537,226],[537,251],[497,251]],"score":0.8283},{"poly":[[421,226],[461,226],[461,251],[421,251]],"score":0.815},{"poly":[[585,229],[595,229],[595,238],[585,238]],"score":0.8262},{"poly":[[223,213],[308,213],[308,229],[223,229]],"score":0.9707},{"poly":[[497,208],[537,208],[537,233],[497,233]],"score":0.8236},{"poly":[[421,208],[461,208],[461,234],[421,234]],"score":0.7255},{"poly":[[810,205],[840,205],[840,224],[810,224]],"score":0.76},{"poly":[[496,185],[542,185],[542,206],[496,206]],"score":0.876},{"poly":[[419,185],[462,185],[462,205],[419,205]],"score":0.9189},{"poly":[[223,177],[346,177],[346,195],[223,195]],"score":0.857},{"poly":[[493,163],[543,167],[541,187],[492,183]],"score":0.7708},{"poly":[[397,161],[483,165],[482,188],[395,184]],"score":0.7509},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7418}],"page_no":4,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":84},"conf":0.2801,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":207,"x1":1019,"y0":1420,"y1":1527},"conf":0.2574,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1503,"y1":1521},"font_size":0.0,"text":"6"},{"bbox":{"x0":211,"x1":677,"y0":1442,"y1":1465},"font_size":0.0,"text":"better decoding (e.g. beam search) might help address this issue."},{"bbox":{"x0":239,"x1":1010,"y0":1419,"y1":1447},"font_size":0.0,"text":"4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":231,"x1":990,"y0":159,"y1":304},"conf":0.9748,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":354,"x1":392,"y0":166,"y1":190},"font_size":0.0,"text":"Type"},{"bbox":{"x0":442,"x1":508,"y0":164,"y1":186},"font_size":0.0,"text":"Defnition"},{"bbox":{"x0":471,"x1":471,"y0":172,"y1":185},"font_size":6.0,"text":"i"},{"bbox":{"x0":876,"x1":925,"y0":168,"y1":185},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":942,"x1":975,"y0":168,"y1":186},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":246,"x1":301,"y0":200,"y1":223},"font_size":0.0,"text":"Success"},{"bbox":{"x0":332,"x1":413,"y0":194,"y1":213},"font_size":0.0,"text":"True positive"},{"bbox":{"x0":443,"x1":634,"y0":193,"y1":213},"font_size":0.0,"text":"Correct reasoning trace and facts"},{"bbox":{"x0":878,"x1":907,"y0":196,"y1":210},"font_size":0.0,"text":" $94\\%$ "},{"bbox":{"x0":941,"x1":976,"y0":193,"y1":214},"font_size":0.0,"text":"86%"},{"bbox":{"x0":329,"x1":414,"y0":209,"y1":230},"font_size":0.0,"text":"False positive"},{"bbox":{"x0":445,"x1":654,"y0":212,"y1":227},"font_size":0.0,"text":"Hallucinated reasoning trace or facts"},{"bbox":{"x0":874,"x1":905,"y0":209,"y1":231},"font_size":0.0,"text":"6%"},{"bbox":{"x0":942,"x1":976,"y0":209,"y1":230},"font_size":0.0,"text":"14%"},{"bbox":{"x0":322,"x1":421,"y0":226,"y1":247},"font_size":0.0,"text":"Reasoning error"},{"bbox":{"x0":446,"x1":859,"y0":229,"y1":244},"font_size":0.0,"text":"Wrong reasoning trace (including failing to recover from repetitive steps)"},{"bbox":{"x0":875,"x1":910,"y0":226,"y1":244},"font_size":0.0,"text":"47%"},{"bbox":{"x0":942,"x1":977,"y0":225,"y1":246},"font_size":0.0,"text":"16%"},{"bbox":{"x0":315,"x1":428,"y0":240,"y1":262},"font_size":0.0,"text":"Search result error"},{"bbox":{"x0":248,"x1":295,"y0":251,"y1":268},"font_size":0.0,"text":"Failure"},{"bbox":{"x0":445,"x1":779,"y0":244,"y1":260},"font_size":0.0,"text":"Search return empty or does not contain useful information"},{"bbox":{"x0":878,"x1":906,"y0":245,"y1":258},"font_size":0.0,"text":" $23\\%$ "},{"bbox":{"x0":943,"x1":956,"y0":246,"y1":260},"font_size":0.0,"text":"-"},{"bbox":{"x0":330,"x1":414,"y0":260,"y1":276},"font_size":0.0,"text":"Hallucination"},{"bbox":{"x0":444,"x1":655,"y0":259,"y1":276},"font_size":0.0,"text":"Hallucinated reasoning trace or facts"},{"bbox":{"x0":878,"x1":901,"y0":262,"y1":274},"font_size":0.0,"text":" $0\\%$ "},{"bbox":{"x0":939,"x1":977,"y0":254,"y1":278},"font_size":0.0,"text":"56%"},{"bbox":{"x0":322,"x1":421,"y0":273,"y1":294},"font_size":0.0,"text":"Label ambiguity"},{"bbox":{"x0":443,"x1":746,"y0":274,"y1":293},"font_size":0.0,"text":"Right prediction but did not match the label precisely"},{"bbox":{"x0":878,"x1":906,"y0":278,"y1":291},"font_size":0.0,"text":" $29\\%$ "},{"bbox":{"x0":944,"x1":972,"y0":278,"y1":291},"font_size":0.0,"text":" $28\\%$ "}],"source":"layout det","text":"<html><body><table><thead><tr><td></td><td>Type</td><td>Defnition</td><td>ReAct</td><td>CoT</td></tr></thead><tbody><tr><td rowspan=\"2\">Success</td><td>True positive</td><td>Correct reasoning trace and facts</td><td> $94\\%$ </td><td>86%</td></tr><tr><td>False positive</td><td>Hallucinated reasoning trace or facts</td><td>6%</td><td>14%</td></tr><tr><td rowspan=\"4\">Failure</td><td>Reasoning error</td><td>Wrong reasoning trace (including failing to recover from repetitive steps)</td><td>47%</td><td>16%</td></tr><tr><td>Search result error</td><td>Search return empty or does not contain useful information</td><td> $23\\%$ </td><td>-</td></tr><tr><td>Hallucination</td><td>Hallucinated reasoning trace or facts</td><td> $0\\%$ </td><td>56%</td></tr><tr><td>Label ambiguity</td><td>Right prediction but did not match the label precisely</td><td> $29\\%$ </td><td> $28\\%$ </td></tr></tbody></table></body></html>"},{"bbox":{"x0":209,"x1":1013,"y0":312,"y1":366},"conf":0.8706,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1008,"y0":317,"y1":338},"font_size":2497000000000.0,"text":"Table 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their"},{"bbox":{"x0":211,"x1":712,"y0":338,"y1":363},"font_size":2497000000000.0,"text":"percentages in randomly selected examples studied by human."}],"source":"layout det","text":"Table 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their percentages in randomly selected examples studied by human."},{"bbox":{"x0":207,"x1":1016,"y0":380,"y1":543},"conf":0.9723,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1006,"y0":386,"y1":408},"font_size":2497000000000.0,"text":"ReAct vs. CoTOn the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly"},{"bbox":{"x0":213,"x1":1009,"y0":406,"y1":431},"font_size":2497000000000.0,"text":"lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only"},{"bbox":{"x0":215,"x1":1008,"y0":431,"y1":452},"font_size":2497000000000.0,"text":"differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge"},{"bbox":{"x0":211,"x1":1008,"y0":450,"y1":472},"font_size":2497000000000.0,"text":"is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we"},{"bbox":{"x0":213,"x1":1004,"y0":475,"y1":492},"font_size":2497000000000.0,"text":"randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct"},{"bbox":{"x0":213,"x1":1008,"y0":497,"y1":518},"font_size":2497000000000.0,"text":"and CoT respectively (thus 200 examples in total), and manually labeled their success and failure"},{"bbox":{"x0":215,"x1":669,"y0":518,"y1":540},"font_size":2497000000000.0,"text":"modes in Table 2. Some key observations are as follows:"}],"source":"layout det","text":"ReAct vs. CoTOn the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct and CoT respectively (thus 200 examples in total), and manually labeled their success and failure modes in Table 2. Some key observations are as follows:"},{"bbox":{"x0":208,"x1":1016,"y0":547,"y1":643},"conf":0.9595,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":233,"x1":1009,"y0":551,"y1":573},"font_size":2497000000000.0,"text":"A) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than"},{"bbox":{"x0":213,"x1":1009,"y0":572,"y1":596},"font_size":2497000000000.0,"text":"ReAct ( $14\\%$  vs. $6\\%$  in success mode, and make up its major failure mode $(56\\%)$ . In contrast, the"},{"bbox":{"x0":213,"x1":1009,"y0":596,"y1":617},"font_size":2497000000000.0,"text":"problem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the"},{"bbox":{"x0":213,"x1":521,"y0":617,"y1":639},"font_size":2497000000000.0,"text":"access of an external knowledge base."}],"source":"layout det","text":"A) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than ReAct ( $14\\%$  vs. $6\\%$  in success mode, and make up its major failure mode $(56\\%)$ . In contrast, the problem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the access of an external knowledge base."},{"bbox":{"x0":208,"x1":1016,"y0":647,"y1":788},"conf":0.9625,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":233,"x1":1011,"y0":652,"y1":673},"font_size":2497000000000.0,"text":"B) While interleaving reasoning, action and observation steps improves ReAct’s grounded-"},{"bbox":{"x0":213,"x1":1009,"y0":673,"y1":695},"font_size":2497000000000.0,"text":"ness and trustworthiness, such a structural constraint also reduces its fexibility in formulatingl"},{"bbox":{"x0":215,"x1":1009,"y0":696,"y1":718},"font_size":2497000000000.0,"text":"reasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent"},{"bbox":{"x0":213,"x1":1009,"y0":718,"y1":739},"font_size":2497000000000.0,"text":"error pattern specifc to ReAct, in which the model repetitively generates the previous thoughts andi"},{"bbox":{"x0":213,"x1":1009,"y0":739,"y1":761},"font_size":2497000000000.0,"text":"actions, and we categorize it as part of “reasoning error” as the model fails to reason about what the"},{"bbox":{"x0":211,"x1":634,"y0":761,"y1":785},"font_size":2497000000000.0,"text":"proper next action to take and jump out of the loop4."}],"source":"layout det","text":"B) While interleaving reasoning, action and observation steps improves ReAct’s groundedness and trustworthiness, such a structural constraint also reduces its fexibility in formulatingl reasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent error pattern specifc to ReAct, in which the model repetitively generates the previous thoughts andi actions, and we categorize it as part of “reasoning error” as the model fails to reason about what the proper next action to take and jump out of the loop4."},{"bbox":{"x0":207,"x1":1016,"y0":792,"y1":886},"conf":0.9523,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":234,"x1":1009,"y0":797,"y1":813},"font_size":2497000000000.0,"text":"C) For ReAct, successfully retrieving informative knowledge via search is critical. Non-"},{"bbox":{"x0":213,"x1":1009,"y0":815,"y1":838},"font_size":2497000000000.0,"text":"informative search, which counts for $23\\%$  of the error cases, derails the model reasoning and gives"},{"bbox":{"x0":213,"x1":1008,"y0":838,"y1":860},"font_size":2497000000000.0,"text":"it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between"},{"bbox":{"x0":213,"x1":958,"y0":861,"y1":883},"font_size":2497000000000.0,"text":"factuality and fexibility, which motivates our proposed strategies of combining two methods.l"}],"source":"layout det","text":"C) For ReAct, successfully retrieving informative knowledge via search is critical. Noninformative search, which counts for $23\\%$  of the error cases, derails the model reasoning and gives it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between factuality and fexibility, which motivates our proposed strategies of combining two methods.l"},{"bbox":{"x0":208,"x1":1017,"y0":890,"y1":944},"conf":0.9329,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":893,"y1":916},"font_size":2497000000000.0,"text":"We provide examples for each success and failure modes in Appendix E.1. We also fnd somei"},{"bbox":{"x0":213,"x1":881,"y0":917,"y1":939},"font_size":2497000000000.0,"text":"HotpotQA questions may contain outdated answer labels, see Figure 4 for example."}],"source":"layout det","text":"We provide examples for each success and failure modes in Appendix E.1. We also fnd somei HotpotQA questions may contain outdated answer labels, see Figure 4 for example."},{"bbox":{"x0":207,"x1":1017,"y0":958,"y1":1124},"conf":0.9747,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":962,"y1":987},"font_size":2497000000000.0,"text":"ReAct + CoT-SC perform best for prompting LLMsAlso shown in Table 1, the best prompting"},{"bbox":{"x0":211,"x1":1011,"y0":985,"y1":1008},"font_size":2497000000000.0,"text":"method on HotpotQA and Fever are ReAct $\\rightarrow$ CoT-SC and $\\mathtt{CoT-SC}\\to\\mathtt{ReAct}$  respectively."},{"bbox":{"x0":213,"x1":1008,"y0":1008,"y1":1030},"font_size":2497000000000.0,"text":"Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC"},{"bbox":{"x0":213,"x1":1009,"y0":1031,"y1":1053},"font_size":2497000000000.0,"text":"samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both"},{"bbox":{"x0":211,"x1":1009,"y0":1049,"y1":1074},"font_size":2497000000000.0,"text":"signifcantly and consistently outperform CoT-SC across different number of samples, reachingi"},{"bbox":{"x0":211,"x1":1009,"y0":1071,"y1":1096},"font_size":2497000000000.0,"text":"CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of"},{"bbox":{"x0":211,"x1":941,"y0":1094,"y1":1119},"font_size":2497000000000.0,"text":"properly combining model internal knowledge and external knowledge for reasoning tasks."}],"source":"layout det","text":"ReAct + CoT-SC perform best for prompting LLMsAlso shown in Table 1, the best prompting method on HotpotQA and Fever are ReAct $\\rightarrow$ CoT-SC and $\\mathtt{CoT-SC}\\to\\mathtt{ReAct}$  respectively.Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both signifcantly and consistently outperform CoT-SC across different number of samples, reachingi CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of properly combining model internal knowledge and external knowledge for reasoning tasks."},{"bbox":{"x0":208,"x1":1016,"y0":1137,"y1":1411},"conf":0.9802,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":210,"x1":1011,"y0":1138,"y1":1168},"font_size":2497000000000.0,"text":"ReAct performs best for fne-tuningFigure 3 shows the scaling effect of prompting/fnetuningii"},{"bbox":{"x0":211,"x1":1006,"y0":1163,"y1":1188},"font_size":2497000000000.0,"text":"four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct"},{"bbox":{"x0":211,"x1":1009,"y0":1186,"y1":1209},"font_size":2497000000000.0,"text":"performs worst among four methods due to the diffculty to learn both reasoning and acting fromi"},{"bbox":{"x0":213,"x1":1009,"y0":1209,"y1":1231},"font_size":2497000000000.0,"text":"in-context examples. However, when fnetuned with just 3,000 examples, ReAct becomes the besti"},{"bbox":{"x0":209,"x1":1011,"y0":1226,"y1":1257},"font_size":2497000000000.0,"text":"method among the four, with PaLM-8B fnetuned ReAct outperforming all PaLM-62B promptingi"},{"bbox":{"x0":213,"x1":1009,"y0":1254,"y1":1275},"font_size":2497000000000.0,"text":"methods, and PaLM-62B fnetuned ReAct outperforming all 540B prompting methods. In contrast,i"},{"bbox":{"x0":213,"x1":1011,"y0":1275,"y1":1297},"font_size":2497000000000.0,"text":"fnetuning Standard or CoT is signifcantly worse than fnetuning ReAct or Act for both PaLM-iii"},{"bbox":{"x0":213,"x1":1009,"y0":1297,"y1":1318},"font_size":2497000000000.0,"text":"8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge"},{"bbox":{"x0":213,"x1":1009,"y0":1318,"y1":1340},"font_size":2497000000000.0,"text":"facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a"},{"bbox":{"x0":213,"x1":1008,"y0":1340,"y1":1363},"font_size":2497000000000.0,"text":"more generalizable skill for knowledge reasoning. As all prompting methods are still signifcantlyi"},{"bbox":{"x0":213,"x1":1009,"y0":1363,"y1":1384},"font_size":2497000000000.0,"text":"far from domain-specifc state-of-the-art approaches (Table 1), we believe fnetuning with moreii"},{"bbox":{"x0":213,"x1":812,"y0":1384,"y1":1406},"font_size":2497000000000.0,"text":"human-written data might be a better way to unleash the power of ReAct."}],"source":"layout det","text":"ReAct performs best for fne-tuningFigure 3 shows the scaling effect of prompting/fnetuningii four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct performs worst among four methods due to the diffculty to learn both reasoning and acting fromi in-context examples. However, when fnetuned with just 3,000 examples, ReAct becomes the besti method among the four, with PaLM-8B fnetuned ReAct outperforming all PaLM-62B promptingi methods, and PaLM-62B fnetuned ReAct outperforming all 540B prompting methods. In contrast,i fnetuning Standard or CoT is signifcantly worse than fnetuning ReAct or Act for both PaLM-iii 8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a more generalizable skill for knowledge reasoning. As all prompting methods are still signifcantlyi far from domain-specifc state-of-the-art approaches (Table 1), we believe fnetuning with moreii human-written data might be a better way to unleash the power of ReAct."}],"formula_dets":[{"bbox":{"x0":513,"x1":552,"y0":815,"y1":835},"conf":0.824,"label":"print_embedding","label_id":0},{"bbox":{"x0":878,"x1":906,"y0":278,"y1":291},"conf":0.7331,"label":"print_embedding","label_id":0},{"bbox":{"x0":356,"x1":387,"y0":573,"y1":593},"conf":0.7169,"label":"print_embedding","label_id":0},{"bbox":{"x0":825,"x1":876,"y0":572,"y1":594},"conf":0.711,"label":"print_embedding","label_id":0},{"bbox":{"x0":291,"x1":324,"y0":573,"y1":592},"conf":0.7038,"label":"print_embedding","label_id":0},{"bbox":{"x0":878,"x1":906,"y0":245,"y1":258},"conf":0.6368,"label":"print_embedding","label_id":0},{"bbox":{"x0":735,"x1":901,"y0":987,"y1":1006},"conf":0.5923,"label":"print_embedding","label_id":0},{"bbox":{"x0":944,"x1":972,"y0":278,"y1":291},"conf":0.591,"label":"print_embedding","label_id":0},{"bbox":{"x0":589,"x1":614,"y0":990,"y1":1005},"conf":0.5812,"label":"print_embedding","label_id":0},{"bbox":{"x0":878,"x1":907,"y0":196,"y1":210},"conf":0.5768,"label":"print_embedding","label_id":0},{"bbox":{"x0":878,"x1":901,"y0":262,"y1":274},"conf":0.5547,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":208,"x1":1016,"y0":1137,"y1":1411},"conf":0.9802,"label":"Text","label_id":1},{"bbox":{"x0":231,"x1":990,"y0":159,"y1":304},"conf":0.9748,"label":"Table","label_id":5},{"bbox":{"x0":207,"x1":1017,"y0":958,"y1":1124},"conf":0.9747,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":380,"y1":543},"conf":0.9723,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":647,"y1":788},"conf":0.9625,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":547,"y1":643},"conf":0.9595,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":792,"y1":886},"conf":0.9523,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":890,"y1":944},"conf":0.9329,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1013,"y0":312,"y1":366},"conf":0.8706,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":80},"conf":0.7208,"label":"Abandon","label_id":2},{"bbox":{"x0":206,"x1":1016,"y0":1420,"y1":1470},"conf":0.617,"label":"Abandon","label_id":2},{"bbox":{"x0":599,"x1":623,"y0":1500,"y1":1526},"conf":0.3441,"label":"Abandon","label_id":2},{"bbox":{"x0":602,"x1":621,"y0":1501,"y1":1525},"conf":0.2984,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":84},"conf":0.2801,"label":"Abandon","label_id":2},{"bbox":{"x0":207,"x1":1019,"y0":1420,"y1":1527},"conf":0.2574,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1503],[619,1503],[619,1521],[604,1521]],"score":0.8519},{"poly":[[211,1442],[677,1444],[677,1465],[211,1464]],"score":0.8002},{"poly":[[240,1419],[1010,1424],[1009,1447],[239,1442]],"score":0.6775},{"poly":[[213,1384],[812,1384],[812,1406],[213,1406]],"score":0.8048},{"poly":[[213,1363],[1009,1363],[1009,1384],[213,1384]],"score":0.8114},{"poly":[[213,1340],[1008,1340],[1008,1363],[213,1363]],"score":0.6991},{"poly":[[213,1318],[1009,1318],[1009,1340],[213,1340]],"score":0.804},{"poly":[[213,1297],[1009,1297],[1009,1318],[213,1318]],"score":0.8055},{"poly":[[213,1275],[1011,1275],[1011,1297],[213,1297]],"score":0.7503},{"poly":[[213,1254],[1009,1254],[1009,1275],[213,1275]],"score":0.8199},{"poly":[[210,1226],[1011,1229],[1011,1257],[209,1254]],"score":0.6185},{"poly":[[213,1209],[1009,1209],[1009,1231],[213,1231]],"score":0.77},{"poly":[[211,1188],[1009,1186],[1009,1208],[211,1209]],"score":0.7938},{"poly":[[211,1163],[1006,1165],[1006,1188],[211,1186]],"score":0.7064},{"poly":[[210,1138],[1011,1140],[1011,1168],[210,1167]],"score":0.6501},{"poly":[[211,1096],[941,1094],[941,1117],[211,1119]],"score":0.7109},{"poly":[[211,1071],[1009,1073],[1009,1096],[211,1094]],"score":0.7579},{"poly":[[211,1049],[1009,1051],[1009,1074],[211,1072]],"score":0.6669},{"poly":[[213,1031],[1009,1031],[1009,1053],[213,1053]],"score":0.7901},{"poly":[[213,1008],[1008,1008],[1008,1030],[213,1030]],"score":0.7621},{"poly":[[211,985],[1011,987],[1011,1008],[211,1006]],"score":0.8231},{"poly":[[211,962],[1009,964],[1009,987],[211,985]],"score":0.7434},{"poly":[[213,917],[881,917],[881,939],[213,939]],"score":0.802},{"poly":[[211,893],[1008,894],[1008,916],[211,914]],"score":0.8264},{"poly":[[213,861],[958,861],[958,883],[213,883]],"score":0.8007},{"poly":[[213,838],[1008,838],[1008,860],[213,860]],"score":0.7726},{"poly":[[213,817],[1009,817],[1009,838],[213,838]],"score":0.8046},{"poly":[[234,797],[1009,797],[1009,813],[234,813]],"score":0.9874},{"poly":[[211,762],[634,761],[634,784],[211,785]],"score":0.7507},{"poly":[[213,739],[1009,739],[1009,761],[213,761]],"score":0.7604},{"poly":[[213,718],[1009,718],[1009,739],[213,739]],"score":0.7896},{"poly":[[215,696],[1009,696],[1009,718],[215,718]],"score":0.8064},{"poly":[[213,673],[1009,673],[1009,695],[213,695]],"score":0.7752},{"poly":[[233,652],[1011,652],[1011,673],[233,673]],"score":0.8277},{"poly":[[213,617],[521,617],[521,639],[213,639]],"score":0.7748},{"poly":[[213,596],[1009,596],[1009,617],[213,617]],"score":0.7605},{"poly":[[213,574],[1009,574],[1009,596],[213,596]],"score":0.8136},{"poly":[[233,551],[1009,551],[1009,573],[233,573]],"score":0.7836},{"poly":[[215,518],[669,518],[669,540],[215,540]],"score":0.7464},{"poly":[[213,497],[1008,497],[1008,518],[213,518]],"score":0.7872},{"poly":[[213,475],[1004,475],[1004,492],[213,492]],"score":0.7854},{"poly":[[211,450],[1008,450],[1008,472],[211,472]],"score":0.7238},{"poly":[[215,431],[1008,431],[1008,452],[215,452]],"score":0.8044},{"poly":[[213,406],[1009,408],[1009,431],[213,429]],"score":0.7045},{"poly":[[211,386],[1006,386],[1006,408],[211,408]],"score":0.8001},{"poly":[[211,340],[712,338],[712,361],[211,363]],"score":0.7364},{"poly":[[215,317],[1008,317],[1008,338],[215,338]],"score":0.7993},{"poly":[[446,274],[745,274],[745,295],[446,295]],"score":0.7001},{"poly":[[323,274],[419,276],[419,294],[323,292]],"score":0.7901},{"poly":[[876,274],[908,274],[908,294],[876,294]],"score":0.8359},{"poly":[[943,272],[975,272],[975,294],[943,294]],"score":0.8333},{"poly":[[876,259],[906,259],[906,277],[876,277]],"score":0.8861},{"poly":[[444,257],[654,259],[653,277],[444,276]],"score":0.7538},{"poly":[[328,257],[414,259],[414,277],[327,275]],"score":0.8039},{"poly":[[941,257],[975,257],[975,277],[941,277]],"score":0.8891},{"poly":[[248,251],[294,251],[294,269],[248,269]],"score":0.9179},{"poly":[[446,244],[778,244],[778,261],[446,261]],"score":0.8023},{"poly":[[321,244],[424,244],[424,261],[321,261]],"score":0.8768},{"poly":[[876,241],[908,241],[908,262],[876,262]],"score":0.8216},{"poly":[[447,228],[860,228],[860,244],[447,244]],"score":0.8514},{"poly":[[324,226],[421,226],[421,243],[324,243]],"score":0.6408},{"poly":[[943,226],[980,226],[980,246],[943,246]],"score":0.8551},{"poly":[[875,226],[908,226],[908,246],[875,246]],"score":0.8315},{"poly":[[446,211],[652,211],[652,228],[446,228]],"score":0.7865},{"poly":[[331,211],[412,211],[412,229],[331,229]],"score":0.7492},{"poly":[[943,210],[976,210],[976,229],[943,229]],"score":0.8645},{"poly":[[876,210],[905,210],[905,229],[876,229]],"score":0.7163},{"poly":[[249,203],[298,203],[298,221],[249,221]],"score":0.9395},{"poly":[[446,196],[632,196],[632,213],[446,213]],"score":0.8124},{"poly":[[333,196],[411,196],[411,213],[333,213]],"score":0.9501},{"poly":[[943,193],[975,193],[975,214],[943,214]],"score":0.843},{"poly":[[876,193],[908,193],[908,214],[876,214]],"score":0.9017},{"poly":[[943,168],[975,168],[975,186],[943,186]],"score":0.9373},{"poly":[[876,168],[921,168],[921,186],[876,186]],"score":0.8965},{"poly":[[445,165],[508,169],[506,187],[444,183]],"score":0.8128},{"poly":[[357,164],[390,169],[387,190],[354,186]],"score":0.7773},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7401}],"page_no":5,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":208,"x1":1015,"y0":1419,"y1":1528},"conf":0.5675,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":619,"y0":1502,"y1":1520},"font_size":0.0,"text":"7"},{"bbox":{"x0":213,"x1":886,"y0":1444,"y1":1465},"font_size":0.0,"text":"performance than BUTLER, but it is trained on all task types, thus not included as a baseline."},{"bbox":{"x0":236,"x1":1011,"y0":1419,"y1":1444},"font_size":0.0,"text":"5Micheli & Fleuret (2021) fnetuned a GPT-2 model on 3553 task instances and achieved a much improved"},{"bbox":{"x0":444,"x1":444,"y0":1427,"y1":1443},"font_size":8.0,"text":"i"}],"source":"layout det","text":""},{"bbox":{"x0":208,"x1":1018,"y0":47,"y1":85},"conf":0.3763,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":312,"x1":910,"y0":167,"y1":409},"conf":0.9716,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![2ae384b9b60a913387de123ec05332f5](imgs/2ae384b9b60a913387de123ec05332f5.jpg)"},{"bbox":{"x0":254,"x1":995,"y0":422,"y1":454},"conf":0.665,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":216,"x1":1008,"y0":426,"y1":447},"font_size":-3.011e-17,"text":"re 3: Scaling results for prompting and fnetuning on HotPotQA with ReAct (ours) and baselinei"}],"source":"layout det","text":"re 3: Scaling results for prompting and fnetuning on HotPotQA with ReAct (ours) and baselinei"},{"bbox":{"x0":208,"x1":530,"y0":467,"y1":497},"conf":0.8784,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":526,"y0":469,"y1":493},"font_size":-3.011e-17,"text":"4DECISION MAKING TASKS"}],"source":"layout det","text":"4DECISION MAKING TASKS"},{"bbox":{"x0":206,"x1":1015,"y0":520,"y1":596},"conf":0.9404,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":521,"y1":543},"font_size":-3.011e-17,"text":"We also test ReAct on two language-based interactive decision-making tasks, ALFWorld and"},{"bbox":{"x0":211,"x1":1009,"y0":543,"y1":568},"font_size":-3.011e-17,"text":"WebShop, both of which feature complex environments that require agents to act over long horizons"},{"bbox":{"x0":213,"x1":890,"y0":568,"y1":589},"font_size":-3.011e-17,"text":"with sparse rewards, warranting the need for reasoning to act and explore effectively."}],"source":"layout det","text":"We also test ReAct on two language-based interactive decision-making tasks, ALFWorld and WebShop, both of which feature complex environments that require agents to act over long horizons with sparse rewards, warranting the need for reasoning to act and explore effectively."},{"bbox":{"x0":207,"x1":1017,"y0":615,"y1":1065},"conf":0.9848,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":620,"y1":642},"font_size":-3.011e-17,"text":"ALFWorldALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed"},{"bbox":{"x0":213,"x1":1009,"y0":642,"y1":663},"font_size":-3.011e-17,"text":"to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of"},{"bbox":{"x0":211,"x1":1009,"y0":662,"y1":686},"font_size":-3.011e-17,"text":"tasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by"},{"bbox":{"x0":213,"x1":1009,"y0":686,"y1":708},"font_size":-3.011e-17,"text":"navigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take"},{"bbox":{"x0":211,"x1":1009,"y0":706,"y1":731},"font_size":-3.011e-17,"text":"paper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy"},{"bbox":{"x0":211,"x1":1009,"y0":728,"y1":752},"font_size":-3.011e-17,"text":"more than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore"},{"bbox":{"x0":213,"x1":1009,"y0":752,"y1":774},"font_size":-3.011e-17,"text":"systematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into"},{"bbox":{"x0":213,"x1":1009,"y0":774,"y1":795},"font_size":-3.011e-17,"text":"ALFWorld is the need to determine likely locations for common household items (e.g. desklamps will"},{"bbox":{"x0":213,"x1":1011,"y0":795,"y1":817},"font_size":-3.011e-17,"text":"likely be on desks, shelfs, or dressers), making this environment a good ft for LLMs to exploit theiri"},{"bbox":{"x0":211,"x1":1009,"y0":817,"y1":840},"font_size":-3.011e-17,"text":"pretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories"},{"bbox":{"x0":213,"x1":1009,"y0":840,"y1":861},"font_size":-3.011e-17,"text":"from the training set for each task type, where each trajectory includes sparse thoughts that (1)"},{"bbox":{"x0":213,"x1":1011,"y0":861,"y1":883},"font_size":-3.011e-17,"text":"decompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via"},{"bbox":{"x0":213,"x1":1009,"y0":883,"y1":904},"font_size":-3.011e-17,"text":"commonsense where to fnd an object and what to do with it. We show prompts used for ALFWorldi"},{"bbox":{"x0":211,"x1":1011,"y0":903,"y1":927},"font_size":-3.011e-17,"text":"in Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games"},{"bbox":{"x0":213,"x1":1009,"y0":926,"y1":949},"font_size":-3.011e-17,"text":"in a task-specifc setup. For robustness, we construct 6 prompts for each task type through eachi"},{"bbox":{"x0":213,"x1":1009,"y0":949,"y1":972},"font_size":-3.011e-17,"text":"permutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using"},{"bbox":{"x0":213,"x1":1011,"y0":970,"y1":992},"font_size":-3.011e-17,"text":"the same trajectories, but without thoughts — since task instances are randomly chosen from the"},{"bbox":{"x0":213,"x1":1008,"y0":993,"y1":1010},"font_size":-3.011e-17,"text":"training set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the"},{"bbox":{"x0":213,"x1":1011,"y0":1015,"y1":1036},"font_size":-3.011e-17,"text":"importance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation"},{"bbox":{"x0":213,"x1":760,"y0":1036,"y1":1058},"font_size":-3.011e-17,"text":"learning agent trained on $10^{5}$ expert trajectories for each task type5."}],"source":"layout det","text":"ALFWorldALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of tasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by navigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take paper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy more than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore systematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into ALFWorld is the need to determine likely locations for common household items (e.g. desklamps will likely be on desks, shelfs, or dressers), making this environment a good ft for LLMs to exploit theiri pretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories from the training set for each task type, where each trajectory includes sparse thoughts that (1)decompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via commonsense where to fnd an object and what to do with it. We show prompts used for ALFWorldi in Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games in a task-specifc setup. For robustness, we construct 6 prompts for each task type through eachi permutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using the same trajectories, but without thoughts — since task instances are randomly chosen from the training set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the importance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation learning agent trained on $10^{5}$ expert trajectories for each task type5."},{"bbox":{"x0":207,"x1":1017,"y0":1084,"y1":1403},"conf":0.9845,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":1089,"y1":1112},"font_size":-3.011e-17,"text":"WebShopCan ReAct also interact with noisy real-world language environments for practical"},{"bbox":{"x0":210,"x1":1011,"y0":1107,"y1":1137},"font_size":-3.011e-17,"text":"applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping"},{"bbox":{"x0":211,"x1":1009,"y0":1134,"y1":1155},"font_size":-3.011e-17,"text":"website environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,"},{"bbox":{"x0":211,"x1":1011,"y0":1153,"y1":1178},"font_size":-3.011e-17,"text":"Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,"},{"bbox":{"x0":213,"x1":1011,"y0":1178,"y1":1200},"font_size":-3.011e-17,"text":"and options crawled from Amazon), and requires an agent to purchase a product based on a user"},{"bbox":{"x0":211,"x1":1009,"y0":1196,"y1":1221},"font_size":-3.011e-17,"text":"instruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel fnish, andi"},{"bbox":{"x0":211,"x1":1009,"y0":1219,"y1":1244},"font_size":-3.011e-17,"text":"priced lower than $\\$140\")$  through web interactions (e.g. search “nightstand drawers”, choose buttons"},{"bbox":{"x0":213,"x1":1011,"y0":1242,"y1":1264},"font_size":-3.011e-17,"text":"such as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score"},{"bbox":{"x0":211,"x1":1009,"y0":1262,"y1":1287},"font_size":-3.011e-17,"text":"(percentage of desired attributes covered by the chosen product averaged across all episodes) and"},{"bbox":{"x0":213,"x1":1009,"y0":1287,"y1":1308},"font_size":-3.011e-17,"text":"success rate (percentage of episodes where the chosen product satisfes all requirements) on 500 testi"},{"bbox":{"x0":213,"x1":1011,"y0":1308,"y1":1330},"font_size":-3.011e-17,"text":"instructions. We formulate Act prompts with actions to search, choose product, choose options,"},{"bbox":{"x0":211,"x1":1009,"y0":1332,"y1":1353},"font_size":-3.011e-17,"text":"and buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,"},{"bbox":{"x0":213,"x1":1009,"y0":1353,"y1":1374},"font_size":-3.011e-17,"text":"and what products options are relevant to the instruction. See Table 6 for an example prompt, and"},{"bbox":{"x0":215,"x1":1009,"y0":1374,"y1":1396},"font_size":-3.011e-17,"text":"Table 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method"}],"source":"layout det","text":"WebShopCan ReAct also interact with noisy real-world language environments for practical applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping website environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,and options crawled from Amazon), and requires an agent to purchase a product based on a user instruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel fnish, andi priced lower than $\\$140\")$  through web interactions (e.g. search “nightstand drawers”, choose buttons such as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score(percentage of desired attributes covered by the chosen product averaged across all episodes) and success rate (percentage of episodes where the chosen product satisfes all requirements) on 500 testi instructions. We formulate Act prompts with actions to search, choose product, choose options,and buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,and what products options are relevant to the instruction. See Table 6 for an example prompt, and Table 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method"}],"formula_dets":[{"bbox":{"x0":418,"x1":448,"y0":1036,"y1":1056},"conf":0.8678,"label":"print_embedding","label_id":0},{"bbox":{"x0":357,"x1":412,"y0":1220,"y1":1241},"conf":0.7513,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":207,"x1":1017,"y0":615,"y1":1065},"conf":0.9848,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1017,"y0":1084,"y1":1403},"conf":0.9845,"label":"Text","label_id":1},{"bbox":{"x0":312,"x1":910,"y0":167,"y1":409},"conf":0.9716,"label":"Figure","label_id":3},{"bbox":{"x0":206,"x1":1015,"y0":520,"y1":596},"conf":0.9404,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":530,"y0":467,"y1":497},"conf":0.8784,"label":"Title","label_id":0},{"bbox":{"x0":208,"x1":592,"y0":49,"y1":80},"conf":0.7009,"label":"Abandon","label_id":2},{"bbox":{"x0":254,"x1":995,"y0":422,"y1":454},"conf":0.665,"label":"Figure caption","label_id":4},{"bbox":{"x0":208,"x1":1015,"y0":1419,"y1":1528},"conf":0.5675,"label":"Abandon","label_id":2},{"bbox":{"x0":207,"x1":1017,"y0":1420,"y1":1469},"conf":0.3917,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1018,"y0":47,"y1":85},"conf":0.3763,"label":"Abandon","label_id":2},{"bbox":{"x0":601,"x1":620,"y0":1501,"y1":1525},"conf":0.2214,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1502],[619,1502],[619,1520],[604,1520]],"score":0.922},{"poly":[[213,1444],[886,1444],[886,1465],[213,1465]],"score":0.7555},{"poly":[[236,1419],[1011,1421],[1011,1444],[236,1442]],"score":0.6978},{"poly":[[215,1374],[1009,1374],[1009,1396],[215,1396]],"score":0.8018},{"poly":[[213,1353],[1009,1353],[1009,1374],[213,1374]],"score":0.7863},{"poly":[[211,1332],[1009,1332],[1009,1353],[211,1353]],"score":0.8374},{"poly":[[213,1308],[1011,1308],[1011,1330],[213,1330]],"score":0.8081},{"poly":[[213,1287],[1009,1287],[1009,1308],[213,1308]],"score":0.7961},{"poly":[[211,1264],[1009,1262],[1009,1285],[211,1287]],"score":0.757},{"poly":[[213,1242],[1011,1242],[1011,1264],[213,1264]],"score":0.7609},{"poly":[[211,1221],[1009,1219],[1009,1242],[211,1244]],"score":0.7469},{"poly":[[211,1198],[1009,1196],[1009,1219],[211,1221]],"score":0.7521},{"poly":[[213,1178],[1011,1178],[1011,1200],[213,1200]],"score":0.7826},{"poly":[[211,1153],[1011,1155],[1011,1178],[211,1176]],"score":0.746},{"poly":[[211,1134],[1009,1134],[1009,1155],[211,1155]],"score":0.7744},{"poly":[[210,1107],[1011,1109],[1011,1137],[210,1135]],"score":0.6153},{"poly":[[215,1089],[1009,1089],[1009,1112],[215,1112]],"score":0.705},{"poly":[[213,1036],[760,1036],[760,1058],[213,1058]],"score":0.8057},{"poly":[[213,1015],[1011,1015],[1011,1036],[213,1036]],"score":0.8298},{"poly":[[213,993],[1008,993],[1008,1010],[213,1010]],"score":0.7948},{"poly":[[213,970],[1011,970],[1011,992],[213,992]],"score":0.8084},{"poly":[[213,949],[1009,949],[1009,972],[213,972]],"score":0.6799},{"poly":[[213,926],[1009,926],[1009,949],[213,949]],"score":0.7031},{"poly":[[211,904],[1011,903],[1011,926],[211,927]],"score":0.6866},{"poly":[[213,883],[1009,883],[1009,904],[213,904]],"score":0.8232},{"poly":[[213,861],[1011,861],[1011,883],[213,883]],"score":0.8281},{"poly":[[213,840],[1009,840],[1009,861],[213,861]],"score":0.7906},{"poly":[[211,818],[1009,817],[1009,838],[211,840]],"score":0.818},{"poly":[[213,795],[1011,795],[1011,817],[213,817]],"score":0.8135},{"poly":[[213,774],[1009,774],[1009,795],[213,795]],"score":0.8177},{"poly":[[213,752],[1009,752],[1009,774],[213,774]],"score":0.8202},{"poly":[[211,728],[1009,729],[1009,752],[211,751]],"score":0.7483},{"poly":[[211,708],[1009,706],[1009,729],[211,731]],"score":0.6747},{"poly":[[213,686],[1009,686],[1009,708],[213,708]],"score":0.8149},{"poly":[[211,663],[1009,662],[1009,685],[211,686]],"score":0.7416},{"poly":[[213,642],[1009,642],[1009,663],[213,663]],"score":0.8212},{"poly":[[213,620],[1009,620],[1009,642],[213,642]],"score":0.8204},{"poly":[[213,568],[890,568],[890,589],[213,589]],"score":0.8075},{"poly":[[211,543],[1009,545],[1009,568],[211,566]],"score":0.7681},{"poly":[[211,521],[1008,521],[1008,543],[211,543]],"score":0.7494},{"poly":[[243,469],[526,470],[525,493],[243,492]],"score":0.7406},{"poly":[[215,472],[249,472],[249,490],[215,490]],"score":0.7463},{"poly":[[216,426],[1008,426],[1008,447],[216,447]],"score":0.87},{"poly":[[748,386],[782,386],[782,404],[748,404]],"score":0.9146},{"poly":[[472,384],[506,384],[506,404],[472,404]],"score":0.8286},{"poly":[[833,371],[871,371],[871,389],[833,389]],"score":0.9508},{"poly":[[750,371],[780,371],[780,389],[750,389]],"score":0.9321},{"poly":[[474,371],[506,371],[506,389],[474,389]],"score":0.9023},{"poly":[[392,373],[412,373],[412,388],[392,388]],"score":0.8256},{"poly":[[669,371],[687,371],[687,388],[669,388]],"score":0.814},{"poly":[[559,370],[595,370],[595,389],[559,389]],"score":0.8159},{"poly":[[840,314],[885,314],[885,332],[840,332]],"score":0.8487},{"poly":[[840,297],[870,297],[870,315],[840,315]],"score":0.9221},{"poly":[[827,279],[873,279],[873,297],[827,297]],"score":0.628},{"poly":[[837,262],[901,262],[901,280],[837,280]],"score":0.7888},{"poly":[[832,244],[881,244],[881,262],[832,262]],"score":0.8828},{"poly":[[336,203],[353,203],[353,221],[336,221]],"score":0.7846},{"poly":[[702,173],[830,173],[830,190],[702,190]],"score":0.9756},{"poly":[[429,173],[550,173],[550,191],[429,191]],"score":0.8428},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7335}],"page_no":6,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":600,"x1":623,"y0":1501,"y1":1525},"conf":0.4043,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":620,"y0":1503,"y1":1521},"font_size":0.0,"text":"8"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.3192,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":207,"x1":767,"y0":158,"y1":360},"conf":0.9816,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":222,"x1":285,"y0":165,"y1":189},"font_size":0.0,"text":"Method"},{"bbox":{"x0":374,"x1":417,"y0":164,"y1":189},"font_size":0.0,"text":"Pick"},{"bbox":{"x0":428,"x1":477,"y0":165,"y1":189},"font_size":0.0,"text":"Clean"},{"bbox":{"x0":490,"x1":534,"y0":165,"y1":189},"font_size":0.0,"text":"Heat"},{"bbox":{"x0":544,"x1":588,"y0":165,"y1":189},"font_size":0.0,"text":"Cool"},{"bbox":{"x0":599,"x1":646,"y0":165,"y1":189},"font_size":0.0,"text":"Look"},{"bbox":{"x0":657,"x1":715,"y0":163,"y1":191},"font_size":0.0,"text":"Pick 2"},{"bbox":{"x0":724,"x1":756,"y0":164,"y1":190},"font_size":0.0,"text":"All"},{"bbox":{"x0":220,"x1":310,"y0":193,"y1":218},"font_size":0.0,"text":"Act (best of 6)"},{"bbox":{"x0":382,"x1":409,"y0":194,"y1":217},"font_size":0.0,"text":"88"},{"bbox":{"x0":439,"x1":467,"y0":194,"y1":217},"font_size":0.0,"text":"42"},{"bbox":{"x0":497,"x1":526,"y0":192,"y1":218},"font_size":0.0,"text":"74"},{"bbox":{"x0":551,"x1":580,"y0":191,"y1":218},"font_size":0.0,"text":"67"},{"bbox":{"x0":607,"x1":638,"y0":191,"y1":219},"font_size":0.0,"text":"72"},{"bbox":{"x0":672,"x1":697,"y0":194,"y1":216},"font_size":0.0,"text":"41"},{"bbox":{"x0":725,"x1":754,"y0":192,"y1":219},"font_size":0.0,"text":"45"},{"bbox":{"x0":220,"x1":310,"y0":211,"y1":239},"font_size":0.0,"text":"ReAct (avg)"},{"bbox":{"x0":382,"x1":409,"y0":214,"y1":237},"font_size":0.0,"text":"65"},{"bbox":{"x0":439,"x1":467,"y0":214,"y1":237},"font_size":0.0,"text":"39"},{"bbox":{"x0":498,"x1":525,"y0":214,"y1":237},"font_size":0.0,"text":"83"},{"bbox":{"x0":552,"x1":581,"y0":212,"y1":237},"font_size":0.0,"text":"76"},{"bbox":{"x0":610,"x1":636,"y0":214,"y1":237},"font_size":0.0,"text":"55"},{"bbox":{"x0":672,"x1":698,"y0":214,"y1":237},"font_size":0.0,"text":"24"},{"bbox":{"x0":725,"x1":754,"y0":213,"y1":239},"font_size":0.0,"text":"57"},{"bbox":{"x0":220,"x1":332,"y0":231,"y1":257},"font_size":0.0,"text":"ReAct (best of 6)"},{"bbox":{"x0":382,"x1":409,"y0":233,"y1":255},"font_size":0.0,"text":"92"},{"bbox":{"x0":440,"x1":466,"y0":233,"y1":256},"font_size":0.0,"text":"58"},{"bbox":{"x0":498,"x1":525,"y0":234,"y1":255},"font_size":0.0,"text":"96"},{"bbox":{"x0":553,"x1":579,"y0":234,"y1":255},"font_size":0.0,"text":"86"},{"bbox":{"x0":610,"x1":636,"y0":233,"y1":256},"font_size":0.0,"text":"78"},{"bbox":{"x0":672,"x1":697,"y0":234,"y1":255},"font_size":0.0,"text":"41"},{"bbox":{"x0":727,"x1":753,"y0":233,"y1":256},"font_size":0.0,"text":"71"},{"bbox":{"x0":220,"x1":341,"y0":260,"y1":287},"font_size":0.0,"text":"ReAct-IM (avg)"},{"bbox":{"x0":380,"x1":410,"y0":260,"y1":286},"font_size":0.0,"text":"55"},{"bbox":{"x0":439,"x1":467,"y0":261,"y1":284},"font_size":0.0,"text":"59"},{"bbox":{"x0":498,"x1":525,"y0":262,"y1":284},"font_size":0.0,"text":"60"},{"bbox":{"x0":552,"x1":580,"y0":260,"y1":286},"font_size":0.0,"text":"55"},{"bbox":{"x0":609,"x1":636,"y0":261,"y1":284},"font_size":0.0,"text":"23"},{"bbox":{"x0":671,"x1":698,"y0":261,"y1":284},"font_size":0.0,"text":"24"},{"bbox":{"x0":725,"x1":755,"y0":261,"y1":286},"font_size":0.0,"text":"48"},{"bbox":{"x0":221,"x1":363,"y0":280,"y1":304},"font_size":0.0,"text":"ReAct-IM (best of 6)"},{"bbox":{"x0":381,"x1":410,"y0":279,"y1":305},"font_size":0.0,"text":"62"},{"bbox":{"x0":439,"x1":467,"y0":281,"y1":303},"font_size":0.0,"text":"68"},{"bbox":{"x0":498,"x1":525,"y0":281,"y1":304},"font_size":0.0,"text":"87"},{"bbox":{"x0":553,"x1":580,"y0":281,"y1":304},"font_size":0.0,"text":"57"},{"bbox":{"x0":610,"x1":636,"y0":281,"y1":304},"font_size":0.0,"text":"39"},{"bbox":{"x0":672,"x1":698,"y0":281,"y1":304},"font_size":0.0,"text":"33"},{"bbox":{"x0":725,"x1":755,"y0":279,"y1":306},"font_size":0.0,"text":"53"},{"bbox":{"x0":219,"x1":356,"y0":306,"y1":336},"font_size":0.0,"text":"BUTLERg (best of 8)"},{"bbox":{"x0":381,"x1":410,"y0":308,"y1":335},"font_size":0.0,"text":"33"},{"bbox":{"x0":438,"x1":468,"y0":308,"y1":335},"font_size":0.0,"text":"26"},{"bbox":{"x0":497,"x1":526,"y0":307,"y1":334},"font_size":0.0,"text":"70"},{"bbox":{"x0":551,"x1":581,"y0":307,"y1":334},"font_size":0.0,"text":"76"},{"bbox":{"x0":609,"x1":638,"y0":307,"y1":334},"font_size":0.0,"text":"17"},{"bbox":{"x0":673,"x1":698,"y0":310,"y1":333},"font_size":0.0,"text":"12"},{"bbox":{"x0":721,"x1":757,"y0":305,"y1":357},"font_size":0.0,"text":"2327"},{"bbox":{"x0":221,"x1":347,"y0":327,"y1":353},"font_size":0.0,"text":"BUTLER (best of 8)"},{"bbox":{"x0":382,"x1":409,"y0":328,"y1":352},"font_size":0.0,"text":"46"},{"bbox":{"x0":438,"x1":468,"y0":327,"y1":354},"font_size":0.0,"text":"39"},{"bbox":{"x0":498,"x1":526,"y0":327,"y1":354},"font_size":0.0,"text":"74"},{"bbox":{"x0":549,"x1":584,"y0":327,"y1":353},"font_size":0.0,"text":"100"},{"bbox":{"x0":607,"x1":637,"y0":325,"y1":354},"font_size":0.0,"text":"22"},{"bbox":{"x0":672,"x1":699,"y0":328,"y1":352},"font_size":0.0,"text":"24"}],"source":"layout det","text":"<html><body><table><thead><tr><td>Method</td><td>Pick</td><td>Clean</td><td>Heat</td><td>Cool</td><td>Look</td><td>Pick 2</td><td>All</td></tr></thead><tbody><tr><td>Act (best of 6)</td><td>88</td><td>42</td><td>74</td><td>67</td><td>72</td><td>41</td><td>45</td></tr><tr><td>ReAct (avg)</td><td>65</td><td>39</td><td>83</td><td>76</td><td>55</td><td>24</td><td>57</td></tr><tr><td>ReAct (best of 6)</td><td>92</td><td>58</td><td>96</td><td>86</td><td>78</td><td>41</td><td>71</td></tr><tr><td>ReAct-IM (avg)</td><td>55</td><td>59</td><td>60</td><td>55</td><td>23</td><td>24</td><td>48</td></tr><tr><td>ReAct-IM (best of 6)</td><td>62</td><td>68</td><td>87</td><td>57</td><td>39</td><td>33</td><td>53</td></tr><tr><td>BUTLERg (best of 8)</td><td>33</td><td>26</td><td>70</td><td>76</td><td>17</td><td>12</td><td>2327</td></tr><tr><td>BUTLER (best of 8)</td><td>46</td><td>39</td><td>74</td><td>100</td><td>22</td><td>24</td><td></td></tr></tbody></table></body></html>"},{"bbox":{"x0":208,"x1":768,"y0":372,"y1":446},"conf":0.8043,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":213,"x1":768,"y0":375,"y1":396},"font_size":-6.614e+26,"text":"Table 3: AlfWorld task-specifc success rates (%). BUTLER andi"},{"bbox":{"x0":213,"x1":961,"y0":396,"y1":417},"font_size":-6.614e+26,"text":"BUTLERg results are from Table 4 of Shridhar et al. (2020b). All"},{"bbox":{"x0":213,"x1":960,"y0":417,"y1":441},"font_size":-6.614e+26,"text":"methods use greedy decoding, except that BUTLER uses beam search."}],"source":"layout det","text":"Table 3: AlfWorld task-specifc success rates (%). BUTLER andi BUTLERg results are from Table 4 of Shridhar et al. (2020b). All methods use greedy decoding, except that BUTLER uses beam search."},{"bbox":{"x0":772,"x1":967,"y0":158,"y1":339},"conf":0.9655,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":782,"x1":846,"y0":165,"y1":188},"font_size":0.0,"text":"Method"},{"bbox":{"x0":853,"x1":905,"y0":162,"y1":189},"font_size":0.0,"text":"Score"},{"bbox":{"x0":917,"x1":951,"y0":165,"y1":189},"font_size":0.0,"text":"SR"},{"bbox":{"x0":793,"x1":833,"y0":194,"y1":216},"font_size":0.0,"text":"Act"},{"bbox":{"x0":858,"x1":901,"y0":191,"y1":216},"font_size":0.0,"text":"62.3"},{"bbox":{"x0":913,"x1":955,"y0":191,"y1":217},"font_size":0.0,"text":"30.1"},{"bbox":{"x0":782,"x1":842,"y0":211,"y1":235},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":860,"x1":901,"y0":211,"y1":232},"font_size":0.0,"text":"66.6"},{"bbox":{"x0":914,"x1":954,"y0":211,"y1":233},"font_size":0.0,"text":"40.0"},{"bbox":{"x0":799,"x1":829,"y0":237,"y1":262},"font_size":0.0,"text":"IL"},{"bbox":{"x0":858,"x1":902,"y0":237,"y1":263},"font_size":0.0,"text":"59.9"},{"bbox":{"x0":911,"x1":955,"y0":235,"y1":263},"font_size":0.0,"text":"29.1"},{"bbox":{"x0":784,"x1":842,"y0":256,"y1":280},"font_size":0.0,"text":"IL+RL"},{"bbox":{"x0":860,"x1":901,"y0":256,"y1":279},"font_size":0.0,"text":"62.4"},{"bbox":{"x0":912,"x1":955,"y0":255,"y1":281},"font_size":0.0,"text":"28.7"},{"bbox":{"x0":783,"x1":844,"y0":288,"y1":312},"font_size":0.0,"text":"Human"},{"bbox":{"x0":782,"x1":845,"y0":304,"y1":335},"font_size":0.0,"text":"Expert"},{"bbox":{"x0":859,"x1":901,"y0":297,"y1":322},"font_size":0.0,"text":"82.1"},{"bbox":{"x0":912,"x1":956,"y0":297,"y1":322},"font_size":0.0,"text":"59.6"}],"source":"layout det","text":"<html><body><table><tr><td>Method</td><td>Score</td><td>SR</td></tr><tr><td>Act ReAct</td><td>62.3 66.6</td><td>30.1 40.0</td></tr><tr><td>IL</td><td>59.9</td><td>29.1</td></tr><tr><td>IL+RL</td><td>62.4</td><td>28.7</td></tr><tr><td>Human Expert</td><td>82.1</td><td>59.6</td></tr></table></body></html>"},{"bbox":{"x0":770,"x1":967,"y0":351,"y1":444},"conf":0.3558,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":771,"x1":963,"y0":350,"y1":376},"font_size":-6.614e+26,"text":"Table 4: Score and suc-"},{"bbox":{"x0":770,"x1":963,"y0":375,"y1":396},"font_size":-6.614e+26,"text":"cess rate (SR) on Web-"},{"bbox":{"x0":777,"x1":959,"y0":400,"y1":418},"font_size":10.0,"text":"shop. IL/IL+RL taken"},{"bbox":{"x0":777,"x1":959,"y0":422,"y1":440},"font_size":9.0,"text":"from Yao et al. (2022)."}],"source":"layout det","text":"Table 4: Score and success rate (SR) on Webshop. IL/IL+RL taken from Yao et al. (2022)."},{"bbox":{"x0":208,"x1":1016,"y0":492,"y1":544},"conf":0.9318,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":495,"y1":516},"font_size":-6.614e+26,"text":"trained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)"},{"bbox":{"x0":213,"x1":707,"y0":518,"y1":540},"font_size":-6.614e+26,"text":"method additionally trained with 10,587 training instructions."}],"source":"layout det","text":"trained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)method additionally trained with 10,587 training instructions."},{"bbox":{"x0":209,"x1":1015,"y0":603,"y1":787},"conf":0.9772,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":606,"y1":627},"font_size":-6.614e+26,"text":"ResultsReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On"},{"bbox":{"x0":211,"x1":1010,"y0":625,"y1":652},"font_size":-6.614e+26,"text":"ALFWorld, the best ReAct trial achieves an average success rate of $71\\%,$  signifcantly outperformingi"},{"bbox":{"x0":213,"x1":1009,"y0":650,"y1":673},"font_size":-6.614e+26,"text":"the best Act $(45\\%)$  and BUTLER $(37\\%)$  trials. In fact, even the worse ReAct trial $$  beats"},{"bbox":{"x0":213,"x1":1009,"y0":672,"y1":693},"font_size":-6.614e+26,"text":"the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across"},{"bbox":{"x0":213,"x1":1010,"y0":694,"y1":716},"font_size":-6.614e+26,"text":"six controlled trials, with relative performance gain ranging from $33\\%$ to $90\\%$  and averaging $62\\%.$"},{"bbox":{"x0":213,"x1":1009,"y0":716,"y1":739},"font_size":-6.614e+26,"text":"Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals"},{"bbox":{"x0":213,"x1":1011,"y0":738,"y1":759},"font_size":-6.614e+26,"text":"into smaller subgoals, or loses track of the current state of the environment. Example trajectories"},{"bbox":{"x0":211,"x1":871,"y0":757,"y1":782},"font_size":-6.614e+26,"text":"comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2."}],"source":"layout det","text":"ResultsReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On ALFWorld, the best ReAct trial achieves an average success rate of $71\\%,$  signifcantly outperformingi the best Act $(45\\%)$  and BUTLER $(37\\%)$  trials. In fact, even the worse ReAct trial $$  beats the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across six controlled trials, with relative performance gain ranging from $33\\%$ to $90\\%$  and averaging $62\\%.$ Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals into smaller subgoals, or loses track of the current state of the environment. Example trajectories comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2."},{"bbox":{"x0":209,"x1":1015,"y0":791,"y1":975},"conf":0.9779,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":794,"y1":815},"font_size":-6.614e+26,"text":"On Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With"},{"bbox":{"x0":211,"x1":1010,"y0":815,"y1":837},"font_size":-6.614e+26,"text":"additional sparse reasoning, ReAct achieves signifcantly better performance, with an absolutei $10\\%$"},{"bbox":{"x0":215,"x1":1008,"y0":840,"y1":859},"font_size":-6.614e+26,"text":"improvement over the previous best success rate. By checking examples, we fnd that ReAct is morei"},{"bbox":{"x0":213,"x1":1009,"y0":860,"y1":881},"font_size":-6.614e+26,"text":"likely to identify instruction-relevant products and options by reasoning to bridge the gap between"},{"bbox":{"x0":211,"x1":1011,"y0":879,"y1":904},"font_size":-6.614e+26,"text":"noisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item"},{"bbox":{"x0":215,"x1":1008,"y0":906,"y1":922},"font_size":-6.614e+26,"text":"has options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are"},{"bbox":{"x0":213,"x1":1009,"y0":926,"y1":947},"font_size":-6.614e+26,"text":"still far from the performance of expert humans (Table 4), who perform signifcantly more producti"},{"bbox":{"x0":213,"x1":970,"y0":947,"y1":970},"font_size":-6.614e+26,"text":"explorations and query re-formulations that are still challenging for prompting-based methods."}],"source":"layout det","text":"On Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With additional sparse reasoning, ReAct achieves signifcantly better performance, with an absolutei $10\\%$ improvement over the previous best success rate. By checking examples, we fnd that ReAct is morei likely to identify instruction-relevant products and options by reasoning to bridge the gap between noisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item has options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are still far from the performance of expert humans (Table 4), who perform signifcantly more producti explorations and query re-formulations that are still challenging for prompting-based methods."},{"bbox":{"x0":207,"x1":1016,"y0":1030,"y1":1214},"conf":0.9715,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1035,"y1":1057},"font_size":-6.614e+26,"text":"On the value of internal reasoning vs. external feedbackTo our knowledge, ReAct is the frsti"},{"bbox":{"x0":213,"x1":1009,"y0":1058,"y1":1079},"font_size":-6.614e+26,"text":"demonstration of combined reasoning and action using an LLM applied to an interactive environment"},{"bbox":{"x0":211,"x1":1009,"y0":1077,"y1":1102},"font_size":-6.614e+26,"text":"within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang"},{"bbox":{"x0":213,"x1":1009,"y0":1101,"y1":1122},"font_size":-6.614e+26,"text":"et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner"},{"bbox":{"x0":213,"x1":1009,"y0":1124,"y1":1145},"font_size":-6.614e+26,"text":"monologue”. However, IM’s “inner monologue” is limited to observations of the environment"},{"bbox":{"x0":213,"x1":1011,"y0":1145,"y1":1167},"font_size":-6.614e+26,"text":"state and what needs to be completed by the agent for the goal to be satisfed. In contrast, thei"},{"bbox":{"x0":211,"x1":1011,"y0":1165,"y1":1191},"font_size":-6.614e+26,"text":"reasoning traces in ReAct for decision making is fexible and sparse, allowing diverse reasoningl"},{"bbox":{"x0":211,"x1":645,"y0":1188,"y1":1213},"font_size":-6.614e+26,"text":"types (see Section 2) to be induced for different tasks."}],"source":"layout det","text":"On the value of internal reasoning vs. external feedbackTo our knowledge, ReAct is the frsti demonstration of combined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”. However, IM’s “inner monologue” is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisfed. In contrast, thei reasoning traces in ReAct for decision making is fexible and sparse, allowing diverse reasoningl types (see Section 2) to be induced for different tasks."},{"bbox":{"x0":208,"x1":1015,"y0":1220,"y1":1469},"conf":0.9787,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1223,"y1":1244},"font_size":-6.614e+26,"text":"To demonstrate the differences between ReAct and IM, and to highlight the importance of internal"},{"bbox":{"x0":211,"x1":1009,"y0":1242,"y1":1267},"font_size":-6.614e+26,"text":"reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought"},{"bbox":{"x0":211,"x1":1009,"y0":1266,"y1":1289},"font_size":-6.614e+26,"text":"pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially"},{"bbox":{"x0":213,"x1":1011,"y0":1289,"y1":1310},"font_size":-6.614e+26,"text":"outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent"},{"bbox":{"x0":213,"x1":1009,"y0":1310,"y1":1332},"font_size":-6.614e+26,"text":"advantages on fve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakesi"},{"bbox":{"x0":213,"x1":1008,"y0":1335,"y1":1354},"font_size":-6.614e+26,"text":"in identifying when subgoals were fnished, or what the next subgoal should be, due to a lack of high-i"},{"bbox":{"x0":213,"x1":1011,"y0":1355,"y1":1376},"font_size":-6.614e+26,"text":"level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where"},{"bbox":{"x0":211,"x1":1011,"y0":1374,"y1":1399},"font_size":-6.614e+26,"text":"an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning."},{"bbox":{"x0":211,"x1":1011,"y0":1396,"y1":1421},"font_size":-6.614e+26,"text":"Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in"},{"bbox":{"x0":213,"x1":1011,"y0":1421,"y1":1442},"font_size":-6.614e+26,"text":"Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example"},{"bbox":{"x0":211,"x1":451,"y0":1439,"y1":1465},"font_size":-6.614e+26,"text":"trajectory in Appendix D.2.3."}],"source":"layout det","text":"To demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on fve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakesi in identifying when subgoals were fnished, or what the next subgoal should be, due to a lack of high-i level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example trajectory in Appendix D.2.3."}],"formula_dets":[{"bbox":{"x0":744,"x1":784,"y0":694,"y1":714},"conf":0.8394,"label":"print_embedding","label_id":0},{"bbox":{"x0":806,"x1":846,"y0":694,"y1":714},"conf":0.8235,"label":"print_embedding","label_id":0},{"bbox":{"x0":967,"x1":1010,"y0":695,"y1":714},"conf":0.8082,"label":"print_embedding","label_id":0},{"bbox":{"x0":972,"x1":1010,"y0":816,"y1":835},"conf":0.7963,"label":"print_embedding","label_id":0},{"bbox":{"x0":747,"x1":789,"y0":628,"y1":649},"conf":0.7844,"label":"print_embedding","label_id":0},{"bbox":{"x0":908,"x1":959,"y0":650,"y1":673},"conf":0.744,"label":"print_embedding","label_id":0},{"bbox":{"x0":326,"x1":374,"y0":651,"y1":672},"conf":0.5578,"label":"print_embedding","label_id":0},{"bbox":{"x0":501,"x1":553,"y0":650,"y1":673},"conf":0.5143,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":207,"x1":767,"y0":158,"y1":360},"conf":0.9816,"label":"Table","label_id":5},{"bbox":{"x0":208,"x1":1015,"y0":1220,"y1":1469},"conf":0.9787,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1015,"y0":791,"y1":975},"conf":0.9779,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1015,"y0":603,"y1":787},"conf":0.9772,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1030,"y1":1214},"conf":0.9715,"label":"Text","label_id":1},{"bbox":{"x0":772,"x1":967,"y0":158,"y1":339},"conf":0.9655,"label":"Table","label_id":5},{"bbox":{"x0":208,"x1":1016,"y0":492,"y1":544},"conf":0.9318,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":768,"y0":372,"y1":446},"conf":0.8043,"label":"Table footnote","label_id":7},{"bbox":{"x0":210,"x1":591,"y0":50,"y1":79},"conf":0.787,"label":"Abandon","label_id":2},{"bbox":{"x0":603,"x1":620,"y0":1502,"y1":1524},"conf":0.4824,"label":"Abandon","label_id":2},{"bbox":{"x0":600,"x1":623,"y0":1501,"y1":1525},"conf":0.4043,"label":"Abandon","label_id":2},{"bbox":{"x0":770,"x1":967,"y0":351,"y1":444},"conf":0.3558,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.3192,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1503],[620,1503],[620,1521],[604,1521]],"score":0.7946},{"poly":[[211,1442],[450,1439],[451,1462],[211,1465]],"score":0.7919},{"poly":[[213,1421],[1011,1421],[1011,1442],[213,1442]],"score":0.8227},{"poly":[[211,1396],[1011,1398],[1011,1421],[211,1419]],"score":0.7515},{"poly":[[211,1374],[1011,1376],[1011,1399],[211,1398]],"score":0.7044},{"poly":[[213,1355],[1011,1355],[1011,1376],[213,1376]],"score":0.7865},{"poly":[[213,1335],[1008,1335],[1008,1351],[213,1351]],"score":0.9877},{"poly":[[213,1310],[1009,1310],[1009,1332],[213,1332]],"score":0.817},{"poly":[[213,1289],[1011,1289],[1011,1310],[213,1310]],"score":0.7927},{"poly":[[211,1267],[1009,1266],[1009,1287],[211,1289]],"score":0.8154},{"poly":[[211,1244],[1009,1242],[1009,1266],[211,1267]],"score":0.7258},{"poly":[[213,1223],[1009,1223],[1009,1244],[213,1244]],"score":0.8065},{"poly":[[211,1190],[645,1188],[645,1211],[211,1213]],"score":0.7137},{"poly":[[211,1165],[1011,1168],[1011,1191],[211,1188]],"score":0.7322},{"poly":[[213,1145],[1011,1145],[1011,1167],[213,1167]],"score":0.7853},{"poly":[[213,1124],[1009,1124],[1009,1145],[213,1145]],"score":0.8237},{"poly":[[213,1101],[1009,1101],[1009,1122],[213,1122]],"score":0.8155},{"poly":[[211,1077],[1009,1079],[1009,1102],[211,1101]],"score":0.6884},{"poly":[[213,1058],[1009,1058],[1009,1079],[213,1079]],"score":0.7992},{"poly":[[213,1035],[1009,1035],[1009,1056],[213,1056]],"score":0.7944},{"poly":[[213,947],[970,947],[970,970],[213,970]],"score":0.7193},{"poly":[[213,926],[1009,926],[1009,947],[213,947]],"score":0.7758},{"poly":[[215,906],[1008,906],[1008,922],[215,922]],"score":0.9602},{"poly":[[211,881],[1011,879],[1011,903],[211,904]],"score":0.6885},{"poly":[[213,860],[1009,860],[1009,881],[213,881]],"score":0.7866},{"poly":[[215,840],[1008,840],[1008,856],[215,856]],"score":0.9879},{"poly":[[211,815],[1009,815],[1009,837],[211,837]],"score":0.7823},{"poly":[[211,794],[1008,794],[1008,815],[211,815]],"score":0.8034},{"poly":[[211,759],[871,757],[871,780],[211,782]],"score":0.7201},{"poly":[[213,738],[1011,738],[1011,759],[213,759]],"score":0.7846},{"poly":[[213,716],[1009,716],[1009,739],[213,739]],"score":0.7031},{"poly":[[213,695],[1011,695],[1011,716],[213,716]],"score":0.8158},{"poly":[[213,672],[1009,672],[1009,693],[213,693]],"score":0.8152},{"poly":[[213,650],[1009,650],[1009,672],[213,672]],"score":0.8321},{"poly":[[211,625],[1010,629],[1009,652],[211,648]],"score":0.6705},{"poly":[[213,606],[1011,606],[1011,627],[213,627]],"score":0.8154},{"poly":[[213,518],[707,518],[707,540],[213,540]],"score":0.8277},{"poly":[[213,495],[1009,495],[1009,516],[213,516]],"score":0.822},{"poly":[[213,417],[960,417],[960,441],[213,441]],"score":0.7219},{"poly":[[213,396],[961,396],[961,417],[213,417]],"score":0.7961},{"poly":[[770,375],[963,375],[963,396],[770,396]],"score":0.822},{"poly":[[213,375],[768,375],[768,396],[213,396]],"score":0.7886},{"poly":[[772,350],[963,353],[963,376],[771,373]],"score":0.724},{"poly":[[220,326],[349,330],[349,353],[219,350]],"score":0.7564},{"poly":[[550,330],[584,330],[584,350],[550,350]],"score":0.926},{"poly":[[672,328],[698,328],[698,351],[672,351]],"score":0.8111},{"poly":[[610,328],[635,328],[635,351],[610,351]],"score":0.9419},{"poly":[[499,328],[526,328],[526,351],[499,351]],"score":0.736},{"poly":[[441,328],[467,328],[467,351],[441,351]],"score":0.8328},{"poly":[[382,326],[410,331],[406,352],[378,347]],"score":0.753},{"poly":[[727,327],[752,327],[752,351],[727,351]],"score":0.7799},{"poly":[[220,308],[354,310],[354,333],[219,332]],"score":0.7735},{"poly":[[672,310],[698,310],[698,333],[672,333]],"score":0.8179},{"poly":[[609,310],[635,310],[635,333],[609,333]],"score":0.812},{"poly":[[549,309],[584,309],[584,335],[549,335]],"score":0.7829},{"poly":[[381,310],[409,310],[409,333],[381,333]],"score":0.8398},{"poly":[[785,305],[842,309],[840,334],[783,329]],"score":0.7925},{"poly":[[725,307],[755,307],[755,335],[725,335]],"score":0.903},{"poly":[[497,304],[528,308],[525,336],[493,332]],"score":0.7671},{"poly":[[437,307],[469,307],[469,335],[437,335]],"score":0.7553},{"poly":[[915,299],[953,299],[953,318],[915,318]],"score":0.8778},{"poly":[[860,297],[901,297],[901,322],[860,322]],"score":0.83},{"poly":[[783,287],[844,291],[842,314],[781,310]],"score":0.8475},{"poly":[[221,282],[363,282],[363,304],[221,304]],"score":0.8729},{"poly":[[727,280],[753,280],[753,304],[727,304]],"score":0.8467},{"poly":[[497,280],[524,280],[524,304],[497,304]],"score":0.8326},{"poly":[[439,280],[467,280],[467,304],[439,304]],"score":0.8167},{"poly":[[670,277],[700,277],[700,305],[670,305]],"score":0.7743},{"poly":[[223,266],[339,266],[339,284],[223,284]],"score":0.8248},{"poly":[[727,262],[753,262],[753,284],[727,284]],"score":0.9302},{"poly":[[497,262],[526,262],[526,285],[497,285]],"score":0.8373},{"poly":[[670,259],[700,259],[700,287],[670,287]],"score":0.7932},{"poly":[[607,257],[640,257],[640,307],[607,307]],"score":0.7732},{"poly":[[549,257],[584,257],[584,307],[549,307]],"score":0.7344},{"poly":[[437,259],[469,259],[469,287],[437,287]],"score":0.7977},{"poly":[[378,257],[412,257],[412,307],[378,307]],"score":0.7114},{"poly":[[861,259],[900,259],[900,279],[861,279]],"score":0.8893},{"poly":[[785,257],[840,257],[840,280],[785,280]],"score":0.955},{"poly":[[915,256],[955,256],[955,280],[915,280]],"score":0.8497},{"poly":[[913,238],[955,238],[955,262],[913,262]],"score":0.8149},{"poly":[[860,238],[901,238],[901,262],[860,262]],"score":0.9002},{"poly":[[800,239],[827,239],[827,261],[800,261]],"score":0.8214},{"poly":[[220,234],[326,234],[326,252],[220,252]],"score":0.6964},{"poly":[[727,233],[753,233],[753,256],[727,256]],"score":0.8376},{"poly":[[670,233],[698,233],[698,256],[670,256]],"score":0.7814},{"poly":[[609,233],[637,233],[637,256],[609,256]],"score":0.8042},{"poly":[[552,233],[580,233],[580,256],[552,256]],"score":0.7982},{"poly":[[497,233],[526,233],[526,254],[497,254]],"score":0.856},{"poly":[[441,233],[467,233],[467,254],[441,254]],"score":0.9402},{"poly":[[381,233],[409,233],[409,254],[381,254]],"score":0.9564},{"poly":[[222,214],[308,218],[307,236],[221,232]],"score":0.769},{"poly":[[915,213],[955,213],[955,233],[915,233]],"score":0.9159},{"poly":[[861,213],[900,213],[900,233],[861,233]],"score":0.9233},{"poly":[[782,211],[843,211],[843,234],[782,234]],"score":0.8129},{"poly":[[727,213],[753,213],[753,236],[727,236]],"score":0.891},{"poly":[[670,213],[698,213],[698,236],[670,236]],"score":0.8349},{"poly":[[609,213],[635,213],[635,236],[609,236]],"score":0.8617},{"poly":[[552,213],[580,213],[580,236],[552,236]],"score":0.8587},{"poly":[[497,213],[524,213],[524,236],[497,236]],"score":0.9061},{"poly":[[441,213],[467,213],[467,236],[441,236]],"score":0.8579},{"poly":[[381,213],[409,213],[409,236],[381,236]],"score":0.8721},{"poly":[[220,191],[310,195],[309,218],[219,214]],"score":0.7607},{"poly":[[793,195],[832,195],[832,214],[793,214]],"score":0.8554},{"poly":[[670,195],[698,195],[698,216],[670,216]],"score":0.8512},{"poly":[[550,195],[580,195],[580,216],[550,216]],"score":0.8612},{"poly":[[915,191],[955,191],[955,216],[915,216]],"score":0.8912},{"poly":[[860,191],[901,191],[901,216],[860,216]],"score":0.8825},{"poly":[[727,193],[753,193],[753,216],[727,216]],"score":0.8122},{"poly":[[610,193],[635,193],[635,216],[610,216]],"score":0.9406},{"poly":[[497,193],[526,193],[526,216],[497,216]],"score":0.7901},{"poly":[[439,193],[467,193],[467,216],[439,216]],"score":0.8183},{"poly":[[381,193],[409,193],[409,216],[381,216]],"score":0.826},{"poly":[[855,168],[903,168],[903,186],[855,186]],"score":0.8529},{"poly":[[659,168],[712,168],[712,186],[659,186]],"score":0.9532},{"poly":[[600,168],[645,168],[645,186],[600,186]],"score":0.9352},{"poly":[[545,168],[589,168],[589,186],[545,186]],"score":0.9104},{"poly":[[491,168],[534,168],[534,186],[491,186]],"score":0.9016},{"poly":[[918,167],[948,167],[948,186],[918,186]],"score":0.8937},{"poly":[[784,165],[857,169],[856,187],[783,183]],"score":0.699},{"poly":[[722,167],[755,167],[755,186],[722,186]],"score":0.8078},{"poly":[[427,165],[481,165],[481,188],[427,188]],"score":0.8115},{"poly":[[373,165],[419,165],[419,190],[373,190]],"score":0.7282},{"poly":[[220,165],[284,165],[284,188],[220,188]],"score":0.8529},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7412}],"page_no":7,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":209,"x1":591,"y0":50,"y1":79},"conf":0.8083,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":233,"x1":991,"y0":1442,"y1":1526},"conf":0.5168,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":604,"x1":620,"y0":1503,"y1":1521},"font_size":0.0,"text":"9"},{"bbox":{"x0":241,"x1":980,"y0":1444,"y1":1465},"font_size":0.0,"text":"6Human feedback can also be incorporated in a complementary manner but we leave it for future work."}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":209,"x1":428,"y0":160,"y1":191},"conf":0.8924,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":424,"y0":165,"y1":188},"font_size":9.477e-27,"text":"5RELATED WORK"}],"source":"layout det","text":"5RELATED WORK"},{"bbox":{"x0":208,"x1":1019,"y0":209,"y1":591},"conf":0.9838,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":211,"y1":236},"font_size":9.477e-27,"text":"Language model for reasoningPerhaps the most well-known work of using LLMs for reasoning"},{"bbox":{"x0":211,"x1":1009,"y0":233,"y1":257},"font_size":9.477e-27,"text":"is Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their"},{"bbox":{"x0":213,"x1":1009,"y0":257,"y1":279},"font_size":9.477e-27,"text":"own “thinking procedure” for problem solving. Several follow-up works have since been performed,"},{"bbox":{"x0":211,"x1":1011,"y0":277,"y1":302},"font_size":9.477e-27,"text":"including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-"},{"bbox":{"x0":213,"x1":1009,"y0":299,"y1":322},"font_size":9.477e-27,"text":"CoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,"},{"bbox":{"x0":211,"x1":1008,"y0":323,"y1":345},"font_size":9.477e-27,"text":"(Madaan $\\&$  Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and"},{"bbox":{"x0":213,"x1":1009,"y0":345,"y1":366},"font_size":9.477e-27,"text":"observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other"},{"bbox":{"x0":211,"x1":1010,"y0":365,"y1":391},"font_size":9.477e-27,"text":"work has also been extended to more sophisticated reasoning architecture beyond simple prompting."},{"bbox":{"x0":213,"x1":1009,"y0":389,"y1":411},"font_size":9.477e-27,"text":"For example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps"},{"bbox":{"x0":213,"x1":1009,"y0":411,"y1":434},"font_size":9.477e-27,"text":"of “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by"},{"bbox":{"x0":213,"x1":1009,"y0":434,"y1":455},"font_size":9.477e-27,"text":"fnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswelli"},{"bbox":{"x0":211,"x1":1008,"y0":454,"y1":475},"font_size":9.477e-27,"text":"& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated"},{"bbox":{"x0":213,"x1":1009,"y0":477,"y1":498},"font_size":9.477e-27,"text":"LM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which fnetunes a LM oni"},{"bbox":{"x0":213,"x1":1011,"y0":498,"y1":520},"font_size":9.477e-27,"text":"intermediate computation steps, also demonstrate improvement on multi-step computation problems."},{"bbox":{"x0":213,"x1":1009,"y0":521,"y1":543},"font_size":9.477e-27,"text":"In contrast to these methods, ReAct performs more than just isolated, fxed reasoning, and integratesi"},{"bbox":{"x0":213,"x1":1009,"y0":543,"y1":564},"font_size":9.477e-27,"text":"model actions and their corresponding observations into a coherent stream of inputs for the model to"},{"bbox":{"x0":211,"x1":953,"y0":563,"y1":587},"font_size":9.477e-27,"text":"reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making)."}],"source":"layout det","text":"Language model for reasoningPerhaps the most well-known work of using LLMs for reasoning is Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their own “thinking procedure” for problem solving. Several follow-up works have since been performed,including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shotCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,(Madaan $\\&$  Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other work has also been extended to more sophisticated reasoning architecture beyond simple prompting.For example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps of “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by fnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswelli& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated LM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which fnetunes a LM oni intermediate computation steps, also demonstrate improvement on multi-step computation problems.In contrast to these methods, ReAct performs more than just isolated, fxed reasoning, and integratesi model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making)."},{"bbox":{"x0":208,"x1":1016,"y0":606,"y1":878},"conf":0.9808,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":609,"y1":632},"font_size":9.477e-27,"text":"Language model for decision makingThe strong capability of LLMs has enabled them to perform"},{"bbox":{"x0":213,"x1":1009,"y0":634,"y1":655},"font_size":9.477e-27,"text":"tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a"},{"bbox":{"x0":213,"x1":1011,"y0":655,"y1":678},"font_size":9.477e-27,"text":"policy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,"},{"bbox":{"x0":213,"x1":1009,"y0":675,"y1":700},"font_size":9.477e-27,"text":"2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to"},{"bbox":{"x0":211,"x1":1008,"y0":698,"y1":719},"font_size":9.477e-27,"text":"complicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not"},{"bbox":{"x0":213,"x1":1009,"y0":721,"y1":742},"font_size":9.477e-27,"text":"explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for"},{"bbox":{"x0":213,"x1":1009,"y0":742,"y1":764},"font_size":9.477e-27,"text":"reinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)"},{"bbox":{"x0":213,"x1":1008,"y0":764,"y1":785},"font_size":9.477e-27,"text":"and Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl"},{"bbox":{"x0":213,"x1":1008,"y0":787,"y1":808},"font_size":9.477e-27,"text":"et al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly"},{"bbox":{"x0":213,"x1":1009,"y0":808,"y1":830},"font_size":9.477e-27,"text":"consider the reasoning procedure either, and also relies on expensive datasets and human feedback"},{"bbox":{"x0":211,"x1":1011,"y0":828,"y1":853},"font_size":9.477e-27,"text":"collections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the"},{"bbox":{"x0":211,"x1":925,"y0":850,"y1":875},"font_size":9.477e-27,"text":"decision making process only requires language description of the reasoning procedure.6"}],"source":"layout det","text":"Language model for decision makingThe strong capability of LLMs has enabled them to perform tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a policy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for reinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)and Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl et al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly consider the reasoning procedure either, and also relies on expensive datasets and human feedback collections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the decision making process only requires language description of the reasoning procedure.6"},{"bbox":{"x0":208,"x1":1016,"y0":883,"y1":1218},"conf":0.9827,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":884,"y1":908},"font_size":9.477e-27,"text":"LLMS have also been increasingly employed in interactive and embodied environments for planning"},{"bbox":{"x0":211,"x1":1009,"y0":906,"y1":931},"font_size":9.477e-27,"text":"and decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)"},{"bbox":{"x0":213,"x1":1009,"y0":931,"y1":952},"font_size":9.477e-27,"text":"and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision"},{"bbox":{"x0":213,"x1":1009,"y0":952,"y1":974},"font_size":9.477e-27,"text":"making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which"},{"bbox":{"x0":213,"x1":1011,"y0":974,"y1":995},"font_size":9.477e-27,"text":"is then reranked by an affordance model grounded on the visual environments for fnal prediction.i"},{"bbox":{"x0":213,"x1":1009,"y0":997,"y1":1018},"font_size":9.477e-27,"text":"Inner Monologue made further improvements by adding the eponymous “inner monologue\", which is"},{"bbox":{"x0":211,"x1":1008,"y0":1016,"y1":1038},"font_size":9.477e-27,"text":"implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the"},{"bbox":{"x0":213,"x1":1009,"y0":1040,"y1":1061},"font_size":9.477e-27,"text":"frst work that demonstrates such a closed-loop system, which ReAct builds on. However, we arguei"},{"bbox":{"x0":211,"x1":1009,"y0":1059,"y1":1084},"font_size":9.477e-27,"text":"that Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We"},{"bbox":{"x0":213,"x1":1009,"y0":1084,"y1":1106},"font_size":9.477e-27,"text":"also note that leveraging language as semantically-rich inputs in the process of interactive decision"},{"bbox":{"x0":215,"x1":1009,"y0":1105,"y1":1127},"font_size":9.477e-27,"text":"making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti"},{"bbox":{"x0":213,"x1":1009,"y0":1127,"y1":1150},"font_size":9.477e-27,"text":"et al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of"},{"bbox":{"x0":213,"x1":1009,"y0":1150,"y1":1172},"font_size":9.477e-27,"text":"LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and"},{"bbox":{"x0":215,"x1":1009,"y0":1171,"y1":1195},"font_size":9.477e-27,"text":"decision making. What is more, progress in LLMs has also inspired the development of versatile and"},{"bbox":{"x0":209,"x1":539,"y0":1190,"y1":1219},"font_size":9.477e-27,"text":"generalist agents like Reed et al. (2022)."}],"source":"layout det","text":"LLMS have also been increasingly employed in interactive and embodied environments for planning and decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which is then reranked by an affordance model grounded on the visual environments for fnal prediction.i Inner Monologue made further improvements by adding the eponymous “inner monologue\", which is implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the frst work that demonstrates such a closed-loop system, which ReAct builds on. However, we arguei that Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We also note that leveraging language as semantically-rich inputs in the process of interactive decision making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti et al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and decision making. What is more, progress in LLMs has also inspired the development of versatile and generalist agents like Reed et al. (2022)."},{"bbox":{"x0":209,"x1":397,"y0":1244,"y1":1274},"conf":0.8921,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":392,"y0":1247,"y1":1270},"font_size":9.477e-27,"text":"6CONCLUSION"}],"source":"layout det","text":"6CONCLUSION"},{"bbox":{"x0":208,"x1":1015,"y0":1294,"y1":1433},"conf":0.9662,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":1297,"y1":1320},"font_size":9.477e-27,"text":"We have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in"},{"bbox":{"x0":215,"x1":1009,"y0":1318,"y1":1341},"font_size":9.477e-27,"text":"large language models. Through a diverse set of experiments on multi-hop question-answering, fact"},{"bbox":{"x0":213,"x1":1008,"y0":1341,"y1":1363},"font_size":9.477e-27,"text":"checking, and interactive decision-making tasks, we show that ReAct leads to superior performance"},{"bbox":{"x0":211,"x1":1006,"y0":1363,"y1":1384},"font_size":9.477e-27,"text":"with interpretable decision traces. Despite the simplicity of our method, complex tasks with large"},{"bbox":{"x0":213,"x1":1009,"y0":1384,"y1":1407},"font_size":9.477e-27,"text":"action spaces require more demonstrations to learn well, which unfortunately can easily go beyond"},{"bbox":{"x0":213,"x1":1008,"y0":1407,"y1":1431},"font_size":9.477e-27,"text":"the input length limit of in-context learning. We explore the fne-tuning approach on HotpotQAi"}],"source":"layout det","text":"We have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experiments on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. Despite the simplicity of our method, complex tasks with large action spaces require more demonstrations to learn well, which unfortunately can easily go beyond the input length limit of in-context learning. We explore the fne-tuning approach on HotpotQAi"}],"formula_dets":[{"bbox":{"x0":290,"x1":306,"y0":326,"y1":341},"conf":0.5829,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":208,"x1":1019,"y0":209,"y1":591},"conf":0.9838,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":883,"y1":1218},"conf":0.9827,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":606,"y1":878},"conf":0.9808,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1015,"y0":1294,"y1":1433},"conf":0.9662,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":428,"y0":160,"y1":191},"conf":0.8924,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":397,"y0":1244,"y1":1274},"conf":0.8921,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":591,"y0":50,"y1":79},"conf":0.8083,"label":"Abandon","label_id":2},{"bbox":{"x0":233,"x1":991,"y0":1442,"y1":1526},"conf":0.5168,"label":"Abandon","label_id":2},{"bbox":{"x0":235,"x1":987,"y0":1441,"y1":1469},"conf":0.3183,"label":"Abandon","label_id":2},{"bbox":{"x0":602,"x1":620,"y0":1502,"y1":1523},"conf":0.275,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[604,1503],[620,1503],[620,1521],[604,1521]],"score":0.761},{"poly":[[241,1444],[980,1444],[980,1465],[241,1465]],"score":0.8399},{"poly":[[213,1407],[1008,1407],[1008,1431],[213,1431]],"score":0.728},{"poly":[[213,1384],[1009,1384],[1009,1407],[213,1407]],"score":0.6979},{"poly":[[211,1363],[1006,1363],[1006,1384],[211,1384]],"score":0.8012},{"poly":[[213,1341],[1008,1341],[1008,1363],[213,1363]],"score":0.8175},{"poly":[[215,1318],[1009,1318],[1009,1341],[215,1341]],"score":0.6847},{"poly":[[213,1297],[1008,1297],[1008,1320],[213,1320]],"score":0.6962},{"poly":[[213,1247],[392,1247],[392,1270],[213,1270]],"score":0.7877},{"poly":[[209,1191],[539,1190],[539,1218],[210,1219]],"score":0.6743},{"poly":[[215,1171],[1009,1171],[1009,1195],[215,1195]],"score":0.6883},{"poly":[[213,1150],[1009,1150],[1009,1172],[213,1172]],"score":0.8063},{"poly":[[213,1127],[1009,1127],[1009,1150],[213,1150]],"score":0.7213},{"poly":[[215,1105],[1009,1105],[1009,1127],[215,1127]],"score":0.8051},{"poly":[[213,1084],[1009,1084],[1009,1106],[213,1106]],"score":0.7689},{"poly":[[211,1061],[1009,1059],[1009,1082],[211,1084]],"score":0.7473},{"poly":[[213,1040],[1009,1040],[1009,1061],[213,1061]],"score":0.8166},{"poly":[[211,1016],[1008,1016],[1008,1038],[211,1038]],"score":0.8009},{"poly":[[213,997],[1009,997],[1009,1018],[213,1018]],"score":0.8282},{"poly":[[213,974],[1011,974],[1011,995],[213,995]],"score":0.8121},{"poly":[[213,952],[1009,952],[1009,974],[213,974]],"score":0.7922},{"poly":[[213,931],[1009,931],[1009,952],[213,952]],"score":0.7994},{"poly":[[211,908],[1009,906],[1009,929],[211,931]],"score":0.7573},{"poly":[[211,884],[1008,884],[1008,908],[211,908]],"score":0.6846},{"poly":[[211,851],[925,850],[925,873],[211,875]],"score":0.7314},{"poly":[[211,828],[1011,830],[1011,853],[211,851]],"score":0.7626},{"poly":[[213,808],[1009,808],[1009,830],[213,830]],"score":0.8118},{"poly":[[213,787],[1008,787],[1008,808],[213,808]],"score":0.8272},{"poly":[[213,764],[1008,764],[1008,785],[213,785]],"score":0.8051},{"poly":[[213,742],[1009,742],[1009,764],[213,764]],"score":0.796},{"poly":[[213,721],[1009,721],[1009,742],[213,742]],"score":0.815},{"poly":[[211,698],[1008,698],[1008,719],[211,719]],"score":0.8053},{"poly":[[213,675],[1009,677],[1009,700],[213,698]],"score":0.7468},{"poly":[[213,655],[1011,655],[1011,678],[213,678]],"score":0.6707},{"poly":[[213,634],[1009,634],[1009,655],[213,655]],"score":0.8001},{"poly":[[211,609],[1008,609],[1008,632],[211,632]],"score":0.6847},{"poly":[[211,563],[953,564],[953,587],[211,586]],"score":0.6978},{"poly":[[213,543],[1009,543],[1009,564],[213,564]],"score":0.746},{"poly":[[213,521],[1009,521],[1009,543],[213,543]],"score":0.8083},{"poly":[[213,498],[1011,498],[1011,520],[213,520]],"score":0.7827},{"poly":[[213,477],[1009,477],[1009,498],[213,498]],"score":0.7852},{"poly":[[211,454],[1008,454],[1008,475],[211,475]],"score":0.7643},{"poly":[[213,434],[1009,434],[1009,455],[213,455]],"score":0.8265},{"poly":[[213,411],[1009,411],[1009,434],[213,434]],"score":0.7039},{"poly":[[213,389],[1009,389],[1009,411],[213,411]],"score":0.8121},{"poly":[[211,365],[1010,368],[1009,391],[211,388]],"score":0.7153},{"poly":[[213,345],[1009,345],[1009,366],[213,366]],"score":0.798},{"poly":[[211,323],[1008,323],[1008,345],[211,345]],"score":0.8007},{"poly":[[213,299],[1009,299],[1009,322],[213,322]],"score":0.7034},{"poly":[[211,279],[1011,277],[1011,300],[211,302]],"score":0.7181},{"poly":[[213,257],[1009,257],[1009,279],[213,279]],"score":0.8062},{"poly":[[211,234],[1009,233],[1009,256],[211,257]],"score":0.7257},{"poly":[[211,211],[1009,213],[1009,236],[211,234]],"score":0.7376},{"poly":[[213,165],[424,165],[424,188],[213,188]],"score":0.7301},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7547}],"page_no":8,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1526},"conf":0.687,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1502,"y1":1523},"font_size":0.0,"text":"10"}],"source":"layout det","text":""},{"bbox":{"x0":208,"x1":1016,"y0":47,"y1":85},"conf":0.234,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":207,"x1":1016,"y0":163,"y1":260},"conf":0.9548,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":165,"y1":190},"font_size":1.169e-19,"text":"with initial promising results, but learning from more high-quality human annotations will be the"},{"bbox":{"x0":215,"x1":1008,"y0":190,"y1":213},"font_size":1.169e-19,"text":"desiderata to further improve the performance. Scaling up ReAct with multi-task training and"},{"bbox":{"x0":211,"x1":1008,"y0":210,"y1":234},"font_size":1.169e-19,"text":"combining it with complementary paradigms like reinforcement learning could result in stronger"},{"bbox":{"x0":213,"x1":780,"y0":231,"y1":256},"font_size":1.169e-19,"text":"agents that further unlock the potential of LLMs for more applications."}],"source":"layout det","text":"with initial promising results, but learning from more high-quality human annotations will be the desiderata to further improve the performance. Scaling up ReAct with multi-task training and combining it with complementary paradigms like reinforcement learning could result in stronger agents that further unlock the potential of LLMs for more applications."},{"bbox":{"x0":209,"x1":407,"y0":281,"y1":308},"conf":0.804,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":401,"y0":284,"y1":305},"font_size":1.169e-19,"text":"ACKNOWLEDGMENTS"}],"source":"layout det","text":"ACKNOWLEDGMENTS"},{"bbox":{"x0":208,"x1":1016,"y0":319,"y1":416},"conf":0.9478,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":322,"y1":347},"font_size":1.169e-19,"text":"We thank the support and feedback of many people from Google Brain team and Princeton NLP"},{"bbox":{"x0":215,"x1":1011,"y0":346,"y1":368},"font_size":1.169e-19,"text":"Group. This work was supported in part by the National Science Foundation under Grant No."},{"bbox":{"x0":215,"x1":1008,"y0":368,"y1":389},"font_size":1.169e-19,"text":"2107048. Any opinions, fndings, and conclusions or recommendations expressed in this material arei"},{"bbox":{"x0":213,"x1":994,"y0":389,"y1":411},"font_size":1.169e-19,"text":"those of the author(s) and do not necessarily refect the views of the National Science Foundation.l"}],"source":"layout det","text":"We thank the support and feedback of many people from Google Brain team and Princeton NLP Group. This work was supported in part by the National Science Foundation under Grant No.2107048. Any opinions, fndings, and conclusions or recommendations expressed in this material arei those of the author(s) and do not necessarily refect the views of the National Science Foundation.l"},{"bbox":{"x0":209,"x1":489,"y0":438,"y1":465},"conf":0.8094,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":484,"y0":444,"y1":460},"font_size":1.169e-19,"text":"REPRODUCIBILITY STATEMENT"}],"source":"layout det","text":"REPRODUCIBILITY STATEMENT"},{"bbox":{"x0":208,"x1":1015,"y0":477,"y1":573},"conf":0.9537,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":482,"y1":503},"font_size":1.169e-19,"text":"Our main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible"},{"bbox":{"x0":213,"x1":1008,"y0":503,"y1":526},"font_size":1.169e-19,"text":"model yet. To increase reproducibility, we have included all used prompts in Appendix C, additional"},{"bbox":{"x0":213,"x1":1006,"y0":525,"y1":546},"font_size":1.169e-19,"text":"experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct"},{"bbox":{"x0":211,"x1":910,"y0":544,"y1":571},"font_size":1.169e-19,"text":"prompting code at https://anonymous.4open.science/r/ReAct-2268/."}],"source":"layout det","text":"Our main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible model yet. To increase reproducibility, we have included all used prompts in Appendix C, additional experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct prompting code at https://anonymous.4open.science/r/ReAct-2268/."},{"bbox":{"x0":209,"x1":394,"y0":595,"y1":622},"conf":0.8746,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":386,"y0":599,"y1":615},"font_size":1.169e-19,"text":"ETHICS STATEMENT"}],"source":"layout det","text":"ETHICS STATEMENT"},{"bbox":{"x0":208,"x1":1017,"y0":634,"y1":838},"conf":0.9746,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":639,"y1":660},"font_size":1.169e-19,"text":"ReAct prompts large language models to generate more human interpretable, diagnosable, and"},{"bbox":{"x0":211,"x1":1009,"y0":658,"y1":683},"font_size":1.169e-19,"text":"controllable task-solving trajectories than previous methods. However, hooking up a large language"},{"bbox":{"x0":213,"x1":1008,"y0":681,"y1":703},"font_size":1.169e-19,"text":"model with an action space to interact with external environments (e.g. the web, physical environ-"},{"bbox":{"x0":213,"x1":1008,"y0":705,"y1":726},"font_size":1.169e-19,"text":"ments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful"},{"bbox":{"x0":213,"x1":1009,"y0":726,"y1":747},"font_size":1.169e-19,"text":"actions in an environment. Our experiments minimize such risks by limiting the interactions to"},{"bbox":{"x0":213,"x1":1009,"y0":747,"y1":771},"font_size":1.169e-19,"text":"specifc websites (Wikipedia or WebShop) that are free of private information, without any dangerousi"},{"bbox":{"x0":211,"x1":1009,"y0":767,"y1":792},"font_size":1.169e-19,"text":"actions in the action space design (i.e. models cannot really buy products on WebShop the research"},{"bbox":{"x0":211,"x1":1010,"y0":789,"y1":815},"font_size":1.169e-19,"text":"benchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing"},{"bbox":{"x0":213,"x1":544,"y0":813,"y1":835},"font_size":1.169e-19,"text":"more extensive experiments in the future."}],"source":"layout det","text":"ReAct prompts large language models to generate more human interpretable, diagnosable, and controllable task-solving trajectories than previous methods. However, hooking up a large language model with an action space to interact with external environments (e.g. the web, physical environments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful actions in an environment. Our experiments minimize such risks by limiting the interactions to specifc websites (Wikipedia or WebShop) that are free of private information, without any dangerousi actions in the action space design (i.e. models cannot really buy products on WebShop the research benchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing more extensive experiments in the future."},{"bbox":{"x0":209,"x1":357,"y0":870,"y1":899},"conf":0.8451,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":353,"y0":874,"y1":896},"font_size":1.169e-19,"text":"REFERENCES"}],"source":"layout det","text":"REFERENCES"},{"bbox":{"x0":208,"x1":1018,"y0":909,"y1":1049},"conf":0.9712,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":912,"y1":934},"font_size":1.169e-19,"text":"Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita"},{"bbox":{"x0":233,"x1":1008,"y0":932,"y1":955},"font_size":1.169e-19,"text":"Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim"},{"bbox":{"x0":233,"x1":1006,"y0":955,"y1":977},"font_size":1.169e-19,"text":"Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew"},{"bbox":{"x0":233,"x1":1009,"y0":978,"y1":1000},"font_size":1.169e-19,"text":"son, Sonˇa Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,"},{"bbox":{"x0":234,"x1":1011,"y0":1000,"y1":1023},"font_size":1.169e-19,"text":"Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,"},{"bbox":{"x0":233,"x1":720,"y0":1021,"y1":1043},"font_size":1.169e-19,"text":"2020. URL https://arxiv.org/abs/2012.05672."}],"source":"layout det","text":"Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew son, Sonˇa Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,2020. URL https://arxiv.org/abs/2012.05672."},{"bbox":{"x0":208,"x1":1017,"y0":1061,"y1":1269},"conf":0.9755,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":1066,"y1":1087},"font_size":1.169e-19,"text":"Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea"},{"bbox":{"x0":231,"x1":1009,"y0":1087,"y1":1112},"font_size":1.169e-19,"text":"Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine"},{"bbox":{"x0":233,"x1":1008,"y0":1112,"y1":1134},"font_size":1.169e-19,"text":"Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally"},{"bbox":{"x0":233,"x1":1009,"y0":1134,"y1":1155},"font_size":1.169e-19,"text":"Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,"},{"bbox":{"x0":231,"x1":1009,"y0":1153,"y1":1178},"font_size":1.169e-19,"text":"Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka"},{"bbox":{"x0":233,"x1":1009,"y0":1178,"y1":1200},"font_size":1.169e-19,"text":"Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander"},{"bbox":{"x0":233,"x1":1009,"y0":1196,"y1":1221},"font_size":1.169e-19,"text":"Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and"},{"bbox":{"x0":233,"x1":1009,"y0":1221,"y1":1242},"font_size":1.169e-19,"text":"Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL"},{"bbox":{"x0":234,"x1":624,"y0":1246,"y1":1262},"font_size":1.169e-19,"text":"https://arxiv.org/abs/2204.01691."}],"source":"layout det","text":"Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/2204.01691."},{"bbox":{"x0":207,"x1":1018,"y0":1282,"y1":1336},"conf":0.9169,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":1285,"y1":1310},"font_size":1.169e-19,"text":"Ben Alderson-Day and Charles Fernyhough.Inner speech: development, cognitive functions,"},{"bbox":{"x0":234,"x1":853,"y0":1310,"y1":1332},"font_size":1.169e-19,"text":"phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015."}],"source":"layout det","text":"Ben Alderson-Day and Charles Fernyhough.Inner speech: development, cognitive functions,phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015."},{"bbox":{"x0":207,"x1":787,"y0":1349,"y1":1380},"conf":0.879,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":780,"y0":1355,"y1":1376},"font_size":1.169e-19,"text":"Alan Baddeley. Working memory. Science, 255(5044):556–559, 1992."}],"source":"layout det","text":"Alan Baddeley. Working memory. Science, 255(5044):556–559, 1992."},{"bbox":{"x0":208,"x1":1017,"y0":1395,"y1":1471},"conf":0.9162,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":1399,"y1":1421},"font_size":1.169e-19,"text":"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,"},{"bbox":{"x0":234,"x1":1009,"y0":1422,"y1":1444},"font_size":1.169e-19,"text":"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are"},{"bbox":{"x0":234,"x1":973,"y0":1444,"y1":1465},"font_size":1.169e-19,"text":"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020."}],"source":"layout det","text":"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":208,"x1":1017,"y0":1061,"y1":1269},"conf":0.9755,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":634,"y1":838},"conf":0.9746,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1018,"y0":909,"y1":1049},"conf":0.9712,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":163,"y1":260},"conf":0.9548,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1015,"y0":477,"y1":573},"conf":0.9537,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":319,"y1":416},"conf":0.9478,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1018,"y0":1282,"y1":1336},"conf":0.9169,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":1395,"y1":1471},"conf":0.9162,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":787,"y0":1349,"y1":1380},"conf":0.879,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":394,"y0":595,"y1":622},"conf":0.8746,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":357,"y0":870,"y1":899},"conf":0.8451,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":489,"y0":438,"y1":465},"conf":0.8094,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":407,"y0":281,"y1":308},"conf":0.804,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":592,"y0":50,"y1":80},"conf":0.8037,"label":"Abandon","label_id":2},{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1526},"conf":0.687,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1016,"y0":47,"y1":85},"conf":0.234,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[625,1502],[625,1523],[599,1523]],"score":0.7983},{"poly":[[234,1444],[973,1444],[973,1465],[234,1465]],"score":0.8329},{"poly":[[234,1422],[1009,1422],[1009,1444],[234,1444]],"score":0.8206},{"poly":[[215,1399],[1009,1399],[1009,1421],[215,1421]],"score":0.8262},{"poly":[[213,1355],[780,1355],[780,1376],[213,1376]],"score":0.8177},{"poly":[[234,1310],[853,1310],[853,1332],[234,1332]],"score":0.8157},{"poly":[[213,1285],[1011,1287],[1011,1310],[213,1308]],"score":0.7554},{"poly":[[234,1246],[624,1246],[624,1262],[234,1262]],"score":0.9486},{"poly":[[233,1221],[1009,1221],[1009,1242],[233,1242]],"score":0.7895},{"poly":[[233,1198],[1009,1196],[1009,1219],[233,1221]],"score":0.6996},{"poly":[[233,1178],[1009,1178],[1009,1200],[233,1200]],"score":0.8131},{"poly":[[231,1155],[1009,1153],[1009,1176],[231,1178]],"score":0.7104},{"poly":[[233,1134],[1009,1134],[1009,1155],[233,1155]],"score":0.7945},{"poly":[[233,1112],[1008,1112],[1008,1134],[233,1134]],"score":0.7906},{"poly":[[231,1087],[1009,1089],[1009,1112],[231,1110]],"score":0.719},{"poly":[[213,1066],[1008,1066],[1008,1087],[213,1087]],"score":0.8212},{"poly":[[233,1021],[720,1021],[720,1043],[233,1043]],"score":0.7366},{"poly":[[234,1000],[1011,1000],[1011,1023],[234,1023]],"score":0.7171},{"poly":[[233,978],[1009,978],[1009,1000],[233,1000]],"score":0.7693},{"poly":[[233,955],[1006,955],[1006,977],[233,977]],"score":0.7622},{"poly":[[233,932],[1008,932],[1008,955],[233,955]],"score":0.6988},{"poly":[[213,912],[1009,912],[1009,934],[213,934]],"score":0.8245},{"poly":[[213,874],[353,874],[353,896],[213,896]],"score":0.8452},{"poly":[[213,813],[544,813],[544,835],[213,835]],"score":0.8187},{"poly":[[211,789],[1010,792],[1009,815],[211,812]],"score":0.7352},{"poly":[[211,769],[1009,767],[1009,790],[211,792]],"score":0.753},{"poly":[[213,747],[1009,747],[1009,771],[213,771]],"score":0.664},{"poly":[[213,726],[1009,726],[1009,747],[213,747]],"score":0.7796},{"poly":[[213,705],[1008,705],[1008,726],[213,726]],"score":0.8022},{"poly":[[213,681],[1008,681],[1008,703],[213,703]],"score":0.753},{"poly":[[211,658],[1009,660],[1009,683],[211,681]],"score":0.7424},{"poly":[[213,639],[1009,639],[1009,660],[213,660]],"score":0.8042},{"poly":[[215,599],[386,599],[386,615],[215,615]],"score":0.7898},{"poly":[[211,548],[910,544],[910,568],[211,571]],"score":0.7288},{"poly":[[213,525],[1006,525],[1006,546],[213,546]],"score":0.7614},{"poly":[[213,503],[1008,503],[1008,526],[213,526]],"score":0.6481},{"poly":[[213,482],[1008,482],[1008,503],[213,503]],"score":0.8},{"poly":[[215,444],[484,444],[484,460],[215,460]],"score":0.9902},{"poly":[[213,389],[994,389],[994,411],[213,411]],"score":0.7733},{"poly":[[215,368],[1008,368],[1008,389],[215,389]],"score":0.7952},{"poly":[[215,346],[1011,346],[1011,368],[215,368]],"score":0.7865},{"poly":[[211,322],[1008,323],[1008,347],[211,345]],"score":0.7548},{"poly":[[213,284],[401,284],[401,305],[213,305]],"score":0.8243},{"poly":[[213,233],[780,231],[780,254],[213,256]],"score":0.7328},{"poly":[[211,210],[1008,211],[1008,234],[211,233]],"score":0.7298},{"poly":[[215,190],[1008,190],[1008,213],[215,213]],"score":0.6958},{"poly":[[211,165],[1009,167],[1009,190],[211,188]],"score":0.7281},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7428}],"page_no":9,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":626,"y0":1499,"y1":1527},"conf":0.6604,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":622,"y0":1503,"y1":1523},"font_size":0.0,"text":"11"}],"source":"layout det","text":""},{"bbox":{"x0":208,"x1":1016,"y0":46,"y1":85},"conf":0.3639,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":208,"x1":1016,"y0":162,"y1":238},"conf":0.9496,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":168,"y1":190},"font_size":9.0,"text":"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam"},{"bbox":{"x0":233,"x1":1009,"y0":190,"y1":211},"font_size":9.0,"text":"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:"},{"bbox":{"x0":234,"x1":903,"y0":211,"y1":233},"font_size":9.0,"text":"Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022."}],"source":"layout det","text":"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022."},{"bbox":{"x0":208,"x1":1017,"y0":244,"y1":299},"conf":0.9354,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":251,"y1":272},"font_size":9.0,"text":"Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL"},{"bbox":{"x0":231,"x1":624,"y0":272,"y1":294},"font_size":9.0,"text":"https://arxiv.org/abs/2208.14271."}],"source":"layout det","text":"Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL https://arxiv.org/abs/2208.14271."},{"bbox":{"x0":208,"x1":1016,"y0":306,"y1":381},"conf":0.9433,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":310,"y1":335},"font_size":9.0,"text":"Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large"},{"bbox":{"x0":233,"x1":1006,"y0":333,"y1":356},"font_size":9.0,"text":"language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/"},{"bbox":{"x0":236,"x1":363,"y0":358,"y1":375},"font_size":9.0,"text":"2205.09712."}],"source":"layout det","text":"Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/2205.09712."},{"bbox":{"x0":207,"x1":1017,"y0":389,"y1":508},"conf":0.9532,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":394,"y1":416},"font_size":-586800000000000.0,"text":"Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:"},{"bbox":{"x0":233,"x1":1009,"y0":417,"y1":439},"font_size":-586800000000000.0,"text":"Long form question answering. In Proceedings of the 57th Annual Meeting of the Association"},{"bbox":{"x0":233,"x1":1009,"y0":439,"y1":462},"font_size":-586800000000000.0,"text":"for Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Com-"},{"bbox":{"x0":235,"x1":1010,"y0":462,"y1":480},"font_size":9.0,"text":"putational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/"},{"bbox":{"x0":233,"x1":339,"y0":482,"y1":500},"font_size":-586800000000000.0,"text":"P19-1346."}],"source":"layout det","text":"Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/P19-1346."},{"bbox":{"x0":208,"x1":1016,"y0":516,"y1":570},"conf":0.9327,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":521,"y1":543},"font_size":9.0,"text":"Charles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social"},{"bbox":{"x0":234,"x1":1011,"y0":543,"y1":566},"font_size":9.0,"text":"interaction and the development of social understanding and executive functions, pp. 56–79, 2010."}],"source":"layout det","text":"Charles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social interaction and the development of social understanding and executive functions, pp. 56–79, 2010."},{"bbox":{"x0":207,"x1":1018,"y0":577,"y1":784},"conf":0.9735,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":582,"y1":604},"font_size":-586800000000000.0,"text":"Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-"},{"bbox":{"x0":233,"x1":1009,"y0":604,"y1":625},"font_size":-586800000000000.0,"text":"beth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,"},{"bbox":{"x0":233,"x1":1009,"y0":625,"y1":648},"font_size":-586800000000000.0,"text":"Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth"},{"bbox":{"x0":233,"x1":1011,"y0":648,"y1":670},"font_size":-586800000000000.0,"text":"Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,"},{"bbox":{"x0":233,"x1":1009,"y0":670,"y1":691},"font_size":-586800000000000.0,"text":"Sonˇa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,"},{"bbox":{"x0":233,"x1":1009,"y0":691,"y1":713},"font_size":-586800000000000.0,"text":"William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and"},{"bbox":{"x0":231,"x1":1011,"y0":711,"y1":736},"font_size":-586800000000000.0,"text":"Geoffrey Irving.Improving alignment of dialogue agents via targeted human judgements,"},{"bbox":{"x0":231,"x1":1013,"y0":732,"y1":759},"font_size":-586800000000000.0,"text":"2022.URL https://storage.googleapis.com/deepmind-media/DeepMind."},{"bbox":{"x0":234,"x1":757,"y0":759,"y1":780},"font_size":-586800000000000.0,"text":"com/Authors-Notes/sparrow/sparrow-final.pdf."}],"source":"layout det","text":"Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,Sonˇa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving.Improving alignment of dialogue agents via targeted human judgements,2022.URL https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf."},{"bbox":{"x0":207,"x1":1018,"y0":790,"y1":866},"conf":0.9471,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":797,"y1":818},"font_size":9.0,"text":"Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple"},{"bbox":{"x0":233,"x1":1008,"y0":818,"y1":842},"font_size":9.0,"text":"language model for task-oriented dialogue. Advances in Neural Information Processing Systems,"},{"bbox":{"x0":234,"x1":426,"y0":840,"y1":861},"font_size":9.0,"text":"33:20179–20191, 2020."}],"source":"layout det","text":"Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems,33:20179–20191, 2020."},{"bbox":{"x0":207,"x1":1017,"y0":873,"y1":950},"conf":0.9455,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":879,"y1":903},"font_size":9.0,"text":"Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot"},{"bbox":{"x0":233,"x1":1009,"y0":901,"y1":926},"font_size":9.0,"text":"planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,"},{"bbox":{"x0":231,"x1":292,"y0":919,"y1":948},"font_size":9.0,"text":"2022a."}],"source":"layout det","text":"Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,2022a."},{"bbox":{"x0":207,"x1":1016,"y0":957,"y1":1033},"conf":0.9536,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":216,"x1":1008,"y0":962,"y1":985},"font_size":-586800000000000.0,"text":"Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan"},{"bbox":{"x0":234,"x1":1009,"y0":985,"y1":1006},"font_size":-586800000000000.0,"text":"Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through"},{"bbox":{"x0":233,"x1":825,"y0":1005,"y1":1030},"font_size":-586800000000000.0,"text":"planning with language models. arXiv preprint arXiv:2207.05608, 2022b."}],"source":"layout det","text":"Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b."},{"bbox":{"x0":208,"x1":1016,"y0":1039,"y1":1114},"conf":0.9538,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":1046,"y1":1068},"font_size":-586800000000000.0,"text":"Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed"},{"bbox":{"x0":233,"x1":1004,"y0":1066,"y1":1089},"font_size":-586800000000000.0,"text":"latent actions. In CoRL, pp. 1379–1390, 2021. URL https://proceedings.mlr.press/"},{"bbox":{"x0":234,"x1":519,"y0":1092,"y1":1109},"font_size":-586800000000000.0,"text":"v164/karamcheti22a.html."}],"source":"layout det","text":"Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed latent actions. In CoRL, pp. 1379–1390, 2021. URL https://proceedings.mlr.press/v164/karamcheti22a.html."},{"bbox":{"x0":207,"x1":1016,"y0":1123,"y1":1177},"conf":0.939,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":1127,"y1":1152},"font_size":9.0,"text":"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large"},{"bbox":{"x0":231,"x1":886,"y0":1150,"y1":1172},"font_size":9.0,"text":"language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022"}],"source":"layout det","text":"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022"},{"bbox":{"x0":207,"x1":1017,"y0":1183,"y1":1259},"conf":0.9489,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":1190,"y1":1211},"font_size":9.0,"text":"Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-"},{"bbox":{"x0":229,"x1":1013,"y0":1208,"y1":1238},"font_size":9.0,"text":"augmented language models through few-shot prompting for open-domain question answering."},{"bbox":{"x0":234,"x1":555,"y0":1234,"y1":1256},"font_size":9.0,"text":"arXiv preprint arXiv:2203.05115, 2022."}],"source":"layout det","text":"Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering.arXiv preprint arXiv:2203.05115, 2022."},{"bbox":{"x0":207,"x1":1018,"y0":1267,"y1":1363},"conf":0.9562,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":1272,"y1":1294},"font_size":-586800000000000.0,"text":"Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,"},{"bbox":{"x0":233,"x1":1009,"y0":1295,"y1":1317},"font_size":-586800000000000.0,"text":"Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-"},{"bbox":{"x0":234,"x1":1011,"y0":1317,"y1":1338},"font_size":-586800000000000.0,"text":"tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:"},{"bbox":{"x0":234,"x1":379,"y0":1340,"y1":1356},"font_size":-586800000000000.0,"text":"9459–9474, 2020."}],"source":"layout det","text":"Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020."},{"bbox":{"x0":207,"x1":1018,"y0":1371,"y1":1470},"conf":0.9452,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":1378,"y1":1399},"font_size":9.0,"text":"Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An"},{"bbox":{"x0":233,"x1":1011,"y0":1399,"y1":1421},"font_size":9.0,"text":"Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,"},{"bbox":{"x0":231,"x1":1013,"y0":1419,"y1":1445},"font_size":9.0,"text":"and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:"},{"bbox":{"x0":233,"x1":550,"y0":1444,"y1":1465},"font_size":9.0,"text":"//arxiv.org/abs/2202.01771."}],"source":"layout det","text":"Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https://arxiv.org/abs/2202.01771."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":207,"x1":1018,"y0":577,"y1":784},"conf":0.9735,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1018,"y0":1267,"y1":1363},"conf":0.9562,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":1039,"y1":1114},"conf":0.9538,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":957,"y1":1033},"conf":0.9536,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1017,"y0":389,"y1":508},"conf":0.9532,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":162,"y1":238},"conf":0.9496,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1017,"y0":1183,"y1":1259},"conf":0.9489,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1018,"y0":790,"y1":866},"conf":0.9471,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1017,"y0":873,"y1":950},"conf":0.9455,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1018,"y0":1371,"y1":1470},"conf":0.9452,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":306,"y1":381},"conf":0.9433,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1123,"y1":1177},"conf":0.939,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":244,"y1":299},"conf":0.9354,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":516,"y1":570},"conf":0.9327,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":592,"y0":49,"y1":81},"conf":0.7694,"label":"Abandon","label_id":2},{"bbox":{"x0":595,"x1":626,"y0":1499,"y1":1527},"conf":0.6604,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1016,"y0":46,"y1":85},"conf":0.3639,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1503],[622,1503],[622,1523],[599,1523]],"score":0.9066},{"poly":[[233,1444],[550,1444],[550,1465],[233,1465]],"score":0.8336},{"poly":[[231,1419],[1013,1422],[1013,1445],[231,1442]],"score":0.7036},{"poly":[[233,1399],[1011,1399],[1011,1421],[233,1421]],"score":0.8174},{"poly":[[215,1378],[1009,1378],[1009,1399],[215,1399]],"score":0.8193},{"poly":[[234,1340],[379,1340],[379,1356],[234,1356]],"score":0.9841},{"poly":[[234,1317],[1011,1317],[1011,1338],[234,1338]],"score":0.7943},{"poly":[[233,1295],[1009,1295],[1009,1317],[233,1317]],"score":0.8106},{"poly":[[215,1272],[1009,1272],[1009,1294],[215,1294]],"score":0.7668},{"poly":[[234,1234],[555,1234],[555,1256],[234,1256]],"score":0.8243},{"poly":[[230,1208],[1013,1209],[1013,1238],[229,1236]],"score":0.6608},{"poly":[[213,1190],[1011,1190],[1011,1211],[213,1211]],"score":0.7851},{"poly":[[231,1150],[886,1150],[886,1172],[231,1172]],"score":0.7918},{"poly":[[213,1127],[1008,1129],[1008,1152],[213,1150]],"score":0.7499},{"poly":[[234,1092],[519,1092],[519,1109],[234,1109]],"score":0.9732},{"poly":[[233,1066],[1004,1066],[1004,1089],[233,1089]],"score":0.6736},{"poly":[[215,1046],[1009,1046],[1009,1068],[215,1068]],"score":0.8112},{"poly":[[233,1007],[825,1005],[825,1028],[233,1030]],"score":0.7296},{"poly":[[234,985],[1009,985],[1009,1006],[234,1006]],"score":0.8065},{"poly":[[216,962],[1008,962],[1008,985],[216,985]],"score":0.7124},{"poly":[[232,919],[292,923],[290,948],[231,943]],"score":0.7523},{"poly":[[233,903],[1009,901],[1009,924],[233,926]],"score":0.7699},{"poly":[[215,879],[1009,879],[1009,903],[215,903]],"score":0.6815},{"poly":[[234,840],[426,840],[426,861],[234,861]],"score":0.8093},{"poly":[[233,818],[1008,818],[1008,842],[233,842]],"score":0.6784},{"poly":[[215,797],[1009,797],[1009,818],[215,818]],"score":0.7883},{"poly":[[234,759],[757,759],[757,780],[234,780]],"score":0.8138},{"poly":[[301,736],[1013,736],[1013,757],[301,757]],"score":0.7982},{"poly":[[232,732],[311,736],[310,759],[231,755]],"score":0.7215},{"poly":[[231,711],[1011,713],[1011,736],[231,734]],"score":0.7046},{"poly":[[233,691],[1009,691],[1009,713],[233,713]],"score":0.8003},{"poly":[[233,670],[1009,670],[1009,691],[233,691]],"score":0.7993},{"poly":[[233,648],[1011,648],[1011,670],[233,670]],"score":0.8225},{"poly":[[233,625],[1009,625],[1009,648],[233,648]],"score":0.6881},{"poly":[[233,604],[1009,604],[1009,625],[233,625]],"score":0.7361},{"poly":[[213,582],[1011,582],[1011,604],[213,604]],"score":0.7556},{"poly":[[234,543],[1011,543],[1011,566],[234,566]],"score":0.7015},{"poly":[[215,521],[1009,521],[1009,543],[215,543]],"score":0.8278},{"poly":[[233,482],[339,482],[339,500],[233,500]],"score":0.7633},{"poly":[[233,439],[1009,439],[1009,462],[233,462]],"score":0.6619},{"poly":[[233,417],[1009,417],[1009,439],[233,439]],"score":0.8047},{"poly":[[213,394],[1009,394],[1009,416],[213,416]],"score":0.7892},{"poly":[[236,358],[363,358],[363,375],[236,375]],"score":0.9333},{"poly":[[233,333],[1006,333],[1006,356],[233,356]],"score":0.69},{"poly":[[211,310],[1009,312],[1009,335],[211,333]],"score":0.7165},{"poly":[[231,272],[624,272],[624,294],[231,294]],"score":0.795},{"poly":[[213,251],[1008,251],[1008,272],[213,272]],"score":0.8499},{"poly":[[234,211],[903,211],[903,233],[234,233]],"score":0.7922},{"poly":[[233,190],[1009,190],[1009,211],[233,211]],"score":0.8285},{"poly":[[215,168],[1009,168],[1009,190],[215,190]],"score":0.8334},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7413}],"page_no":10,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1527},"conf":0.6979,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":624,"y0":1502,"y1":1523},"font_size":0.0,"text":"12"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.3374,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":51,"y1":76},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":208,"x1":1018,"y0":162,"y1":215},"conf":0.9301,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1011,"y0":168,"y1":190},"font_size":1.027e+22,"text":"Aleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-"},{"bbox":{"x0":234,"x1":479,"y0":190,"y1":211},"font_size":1.027e+22,"text":"chologia, 3(4):387–392, 1965."}],"source":"layout det","text":"Aleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsychologia, 3(4):387–392, 1965."},{"bbox":{"x0":209,"x1":1014,"y0":220,"y1":273},"conf":0.7962,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":226,"y1":247},"font_size":1.027e+22,"text":"Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes"},{"bbox":{"x0":231,"x1":830,"y0":246,"y1":269},"font_size":1.027e+22,"text":"two to tango, 2022. URL https://arxiv.org/abs/2209.07686."}],"source":"layout det","text":"Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022. URL https://arxiv.org/abs/2209.07686."},{"bbox":{"x0":208,"x1":1016,"y0":279,"y1":332},"conf":0.9362,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":216,"x1":1009,"y0":285,"y1":307},"font_size":1.027e+22,"text":"Vincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint"},{"bbox":{"x0":231,"x1":436,"y0":305,"y1":327},"font_size":1.027e+22,"text":"arXiv:2104.07972, 2021."}],"source":"layout det","text":"Vincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint arXiv:2104.07972, 2021."},{"bbox":{"x0":208,"x1":1017,"y0":339,"y1":458},"conf":0.9555,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":343,"y1":365},"font_size":1.027e+22,"text":"Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher"},{"bbox":{"x0":234,"x1":1009,"y0":366,"y1":388},"font_size":1.027e+22,"text":"Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,"},{"bbox":{"x0":233,"x1":1010,"y0":384,"y1":411},"font_size":1.027e+22,"text":"Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:"},{"bbox":{"x0":231,"x1":1013,"y0":408,"y1":432},"font_size":1.027e+22,"text":"Browser-assisted question-answering with human feedback, 2021. URL https://arxiv."},{"bbox":{"x0":233,"x1":459,"y0":429,"y1":456},"font_size":1.027e+22,"text":"org/abs/2112.09332."}],"source":"layout det","text":"Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:Browser-assisted question-answering with human feedback, 2021. URL https://arxiv.org/abs/2112.09332."},{"bbox":{"x0":208,"x1":1016,"y0":463,"y1":560},"conf":0.9603,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":467,"y1":488},"font_size":1.027e+22,"text":"Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David"},{"bbox":{"x0":233,"x1":1008,"y0":488,"y1":510},"font_size":1.027e+22,"text":"Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and"},{"bbox":{"x0":233,"x1":1008,"y0":510,"y1":535},"font_size":1.027e+22,"text":"Augustus Odena. Show your work: Scratchpads for intermediate computation with language"},{"bbox":{"x0":234,"x1":790,"y0":533,"y1":556},"font_size":1.027e+22,"text":"models, 2021. URL https://arxiv.org/abs/2112.00114."}],"source":"layout det","text":"Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114."},{"bbox":{"x0":209,"x1":1016,"y0":565,"y1":684},"conf":0.967,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":571,"y1":592},"font_size":1.027e+22,"text":"Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,"},{"bbox":{"x0":233,"x1":1008,"y0":591,"y1":612},"font_size":1.027e+22,"text":"Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom"},{"bbox":{"x0":234,"x1":1011,"y0":615,"y1":637},"font_size":1.027e+22,"text":"Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,"},{"bbox":{"x0":233,"x1":1011,"y0":637,"y1":658},"font_size":1.027e+22,"text":"Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:"},{"bbox":{"x0":233,"x1":549,"y0":660,"y1":681},"font_size":1.027e+22,"text":"//arxiv.org/abs/2205.06175."}],"source":"layout det","text":"Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https://arxiv.org/abs/2205.06175."},{"bbox":{"x0":208,"x1":1016,"y0":690,"y1":786},"conf":0.958,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":693,"y1":718},"font_size":1.027e+22,"text":"Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,"},{"bbox":{"x0":233,"x1":1009,"y0":718,"y1":741},"font_size":1.027e+22,"text":"Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions"},{"bbox":{"x0":233,"x1":1009,"y0":739,"y1":761},"font_size":1.027e+22,"text":"for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern"},{"bbox":{"x0":231,"x1":539,"y0":759,"y1":784},"font_size":1.027e+22,"text":"recognition, pp. 10740–10749, 2020a."}],"source":"layout det","text":"Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740–10749, 2020a."},{"bbox":{"x0":208,"x1":1016,"y0":793,"y1":867},"conf":0.9553,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1006,"y0":797,"y1":818},"font_size":1.027e+22,"text":"Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew"},{"bbox":{"x0":233,"x1":1008,"y0":820,"y1":842},"font_size":1.027e+22,"text":"Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv"},{"bbox":{"x0":231,"x1":516,"y0":838,"y1":865},"font_size":1.027e+22,"text":"preprint arXiv:2010.03768, 2020b."}],"source":"layout det","text":"Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b."},{"bbox":{"x0":208,"x1":1017,"y0":873,"y1":949},"conf":0.947,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1011,"y0":878,"y1":899},"font_size":1.027e+22,"text":"Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston."},{"bbox":{"x0":233,"x1":1009,"y0":901,"y1":922},"font_size":1.027e+22,"text":"Language models that seek for knowledge: Modular search $\\&$  generation for dialogue and prompt"},{"bbox":{"x0":233,"x1":664,"y0":921,"y1":945},"font_size":1.027e+22,"text":"completion. arXiv preprint arXiv:2203.13224, 2022a."}],"source":"layout det","text":"Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.Language models that seek for knowledge: Modular search $\\&$  generation for dialogue and prompt completion. arXiv preprint arXiv:2203.13224, 2022a."},{"bbox":{"x0":208,"x1":1017,"y0":955,"y1":1072},"conf":0.9702,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1010,"y0":957,"y1":983},"font_size":1.027e+22,"text":"Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,"},{"bbox":{"x0":234,"x1":1009,"y0":982,"y1":1003},"font_size":1.027e+22,"text":"Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman"},{"bbox":{"x0":233,"x1":1011,"y0":1003,"y1":1025},"font_size":1.027e+22,"text":"Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:"},{"bbox":{"x0":233,"x1":1009,"y0":1025,"y1":1048},"font_size":1.027e+22,"text":"a deployed conversational agent that continually learns to responsibly engage, 2022b. URL"},{"bbox":{"x0":231,"x1":624,"y0":1044,"y1":1069},"font_size":1.027e+22,"text":"https://arxiv.org/abs/2208.03188."}],"source":"layout det","text":"Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:a deployed conversational agent that continually learns to responsibly engage, 2022b. URL https://arxiv.org/abs/2208.03188."},{"bbox":{"x0":207,"x1":1016,"y0":1077,"y1":1133},"conf":0.9165,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1084,"y1":1106},"font_size":1.027e+22,"text":"James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale"},{"bbox":{"x0":234,"x1":898,"y0":1106,"y1":1127},"font_size":1.027e+22,"text":"dataset for fact extraction and verifcation. arXiv preprint arXiv:1803.05355, 2018.i"}],"source":"layout det","text":"James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verifcation. arXiv preprint arXiv:1803.05355, 2018.i"},{"bbox":{"x0":208,"x1":963,"y0":1136,"y1":1169},"conf":0.8829,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":956,"y0":1143,"y1":1165},"font_size":1.027e+22,"text":"Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39–285, 1987."}],"source":"layout det","text":"Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39–285, 1987."},{"bbox":{"x0":208,"x1":1016,"y0":1174,"y1":1249},"conf":0.9377,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1180,"y1":1201},"font_size":1.027e+22,"text":"Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-"},{"bbox":{"x0":233,"x1":1011,"y0":1203,"y1":1224},"font_size":1.027e+22,"text":"ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,"},{"bbox":{"x0":234,"x1":732,"y0":1224,"y1":1246},"font_size":1.027e+22,"text":"2022a. URL https://arxiv.org/abs/2203.11171."}],"source":"layout det","text":"Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,2022a. URL https://arxiv.org/abs/2203.11171."},{"bbox":{"x0":207,"x1":1016,"y0":1254,"y1":1307},"conf":0.9483,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1257,"y1":1282},"font_size":1.027e+22,"text":"Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented"},{"bbox":{"x0":231,"x1":820,"y0":1280,"y1":1305},"font_size":1.027e+22,"text":"ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b."}],"source":"layout det","text":"Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b."},{"bbox":{"x0":207,"x1":1016,"y0":1313,"y1":1388},"conf":0.9481,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1008,"y0":1317,"y1":1341},"font_size":1.027e+22,"text":"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny"},{"bbox":{"x0":234,"x1":1008,"y0":1341,"y1":1363},"font_size":1.027e+22,"text":"Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint"},{"bbox":{"x0":233,"x1":434,"y0":1361,"y1":1384},"font_size":1.027e+22,"text":"arXiv:2201.11903, 2022."}],"source":"layout det","text":"Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022."},{"bbox":{"x0":207,"x1":1016,"y0":1394,"y1":1470},"conf":0.9397,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":1399,"y1":1421},"font_size":1.027e+22,"text":"Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,"},{"bbox":{"x0":233,"x1":1009,"y0":1422,"y1":1444},"font_size":1.027e+22,"text":"and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question"},{"bbox":{"x0":233,"x1":649,"y0":1442,"y1":1467},"font_size":1.027e+22,"text":"answering. arXiv preprint arXiv:1809.09600, 2018."}],"source":"layout det","text":"Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018."}],"formula_dets":[{"bbox":{"x0":707,"x1":722,"y0":903,"y1":919},"conf":0.5216,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":208,"x1":1017,"y0":955,"y1":1072},"conf":0.9702,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1016,"y0":565,"y1":684},"conf":0.967,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":463,"y1":560},"conf":0.9603,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":690,"y1":786},"conf":0.958,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":339,"y1":458},"conf":0.9555,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":793,"y1":867},"conf":0.9553,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1254,"y1":1307},"conf":0.9483,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1313,"y1":1388},"conf":0.9481,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":873,"y1":949},"conf":0.947,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1394,"y1":1470},"conf":0.9397,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":1174,"y1":1249},"conf":0.9377,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":279,"y1":332},"conf":0.9362,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1018,"y0":162,"y1":215},"conf":0.9301,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1077,"y1":1133},"conf":0.9165,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":963,"y0":1136,"y1":1169},"conf":0.8829,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1014,"y0":220,"y1":273},"conf":0.7962,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.7657,"label":"Abandon","label_id":2},{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1527},"conf":0.6979,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.3374,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[624,1502],[624,1523],[599,1523]],"score":0.8996},{"poly":[[233,1445],[648,1442],[649,1463],[233,1467]],"score":0.8166},{"poly":[[233,1422],[1009,1422],[1009,1444],[233,1444]],"score":0.8214},{"poly":[[213,1399],[1011,1399],[1011,1421],[213,1421]],"score":0.8252},{"poly":[[233,1363],[434,1361],[434,1383],[233,1384]],"score":0.8173},{"poly":[[234,1341],[1008,1341],[1008,1363],[234,1363]],"score":0.8299},{"poly":[[211,1317],[1008,1318],[1008,1341],[211,1340]],"score":0.7377},{"poly":[[231,1282],[820,1280],[820,1303],[231,1305]],"score":0.7326},{"poly":[[213,1259],[1009,1257],[1009,1280],[213,1282]],"score":0.7271},{"poly":[[234,1224],[732,1224],[732,1246],[234,1246]],"score":0.7816},{"poly":[[233,1203],[1011,1203],[1011,1224],[233,1224]],"score":0.7922},{"poly":[[213,1180],[1009,1180],[1009,1201],[213,1201]],"score":0.8234},{"poly":[[213,1143],[956,1143],[956,1165],[213,1165]],"score":0.8384},{"poly":[[234,1106],[898,1106],[898,1127],[234,1127]],"score":0.7561},{"poly":[[213,1084],[1009,1084],[1009,1106],[213,1106]],"score":0.7926},{"poly":[[231,1046],[624,1044],[624,1068],[231,1069]],"score":0.7275},{"poly":[[233,1025],[1009,1025],[1009,1048],[233,1048]],"score":0.6936},{"poly":[[233,1003],[1011,1003],[1011,1025],[233,1025]],"score":0.7677},{"poly":[[234,982],[1009,982],[1009,1003],[234,1003]],"score":0.8128},{"poly":[[211,957],[1010,960],[1009,983],[211,980]],"score":0.7403},{"poly":[[233,922],[664,921],[664,944],[233,945]],"score":0.7535},{"poly":[[233,901],[1009,901],[1009,922],[233,922]],"score":0.8233},{"poly":[[215,878],[1011,878],[1011,899],[215,899]],"score":0.8316},{"poly":[[231,842],[515,838],[516,861],[231,865]],"score":0.7197},{"poly":[[233,820],[1008,820],[1008,842],[233,842]],"score":0.7819},{"poly":[[213,797],[1006,797],[1006,818],[213,818]],"score":0.8098},{"poly":[[231,761],[539,759],[539,782],[231,784]],"score":0.7485},{"poly":[[233,739],[1009,739],[1009,761],[233,761]],"score":0.7989},{"poly":[[233,718],[1009,718],[1009,741],[233,741]],"score":0.7078},{"poly":[[211,693],[1009,695],[1009,718],[211,716]],"score":0.7298},{"poly":[[233,660],[549,660],[549,681],[233,681]],"score":0.8286},{"poly":[[233,637],[1011,637],[1011,658],[233,658]],"score":0.7923},{"poly":[[234,615],[1011,615],[1011,637],[234,637]],"score":0.8274},{"poly":[[233,591],[1008,591],[1008,612],[233,612]],"score":0.7451},{"poly":[[215,571],[1009,571],[1009,592],[215,592]],"score":0.825},{"poly":[[234,533],[790,533],[790,556],[234,556]],"score":0.7269},{"poly":[[233,510],[1008,512],[1008,535],[233,533]],"score":0.7032},{"poly":[[233,488],[1008,488],[1008,510],[233,510]],"score":0.7643},{"poly":[[215,467],[1009,467],[1009,488],[215,488]],"score":0.8148},{"poly":[[233,432],[459,429],[459,452],[233,456]],"score":0.6928},{"poly":[[231,408],[1013,409],[1013,432],[231,431]],"score":0.698},{"poly":[[233,384],[1010,388],[1009,411],[233,408]],"score":0.6831},{"poly":[[234,366],[1009,366],[1009,388],[234,388]],"score":0.8087},{"poly":[[213,343],[1009,343],[1009,365],[213,365]],"score":0.8177},{"poly":[[231,305],[436,305],[436,327],[231,327]],"score":0.7625},{"poly":[[216,285],[1009,285],[1009,307],[216,307]],"score":0.8396},{"poly":[[231,246],[830,246],[830,269],[231,269]],"score":0.6788},{"poly":[[215,226],[1009,226],[1009,247],[215,247]],"score":0.8301},{"poly":[[234,190],[479,190],[479,211],[234,211]],"score":0.8371},{"poly":[[215,168],[1011,168],[1011,190],[215,190]],"score":0.839},{"poly":[[213,53],[585,51],[585,74],[213,76]],"score":0.7425}],"page_no":11,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.6848,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":597,"x1":627,"y0":1498,"y1":1526},"font_size":0.0,"text":"13"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1016,"y0":47,"y1":85},"conf":0.2432,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":209,"x1":1018,"y0":163,"y1":281},"conf":0.9669,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1011,"y0":167,"y1":188},"font_size":1.232e-07,"text":"Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:"},{"bbox":{"x0":230,"x1":1009,"y0":185,"y1":213},"font_size":1.232e-07,"text":"Language models for action generation in text-based games. In Proceedings of the 2020 Conference"},{"bbox":{"x0":234,"x1":1009,"y0":210,"y1":231},"font_size":1.232e-07,"text":"on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online, Novem-"},{"bbox":{"x0":234,"x1":1009,"y0":231,"y1":252},"font_size":1.232e-07,"text":"ber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704."},{"bbox":{"x0":233,"x1":813,"y0":251,"y1":276},"font_size":1.232e-07,"text":"URL https://aclanthology.org/2020.emnlp-main.704."}],"source":"layout det","text":"Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.URL https://aclanthology.org/2020.emnlp-main.704."},{"bbox":{"x0":209,"x1":1017,"y0":286,"y1":361},"conf":0.9366,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":290,"y1":314},"font_size":1.232e-07,"text":"Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable"},{"bbox":{"x0":233,"x1":1011,"y0":313,"y1":337},"font_size":1.232e-07,"text":"real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,"},{"bbox":{"x0":233,"x1":284,"y0":333,"y1":358},"font_size":1.232e-07,"text":"2022."}],"source":"layout det","text":"Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,2022."},{"bbox":{"x0":209,"x1":1015,"y0":369,"y1":423},"conf":0.8628,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":373,"y1":396},"font_size":1.232e-07,"text":"Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with"},{"bbox":{"x0":233,"x1":807,"y0":394,"y1":417},"font_size":1.232e-07,"text":"reasoning, 2022. URL https://arxiv.org/abs/2203.14465."}],"source":"layout det","text":"Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465."},{"bbox":{"x0":208,"x1":1016,"y0":429,"y1":504},"conf":0.9436,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":431,"y1":455},"font_size":1.232e-07,"text":"Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,"},{"bbox":{"x0":233,"x1":1011,"y0":455,"y1":478},"font_size":1.232e-07,"text":"Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in"},{"bbox":{"x0":233,"x1":906,"y0":477,"y1":498},"font_size":1.232e-07,"text":"large language models, 2022. URL https://arxiv.org/abs/2205.10625."}],"source":"layout det","text":"Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2022. URL https://arxiv.org/abs/2205.10625."},{"bbox":{"x0":209,"x1":1014,"y0":510,"y1":564},"conf":0.9391,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":511,"y1":536},"font_size":1.232e-07,"text":"Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information"},{"bbox":{"x0":231,"x1":926,"y0":535,"y1":559},"font_size":1.232e-07,"text":"seeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021."}],"source":"layout det","text":"Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information seeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":209,"x1":1018,"y0":163,"y1":281},"conf":0.9669,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1016,"y0":429,"y1":504},"conf":0.9436,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1014,"y0":510,"y1":564},"conf":0.9391,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1017,"y0":286,"y1":361},"conf":0.9366,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1015,"y0":369,"y1":423},"conf":0.8628,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":50,"y1":80},"conf":0.7801,"label":"Abandon","label_id":2},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.6848,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":1016,"y0":47,"y1":85},"conf":0.2432,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[597,1498],[627,1498],[627,1526],[597,1526]],"score":0.7961},{"poly":[[231,536],[926,535],[926,558],[231,559]],"score":0.8077},{"poly":[[215,511],[1009,513],[1009,536],[215,535]],"score":0.8192},{"poly":[[233,477],[906,477],[906,498],[233,498]],"score":0.8301},{"poly":[[233,455],[1011,455],[1011,478],[233,478]],"score":0.7419},{"poly":[[213,431],[1011,432],[1011,455],[213,454]],"score":0.7837},{"poly":[[233,394],[807,394],[807,417],[233,417]],"score":0.7158},{"poly":[[215,373],[1009,373],[1009,396],[215,396]],"score":0.7845},{"poly":[[233,333],[284,333],[284,358],[233,358]],"score":0.9195},{"poly":[[233,313],[1011,313],[1011,337],[233,337]],"score":0.7803},{"poly":[[213,290],[1009,290],[1009,314],[213,314]],"score":0.7281},{"poly":[[233,251],[813,252],[813,276],[233,274]],"score":0.758},{"poly":[[234,231],[1009,231],[1009,252],[234,252]],"score":0.8384},{"poly":[[234,210],[1009,210],[1009,231],[234,231]],"score":0.8247},{"poly":[[230,185],[1009,185],[1009,213],[230,213]],"score":0.6476},{"poly":[[215,167],[1011,167],[1011,188],[215,188]],"score":0.9096},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.775}],"page_no":12,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.6883,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1502,"y1":1521},"font_size":0.0,"text":"14"}],"source":"layout det","text":""},{"bbox":{"x0":207,"x1":1016,"y0":46,"y1":85},"conf":0.4344,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":209,"x1":497,"y0":160,"y1":193},"conf":0.7888,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":491,"y0":165,"y1":186},"font_size":3.979e-34,"text":"AADDITIONAL RESULTS"}],"source":"layout det","text":"AADDITIONAL RESULTS"},{"bbox":{"x0":210,"x1":459,"y0":209,"y1":240},"conf":0.7601,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":456,"y0":213,"y1":234},"font_size":3.979e-34,"text":"A.1GPT-3 EXPERIMENTS"}],"source":"layout det","text":"A.1GPT-3 EXPERIMENTS"},{"bbox":{"x0":382,"x1":840,"y0":254,"y1":354},"conf":0.9609,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":638,"x1":750,"y0":261,"y1":290},"font_size":0.0,"text":"PaLM-540B"},{"bbox":{"x0":765,"x1":828,"y0":262,"y1":289},"font_size":0.0,"text":"GPT-3"},{"bbox":{"x0":410,"x1":613,"y0":298,"y1":320},"font_size":0.0,"text":"HotpotQA (exact match)"},{"bbox":{"x0":673,"x1":720,"y0":295,"y1":322},"font_size":0.0,"text":"29.4"},{"bbox":{"x0":773,"x1":821,"y0":295,"y1":322},"font_size":0.0,"text":"30.8"},{"bbox":{"x0":397,"x1":626,"y0":315,"y1":344},"font_size":0.0,"text":"ALFWorld (success rate %)"},{"bbox":{"x0":673,"x1":719,"y0":317,"y1":345},"font_size":0.0,"text":"70.9"},{"bbox":{"x0":773,"x1":820,"y0":317,"y1":344},"font_size":0.0,"text":"78.4"}],"source":"layout det","text":"<html><body><table><tr><td></td><td>PaLM-540B</td><td>GPT-3</td></tr><tr><td>HotpotQA (exact match)</td><td>29.4</td><td>30.8</td></tr><tr><td>ALFWorld (success rate %)</td><td>70.9</td><td>78.4</td></tr></table></body></html>"},{"bbox":{"x0":206,"x1":1017,"y0":364,"y1":441},"conf":0.8419,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1009,"y0":366,"y1":389},"font_size":3.979e-34,"text":"Table 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding)."},{"bbox":{"x0":213,"x1":1008,"y0":389,"y1":411},"font_size":3.979e-34,"text":"On HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all"},{"bbox":{"x0":213,"x1":941,"y0":411,"y1":432},"font_size":3.979e-34,"text":"134 unseen validation task instances, and use the best prompt set according to PaLM-540B."}],"source":"layout det","text":"Table 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).On HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all 134 unseen validation task instances, and use the best prompt set according to PaLM-540B."},{"bbox":{"x0":207,"x1":1017,"y0":457,"y1":602},"conf":0.9716,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":462,"y1":484},"font_size":3.979e-34,"text":"We run additional GPT-3 (Brown et al., 2020) experiments to confrm ReAct prompting performancei"},{"bbox":{"x0":213,"x1":1009,"y0":483,"y1":507},"font_size":3.979e-34,"text":"is general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,"},{"bbox":{"x0":213,"x1":1009,"y0":507,"y1":528},"font_size":3.979e-34,"text":"greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly"},{"bbox":{"x0":211,"x1":1008,"y0":526,"y1":550},"font_size":3.979e-34,"text":"because it is fnetuned with human instruction following. This indicates ReAct prompting is effectivei"},{"bbox":{"x0":213,"x1":1009,"y0":549,"y1":573},"font_size":3.979e-34,"text":"across different large language models on different tasks. The code for these experiments are at"},{"bbox":{"x0":210,"x1":545,"y0":571,"y1":592},"font_size":3.979e-34,"text":"https://react-lm.github.io/."}],"source":"layout det","text":"We run additional GPT-3 (Brown et al., 2020) experiments to confrm ReAct prompting performancei is general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly because it is fnetuned with human instruction following. This indicates ReAct prompting is effectivei across different large language models on different tasks. The code for these experiments are at https://react-lm.github.io/."},{"bbox":{"x0":208,"x1":775,"y0":616,"y1":650},"conf":0.3489,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":768,"y0":622,"y1":644},"font_size":3.979e-34,"text":"A.2REACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA"}],"source":"layout det","text":"A.2REACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA"},{"bbox":{"x0":208,"x1":775,"y0":616,"y1":650},"conf":0.2979,"font_size":0.0,"label":"Table caption","label_id":6,"lines":[],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1014,"y0":663,"y1":1018},"conf":0.9515,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![8d596dbf2460c78be09b1773e5db7206](imgs/8d596dbf2460c78be09b1773e5db7206.jpg)"},{"bbox":{"x0":207,"x1":1016,"y0":1034,"y1":1086},"conf":0.8308,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":1038,"y1":1059},"font_size":3.979e-34,"text":"Figure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is"},{"bbox":{"x0":211,"x1":921,"y0":1058,"y1":1084},"font_size":3.979e-34,"text":"able to obtain the up-to-date answer thanks to real-world web interaction plus reasoning."}],"source":"layout det","text":"Figure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is able to obtain the up-to-date answer thanks to real-world web interaction plus reasoning."},{"bbox":{"x0":208,"x1":1016,"y0":1088,"y1":1271},"conf":0.9516,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1089,"y1":1110},"font_size":3.979e-34,"text":"During trajectory inspection, we also fnd that sometimes ReAct does not agree with dataset labels asi"},{"bbox":{"x0":213,"x1":1009,"y0":1109,"y1":1130},"font_size":3.979e-34,"text":"the labels themselves could be outdated. For example, as shown in Figure 4, the question asks about"},{"bbox":{"x0":213,"x1":1009,"y0":1132,"y1":1153},"font_size":3.979e-34,"text":"the size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT"},{"bbox":{"x0":213,"x1":1011,"y0":1155,"y1":1176},"font_size":3.979e-34,"text":"give wrong answers due to hallucination, Act fails despite the access of real-world web interaction,"},{"bbox":{"x0":213,"x1":1011,"y0":1176,"y1":1198},"font_size":3.979e-34,"text":"due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to"},{"bbox":{"x0":213,"x1":1011,"y0":1200,"y1":1221},"font_size":3.979e-34,"text":"retrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better"},{"bbox":{"x0":213,"x1":1011,"y0":1221,"y1":1242},"font_size":3.979e-34,"text":"incorporation of reasoning abilities might beneft recent Internet-augmented language models (Nakanoi"},{"bbox":{"x0":211,"x1":888,"y0":1239,"y1":1266},"font_size":3.979e-34,"text":"et al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving."}],"source":"layout det","text":"During trajectory inspection, we also fnd that sometimes ReAct does not agree with dataset labels asi the labels themselves could be outdated. For example, as shown in Figure 4, the question asks about the size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT give wrong answers due to hallucination, Act fails despite the access of real-world web interaction,due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to retrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better incorporation of reasoning abilities might beneft recent Internet-augmented language models (Nakanoi et al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving."},{"bbox":{"x0":209,"x1":811,"y0":1288,"y1":1317},"conf":0.8212,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":807,"y0":1290,"y1":1312},"font_size":3.979e-34,"text":"A.3HUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD"}],"source":"layout det","text":"A.3HUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD"},{"bbox":{"x0":208,"x1":1017,"y0":1328,"y1":1472},"conf":0.9696,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1332,"y1":1353},"font_size":3.979e-34,"text":"We also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit"},{"bbox":{"x0":213,"x1":1011,"y0":1355,"y1":1376},"font_size":3.979e-34,"text":"ReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act"},{"bbox":{"x0":213,"x1":1009,"y0":1376,"y1":1398},"font_size":3.979e-34,"text":"17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align"},{"bbox":{"x0":210,"x1":1011,"y0":1396,"y1":1421},"font_size":3.979e-34,"text":"with these human thought edits and succeed in the task. From a human perspective, solving such a"},{"bbox":{"x0":213,"x1":1011,"y0":1421,"y1":1442},"font_size":3.979e-34,"text":"task becomes signifcantly easier, from typing tens of actions to only editing a couple of thoughts,i"},{"bbox":{"x0":211,"x1":1009,"y0":1440,"y1":1465},"font_size":3.979e-34,"text":"which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go"}],"source":"layout det","text":"We also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit ReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act 17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align with these human thought edits and succeed in the task. From a human perspective, solving such a task becomes signifcantly easier, from typing tens of actions to only editing a couple of thoughts,i which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":207,"x1":1017,"y0":457,"y1":602},"conf":0.9716,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":1328,"y1":1472},"conf":0.9696,"label":"Text","label_id":1},{"bbox":{"x0":382,"x1":840,"y0":254,"y1":354},"conf":0.9609,"label":"Table","label_id":5},{"bbox":{"x0":208,"x1":1016,"y0":1088,"y1":1271},"conf":0.9516,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1014,"y0":663,"y1":1018},"conf":0.9515,"label":"Figure","label_id":3},{"bbox":{"x0":206,"x1":1017,"y0":364,"y1":441},"conf":0.8419,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1016,"y0":1034,"y1":1086},"conf":0.8308,"label":"Figure caption","label_id":4},{"bbox":{"x0":209,"x1":811,"y0":1288,"y1":1317},"conf":0.8212,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":497,"y0":160,"y1":193},"conf":0.7888,"label":"Title","label_id":0},{"bbox":{"x0":210,"x1":459,"y0":209,"y1":240},"conf":0.7601,"label":"Title","label_id":0},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.6883,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":592,"y0":49,"y1":81},"conf":0.6773,"label":"Abandon","label_id":2},{"bbox":{"x0":207,"x1":1016,"y0":46,"y1":85},"conf":0.4344,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":775,"y0":616,"y1":650},"conf":0.3489,"label":"Title","label_id":0},{"bbox":{"x0":208,"x1":775,"y0":616,"y1":650},"conf":0.2979,"label":"Table caption","label_id":6}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[625,1502],[625,1521],[599,1521]],"score":0.9777},{"poly":[[211,1440],[1009,1442],[1009,1465],[211,1464]],"score":0.7196},{"poly":[[213,1421],[1011,1421],[1011,1442],[213,1442]],"score":0.7823},{"poly":[[210,1396],[1011,1398],[1011,1421],[210,1419]],"score":0.7493},{"poly":[[213,1376],[1009,1376],[1009,1398],[213,1398]],"score":0.8214},{"poly":[[213,1355],[1011,1355],[1011,1376],[213,1376]],"score":0.823},{"poly":[[213,1332],[1009,1332],[1009,1353],[213,1353]],"score":0.8191},{"poly":[[213,1290],[807,1290],[807,1312],[213,1312]],"score":0.8114},{"poly":[[211,1239],[888,1242],[888,1266],[211,1262]],"score":0.7155},{"poly":[[213,1221],[1011,1221],[1011,1242],[213,1242]],"score":0.786},{"poly":[[213,1200],[1011,1200],[1011,1221],[213,1221]],"score":0.8044},{"poly":[[213,1176],[1011,1176],[1011,1198],[213,1198]],"score":0.7994},{"poly":[[213,1155],[1011,1155],[1011,1176],[213,1176]],"score":0.8198},{"poly":[[213,1132],[1009,1132],[1009,1153],[213,1153]],"score":0.76},{"poly":[[213,1109],[1009,1109],[1009,1130],[213,1130]],"score":0.7585},{"poly":[[213,1089],[1009,1089],[1009,1110],[213,1110]],"score":0.7846},{"poly":[[211,1058],[921,1061],[921,1084],[211,1081]],"score":0.6889},{"poly":[[213,1038],[1011,1038],[1011,1059],[213,1059]],"score":0.8023},{"poly":[[424,990],[542,990],[542,1006],[424,1006]],"score":0.6198},{"poly":[[373,988],[394,988],[394,1010],[373,1010]],"score":0.7672},{"poly":[[893,987],[963,987],[963,1003],[893,1003]],"score":0.8671},{"poly":[[627,987],[747,987],[747,998],[627,998]],"score":0.9504},{"poly":[[589,985],[609,981],[615,1005],[594,1010]],"score":0.696},{"poly":[[411,975],[517,975],[517,992],[411,992]],"score":0.7949},{"poly":[[629,975],[693,975],[693,987],[629,987]],"score":0.7475},{"poly":[[690,960],[990,960],[990,977],[690,977]],"score":0.7706},{"poly":[[625,960],[695,960],[695,977],[625,977]],"score":0.7811},{"poly":[[411,960],[590,960],[590,977],[411,977]],"score":0.7613},{"poly":[[228,954],[306,954],[306,965],[228,965]],"score":0.9584},{"poly":[[409,945],[477,945],[477,962],[409,962]],"score":0.7371},{"poly":[[627,936],[717,936],[717,952],[627,952]],"score":0.7371},{"poly":[[409,931],[590,931],[590,947],[409,947]],"score":0.7513},{"poly":[[228,926],[353,926],[353,942],[228,942]],"score":0.6772},{"poly":[[627,926],[871,926],[871,937],[627,937]],"score":0.7772},{"poly":[[877,921],[985,922],[984,941],[876,939]],"score":0.674},{"poly":[[411,916],[459,916],[459,934],[411,934]],"score":0.8315},{"poly":[[368,914],[387,914],[387,926],[368,926]],"score":0.6638},{"poly":[[762,912],[910,912],[910,924],[762,924]],"score":0.8351},{"poly":[[624,911],[762,909],[762,926],[624,927]],"score":0.7175},{"poly":[[226,911],[329,911],[329,927],[226,927]],"score":0.7605},{"poly":[[409,903],[610,903],[610,919],[409,919]],"score":0.8444},{"poly":[[627,903],[757,903],[757,914],[627,914]],"score":0.7829},{"poly":[[333,899],[382,899],[382,911],[333,911]],"score":0.9203},{"poly":[[226,898],[326,898],[326,914],[226,914]],"score":0.8242},{"poly":[[409,888],[544,888],[544,904],[409,904]],"score":0.7858},{"poly":[[625,886],[973,886],[973,903],[625,903]],"score":0.7623},{"poly":[[228,886],[359,886],[359,898],[228,898]],"score":0.9653},{"poly":[[625,875],[950,873],[950,889],[625,891]],"score":0.6647},{"poly":[[409,873],[496,873],[496,889],[409,889]],"score":0.7376},{"poly":[[225,870],[358,870],[358,886],[225,886]],"score":0.747},{"poly":[[625,861],[950,861],[950,878],[625,878]],"score":0.7931},{"poly":[[446,860],[609,860],[609,876],[446,876]],"score":0.8417},{"poly":[[411,861],[449,861],[449,874],[411,874]],"score":0.7979},{"poly":[[226,858],[382,858],[382,870],[226,870]],"score":0.9722},{"poly":[[409,843],[545,843],[545,860],[409,860]],"score":0.8043},{"poly":[[223,838],[381,840],[381,858],[223,856]],"score":0.689},{"poly":[[627,837],[848,837],[848,853],[627,853]],"score":0.6034},{"poly":[[409,828],[545,828],[545,845],[409,845]],"score":0.7641},{"poly":[[802,828],[993,828],[993,840],[802,840]],"score":0.89},{"poly":[[629,828],[778,828],[778,840],[629,840]],"score":0.8504},{"poly":[[238,823],[376,823],[376,840],[238,840]],"score":0.8603},{"poly":[[414,817],[599,817],[599,828],[414,828]],"score":0.9553},{"poly":[[625,813],[876,813],[876,830],[625,830]],"score":0.6663},{"poly":[[629,804],[690,804],[690,815],[629,815]],"score":0.8881},{"poly":[[409,799],[584,799],[584,815],[409,815]],"score":0.7456},{"poly":[[373,790],[392,790],[392,815],[373,815]],"score":0.7001},{"poly":[[693,789],[988,789],[988,805],[693,805]],"score":0.8075},{"poly":[[625,789],[688,789],[688,805],[625,805]],"score":0.8034},{"poly":[[228,787],[313,787],[313,804],[228,804]],"score":0.8985},{"poly":[[407,784],[602,784],[602,800],[407,800]],"score":0.8355},{"poly":[[461,766],[552,766],[552,784],[461,784]],"score":0.8159},{"poly":[[263,766],[351,766],[351,784],[263,784]],"score":0.8067},{"poly":[[624,764],[798,764],[798,780],[624,780]],"score":0.7427},{"poly":[[625,752],[941,752],[941,769],[625,769]],"score":0.7619},{"poly":[[625,741],[896,741],[896,757],[625,757]],"score":0.7294},{"poly":[[226,734],[366,734],[366,751],[226,751]],"score":0.9199},{"poly":[[526,729],[605,729],[605,754],[526,754]],"score":0.7055},{"poly":[[627,728],[693,728],[693,744],[627,744]],"score":0.6918},{"poly":[[228,719],[449,719],[449,736],[228,736]],"score":0.7874},{"poly":[[629,718],[973,718],[973,729],[629,729]],"score":0.951},{"poly":[[228,705],[597,705],[597,721],[228,721]],"score":0.7965},{"poly":[[627,703],[976,703],[976,719],[627,719]],"score":0.7493},{"poly":[[737,683],[891,683],[891,700],[737,700]],"score":0.9345},{"poly":[[369,683],[462,683],[462,700],[369,700]],"score":0.9679},{"poly":[[213,622],[768,622],[768,644],[213,644]],"score":0.7318},{"poly":[[210,571],[545,571],[545,592],[210,592]],"score":0.7149},{"poly":[[213,549],[1009,549],[1009,573],[213,573]],"score":0.7043},{"poly":[[211,526],[1008,528],[1008,549],[211,548]],"score":0.7911},{"poly":[[213,507],[1009,507],[1009,528],[213,528]],"score":0.8049},{"poly":[[213,483],[1009,483],[1009,507],[213,507]],"score":0.7034},{"poly":[[213,462],[1009,462],[1009,483],[213,483]],"score":0.7613},{"poly":[[213,411],[941,411],[941,432],[213,432]],"score":0.7697},{"poly":[[213,389],[1008,389],[1008,411],[213,411]],"score":0.7859},{"poly":[[215,366],[1009,366],[1009,389],[215,389]],"score":0.7012},{"poly":[[399,320],[625,320],[625,342],[399,342]],"score":0.9218},{"poly":[[773,317],[820,317],[820,343],[773,343]],"score":0.8625},{"poly":[[675,317],[718,317],[718,343],[675,343]],"score":0.9},{"poly":[[409,297],[614,297],[614,318],[409,318]],"score":0.8181},{"poly":[[773,295],[820,295],[820,322],[773,322]],"score":0.8572},{"poly":[[674,295],[718,295],[718,322],[674,322]],"score":0.8257},{"poly":[[642,266],[750,266],[750,284],[642,284]],"score":0.807},{"poly":[[763,264],[827,264],[827,287],[763,287]],"score":0.9212},{"poly":[[213,213],[456,213],[456,234],[213,234]],"score":0.8246},{"poly":[[213,165],[491,165],[491,186],[213,186]],"score":0.9622},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7314}],"page_no":13,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1528},"conf":0.5837,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1500,"y1":1523},"font_size":0.0,"text":"15"}],"source":"layout det","text":""},{"bbox":{"x0":208,"x1":1017,"y0":46,"y1":86},"conf":0.5516,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":587,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":213,"x1":1015,"y0":158,"y1":476},"conf":0.9722,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![84680182d56688a02452e83527d1856e](imgs/84680182d56688a02452e83527d1856e.jpg)"},{"bbox":{"x0":207,"x1":1018,"y0":487,"y1":564},"conf":0.9482,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":213,"x1":1008,"y0":492,"y1":513},"font_size":-9.706e+21,"text":"Figure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct"},{"bbox":{"x0":213,"x1":1009,"y0":513,"y1":535},"font_size":-9.706e+21,"text":"trajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts"},{"bbox":{"x0":213,"x1":990,"y0":535,"y1":558},"font_size":-9.706e+21,"text":"(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds."}],"source":"layout det","text":"Figure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct trajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds."},{"bbox":{"x0":209,"x1":1016,"y0":643,"y1":786},"conf":0.9691,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":647,"y1":669},"font_size":-9.706e+21,"text":"is diffcult for Act and previous RL methods, as a human cannot change the model parameters, andi"},{"bbox":{"x0":213,"x1":1009,"y0":670,"y1":691},"font_size":-9.706e+21,"text":"changing a few actions might not edit the rest of the model behavior. This paradigm is also more than"},{"bbox":{"x0":213,"x1":1009,"y0":691,"y1":713},"font_size":-9.706e+21,"text":"human dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct"},{"bbox":{"x0":211,"x1":1009,"y0":711,"y1":736},"font_size":-9.706e+21,"text":"thoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the"},{"bbox":{"x0":213,"x1":1011,"y0":736,"y1":757},"font_size":-9.706e+21,"text":"fexible thought space supports, for better task solving. We believe this is an exciting direction forl"},{"bbox":{"x0":213,"x1":743,"y0":757,"y1":779},"font_size":-9.706e+21,"text":"human alignment and leave more systematic study as future work."}],"source":"layout det","text":"is diffcult for Act and previous RL methods, as a human cannot change the model parameters, andi changing a few actions might not edit the rest of the model behavior. This paradigm is also more than human dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct thoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the fexible thought space supports, for better task solving. We believe this is an exciting direction forl human alignment and leave more systematic study as future work."},{"bbox":{"x0":209,"x1":494,"y0":881,"y1":914},"conf":0.8995,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":211,"x1":491,"y0":883,"y1":909},"font_size":-9.706e+21,"text":"BEXPERIMENT DETAILS"}],"source":"layout det","text":"BEXPERIMENT DETAILS"},{"bbox":{"x0":209,"x1":569,"y0":975,"y1":1008},"conf":0.8911,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":564,"y0":980,"y1":1002},"font_size":-9.706e+21,"text":"B.1HOTPOTQA FINETUNING DETAILS"}],"source":"layout det","text":"B.1HOTPOTQA FINETUNING DETAILS"},{"bbox":{"x0":208,"x1":1017,"y0":1045,"y1":1167},"conf":0.9674,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1049,"y1":1072},"font_size":-9.706e+21,"text":"For all fnetuning we use a batch size of 64. On PaLM-8B, we fnetune ReAct and Act methodsii"},{"bbox":{"x0":211,"x1":1008,"y0":1071,"y1":1094},"font_size":-9.706e+21,"text":"for 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we fnetunei"},{"bbox":{"x0":213,"x1":1009,"y0":1094,"y1":1115},"font_size":-9.706e+21,"text":"ReAct and Act methods for 4, 000 steps and Standard and CoT methods for 1, 000 steps. We"},{"bbox":{"x0":213,"x1":1011,"y0":1117,"y1":1138},"font_size":-9.706e+21,"text":"fnd ReAct and Act methods generally beneft from more training steps (and more training data),ii"},{"bbox":{"x0":211,"x1":750,"y0":1135,"y1":1162},"font_size":-9.706e+21,"text":"while Standard and CoT methods degrade soon after fnetuning.i"}],"source":"layout det","text":"For all fnetuning we use a batch size of 64. On PaLM-8B, we fnetune ReAct and Act methodsii for 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we fnetunei ReAct and Act methods for 4, 000 steps and Standard and CoT methods for 1, 000 steps. We fnd ReAct and Act methods generally beneft from more training steps (and more training data),ii while Standard and CoT methods degrade soon after fnetuning.i"},{"bbox":{"x0":209,"x1":552,"y0":1258,"y1":1289},"conf":0.8627,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":211,"x1":547,"y0":1259,"y1":1284},"font_size":-9.706e+21,"text":"B.2ALFWORLD IM-STYLE DETAILS"}],"source":"layout det","text":"B.2ALFWORLD IM-STYLE DETAILS"},{"bbox":{"x0":208,"x1":1017,"y0":1328,"y1":1470},"conf":0.9661,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1332,"y1":1353},"font_size":-9.706e+21,"text":"For the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense"},{"bbox":{"x0":213,"x1":1011,"y0":1353,"y1":1376},"font_size":-9.706e+21,"text":"external feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)"},{"bbox":{"x0":213,"x1":1011,"y0":1376,"y1":1398},"font_size":-9.706e+21,"text":"decomposing the current goal and (2) the current subgoal that needs to be completed. In particular,"},{"bbox":{"x0":213,"x1":1009,"y0":1398,"y1":1419},"font_size":-9.706e+21,"text":"ReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the"},{"bbox":{"x0":213,"x1":1009,"y0":1421,"y1":1442},"font_size":-9.706e+21,"text":"next subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify"},{"bbox":{"x0":213,"x1":565,"y0":1442,"y1":1464},"font_size":-9.706e+21,"text":"where items can be within the environment."}],"source":"layout det","text":"For the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense external feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)decomposing the current goal and (2) the current subgoal that needs to be completed. In particular,ReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the next subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify where items can be within the environment."}],"formula_dets":[{"bbox":{"x0":405,"x1":441,"y0":195,"y1":205},"conf":0.6482,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":213,"x1":1015,"y0":158,"y1":476},"conf":0.9722,"label":"Figure","label_id":3},{"bbox":{"x0":209,"x1":1016,"y0":643,"y1":786},"conf":0.9691,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":1045,"y1":1167},"conf":0.9674,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":1017,"y0":1328,"y1":1470},"conf":0.9661,"label":"Text","label_id":1},{"bbox":{"x0":207,"x1":1018,"y0":487,"y1":564},"conf":0.9482,"label":"Figure caption","label_id":4},{"bbox":{"x0":209,"x1":494,"y0":881,"y1":914},"conf":0.8995,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":569,"y0":975,"y1":1008},"conf":0.8911,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":552,"y0":1258,"y1":1289},"conf":0.8627,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":593,"y0":49,"y1":81},"conf":0.6174,"label":"Abandon","label_id":2},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1528},"conf":0.5837,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1017,"y0":46,"y1":86},"conf":0.5516,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[625,1500],[625,1523],[599,1523]],"score":0.8161},{"poly":[[213,1442],[565,1442],[565,1464],[213,1464]],"score":0.8052},{"poly":[[213,1421],[1009,1421],[1009,1442],[213,1442]],"score":0.8447},{"poly":[[213,1398],[1009,1398],[1009,1419],[213,1419]],"score":0.8155},{"poly":[[213,1376],[1011,1376],[1011,1398],[213,1398]],"score":0.8293},{"poly":[[213,1353],[1011,1353],[1011,1376],[213,1376]],"score":0.7223},{"poly":[[213,1332],[1009,1332],[1009,1353],[213,1353]],"score":0.8193},{"poly":[[211,1259],[547,1261],[547,1284],[211,1282]],"score":0.7265},{"poly":[[211,1135],[750,1139],[750,1162],[211,1158]],"score":0.665},{"poly":[[213,1117],[1011,1117],[1011,1138],[213,1138]],"score":0.8298},{"poly":[[213,1094],[1009,1094],[1009,1115],[213,1115]],"score":0.8102},{"poly":[[211,1071],[1008,1071],[1008,1092],[211,1092]],"score":0.7875},{"poly":[[213,1049],[1009,1049],[1009,1072],[213,1072]],"score":0.7034},{"poly":[[213,980],[564,980],[564,1002],[213,1002]],"score":0.8056},{"poly":[[211,883],[491,884],[490,909],[211,907]],"score":0.6661},{"poly":[[213,757],[743,757],[743,779],[213,779]],"score":0.8397},{"poly":[[213,736],[1011,736],[1011,757],[213,757]],"score":0.8275},{"poly":[[211,711],[1009,713],[1009,736],[211,734]],"score":0.6943},{"poly":[[213,691],[1009,691],[1009,713],[213,713]],"score":0.7912},{"poly":[[213,670],[1009,670],[1009,691],[213,691]],"score":0.8404},{"poly":[[213,647],[1009,647],[1009,668],[213,668]],"score":0.7998},{"poly":[[213,535],[990,535],[990,558],[213,558]],"score":0.7075},{"poly":[[213,513],[1009,513],[1009,535],[213,535]],"score":0.8252},{"poly":[[213,492],[1008,492],[1008,513],[213,513]],"score":0.7928},{"poly":[[607,447],[629,447],[629,459],[607,459]],"score":0.6993},{"poly":[[266,444],[358,446],[357,462],[266,460]],"score":0.7255},{"poly":[[228,447],[269,447],[269,462],[228,462]],"score":0.8539},{"poly":[[549,441],[565,441],[565,459],[549,459]],"score":0.8697},{"poly":[[590,434],[639,434],[639,469],[590,469]],"score":0.7587},{"poly":[[294,437],[424,437],[424,449],[294,449]],"score":0.7347},{"poly":[[230,427],[294,427],[294,439],[230,439]],"score":0.7457},{"poly":[[693,419],[986,419],[986,436],[693,436]],"score":0.6788},{"poly":[[635,421],[693,421],[693,432],[635,432]],"score":0.665},{"poly":[[266,411],[569,413],[569,429],[266,427]],"score":0.6237},{"poly":[[230,414],[271,414],[271,429],[230,429]],"score":0.7021},{"poly":[[228,393],[341,393],[341,409],[228,409]],"score":0.7671},{"poly":[[614,381],[622,389],[614,397],[606,389]],"score":0.6665},{"poly":[[654,378],[886,378],[886,389],[654,389]],"score":0.7186},{"poly":[[271,369],[369,371],[369,388],[271,386]],"score":0.6069},{"poly":[[246,366],[255,375],[248,383],[239,374]],"score":0.6345},{"poly":[[269,365],[472,365],[472,376],[269,376]],"score":0.6595},{"poly":[[223,358],[273,358],[273,391],[223,391]],"score":0.7558},{"poly":[[226,350],[409,350],[409,366],[226,366]],"score":0.6061},{"poly":[[597,327],[637,327],[637,338],[597,338]],"score":0.7866},{"poly":[[590,289],[961,289],[961,323],[590,323]],"score":0.6233},{"poly":[[607,277],[625,277],[625,290],[607,290]],"score":0.6464},{"poly":[[594,271],[639,271],[639,297],[594,297]],"score":0.7288},{"poly":[[632,261],[780,261],[780,277],[632,277]],"score":0.7249},{"poly":[[595,259],[639,259],[639,277],[595,277]],"score":0.7392},{"poly":[[374,255],[428,250],[430,269],[375,273]],"score":0.7535},{"poly":[[595,239],[876,239],[876,256],[595,256]],"score":0.7082},{"poly":[[595,229],[790,229],[790,246],[595,246]],"score":0.6542},{"poly":[[225,221],[462,221],[462,243],[225,243]],"score":0.6821},{"poly":[[231,216],[239,216],[239,224],[231,224]],"score":0.7515},{"poly":[[880,208],[999,208],[999,224],[880,224]],"score":0.6856},{"poly":[[228,201],[570,200],[570,218],[228,219]],"score":0.6527},{"poly":[[497,193],[567,193],[567,205],[497,205]],"score":0.6206},{"poly":[[431,193],[499,193],[499,205],[431,205]],"score":0.6363},{"poly":[[230,193],[414,193],[414,205],[230,205]],"score":0.6841},{"poly":[[407,180],[562,180],[562,196],[407,196]],"score":0.6057},{"poly":[[713,165],[881,165],[881,182],[713,182]],"score":0.7885},{"poly":[[376,165],[426,165],[426,182],[376,182]],"score":0.9176},{"poly":[[213,51],[587,49],[587,73],[213,74]],"score":0.728}],"page_no":14,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":813,"x1":1009,"y0":1442,"y1":1462},"conf":0.7548,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":817,"x1":1004,"y0":1442,"y1":1459},"font_size":0.0,"text":"Continued on next page"}],"source":"layout det","text":""},{"bbox":{"x0":210,"x1":591,"y0":50,"y1":80},"conf":0.7031,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":596,"x1":626,"y0":1500,"y1":1526},"conf":0.6435,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1502,"y1":1523},"font_size":0.0,"text":"16"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":210,"x1":359,"y0":159,"y1":192},"conf":0.8673,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":356,"y0":163,"y1":186},"font_size":6.692e-10,"text":"CPROMPTS"}],"source":"layout det","text":"CPROMPTS"},{"bbox":{"x0":210,"x1":372,"y0":209,"y1":240},"conf":0.7827,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":211,"x1":368,"y0":210,"y1":233},"font_size":6.692e-10,"text":"C.1HOTPOTQA"}],"source":"layout det","text":"C.1HOTPOTQA"},{"bbox":{"x0":511,"x1":633,"y0":260,"y1":281},"conf":0.5836,"font_size":0.0,"label":"Table caption","label_id":6,"lines":[{"bbox":{"x0":512,"x1":629,"y0":261,"y1":277},"font_size":6.692e-10,"text":"Hotpot QA Prompts"}],"source":"layout det","text":"Hotpot QA Prompts"},{"bbox":{"x0":213,"x1":1017,"y0":273,"y1":1451},"conf":0.9709,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":227,"x1":295,"y0":290,"y1":304},"font_size":0.0,"text":"Original"},{"bbox":{"x0":227,"x1":296,"y0":307,"y1":321},"font_size":0.0,"text":"Question"},{"bbox":{"x0":359,"x1":957,"y0":306,"y1":322},"font_size":0.0,"text":"What is the elevation range for the area that the eastern sector of the"},{"bbox":{"x0":359,"x1":614,"y0":322,"y1":338},"font_size":0.0,"text":"Colorado orogeny extends into?"},{"bbox":{"x0":226,"x1":278,"y0":339,"y1":353},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":360,"x1":504,"y0":339,"y1":352},"font_size":0.0,"text":"1,800 to 7,000 ft"},{"bbox":{"x0":227,"x1":295,"y0":370,"y1":383},"font_size":0.0,"text":"Question"},{"bbox":{"x0":356,"x1":964,"y0":365,"y1":388},"font_size":0.0,"text":"Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\""},{"bbox":{"x0":359,"x1":812,"y0":385,"y1":401},"font_size":0.0,"text":"character Milhouse, who Matt Groening named after who?"},{"bbox":{"x0":226,"x1":278,"y0":403,"y1":417},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":359,"x1":472,"y0":402,"y1":417},"font_size":0.0,"text":"Richard Nixon"},{"bbox":{"x0":227,"x1":296,"y0":435,"y1":448},"font_size":0.0,"text":"Question"},{"bbox":{"x0":359,"x1":981,"y0":434,"y1":450},"font_size":0.0,"text":"Which documentary is about Finnish rock groups, Adam Clayton Powell or The"},{"bbox":{"x0":359,"x1":487,"y0":450,"y1":464},"font_size":0.0,"text":"Saimaa Gesture?"},{"bbox":{"x0":226,"x1":278,"y0":467,"y1":480},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":359,"x1":513,"y0":466,"y1":480},"font_size":0.0,"text":"The Saimaa Gesture"},{"bbox":{"x0":227,"x1":295,"y0":498,"y1":511},"font_size":0.0,"text":"Question"},{"bbox":{"x0":359,"x1":896,"y0":498,"y1":514},"font_size":0.0,"text":"What profession does Nicholas Ray and Elia Kazan have in common?"},{"bbox":{"x0":226,"x1":278,"y0":515,"y1":528},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":360,"x1":604,"y0":516,"y1":528},"font_size":0.0,"text":"director, screenwriter, actor"},{"bbox":{"x0":227,"x1":296,"y0":547,"y1":560},"font_size":0.0,"text":"Question"},{"bbox":{"x0":360,"x1":945,"y0":547,"y1":559},"font_size":0.0,"text":"Which magazine was started first Arthur’s Magazine or First for Women?"},{"bbox":{"x0":226,"x1":278,"y0":563,"y1":576},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":359,"x1":507,"y0":561,"y1":577},"font_size":0.0,"text":"Arthur’s Magazine"},{"bbox":{"x0":227,"x1":296,"y0":594,"y1":608},"font_size":0.0,"text":"Question"},{"bbox":{"x0":357,"x1":930,"y0":593,"y1":609},"font_size":0.0,"text":"Were Pavel Urysohn and Leonid Levin known for the same type of work?"},{"bbox":{"x0":227,"x1":278,"y0":610,"y1":623},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":357,"x1":389,"y0":608,"y1":626},"font_size":0.0,"text":"Yes"},{"bbox":{"x0":224,"x1":254,"y0":634,"y1":652},"font_size":0.0,"text":"Act"},{"bbox":{"x0":226,"x1":296,"y0":652,"y1":666},"font_size":0.0,"text":"Question"},{"bbox":{"x0":359,"x1":957,"y0":651,"y1":667},"font_size":0.0,"text":"What is the elevation range for the area that the eastern sector of the"},{"bbox":{"x0":359,"x1":614,"y0":667,"y1":683},"font_size":0.0,"text":"Colorado orogeny extends into?"},{"bbox":{"x0":224,"x1":296,"y0":681,"y1":698},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":359,"x1":560,"y0":682,"y1":699},"font_size":0.0,"text":"Search[Colorado orogeny]"},{"bbox":{"x0":227,"x1":337,"y0":700,"y1":712},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":359,"x1":964,"y0":699,"y1":715},"font_size":0.0,"text":"The Colorado orogeny was an episode of mountain building (an orogeny) in"},{"bbox":{"x0":357,"x1":624,"y0":713,"y1":731},"font_size":0.0,"text":"Colorado and surrounding areas."},{"bbox":{"x0":226,"x1":297,"y0":732,"y1":744},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":545,"y0":729,"y1":747},"font_size":0.0,"text":"Lookup[eastern sector]"},{"bbox":{"x0":226,"x1":340,"y0":747,"y1":763},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":360,"x1":998,"y0":747,"y1":763},"font_size":0.0,"text":"(Result 1 / 1) The eastern sector extends into the High Plains and is called"},{"bbox":{"x0":357,"x1":586,"y0":760,"y1":780},"font_size":0.0,"text":"the Central Plains orogeny."},{"bbox":{"x0":226,"x1":297,"y0":780,"y1":792},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":357,"x1":521,"y0":779,"y1":795},"font_size":0.0,"text":"Search[High Plains]"},{"bbox":{"x0":226,"x1":340,"y0":795,"y1":810},"font_size":0.0,"text":"Observation 3"},{"bbox":{"x0":357,"x1":821,"y0":795,"y1":810},"font_size":0.0,"text":"High Plains refers to one of two distinct land regions:"},{"bbox":{"x0":227,"x1":297,"y0":812,"y1":825},"font_size":0.0,"text":"Action 4"},{"bbox":{"x0":357,"x1":654,"y0":809,"y1":826},"font_size":0.0,"text":"Search[High Plains (United States)]"},{"bbox":{"x0":226,"x1":340,"y0":826,"y1":842},"font_size":0.0,"text":"Observation 4"},{"bbox":{"x0":356,"x1":998,"y0":825,"y1":842},"font_size":0.0,"text":"The High Plains are a subregion of the Great Plains.From east to west, the"},{"bbox":{"x0":360,"x1":969,"y0":845,"y1":857},"font_size":0.0,"text":"High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130"},{"bbox":{"x0":356,"x1":413,"y0":857,"y1":877},"font_size":0.0,"text":"m).[3]"},{"bbox":{"x0":226,"x1":296,"y0":875,"y1":889},"font_size":0.0,"text":"Action 5"},{"bbox":{"x0":357,"x1":571,"y0":874,"y1":890},"font_size":0.0,"text":"Finish[1,800 to 7,000 ft]"},{"bbox":{"x0":227,"x1":296,"y0":909,"y1":922},"font_size":0.0,"text":"Question"},{"bbox":{"x0":356,"x1":963,"y0":905,"y1":923},"font_size":0.0,"text":"Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\""},{"bbox":{"x0":357,"x1":815,"y0":921,"y1":938},"font_size":0.0,"text":"character Milhouse, who Matt Groening named after who?"},{"bbox":{"x0":224,"x1":296,"y0":937,"y1":952},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":359,"x1":493,"y0":939,"y1":952},"font_size":0.0,"text":"Search[Milhouse]"},{"bbox":{"x0":227,"x1":337,"y0":955,"y1":968},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":357,"x1":982,"y0":954,"y1":970},"font_size":0.0,"text":"Milhouse Mussolini Van Houten is a recurring character in the Fox animated"},{"bbox":{"x0":357,"x1":978,"y0":969,"y1":985},"font_size":0.0,"text":"television series The Simpsons voiced by Pamela Hayden and created by Matt"},{"bbox":{"x0":359,"x1":437,"y0":986,"y1":1001},"font_size":0.0,"text":"Groening."},{"bbox":{"x0":226,"x1":296,"y0":1002,"y1":1015},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":357,"x1":520,"y0":1002,"y1":1018},"font_size":0.0,"text":"Lookup[named after]"},{"bbox":{"x0":226,"x1":340,"y0":1018,"y1":1034},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":360,"x1":989,"y0":1018,"y1":1034},"font_size":0.0,"text":"(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose"},{"bbox":{"x0":356,"x1":564,"y0":1033,"y1":1050},"font_size":0.0,"text":"middle name was Milhous."},{"bbox":{"x0":226,"x1":297,"y0":1051,"y1":1063},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":357,"x1":536,"y0":1048,"y1":1066},"font_size":0.0,"text":"Finish[Richard Nixon]"},{"bbox":{"x0":227,"x1":296,"y0":1083,"y1":1096},"font_size":0.0,"text":"Question"},{"bbox":{"x0":357,"x1":981,"y0":1082,"y1":1098},"font_size":0.0,"text":"Which documentary is about Finnish rock groups, Adam Clayton Powell or The"},{"bbox":{"x0":357,"x1":487,"y0":1098,"y1":1112},"font_size":0.0,"text":"Saimaa Gesture?"},{"bbox":{"x0":226,"x1":296,"y0":1115,"y1":1128},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":359,"x1":587,"y0":1114,"y1":1130},"font_size":0.0,"text":"Search[Adam Clayton Powell]"},{"bbox":{"x0":226,"x1":339,"y0":1130,"y1":1145},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":357,"x1":689,"y0":1130,"y1":1145},"font_size":0.0,"text":"Could not find [Adam Clayton Powell]."},{"bbox":{"x0":681,"x1":757,"y0":1129,"y1":1144},"font_size":0.0,"text":"Similar:"},{"bbox":{"x0":767,"x1":947,"y0":1128,"y1":1144},"font_size":0.0,"text":"[’Adam Clayton Powell"},{"bbox":{"x0":359,"x1":981,"y0":1145,"y1":1161},"font_size":0.0,"text":"III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office"},{"bbox":{"x0":359,"x1":979,"y0":1161,"y1":1179},"font_size":0.0,"text":"Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell"},{"bbox":{"x0":359,"x1":618,"y0":1177,"y1":1193},"font_size":0.0,"text":"(film)’, ’Giancarlo Esposito’]."},{"bbox":{"x0":227,"x1":296,"y0":1193,"y1":1207},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":644,"y0":1195,"y1":1207},"font_size":0.0,"text":"Search[Adam Clayton Powell (film)]"},{"bbox":{"x0":227,"x1":337,"y0":1210,"y1":1223},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":359,"x1":922,"y0":1209,"y1":1222},"font_size":0.0,"text":"Adam Clayton Powell is a 1989 American documentary film directed by"},{"bbox":{"x0":357,"x1":930,"y0":1225,"y1":1241},"font_size":0.0,"text":"Richard Kilberg.The film is about the rise and fall of influential"},{"bbox":{"x0":357,"x1":998,"y0":1241,"y1":1257},"font_size":0.0,"text":"African-American politician Adam Clayton Powell Jr.[3][4] It was later aired"},{"bbox":{"x0":357,"x1":780,"y0":1257,"y1":1273},"font_size":0.0,"text":"as part of the PBS series The American Experience."},{"bbox":{"x0":226,"x1":297,"y0":1273,"y1":1287},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":359,"x1":576,"y0":1274,"y1":1287},"font_size":0.0,"text":"Finish[The Saimaa Gesture]"},{"bbox":{"x0":227,"x1":295,"y0":1306,"y1":1320},"font_size":0.0,"text":"Question"},{"bbox":{"x0":359,"x1":896,"y0":1305,"y1":1321},"font_size":0.0,"text":"What profession does Nicholas Ray and Elia Kazan have in common?"},{"bbox":{"x0":227,"x1":296,"y0":1321,"y1":1334},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":359,"x1":528,"y0":1320,"y1":1337},"font_size":0.0,"text":"Search[Nicholas Ray]"},{"bbox":{"x0":226,"x1":340,"y0":1337,"y1":1353},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":357,"x1":979,"y0":1337,"y1":1353},"font_size":0.0,"text":"Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,"},{"bbox":{"x0":357,"x1":988,"y0":1352,"y1":1369},"font_size":0.0,"text":"1979) was an American film director, screenwriter, and actor best known for"},{"bbox":{"x0":357,"x1":663,"y0":1368,"y1":1385},"font_size":0.0,"text":"the 1955 film Rebel Without a Cause."},{"bbox":{"x0":227,"x1":297,"y0":1385,"y1":1398},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":511,"y0":1385,"y1":1400},"font_size":0.0,"text":"Search[Elia Kazan]"},{"bbox":{"x0":226,"x1":340,"y0":1401,"y1":1417},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":357,"x1":997,"y0":1399,"y1":1417},"font_size":0.0,"text":"Elia Kazan was an American film and theatre director, producer, screenwriter"},{"bbox":{"x0":360,"x1":445,"y0":1419,"y1":1431},"font_size":0.0,"text":"and actor."},{"bbox":{"x0":818,"x1":826,"y0":1446,"y1":1456},"font_size":6.0,"text":"C"},{"bbox":{"x0":826,"x1":834,"y0":1446,"y1":1456},"font_size":6.0,"text":"o"},{"bbox":{"x0":834,"x1":843,"y0":1446,"y1":1456},"font_size":6.0,"text":"n"},{"bbox":{"x0":843,"x1":851,"y0":1446,"y1":1456},"font_size":6.0,"text":"t"},{"bbox":{"x0":851,"x1":859,"y0":1446,"y1":1456},"font_size":6.0,"text":"i"},{"bbox":{"x0":859,"x1":868,"y0":1446,"y1":1456},"font_size":6.0,"text":"n"},{"bbox":{"x0":868,"x1":876,"y0":1446,"y1":1456},"font_size":6.0,"text":"u"},{"bbox":{"x0":876,"x1":885,"y0":1446,"y1":1456},"font_size":6.0,"text":"e"},{"bbox":{"x0":885,"x1":893,"y0":1446,"y1":1456},"font_size":6.0,"text":"d"},{"bbox":{"x0":893,"x1":901,"y0":1446,"y1":1456},"font_size":6.0,"text":" "},{"bbox":{"x0":901,"x1":910,"y0":1446,"y1":1456},"font_size":6.0,"text":"o"},{"bbox":{"x0":910,"x1":918,"y0":1446,"y1":1456},"font_size":6.0,"text":"n"},{"bbox":{"x0":918,"x1":926,"y0":1446,"y1":1456},"font_size":6.0,"text":" "},{"bbox":{"x0":926,"x1":935,"y0":1446,"y1":1456},"font_size":6.0,"text":"n"},{"bbox":{"x0":935,"x1":943,"y0":1446,"y1":1456},"font_size":6.0,"text":"e"},{"bbox":{"x0":943,"x1":951,"y0":1446,"y1":1456},"font_size":6.0,"text":"x"},{"bbox":{"x0":951,"x1":960,"y0":1446,"y1":1456},"font_size":6.0,"text":"t"},{"bbox":{"x0":960,"x1":968,"y0":1446,"y1":1456},"font_size":6.0,"text":" "},{"bbox":{"x0":968,"x1":977,"y0":1446,"y1":1456},"font_size":6.0,"text":"p"},{"bbox":{"x0":977,"x1":985,"y0":1446,"y1":1456},"font_size":6.0,"text":"a"},{"bbox":{"x0":985,"x1":993,"y0":1446,"y1":1456},"font_size":6.0,"text":"g"},{"bbox":{"x0":993,"x1":1002,"y0":1446,"y1":1456},"font_size":6.0,"text":"e"}],"source":"layout det","text":"<html><body><table><thead><tr><td></td><td></td></tr></thead><tbody><tr><td>Original Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Answer</td><td>1,800 to 7,000 ft</td></tr><tr><td>Question</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"</td></tr><tr><td>Answer</td><td>character Milhouse, who Matt Groening named after who? Richard Nixon</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The</td></tr><tr><td></td><td>Saimaa Gesture? The Saimaa Gesture</td></tr><tr><td>Answer</td><td></td></tr><tr><td>Question Answer</td><td>What profession does Nicholas Ray and Elia Kazan have in common? director, screenwriter, actor</td></tr><tr><td></td><td></td></tr><tr><td>Question Answer</td><td>Which magazine was started first Arthur’s Magazine or First for Women? Arthur’s Magazine</td></tr><tr><td></td><td></td></tr><tr><td>Question Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Yes</td></tr><tr><td>Act</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the</td></tr><tr><td>Action 1</td><td>Colorado orogeny extends into? Search[Colorado orogeny]</td></tr><tr><td>Observation 1</td><td>The Colorado orogeny was an episode of mountain building (an orogeny) in</td></tr><tr><td></td><td>Colorado and surrounding areas.</td></tr><tr><td>Action 2</td><td>Lookup[eastern sector] (Result 1 / 1) The eastern sector extends into the High Plains and is called</td></tr><tr><td>Observation 2</td><td>the Central Plains orogeny.</td></tr><tr><td>Action 3</td><td>Search[High Plains]</td></tr><tr><td>Observation 3</td><td>High Plains refers to one of two distinct land regions: Search[High Plains (United States)]</td></tr><tr><td>Action 4 Observation 4</td><td>The High Plains are a subregion of the Great Plains.From east to west, the</td></tr><tr><td></td><td>High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]</td></tr><tr><td>Action 5</td><td>Finish[1,800 to 7,000 ft]</td></tr><tr><td>Question</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?</td></tr><tr><td>Action 1</td><td>Search[Milhouse] Milhouse Mussolini Van Houten is a recurring character in the Fox animated</td></tr><tr><td>Observation 1</td><td>television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.</td></tr><tr><td>Action 2</td><td>Lookup[named after]</td></tr><tr><td>Observation 2</td><td>(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.</td></tr><tr><td>Action 3</td><td>Finish[Richard Nixon]</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The</td></tr><tr><td></td><td>Saimaa Gesture? Search[Adam Clayton Powell]</td></tr><tr><td>Action 1 Observation 1</td><td>Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office</td></tr><tr><td></td><td>Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell</td></tr><tr><td></td><td>(film)’, ’Giancarlo Esposito’]. Search[Adam Clayton Powell (film)]</td></tr><tr><td>Action 2 Observation 2</td><td>Adam Clayton Powell is a 1989 American documentary film directed by</td></tr><tr><td></td><td>Richard Kilberg.The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired</td></tr><tr><td></td><td>as part of the PBS series The American Experience.</td></tr><tr><td>Action 3</td><td>Finish[The Saimaa Gesture]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Question</td><td>What profession does Nicholas Ray and Elia Kazan have in common?</td></tr><tr><td></td><td></td></tr><tr><td>Action 1</td><td>Search[Nicholas Ray]</td></tr><tr><td></td><td></td></tr><tr><td>Observation 1</td><td></td></tr><tr><td></td><td>Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>1979) was an American film director, screenwriter, and actor best known for</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 2</td><td>the 1955 film Rebel Without a Cause.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Search[Elia Kazan]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Observation 2</td><td>Elia Kazan was an American film and theatre director, producer, screenwriter</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>and actor. C o n t i n u e d  o n  n e x t  p a g e</td></tr></table></body></html>"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":213,"x1":1017,"y0":273,"y1":1451},"conf":0.9709,"label":"Table","label_id":5},{"bbox":{"x0":210,"x1":359,"y0":159,"y1":192},"conf":0.8673,"label":"Title","label_id":0},{"bbox":{"x0":210,"x1":372,"y0":209,"y1":240},"conf":0.7827,"label":"Title","label_id":0},{"bbox":{"x0":813,"x1":1009,"y0":1442,"y1":1462},"conf":0.7548,"label":"Abandon","label_id":2},{"bbox":{"x0":210,"x1":591,"y0":50,"y1":80},"conf":0.7031,"label":"Abandon","label_id":2},{"bbox":{"x0":596,"x1":626,"y0":1500,"y1":1526},"conf":0.6435,"label":"Abandon","label_id":2},{"bbox":{"x0":511,"x1":633,"y0":260,"y1":281},"conf":0.5836,"label":"Table caption","label_id":6}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[625,1502],[625,1523],[599,1523]],"score":0.9256},{"poly":[[817,1442],[1004,1442],[1004,1459],[817,1459]],"score":0.779},{"poly":[[358,1416],[444,1416],[444,1432],[358,1432]],"score":0.9117},{"poly":[[356,1396],[998,1398],[998,1419],[356,1417]],"score":0.7121},{"poly":[[225,1399],[339,1399],[339,1416],[225,1416]],"score":0.8125},{"poly":[[358,1384],[514,1384],[514,1401],[358,1401]],"score":0.8073},{"poly":[[223,1385],[299,1381],[300,1399],[223,1403]],"score":0.7246},{"poly":[[358,1368],[662,1368],[662,1384],[358,1384]],"score":0.8157},{"poly":[[358,1351],[990,1351],[990,1368],[358,1368]],"score":0.7642},{"poly":[[356,1333],[980,1333],[980,1355],[356,1355]],"score":0.6487},{"poly":[[225,1335],[338,1335],[338,1351],[225,1351]],"score":0.8154},{"poly":[[358,1320],[529,1320],[529,1336],[358,1336]],"score":0.8141},{"poly":[[223,1320],[295,1316],[296,1335],[223,1338]],"score":0.8144},{"poly":[[358,1304],[896,1304],[896,1320],[358,1320]],"score":0.7521},{"poly":[[225,1304],[296,1304],[296,1320],[225,1320]],"score":0.8993},{"poly":[[358,1272],[577,1272],[577,1289],[358,1289]],"score":0.7669},{"poly":[[223,1272],[297,1269],[298,1287],[223,1291]],"score":0.8203},{"poly":[[358,1256],[782,1256],[782,1272],[358,1272]],"score":0.7333},{"poly":[[358,1237],[996,1237],[996,1254],[358,1254]],"score":0.6139},{"poly":[[356,1223],[930,1221],[930,1239],[356,1241]],"score":0.6767},{"poly":[[358,1208],[923,1208],[923,1224],[358,1224]],"score":0.7938},{"poly":[[226,1206],[339,1206],[339,1224],[226,1224]],"score":0.6789},{"poly":[[358,1191],[645,1191],[645,1208],[358,1208]],"score":0.7929},{"poly":[[225,1191],[296,1191],[296,1208],[225,1208]],"score":0.8804},{"poly":[[361,1178],[432,1178],[432,1190],[361,1190]],"score":0.9017},{"poly":[[428,1173],[620,1175],[620,1193],[427,1191]],"score":0.6844},{"poly":[[354,1157],[983,1158],[983,1180],[354,1178]],"score":0.7042},{"poly":[[359,1147],[404,1147],[404,1158],[359,1158]],"score":0.9236},{"poly":[[406,1142],[983,1142],[983,1163],[406,1163]],"score":0.6359},{"poly":[[768,1129],[946,1129],[946,1145],[768,1145]],"score":0.7982},{"poly":[[682,1129],[758,1129],[758,1145],[682,1145]],"score":0.7948},{"poly":[[358,1129],[682,1129],[682,1145],[358,1145]],"score":0.7838},{"poly":[[225,1129],[338,1129],[338,1145],[225,1145]],"score":0.8477},{"poly":[[359,1115],[585,1115],[585,1127],[359,1127]],"score":0.995},{"poly":[[223,1114],[295,1110],[296,1128],[223,1132]],"score":0.7136},{"poly":[[359,1099],[487,1099],[487,1110],[359,1110]],"score":0.9268},{"poly":[[358,1081],[981,1081],[981,1097],[358,1097]],"score":0.7952},{"poly":[[225,1081],[296,1081],[296,1097],[225,1097]],"score":0.9812},{"poly":[[358,1049],[536,1049],[536,1066],[358,1066]],"score":0.807},{"poly":[[225,1048],[298,1048],[298,1066],[225,1066]],"score":0.7786},{"poly":[[358,1033],[565,1033],[565,1049],[358,1049]],"score":0.7173},{"poly":[[359,1016],[990,1016],[990,1033],[359,1033]],"score":0.7875},{"poly":[[225,1016],[339,1016],[339,1033],[225,1033]],"score":0.8371},{"poly":[[358,1002],[521,1002],[521,1018],[358,1018]],"score":0.8104},{"poly":[[223,1002],[297,998],[298,1016],[223,1020]],"score":0.7277},{"poly":[[356,981],[436,985],[435,1003],[356,1000]],"score":0.7588},{"poly":[[358,967],[980,967],[980,983],[358,983]],"score":0.6755},{"poly":[[358,952],[981,952],[981,969],[358,969]],"score":0.753},{"poly":[[225,952],[339,952],[339,969],[225,969]],"score":0.8071},{"poly":[[361,939],[494,939],[494,950],[361,950]],"score":0.9891},{"poly":[[223,937],[295,934],[296,952],[223,956]],"score":0.7891},{"poly":[[358,921],[817,921],[817,937],[358,937]],"score":0.735},{"poly":[[358,904],[961,904],[961,921],[358,921]],"score":0.7278},{"poly":[[225,906],[296,906],[296,922],[225,922]],"score":0.9666},{"poly":[[358,873],[570,873],[570,889],[358,889]],"score":0.8303},{"poly":[[225,873],[298,873],[298,891],[225,891]],"score":0.7422},{"poly":[[358,858],[412,858],[412,874],[358,874]],"score":0.9653},{"poly":[[358,842],[973,842],[973,858],[358,858]],"score":0.7088},{"poly":[[803,823],[1000,825],[999,843],[803,841]],"score":0.6613},{"poly":[[356,823],[797,825],[797,843],[356,841]],"score":0.6814},{"poly":[[225,825],[339,825],[339,842],[225,842]],"score":0.8083},{"poly":[[358,810],[655,810],[655,827],[358,827]],"score":0.7728},{"poly":[[225,810],[298,810],[298,827],[225,827]],"score":0.8229},{"poly":[[358,794],[822,794],[822,810],[358,810]],"score":0.7404},{"poly":[[225,794],[341,794],[341,810],[225,810]],"score":0.8043},{"poly":[[358,777],[522,777],[522,794],[358,794]],"score":0.8257},{"poly":[[225,777],[298,777],[298,794],[225,794]],"score":0.9354},{"poly":[[358,762],[587,762],[587,779],[358,779]],"score":0.79},{"poly":[[479,746],[998,746],[998,762],[479,762]],"score":0.7934},{"poly":[[359,746],[486,746],[486,762],[359,762]],"score":0.7997},{"poly":[[223,742],[339,744],[339,762],[223,761]],"score":0.7307},{"poly":[[361,733],[544,733],[544,744],[361,744]],"score":0.9903},{"poly":[[225,729],[296,729],[296,746],[225,746]],"score":0.8654},{"poly":[[358,714],[622,714],[622,731],[358,731]],"score":0.7711},{"poly":[[358,698],[965,698],[965,714],[358,714]],"score":0.789},{"poly":[[225,698],[339,698],[339,714],[225,714]],"score":0.8224},{"poly":[[358,681],[562,681],[562,698],[358,698]],"score":0.8033},{"poly":[[225,681],[296,681],[296,698],[225,698]],"score":0.9359},{"poly":[[356,663],[617,663],[617,685],[356,685]],"score":0.6633},{"poly":[[358,650],[956,650],[956,667],[358,667]],"score":0.7585},{"poly":[[223,650],[297,646],[298,665],[223,669]],"score":0.8252},{"poly":[[225,634],[254,634],[254,652],[225,652]],"score":0.6727},{"poly":[[358,607],[389,607],[389,625],[358,625]],"score":0.8028},{"poly":[[224,605],[280,609],[278,627],[222,623]],"score":0.7829},{"poly":[[358,592],[930,592],[930,609],[358,609]],"score":0.7771},{"poly":[[225,592],[296,592],[296,609],[225,609]],"score":0.953},{"poly":[[358,559],[507,559],[507,576],[358,576]],"score":0.7658},{"poly":[[225,559],[281,559],[281,576],[225,576]],"score":0.9477},{"poly":[[225,544],[298,544],[298,561],[225,561]],"score":0.8901},{"poly":[[354,540],[948,541],[948,563],[354,561]],"score":0.7113},{"poly":[[439,513],[607,513],[607,530],[439,530]],"score":0.6716},{"poly":[[358,513],[444,517],[444,528],[357,525]],"score":0.7706},{"poly":[[225,513],[281,513],[281,530],[225,530]],"score":0.9615},{"poly":[[358,497],[896,497],[896,513],[358,513]],"score":0.7995},{"poly":[[225,497],[296,497],[296,513],[225,513]],"score":0.9656},{"poly":[[359,465],[516,465],[516,482],[359,482]],"score":0.7842},{"poly":[[225,465],[279,465],[279,482],[225,482]],"score":0.9718},{"poly":[[359,449],[489,449],[489,465],[359,465]],"score":0.8047},{"poly":[[358,432],[983,432],[983,449],[358,449]],"score":0.7879},{"poly":[[225,432],[296,432],[296,450],[225,450]],"score":0.7232},{"poly":[[359,403],[472,403],[472,414],[359,414]],"score":0.9864},{"poly":[[225,401],[279,401],[279,417],[225,417]],"score":0.9778},{"poly":[[358,384],[817,384],[817,401],[358,401]],"score":0.7512},{"poly":[[225,370],[296,370],[296,386],[225,386]],"score":0.9346},{"poly":[[358,368],[963,368],[963,384],[358,384]],"score":0.7966},{"poly":[[359,337],[504,337],[504,353],[359,353]],"score":0.802},{"poly":[[225,337],[279,337],[279,353],[225,353]],"score":0.9726},{"poly":[[359,323],[614,323],[614,335],[359,335]],"score":0.9459},{"poly":[[358,305],[956,305],[956,322],[358,322]],"score":0.7485},{"poly":[[225,305],[296,305],[296,322],[225,322]],"score":0.8824},{"poly":[[225,289],[296,289],[296,307],[225,307]],"score":0.9153},{"poly":[[512,261],[629,261],[629,277],[512,277]],"score":0.7609},{"poly":[[211,210],[368,210],[368,233],[211,233]],"score":0.6948},{"poly":[[213,163],[356,163],[356,186],[213,186]],"score":0.8402},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7206}],"page_no":15,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":815,"x1":1007,"y0":1462,"y1":1479},"conf":0.5666,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":815,"x1":1005,"y0":1459,"y1":1478},"font_size":0.0,"text":"Continued on next page"}],"source":"layout det","text":""},{"bbox":{"x0":211,"x1":591,"y0":51,"y1":78},"conf":0.5428,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":597,"x1":625,"y0":1501,"y1":1525},"conf":0.483,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":600,"x1":624,"y0":1502,"y1":1523},"font_size":0.0,"text":"17"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":216,"x1":1018,"y0":134,"y1":1468},"conf":0.8891,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":421,"x1":723,"y0":158,"y1":178},"font_size":0.0,"text":"Hotpot QA Prompts – Continued from previous page"},{"bbox":{"x0":224,"x1":297,"y0":203,"y1":219},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":358,"x1":669,"y0":202,"y1":219},"font_size":0.0,"text":"Finish[director, screenwriter, actor]"},{"bbox":{"x0":226,"x1":295,"y0":219,"y1":234},"font_size":0.0,"text":"Question"},{"bbox":{"x0":359,"x1":947,"y0":220,"y1":234},"font_size":0.0,"text":"Which magazine was started first Arthur’s Magazine or First for Women?"},{"bbox":{"x0":224,"x1":297,"y0":235,"y1":251},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":359,"x1":570,"y0":237,"y1":251},"font_size":0.0,"text":"Search[Arthur’s Magazine]"},{"bbox":{"x0":226,"x1":339,"y0":252,"y1":266},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":358,"x1":990,"y0":251,"y1":269},"font_size":0.0,"text":"Arthur’s Magazine (1844-1846) was an American literary periodical published"},{"bbox":{"x0":358,"x1":660,"y0":265,"y1":284},"font_size":0.0,"text":"in Philadelphia in the 19th century."},{"bbox":{"x0":221,"x1":298,"y0":278,"y1":301},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":358,"x1":552,"y0":281,"y1":298},"font_size":0.0,"text":"Search[First for Women]"},{"bbox":{"x0":223,"x1":341,"y0":297,"y1":315},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":357,"x1":990,"y0":297,"y1":316},"font_size":0.0,"text":"First for Women is a woman’s magazine published by Bauer Media Group in the"},{"bbox":{"x0":358,"x1":706,"y0":313,"y1":331},"font_size":0.0,"text":"USA.[1] The magazine was started in 1989."},{"bbox":{"x0":224,"x1":297,"y0":331,"y1":345},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":358,"x1":570,"y0":329,"y1":348},"font_size":0.0,"text":"Finish[Arthur’s Magazine]"},{"bbox":{"x0":226,"x1":297,"y0":362,"y1":377},"font_size":0.0,"text":"Question"},{"bbox":{"x0":358,"x1":932,"y0":362,"y1":380},"font_size":0.0,"text":"Were Pavel Urysohn and Leonid Levin known for the same type of work?"},{"bbox":{"x0":224,"x1":294,"y0":379,"y1":394},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":358,"x1":535,"y0":377,"y1":394},"font_size":0.0,"text":"Search[Pavel Urysohn]"},{"bbox":{"x0":226,"x1":339,"y0":395,"y1":409},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":358,"x1":989,"y0":394,"y1":412},"font_size":0.0,"text":"Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet"},{"bbox":{"x0":358,"x1":976,"y0":412,"y1":426},"font_size":0.0,"text":"mathematician who is best known for his contributions in dimension theory."},{"bbox":{"x0":223,"x1":297,"y0":424,"y1":443},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":528,"y0":427,"y1":441},"font_size":0.0,"text":"Search[Leonid Levin]"},{"bbox":{"x0":226,"x1":340,"y0":442,"y1":456},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":358,"x1":972,"y0":441,"y1":458},"font_size":0.0,"text":"Leonid Anatolievich Levin is a Soviet-American mathematician and computer"},{"bbox":{"x0":358,"x1":446,"y0":459,"y1":473},"font_size":0.0,"text":"scientist."},{"bbox":{"x0":226,"x1":297,"y0":474,"y1":490},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":358,"x1":450,"y0":473,"y1":490},"font_size":0.0,"text":"Finish[yes]"},{"bbox":{"x0":224,"x1":256,"y0":498,"y1":519},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":224,"x1":298,"y0":515,"y1":534},"font_size":0.0,"text":"Question"},{"bbox":{"x0":359,"x1":955,"y0":518,"y1":531},"font_size":0.0,"text":"What is the elevation range for the area that the eastern sector of the"},{"bbox":{"x0":358,"x1":616,"y0":531,"y1":548},"font_size":0.0,"text":"Colorado orogeny extends into?"},{"bbox":{"x0":223,"x1":289,"y0":545,"y1":568},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":358,"x1":973,"y0":548,"y1":566},"font_size":0.0,"text":"Let’s think step by step.The eastern sector of Colorado orogeny extends"},{"bbox":{"x0":358,"x1":554,"y0":563,"y1":581},"font_size":0.0,"text":"into the High Plains."},{"bbox":{"x0":547,"x1":973,"y0":565,"y1":579},"font_size":0.0,"text":"High Plains rise in elevation from around 1,800 to"},{"bbox":{"x0":361,"x1":737,"y0":581,"y1":595},"font_size":0.0,"text":"7,000 ft, so the answer is 1,800 to 7,000 ft."},{"bbox":{"x0":224,"x1":279,"y0":597,"y1":612},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":359,"x1":504,"y0":595,"y1":609},"font_size":0.0,"text":"1,800 to 7,000 ft"},{"bbox":{"x0":226,"x1":297,"y0":629,"y1":644},"font_size":0.0,"text":"Question"},{"bbox":{"x0":357,"x1":962,"y0":627,"y1":645},"font_size":0.0,"text":"Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\""},{"bbox":{"x0":358,"x1":819,"y0":643,"y1":661},"font_size":0.0,"text":"character Milhouse, who Matt Groening named after who?"},{"bbox":{"x0":223,"x1":289,"y0":656,"y1":679},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":359,"x1":980,"y0":661,"y1":675},"font_size":0.0,"text":"Let’s think step by step.Milhouse was named after U.S. president Richard"},{"bbox":{"x0":359,"x1":680,"y0":676,"y1":690},"font_size":0.0,"text":"Nixon, so the answer is Richard Nixon."},{"bbox":{"x0":226,"x1":279,"y0":693,"y1":708},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":358,"x1":472,"y0":693,"y1":707},"font_size":0.0,"text":"Richard Nixon"},{"bbox":{"x0":226,"x1":295,"y0":725,"y1":740},"font_size":0.0,"text":"Question"},{"bbox":{"x0":358,"x1":980,"y0":725,"y1":738},"font_size":0.0,"text":"Which documentary is about Finnish rock groups, Adam Clayton Powell or The"},{"bbox":{"x0":359,"x1":489,"y0":740,"y1":754},"font_size":0.0,"text":"Saimaa Gesture?"},{"bbox":{"x0":223,"x1":289,"y0":752,"y1":775},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":359,"x1":586,"y0":757,"y1":770},"font_size":0.0,"text":"Let’s think step by step."},{"bbox":{"x0":579,"x1":994,"y0":757,"y1":770},"font_size":0.0,"text":"Adam Clayton Powell (film) is a documentary about"},{"bbox":{"x0":357,"x1":997,"y0":769,"y1":790},"font_size":0.0,"text":"an African-American politician, not Finnish rock groups.So the documentary"},{"bbox":{"x0":358,"x1":870,"y0":787,"y1":805},"font_size":0.0,"text":"about Finnish rock groups must instead be The Saimaa Gesture."},{"bbox":{"x0":224,"x1":280,"y0":804,"y1":819},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":358,"x1":514,"y0":802,"y1":819},"font_size":0.0,"text":"The Saimaa Gesture"},{"bbox":{"x0":226,"x1":297,"y0":836,"y1":851},"font_size":0.0,"text":"Question"},{"bbox":{"x0":358,"x1":898,"y0":834,"y1":852},"font_size":0.0,"text":"What profession does Nicholas Ray and Elia Kazan have in common?"},{"bbox":{"x0":226,"x1":287,"y0":852,"y1":868},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":358,"x1":930,"y0":851,"y1":869},"font_size":0.0,"text":"Let’s think step by step.Professions of Nicholas Ray are director,"},{"bbox":{"x0":359,"x1":570,"y0":869,"y1":883},"font_size":0.0,"text":"screenwriter, and actor."},{"bbox":{"x0":572,"x1":987,"y0":866,"y1":883},"font_size":0.0,"text":"Professions of Elia Kazan are director, producer,"},{"bbox":{"x0":358,"x1":571,"y0":883,"y1":901},"font_size":0.0,"text":"screenwriter, and actor."},{"bbox":{"x0":572,"x1":992,"y0":883,"y1":901},"font_size":0.0,"text":"So profession Nicholas Ray and Elia Kazan have in"},{"bbox":{"x0":359,"x1":730,"y0":901,"y1":915},"font_size":0.0,"text":"common is director, screenwriter, and actor."},{"bbox":{"x0":224,"x1":280,"y0":915,"y1":930},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":358,"x1":606,"y0":914,"y1":930},"font_size":0.0,"text":"director, screenwriter, actor"},{"bbox":{"x0":226,"x1":297,"y0":947,"y1":962},"font_size":0.0,"text":"Question"},{"bbox":{"x0":358,"x1":948,"y0":946,"y1":964},"font_size":0.0,"text":"Which magazine was started first Arthur’s Magazine or First for Women?"},{"bbox":{"x0":223,"x1":289,"y0":959,"y1":982},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":359,"x1":964,"y0":964,"y1":977},"font_size":0.0,"text":"Let’s think step by step.Arthur’s Magazine was started in 1844.First"},{"bbox":{"x0":358,"x1":982,"y0":977,"y1":996},"font_size":0.0,"text":"for Women was started in 1989.1844 (Arthur’s Magazine) < 1989 (First for"},{"bbox":{"x0":357,"x1":755,"y0":994,"y1":1012},"font_size":0.0,"text":"Women), so Arthur’s Magazine was started first."},{"bbox":{"x0":226,"x1":279,"y0":1011,"y1":1026},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":358,"x1":506,"y0":1009,"y1":1026},"font_size":0.0,"text":"Arthur’s Magazine"},{"bbox":{"x0":226,"x1":297,"y0":1043,"y1":1058},"font_size":0.0,"text":"Question"},{"bbox":{"x0":357,"x1":930,"y0":1040,"y1":1059},"font_size":0.0,"text":"Were Pavel Urysohn and Leonid Levin known for the same type of work?"},{"bbox":{"x0":223,"x1":289,"y0":1055,"y1":1078},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":359,"x1":980,"y0":1059,"y1":1073},"font_size":0.0,"text":"Let’s think step by step.Pavel Urysohn is a mathematician.Leonid Levin"},{"bbox":{"x0":357,"x1":958,"y0":1072,"y1":1091},"font_size":0.0,"text":"is a mathematician and computer scientist.So Pavel Urysohn and Leonid"},{"bbox":{"x0":358,"x1":638,"y0":1090,"y1":1108},"font_size":0.0,"text":"Levin have the same type of work."},{"bbox":{"x0":226,"x1":279,"y0":1108,"y1":1122},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":358,"x1":389,"y0":1107,"y1":1123},"font_size":0.0,"text":"Yes"},{"bbox":{"x0":224,"x1":272,"y0":1130,"y1":1151},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":224,"x1":298,"y0":1147,"y1":1166},"font_size":0.0,"text":"Question"},{"bbox":{"x0":358,"x1":957,"y0":1147,"y1":1165},"font_size":0.0,"text":"What is the elevation range for the area that the eastern sector of the"},{"bbox":{"x0":358,"x1":617,"y0":1164,"y1":1182},"font_size":0.0,"text":"Colorado orogeny extends into?"},{"bbox":{"x0":226,"x1":305,"y0":1180,"y1":1196},"font_size":0.0,"text":"Thought 1"},{"bbox":{"x0":357,"x1":964,"y0":1178,"y1":1197},"font_size":0.0,"text":"I need to search Colorado orogeny, find the area that the eastern sector"},{"bbox":{"x0":358,"x1":982,"y0":1196,"y1":1214},"font_size":0.0,"text":"of the Colorado orogeny extends into, then find the elevation range of the"},{"bbox":{"x0":359,"x1":403,"y0":1214,"y1":1228},"font_size":0.0,"text":"area."},{"bbox":{"x0":224,"x1":295,"y0":1229,"y1":1243},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":358,"x1":561,"y0":1228,"y1":1244},"font_size":0.0,"text":"Search[Colorado orogeny]"},{"bbox":{"x0":224,"x1":340,"y0":1243,"y1":1261},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":357,"x1":965,"y0":1241,"y1":1262},"font_size":0.0,"text":"The Colorado orogeny was an episode of mountain building (an orogeny) in"},{"bbox":{"x0":358,"x1":620,"y0":1260,"y1":1276},"font_size":0.0,"text":"Colorado and surrounding areas."},{"bbox":{"x0":226,"x1":307,"y0":1276,"y1":1292},"font_size":0.0,"text":"Thought 2"},{"bbox":{"x0":358,"x1":705,"y0":1275,"y1":1293},"font_size":0.0,"text":"It does not mention the eastern sector."},{"bbox":{"x0":698,"x1":939,"y0":1278,"y1":1292},"font_size":0.0,"text":"So I need to look up eastern"},{"bbox":{"x0":359,"x1":421,"y0":1294,"y1":1308},"font_size":0.0,"text":"sector."},{"bbox":{"x0":224,"x1":297,"y0":1308,"y1":1322},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":358,"x1":546,"y0":1307,"y1":1325},"font_size":0.0,"text":"Lookup[eastern sector]"},{"bbox":{"x0":224,"x1":341,"y0":1322,"y1":1340},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":359,"x1":999,"y0":1322,"y1":1340},"font_size":0.0,"text":"(Result 1 / 1) The eastern sector extends into the High Plains and is called"},{"bbox":{"x0":358,"x1":586,"y0":1339,"y1":1355},"font_size":0.0,"text":"the Central Plains orogeny."},{"bbox":{"x0":226,"x1":307,"y0":1355,"y1":1371},"font_size":0.0,"text":"Thought 3"},{"bbox":{"x0":358,"x1":980,"y0":1354,"y1":1372},"font_size":0.0,"text":"The eastern sector of Colorado orogeny extends into the High Plains.So I"},{"bbox":{"x0":358,"x1":829,"y0":1369,"y1":1389},"font_size":0.0,"text":"need to search High Plains and find its elevation range."},{"bbox":{"x0":224,"x1":297,"y0":1387,"y1":1403},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":358,"x1":520,"y0":1386,"y1":1403},"font_size":0.0,"text":"Search[High Plains]"},{"bbox":{"x0":226,"x1":340,"y0":1404,"y1":1418},"font_size":0.0,"text":"Observation 3"},{"bbox":{"x0":358,"x1":813,"y0":1401,"y1":1421},"font_size":0.0,"text":"High Plains refers to one of two distinct land regions"},{"bbox":{"x0":224,"x1":305,"y0":1418,"y1":1435},"font_size":0.0,"text":"Thought 4"},{"bbox":{"x0":357,"x1":805,"y0":1417,"y1":1436},"font_size":0.0,"text":"I need to instead search High Plains (United States)."},{"bbox":{"x0":224,"x1":295,"y0":1436,"y1":1451},"font_size":0.0,"text":"Action 4"},{"bbox":{"x0":358,"x1":652,"y0":1433,"y1":1453},"font_size":0.0,"text":"Search[High Plains (United States)]"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Hotpot QA Prompts – Continued from previous page</td></tr></thead><tbody><tr><td colspan=\"2\"></td></tr><tr><td>Action 3 Question</td><td>Finish[director, screenwriter, actor] Which magazine was started first Arthur’s Magazine or First for Women?</td></tr><tr><td>Action 1</td><td>Search[Arthur’s Magazine]</td></tr><tr><td>Observation 1</td><td>Arthur’s Magazine (1844-1846) was an American literary periodical published</td></tr><tr><td></td><td>in Philadelphia in the 19th century. Search[First for Women]</td></tr><tr><td>Action 2 Observation 2</td><td>First for Women is a woman’s magazine published by Bauer Media Group in the</td></tr><tr><td></td><td>USA.[1] The magazine was started in 1989. Finish[Arthur’s Magazine]</td></tr><tr><td>Action 3</td><td></td></tr><tr><td>Question</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Search[Pavel Urysohn]</td></tr><tr><td>Action 1 Observation 1</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet</td></tr><tr><td></td><td>mathematician who is best known for his contributions in dimension theory.</td></tr><tr><td>Action 2</td><td>Search[Leonid Levin]</td></tr><tr><td>Observation 2</td><td>Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.</td></tr><tr><td>Action 3</td><td>Finish[yes]</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Thought</td><td>Let’s think step by step.The eastern sector of Colorado orogeny extends into the High Plains. High Plains rise in elevation from around 1,800 to</td></tr><tr><td></td><td>7,000 ft, so the answer is 1,800 to 7,000 ft. 1,800 to 7,000 ft</td></tr><tr><td>Answer</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"</td></tr><tr><td>Question</td><td>character Milhouse, who Matt Groening named after who?</td></tr><tr><td>Thought</td><td>Let’s think step by step.Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.</td></tr><tr><td>Answer</td><td>Richard Nixon</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td></tr><tr><td>Thought</td><td>Let’s think step by step. Adam Clayton Powell (film) is a documentary about</td></tr><tr><td></td><td>an African-American politician, not Finnish rock groups.So the documentary about Finnish rock groups must instead be The Saimaa Gesture.</td></tr><tr><td>Answer</td><td>The Saimaa Gesture</td></tr><tr><td>Question Thought</td><td>What profession does Nicholas Ray and Elia Kazan have in common? Let’s think step by step.Professions of Nicholas Ray are director, Professions of Elia Kazan are director, producer,</td></tr><tr><td></td><td>screenwriter, and actor. screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.</td></tr><tr><td>Answer</td><td>director, screenwriter, actor</td></tr><tr><td>Question</td><td>Which magazine was started first Arthur’s Magazine or First for Women? Let’s think step by step.Arthur’s Magazine was started in 1844.First</td></tr><tr><td>Thought</td><td>for Women was started in 1989.1844 (Arthur’s Magazine) < 1989 (First for</td></tr><tr><td></td><td>Women), so Arthur’s Magazine was started first. Arthur’s Magazine</td></tr><tr><td>Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work?</td></tr><tr><td>Question Thought</td><td>Let’s think step by step.Pavel Urysohn is a mathematician.Leonid Levin</td></tr><tr><td></td><td>is a mathematician and computer scientist.So Pavel Urysohn and Leonid Levin have the same type of work.</td></tr><tr><td>Answer</td><td>Yes</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Thought 1</td><td>I need to search Colorado orogeny, find the area that the eastern sector</td></tr><tr><td></td><td>of the Colorado orogeny extends into, then find the elevation range of the</td></tr><tr><td></td><td>area. Search[Colorado orogeny]</td></tr><tr><td>Action 1 Observation 1</td><td>The Colorado orogeny was an episode of mountain building (an orogeny) in</td></tr><tr><td></td><td>Colorado and surrounding areas. It does not mention the eastern sector.</td></tr><tr><td>Thought 2</td><td>So I need to look up eastern sector.</td></tr><tr><td>Action 2</td><td>Lookup[eastern sector]</td></tr><tr><td>Observation 2</td><td>(Result 1 / 1) The eastern sector extends into the High Plains and is called</td></tr><tr><td></td><td>the Central Plains orogeny.</td></tr><tr><td></td><td></td></tr><tr><td>Thought 3</td><td>The eastern sector of Colorado orogeny extends into the High Plains.So I</td></tr><tr><td></td><td></td></tr><tr><td></td><td>need to search High Plains and find its elevation range.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 3</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Search[High Plains]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Observation 3 Thought 4 Action 4</td><td>High Plains refers to one of two distinct land regions I need to instead search High Plains (United States). Search[High Plains (United States)]</td></tr></table></body></html>"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":216,"x1":1018,"y0":134,"y1":1468},"conf":0.8891,"label":"Table","label_id":5},{"bbox":{"x0":815,"x1":1007,"y0":1462,"y1":1479},"conf":0.5666,"label":"Abandon","label_id":2},{"bbox":{"x0":211,"x1":591,"y0":51,"y1":78},"conf":0.5428,"label":"Abandon","label_id":2},{"bbox":{"x0":597,"x1":625,"y0":1501,"y1":1525},"conf":0.483,"label":"Abandon","label_id":2},{"bbox":{"x0":418,"x1":724,"y0":157,"y1":179},"conf":0.2595,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[600,1502],[624,1502],[624,1523],[600,1523]],"score":0.8685},{"poly":[[815,1459],[1005,1460],[1004,1478],[815,1477]],"score":0.7125},{"poly":[[361,1437],[650,1437],[650,1449],[361,1449]],"score":0.9974},{"poly":[[223,1436],[297,1432],[298,1450],[223,1454]],"score":0.7412},{"poly":[[659,1417],[805,1419],[805,1436],[658,1434]],"score":0.7961},{"poly":[[358,1419],[667,1419],[667,1436],[358,1436]],"score":0.7887},{"poly":[[225,1417],[306,1417],[306,1436],[225,1436]],"score":0.7544},{"poly":[[358,1402],[815,1402],[815,1419],[358,1419]],"score":0.7013},{"poly":[[225,1402],[339,1402],[339,1419],[225,1419]],"score":0.8313},{"poly":[[358,1388],[521,1388],[521,1404],[358,1404]],"score":0.7773},{"poly":[[223,1388],[297,1384],[298,1402],[223,1406]],"score":0.7255},{"poly":[[358,1371],[828,1371],[828,1388],[358,1388]],"score":0.742},{"poly":[[358,1355],[983,1355],[983,1371],[358,1371]],"score":0.7376},{"poly":[[225,1355],[306,1355],[306,1371],[225,1371]],"score":0.8918},{"poly":[[356,1336],[585,1338],[585,1356],[356,1355]],"score":0.6731},{"poly":[[226,1325],[338,1325],[338,1336],[226,1336]],"score":0.99},{"poly":[[359,1307],[544,1307],[544,1323],[359,1323]],"score":0.7972},{"poly":[[223,1307],[295,1303],[296,1321],[223,1325]],"score":0.732},{"poly":[[358,1292],[419,1292],[419,1308],[358,1308]],"score":0.7803},{"poly":[[697,1275],[940,1275],[940,1292],[697,1292]],"score":0.7922},{"poly":[[359,1275],[702,1275],[702,1292],[359,1292]],"score":0.7469},{"poly":[[225,1275],[306,1275],[306,1292],[225,1292]],"score":0.877},{"poly":[[356,1257],[622,1259],[622,1277],[356,1275]],"score":0.7038},{"poly":[[358,1244],[965,1244],[965,1261],[358,1261]],"score":0.7468},{"poly":[[358,1226],[562,1226],[562,1242],[358,1242]],"score":0.6868},{"poly":[[223,1228],[295,1224],[296,1242],[223,1246]],"score":0.7765},{"poly":[[358,1213],[406,1213],[406,1229],[358,1229]],"score":0.7362},{"poly":[[358,1196],[983,1196],[983,1213],[358,1213]],"score":0.7494},{"poly":[[358,1178],[963,1178],[963,1195],[358,1195]],"score":0.6657},{"poly":[[223,1178],[306,1176],[306,1194],[223,1196]],"score":0.7618},{"poly":[[358,1148],[956,1148],[956,1165],[358,1165]],"score":0.7853},{"poly":[[226,1148],[298,1148],[298,1165],[226,1165]],"score":0.9156},{"poly":[[225,1132],[271,1132],[271,1150],[225,1150]],"score":0.8487},{"poly":[[358,1106],[391,1106],[391,1124],[358,1124]],"score":0.7415},{"poly":[[225,1106],[279,1106],[279,1122],[225,1122]],"score":0.9413},{"poly":[[358,1091],[639,1091],[639,1107],[358,1107]],"score":0.7503},{"poly":[[722,1074],[958,1074],[958,1091],[722,1091]],"score":0.7426},{"poly":[[358,1074],[728,1074],[728,1091],[358,1091]],"score":0.7146},{"poly":[[358,1058],[981,1058],[981,1074],[358,1074]],"score":0.7078},{"poly":[[225,1058],[289,1058],[289,1076],[225,1076]],"score":0.7782},{"poly":[[358,1040],[930,1040],[930,1056],[358,1056]],"score":0.6062},{"poly":[[225,1041],[296,1041],[296,1059],[225,1059]],"score":0.7876},{"poly":[[358,1010],[506,1010],[506,1026],[358,1026]],"score":0.7883},{"poly":[[225,1010],[279,1010],[279,1026],[225,1026]],"score":0.9782},{"poly":[[358,993],[755,993],[755,1010],[358,1010]],"score":0.775},{"poly":[[356,975],[981,977],[981,995],[356,993]],"score":0.628},{"poly":[[358,962],[965,962],[965,978],[358,978]],"score":0.7453},{"poly":[[225,962],[289,962],[289,978],[225,978]],"score":0.9516},{"poly":[[358,945],[948,945],[948,962],[358,962]],"score":0.7446},{"poly":[[225,945],[296,945],[296,964],[225,964]],"score":0.79},{"poly":[[358,914],[605,914],[605,931],[358,931]],"score":0.6802},{"poly":[[225,914],[279,914],[279,931],[225,931]],"score":0.9802},{"poly":[[359,899],[728,899],[728,916],[359,916]],"score":0.6767},{"poly":[[574,883],[991,883],[991,899],[574,899]],"score":0.7625},{"poly":[[472,883],[567,883],[567,899],[472,899]],"score":0.721},{"poly":[[358,883],[479,883],[479,899],[358,899]],"score":0.7237},{"poly":[[359,868],[569,868],[569,884],[359,884]],"score":0.771},{"poly":[[574,865],[988,866],[988,884],[574,883]],"score":0.6613},{"poly":[[574,848],[926,850],[926,868],[574,866]],"score":0.6496},{"poly":[[356,848],[582,851],[582,868],[356,865]],"score":0.736},{"poly":[[225,848],[288,852],[287,870],[224,866]],"score":0.8211},{"poly":[[358,835],[896,835],[896,851],[358,851]],"score":0.7413},{"poly":[[225,835],[298,835],[298,851],[225,851]],"score":0.9059},{"poly":[[356,802],[516,804],[515,820],[356,818]],"score":0.8098},{"poly":[[225,802],[279,802],[279,820],[225,820]],"score":0.8193},{"poly":[[358,787],[870,787],[870,804],[358,804]],"score":0.724},{"poly":[[358,771],[996,771],[996,787],[358,787]],"score":0.71},{"poly":[[580,752],[996,754],[996,772],[580,771]],"score":0.6807},{"poly":[[358,754],[582,754],[582,771],[358,771]],"score":0.7232},{"poly":[[225,754],[288,754],[288,772],[225,772]],"score":0.7827},{"poly":[[359,741],[487,741],[487,752],[359,752]],"score":0.9921},{"poly":[[358,723],[983,723],[983,739],[358,739]],"score":0.7546},{"poly":[[225,723],[296,723],[296,739],[225,739]],"score":0.931},{"poly":[[356,690],[474,691],[474,708],[356,706]],"score":0.8124},{"poly":[[225,691],[279,691],[279,708],[225,708]],"score":0.9661},{"poly":[[358,675],[682,675],[682,691],[358,691]],"score":0.7139},{"poly":[[358,658],[983,658],[983,675],[358,675]],"score":0.7526},{"poly":[[225,658],[288,658],[288,676],[225,676]],"score":0.7627},{"poly":[[358,644],[817,644],[817,660],[358,660]],"score":0.7622},{"poly":[[225,629],[296,629],[296,645],[225,645]],"score":0.9773},{"poly":[[356,625],[965,625],[965,647],[356,647]],"score":0.6181},{"poly":[[358,596],[506,596],[506,612],[358,612]],"score":0.7628},{"poly":[[225,596],[279,596],[279,612],[225,612]],"score":0.967},{"poly":[[358,579],[737,579],[737,596],[358,596]],"score":0.7976},{"poly":[[359,566],[973,566],[973,578],[359,578]],"score":0.9469},{"poly":[[580,548],[971,548],[971,564],[580,564]],"score":0.7561},{"poly":[[358,548],[579,548],[579,564],[358,564]],"score":0.7917},{"poly":[[225,548],[288,548],[288,564],[225,564]],"score":0.9565},{"poly":[[359,535],[615,535],[615,546],[359,546]],"score":0.9808},{"poly":[[358,516],[956,516],[956,533],[358,533]],"score":0.7502},{"poly":[[225,516],[296,516],[296,533],[225,533]],"score":0.9726},{"poly":[[225,498],[256,498],[256,518],[225,518]],"score":0.9496},{"poly":[[358,474],[451,474],[451,490],[358,490]],"score":0.9071},{"poly":[[225,474],[298,474],[298,490],[225,490]],"score":0.9528},{"poly":[[359,460],[442,460],[442,472],[359,472]],"score":0.9532},{"poly":[[356,441],[973,442],[973,459],[356,457]],"score":0.7793},{"poly":[[358,426],[531,426],[531,442],[358,442]],"score":0.7571},{"poly":[[225,426],[296,426],[296,442],[225,442]],"score":0.8368},{"poly":[[358,411],[978,411],[978,427],[358,427]],"score":0.7919},{"poly":[[358,394],[990,394],[990,411],[358,411]],"score":0.7932},{"poly":[[226,396],[338,396],[338,408],[226,408]],"score":0.9937},{"poly":[[358,378],[537,378],[537,394],[358,394]],"score":0.7919},{"poly":[[225,378],[296,378],[296,394],[225,394]],"score":0.9402},{"poly":[[358,361],[930,361],[930,378],[358,378]],"score":0.763},{"poly":[[223,362],[295,358],[296,376],[223,380]],"score":0.7582},{"poly":[[358,330],[567,330],[567,346],[358,346]],"score":0.7601},{"poly":[[225,328],[298,328],[298,346],[225,346]],"score":0.7794},{"poly":[[358,314],[705,314],[705,330],[358,330]],"score":0.7162},{"poly":[[358,299],[990,299],[990,315],[358,315]],"score":0.7559},{"poly":[[223,295],[339,297],[339,315],[223,313]],"score":0.7326},{"poly":[[358,280],[554,280],[554,297],[358,297]],"score":0.7847},{"poly":[[223,282],[295,279],[296,297],[223,301]],"score":0.7783},{"poly":[[358,266],[664,266],[664,282],[358,282]],"score":0.7657},{"poly":[[358,251],[990,251],[990,267],[358,267]],"score":0.78},{"poly":[[225,251],[338,251],[338,267],[225,267]],"score":0.7934},{"poly":[[356,233],[570,234],[570,251],[356,249]],"score":0.7461},{"poly":[[225,234],[296,234],[296,251],[225,251]],"score":0.8108},{"poly":[[358,218],[946,218],[946,234],[358,234]],"score":0.7713},{"poly":[[225,219],[296,219],[296,236],[225,236]],"score":0.9222},{"poly":[[358,203],[669,203],[669,219],[358,219]],"score":0.737},{"poly":[[223,203],[297,199],[298,218],[223,221]],"score":0.7833},{"poly":[[422,160],[720,160],[720,177],[422,177]],"score":0.807},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7021}],"page_no":16,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":417,"x1":726,"y0":156,"y1":181},"conf":0.473,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":422,"x1":720,"y0":160,"y1":177},"font_size":0.0,"text":"Hotpot QA Prompts – Continued from previous page"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1016,"y0":47,"y1":85},"conf":0.4179,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""},{"bbox":{"x0":597,"x1":1010,"y0":1486,"y1":1526},"conf":0.2641,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1502,"y1":1523},"font_size":0.0,"text":"18"},{"bbox":{"x0":815,"x1":1005,"y0":1487,"y1":1505},"font_size":0.0,"text":"Continued on next page"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":417,"x1":726,"y0":156,"y1":181},"conf":0.2617,"font_size":0.0,"label":"Title","label_id":0,"lines":[],"source":"layout det","text":""},{"bbox":{"x0":221,"x1":1004,"y0":199,"y1":250},"conf":0.7347,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":228,"x1":998,"y0":203,"y1":219},"font_size":2.05,"text":"Observation 4The High Plains are a subregion of the Great Plains.From east to west, the"},{"bbox":{"x0":358,"x1":973,"y0":219,"y1":236},"font_size":2.05,"text":"High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130"},{"bbox":{"x0":358,"x1":412,"y0":236,"y1":252},"font_size":2.05,"text":"m).[3]"}],"source":"layout det","text":"Observation 4The High Plains are a subregion of the Great Plains.From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]"},{"bbox":{"x0":222,"x1":989,"y0":252,"y1":283},"conf":0.4089,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":981,"y0":251,"y1":267},"font_size":2.05,"text":"Thought 5High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer"},{"bbox":{"x0":358,"x1":537,"y0":266,"y1":282},"font_size":2.05,"text":"is 1,800 to 7,000 ft."}],"source":"layout det","text":"Thought 5High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft."},{"bbox":{"x0":222,"x1":572,"y0":282,"y1":302},"conf":0.3709,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":570,"y0":280,"y1":299},"font_size":2.05,"text":"Action 5Finish[1,800 to 7,000 ft]"}],"source":"layout det","text":"Action 5Finish[1,800 to 7,000 ft]"},{"bbox":{"x0":222,"x1":933,"y0":316,"y1":346},"conf":0.2006,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":961,"y0":314,"y1":330},"font_size":2.05,"text":"QuestionMusician and satirist Allie Goertz wrote a song about the \"The Simps"},{"bbox":{"x0":359,"x1":815,"y0":330,"y1":346},"font_size":2.05,"text":"character Milhouse, who Matt Groening named after who?"}],"source":"layout det","text":"QuestionMusician and satirist Allie Goertz wrote a song about the \"The Simps character Milhouse, who Matt Groening named after who?"},{"bbox":{"x0":220,"x1":996,"y0":348,"y1":380},"conf":0.2529,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":991,"y0":343,"y1":365},"font_size":2.05,"text":"Thought 1The question simplifies to \"The Simpsons\" character Milhouse is named after"},{"bbox":{"x0":358,"x1":928,"y0":361,"y1":378},"font_size":2.05,"text":"who.I only need to search Milhouse and find who it is named after."}],"source":"layout det","text":"Thought 1The question simplifies to \"The Simpsons\" character Milhouse is named after who.I only need to search Milhouse and find who it is named after."},{"bbox":{"x0":222,"x1":989,"y0":395,"y1":441},"conf":0.2456,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":227,"x1":980,"y0":394,"y1":411},"font_size":2.05,"text":"Observation 1Milhouse Mussolini Van Houten is a recurring character in the Fox animated"},{"bbox":{"x0":359,"x1":978,"y0":412,"y1":424},"font_size":2.05,"text":"television series The Simpsons voiced by Pamela Hayden and created by Matt"},{"bbox":{"x0":356,"x1":438,"y0":422,"y1":444},"font_size":2.05,"text":"Groening."}],"source":"layout det","text":"Observation 1Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening."},{"bbox":{"x0":222,"x1":1001,"y0":443,"y1":476},"conf":0.3168,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":998,"y0":442,"y1":459},"font_size":2.05,"text":"Thought 2The paragraph does not tell who Milhouse is named after, maybe I can look up"},{"bbox":{"x0":359,"x1":474,"y0":455,"y1":472},"font_size":2.05,"text":"\"named after\"."}],"source":"layout det","text":"Thought 2The paragraph does not tell who Milhouse is named after, maybe I can look up\"named after\"."},{"bbox":{"x0":221,"x1":996,"y0":490,"y1":522},"conf":0.2018,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":988,"y0":490,"y1":507},"font_size":2.05,"text":"Observation 2(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose"},{"bbox":{"x0":356,"x1":567,"y0":505,"y1":523},"font_size":2.05,"text":"middle name was Milhous."}],"source":"layout det","text":"Observation 2(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous."},{"bbox":{"x0":229,"x1":968,"y0":523,"y1":553},"conf":0.4724,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":956,"y0":521,"y1":540},"font_size":2.05,"text":"Thought 3Milhouse was named after U.S. president Richard Nixon, so the answer is"},{"bbox":{"x0":358,"x1":479,"y0":536,"y1":553},"font_size":2.05,"text":"Richard Nixon."}],"source":"layout det","text":"Thought 3Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon."},{"bbox":{"x0":254,"x1":993,"y0":582,"y1":616},"conf":0.4179,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":981,"y0":586,"y1":602},"font_size":2.05,"text":"stionWhich documentary is about Finnish rock groups, Adam Clayton Powell or The"},{"bbox":{"x0":359,"x1":489,"y0":601,"y1":617},"font_size":2.05,"text":"Saimaa Gesture?"}],"source":"layout det","text":"stionWhich documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?"},{"bbox":{"x0":253,"x1":995,"y0":618,"y1":649},"conf":0.3628,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":990,"y0":617,"y1":634},"font_size":2.05,"text":"ught 1I need to search Adam Clayton Powell and The Saimaa Gesture, and find which"},{"bbox":{"x0":359,"x1":703,"y0":634,"y1":650},"font_size":2.05,"text":"documentary is about Finnish rock groups."}],"source":"layout det","text":"ught 1I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups."},{"bbox":{"x0":220,"x1":986,"y0":667,"y1":730},"conf":0.5012,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":946,"y0":665,"y1":681},"font_size":2.05,"text":"Observation 1Could not find [Adam Clayton Powell].Similar:[’Adam Clayton Powell"},{"bbox":{"x0":358,"x1":981,"y0":680,"y1":698},"font_size":2.05,"text":"III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office"},{"bbox":{"x0":356,"x1":983,"y0":695,"y1":716},"font_size":2.05,"text":"Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell"},{"bbox":{"x0":359,"x1":619,"y0":713,"y1":729},"font_size":2.05,"text":"(film)’, ’Giancarlo Esposito’]."}],"source":"layout det","text":"Observation 1Could not find [Adam Clayton Powell].Similar:[’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell(film)’, ’Giancarlo Esposito’]."},{"bbox":{"x0":222,"x1":648,"y0":740,"y1":761},"conf":0.2579,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":644,"y0":744,"y1":761},"font_size":2.05,"text":"Action 2Search[Adam Clayton Powell (film)]"}],"source":"layout det","text":"Action 2Search[Adam Clayton Powell (film)]"},{"bbox":{"x0":222,"x1":933,"y0":762,"y1":780},"conf":0.3401,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":921,"y0":761,"y1":777},"font_size":2.05,"text":"Observation 2Adam Clayton Powell is a 1989 American documentary film directed by"}],"source":"layout det","text":"Observation 2Adam Clayton Powell is a 1989 American documentary film directed by"},{"bbox":{"x0":223,"x1":1001,"y0":792,"y1":825},"conf":0.4527,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":358,"x1":998,"y0":792,"y1":808},"font_size":2.05,"text":"African-American politician Adam Clayton Powell Jr.[3][4] It was later aired"},{"bbox":{"x0":358,"x1":780,"y0":808,"y1":825},"font_size":2.05,"text":"as part of the PBS series The American Experience."}],"source":"layout det","text":"African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience."},{"bbox":{"x0":229,"x1":949,"y0":825,"y1":841},"conf":0.3313,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":940,"y0":823,"y1":840},"font_size":2.05,"text":"Thought 3Adam Clayton Powell (film) is a documentary about an African-American"}],"source":"layout det","text":"Thought 3Adam Clayton Powell (film) is a documentary about an African-American"},{"bbox":{"x0":223,"x1":998,"y0":842,"y1":883},"conf":0.217,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":358,"x1":990,"y0":840,"y1":856},"font_size":2.05,"text":"politician, not Finnish rock groups.So the documentary about Finnish rock"},{"bbox":{"x0":358,"x1":713,"y0":856,"y1":873},"font_size":2.05,"text":"groups must instead be The Saimaa Gesture."},{"bbox":{"x0":225,"x1":579,"y0":871,"y1":889},"font_size":2.05,"text":"Action 3Finish[The Saimaa Gesture]"}],"source":"layout det","text":"politician, not Finnish rock groups.So the documentary about Finnish rock groups must instead be The Saimaa Gesture.Action 3Finish[The Saimaa Gesture]"},{"bbox":{"x0":221,"x1":898,"y0":901,"y1":921},"conf":0.2791,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":896,"y0":904,"y1":922},"font_size":2.05,"text":"QuestionWhat profession does Nicholas Ray and Elia Kazan have in common?"}],"source":"layout det","text":"QuestionWhat profession does Nicholas Ray and Elia Kazan have in common?"},{"bbox":{"x0":254,"x1":990,"y0":921,"y1":938},"conf":0.3382,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":253,"x1":983,"y0":916,"y1":939},"font_size":2.05,"text":"ught 1I need to search Nicholas Ray and Elia Kazan, find their professions, then"}],"source":"layout det","text":"ught 1I need to search Nicholas Ray and Elia Kazan, find their professions, then"},{"bbox":{"x0":226,"x1":532,"y0":953,"y1":968},"conf":0.2355,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":529,"y0":952,"y1":969},"font_size":2.05,"text":"Action 1Search[Nicholas Ray]"}],"source":"layout det","text":"Action 1Search[Nicholas Ray]"},{"bbox":{"x0":229,"x1":971,"y0":969,"y1":1016},"conf":0.2749,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":227,"x1":980,"y0":969,"y1":985},"font_size":2.05,"text":"Observation 1Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16"},{"bbox":{"x0":358,"x1":988,"y0":983,"y1":1000},"font_size":2.05,"text":"1979) was an American film director, screenwriter, and actor best known f"},{"bbox":{"x0":358,"x1":662,"y0":1000,"y1":1016},"font_size":2.05,"text":"the 1955 film Rebel Without a Cause."}],"source":"layout det","text":"Observation 1Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16 1979) was an American film director, screenwriter, and actor best known f the 1955 film Rebel Without a Cause."},{"bbox":{"x0":229,"x1":971,"y0":1017,"y1":1043},"conf":0.4128,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":981,"y0":1016,"y1":1033},"font_size":2.05,"text":"Thought 2Professions of Nicholas Ray are director, screenwriter, and actor.I nee"},{"bbox":{"x0":358,"x1":788,"y0":1031,"y1":1048},"font_size":2.05,"text":"to search Elia Kazan next and find his professions."}],"source":"layout det","text":"Thought 2Professions of Nicholas Ray are director, screenwriter, and actor.I nee to search Elia Kazan next and find his professions."},{"bbox":{"x0":229,"x1":971,"y0":1096,"y1":1111},"conf":0.212,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":224,"x1":980,"y0":1092,"y1":1114},"font_size":2.05,"text":"Thought 3Professions of Elia Kazan are director, producer, screenwriter, and actor"}],"source":"layout det","text":"Thought 3Professions of Elia Kazan are director, producer, screenwriter, and actor"},{"bbox":{"x0":222,"x1":673,"y0":1144,"y1":1163},"conf":0.2578,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":670,"y0":1142,"y1":1160},"font_size":2.05,"text":"Action 3Finish[director, screenwriter, actor]"}],"source":"layout det","text":"Action 3Finish[director, screenwriter, actor]"},{"bbox":{"x0":222,"x1":937,"y0":1411,"y1":1430},"conf":0.3426,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":225,"x1":931,"y0":1414,"y1":1431},"font_size":2.05,"text":"QuestionWere Pavel Urysohn and Leonid Levin known for the same type of work?"}],"source":"layout det","text":"QuestionWere Pavel Urysohn and Leonid Levin known for the same type of work?"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":221,"x1":1004,"y0":199,"y1":250},"conf":0.7347,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.5524,"label":"Abandon","label_id":2},{"bbox":{"x0":220,"x1":986,"y0":667,"y1":730},"conf":0.5012,"label":"Text","label_id":1},{"bbox":{"x0":417,"x1":726,"y0":156,"y1":181},"conf":0.473,"label":"Abandon","label_id":2},{"bbox":{"x0":229,"x1":968,"y0":523,"y1":553},"conf":0.4724,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":1001,"y0":792,"y1":825},"conf":0.4527,"label":"Text","label_id":1},{"bbox":{"x0":254,"x1":993,"y0":582,"y1":616},"conf":0.4179,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1016,"y0":47,"y1":85},"conf":0.4179,"label":"Abandon","label_id":2},{"bbox":{"x0":229,"x1":971,"y0":1017,"y1":1043},"conf":0.4128,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":989,"y0":252,"y1":283},"conf":0.4089,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":572,"y0":282,"y1":302},"conf":0.3709,"label":"Text","label_id":1},{"bbox":{"x0":253,"x1":995,"y0":618,"y1":649},"conf":0.3628,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":937,"y0":1411,"y1":1430},"conf":0.3426,"label":"Table footnote","label_id":7},{"bbox":{"x0":222,"x1":933,"y0":762,"y1":780},"conf":0.3401,"label":"Text","label_id":1},{"bbox":{"x0":254,"x1":990,"y0":921,"y1":938},"conf":0.3382,"label":"Text","label_id":1},{"bbox":{"x0":229,"x1":949,"y0":825,"y1":841},"conf":0.3313,"label":"Text","label_id":1},{"bbox":{"x0":229,"x1":970,"y0":839,"y1":859},"conf":0.3214,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":1001,"y0":443,"y1":476},"conf":0.3168,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":582,"y0":872,"y1":892},"conf":0.3164,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":973,"y0":310,"y1":337},"conf":0.3007,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":898,"y0":901,"y1":921},"conf":0.2791,"label":"Text","label_id":1},{"bbox":{"x0":229,"x1":971,"y0":969,"y1":1016},"conf":0.2749,"label":"Text","label_id":1},{"bbox":{"x0":596,"x1":626,"y0":1501,"y1":1526},"conf":0.2716,"label":"Abandon","label_id":2},{"bbox":{"x0":597,"x1":1010,"y0":1486,"y1":1526},"conf":0.2641,"label":"Abandon","label_id":2},{"bbox":{"x0":417,"x1":726,"y0":156,"y1":181},"conf":0.2617,"label":"Title","label_id":0},{"bbox":{"x0":229,"x1":899,"y0":901,"y1":921},"conf":0.2611,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":648,"y0":740,"y1":761},"conf":0.2579,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":673,"y0":1144,"y1":1163},"conf":0.2578,"label":"Text","label_id":1},{"bbox":{"x0":254,"x1":991,"y0":252,"y1":282},"conf":0.2539,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":996,"y0":348,"y1":380},"conf":0.2529,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":989,"y0":395,"y1":441},"conf":0.2456,"label":"Text","label_id":1},{"bbox":{"x0":226,"x1":532,"y0":953,"y1":968},"conf":0.2355,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":998,"y0":842,"y1":883},"conf":0.217,"label":"Text","label_id":1},{"bbox":{"x0":229,"x1":971,"y0":1096,"y1":1111},"conf":0.212,"label":"Text","label_id":1},{"bbox":{"x0":254,"x1":994,"y0":491,"y1":521},"conf":0.2116,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":996,"y0":490,"y1":522},"conf":0.2018,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":933,"y0":316,"y1":346},"conf":0.2006,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[625,1502],[625,1523],[599,1523]],"score":0.8792},{"poly":[[815,1487],[1005,1488],[1004,1505],[815,1503]],"score":0.7963},{"poly":[[359,1462],[536,1462],[536,1478],[359,1478]],"score":0.7995},{"poly":[[223,1462],[295,1458],[296,1476],[223,1480]],"score":0.76},{"poly":[[358,1445],[620,1445],[620,1462],[358,1462]],"score":0.7699},{"poly":[[358,1431],[980,1431],[980,1447],[358,1447]],"score":0.7667},{"poly":[[225,1427],[306,1431],[305,1449],[224,1445]],"score":0.7734},{"poly":[[358,1414],[931,1414],[931,1431],[358,1431]],"score":0.7437},{"poly":[[225,1414],[299,1414],[299,1431],[225,1431]],"score":0.9069},{"poly":[[356,1379],[571,1383],[570,1399],[356,1396]],"score":0.7446},{"poly":[[225,1381],[298,1381],[298,1398],[225,1398]],"score":0.8009},{"poly":[[358,1366],[788,1366],[788,1383],[358,1383]],"score":0.7524},{"poly":[[358,1350],[996,1350],[996,1366],[358,1366]],"score":0.721},{"poly":[[226,1350],[306,1350],[306,1366],[226,1366]],"score":0.9059},{"poly":[[358,1333],[707,1333],[707,1350],[358,1350]],"score":0.7779},{"poly":[[358,1318],[990,1318],[990,1335],[358,1335]],"score":0.7868},{"poly":[[223,1318],[338,1318],[338,1335],[223,1335]],"score":0.8334},{"poly":[[359,1305],[552,1305],[552,1317],[359,1317]],"score":0.9864},{"poly":[[225,1302],[296,1302],[296,1318],[225,1318]],"score":0.8067},{"poly":[[358,1287],[402,1287],[402,1304],[358,1304]],"score":0.9442},{"poly":[[358,1270],[963,1270],[963,1287],[358,1287]],"score":0.7455},{"poly":[[225,1270],[306,1270],[306,1287],[225,1287]],"score":0.8745},{"poly":[[356,1254],[665,1256],[665,1272],[356,1270]],"score":0.7827},{"poly":[[358,1239],[990,1239],[990,1256],[358,1256]],"score":0.8045},{"poly":[[225,1239],[339,1239],[339,1256],[225,1256]],"score":0.8003},{"poly":[[358,1223],[570,1223],[570,1239],[358,1239]],"score":0.7957},{"poly":[[225,1223],[296,1223],[296,1239],[225,1239]],"score":0.9326},{"poly":[[359,1206],[481,1206],[481,1223],[359,1223]],"score":0.7567},{"poly":[[225,1191],[304,1191],[304,1208],[225,1208]],"score":0.883},{"poly":[[356,1188],[983,1188],[983,1209],[356,1209]],"score":0.6167},{"poly":[[358,1173],[946,1173],[946,1190],[358,1190]],"score":0.7594},{"poly":[[225,1175],[298,1175],[298,1193],[225,1193]],"score":0.8044},{"poly":[[358,1143],[670,1143],[670,1160],[358,1160]],"score":0.7296},{"poly":[[225,1142],[298,1142],[298,1160],[225,1160]],"score":0.7526},{"poly":[[359,1127],[562,1127],[562,1143],[359,1143]],"score":0.6489},{"poly":[[358,1110],[938,1110],[938,1127],[358,1127]],"score":0.7053},{"poly":[[358,1096],[980,1096],[980,1112],[358,1112]],"score":0.7361},{"poly":[[224,1096],[305,1092],[306,1110],[225,1114]],"score":0.7547},{"poly":[[356,1077],[446,1079],[445,1096],[356,1094]],"score":0.7806},{"poly":[[358,1063],[998,1063],[998,1079],[358,1079]],"score":0.6949},{"poly":[[226,1063],[339,1063],[339,1079],[226,1079]],"score":0.8631},{"poly":[[358,1048],[514,1048],[514,1064],[358,1064]],"score":0.732},{"poly":[[223,1048],[295,1044],[296,1062],[223,1066]],"score":0.7403},{"poly":[[358,1031],[788,1031],[788,1048],[358,1048]],"score":0.7579},{"poly":[[358,1016],[981,1016],[981,1033],[358,1033]],"score":0.7544},{"poly":[[226,1016],[306,1016],[306,1033],[226,1033]],"score":0.9401},{"poly":[[358,1000],[662,1000],[662,1016],[358,1016]],"score":0.755},{"poly":[[358,983],[988,983],[988,1000],[358,1000]],"score":0.7864},{"poly":[[358,969],[980,969],[980,985],[358,985]],"score":0.7807},{"poly":[[358,952],[529,952],[529,969],[358,969]],"score":0.7646},{"poly":[[225,952],[296,952],[296,969],[225,969]],"score":0.8844},{"poly":[[359,936],[695,936],[695,952],[359,952]],"score":0.739},{"poly":[[356,917],[983,916],[983,937],[356,939]],"score":0.6543},{"poly":[[226,919],[304,919],[304,937],[226,937]],"score":0.7034},{"poly":[[358,904],[896,904],[896,921],[358,921]],"score":0.7626},{"poly":[[225,904],[298,904],[298,922],[225,922]],"score":0.7654},{"poly":[[358,873],[579,873],[579,889],[358,889]],"score":0.7911},{"poly":[[225,871],[298,871],[298,889],[225,889]],"score":0.7587},{"poly":[[358,856],[713,856],[713,873],[358,873]],"score":0.7407},{"poly":[[675,840],[990,840],[990,856],[675,856]],"score":0.7267},{"poly":[[358,840],[669,840],[669,856],[358,856]],"score":0.7838},{"poly":[[358,823],[940,823],[940,840],[358,840]],"score":0.7356},{"poly":[[226,823],[308,823],[308,840],[226,840]],"score":0.9353},{"poly":[[358,808],[780,808],[780,825],[358,825]],"score":0.7669},{"poly":[[358,792],[998,792],[998,808],[358,808]],"score":0.73},{"poly":[[358,777],[930,777],[930,794],[358,794]],"score":0.7647},{"poly":[[358,761],[921,761],[921,777],[358,777]],"score":0.7614},{"poly":[[223,761],[338,761],[338,777],[223,777]],"score":0.8339},{"poly":[[359,747],[644,747],[644,759],[359,759]],"score":0.9217},{"poly":[[225,744],[299,744],[299,761],[225,761]],"score":0.8933},{"poly":[[358,729],[903,729],[903,746],[358,746]],"score":0.7751},{"poly":[[226,728],[306,728],[306,746],[226,746]],"score":0.7186},{"poly":[[359,713],[619,713],[619,729],[359,729]],"score":0.7529},{"poly":[[356,695],[983,695],[983,716],[356,716]],"score":0.6203},{"poly":[[872,680],[981,682],[981,698],[871,696]],"score":0.7447},{"poly":[[407,681],[863,681],[863,698],[407,698]],"score":0.7848},{"poly":[[358,681],[406,681],[406,698],[358,698]],"score":0.7549},{"poly":[[753,665],[946,665],[946,681],[753,681]],"score":0.7131},{"poly":[[358,665],[760,665],[760,681],[358,681]],"score":0.7587},{"poly":[[225,665],[339,665],[339,681],[225,681]],"score":0.7981},{"poly":[[358,648],[589,648],[589,665],[358,665]],"score":0.6997},{"poly":[[225,648],[296,648],[296,665],[225,665]],"score":0.8655},{"poly":[[359,634],[703,634],[703,650],[359,650]],"score":0.7811},{"poly":[[358,617],[990,617],[990,634],[358,634]],"score":0.7417},{"poly":[[226,617],[304,617],[304,634],[226,634]],"score":0.8703},{"poly":[[359,601],[489,601],[489,617],[359,617]],"score":0.7018},{"poly":[[358,586],[981,586],[981,602],[358,602]],"score":0.7784},{"poly":[[226,586],[296,586],[296,602],[226,602]],"score":0.9736},{"poly":[[358,553],[537,553],[537,569],[358,569]],"score":0.7953},{"poly":[[225,553],[298,553],[298,569],[225,569]],"score":0.955},{"poly":[[358,536],[479,536],[479,553],[358,553]],"score":0.7595},{"poly":[[358,521],[956,521],[956,538],[358,538]],"score":0.7824},{"poly":[[226,521],[306,521],[306,540],[226,540]],"score":0.7588},{"poly":[[356,505],[567,507],[567,523],[356,521]],"score":0.7328},{"poly":[[359,490],[988,490],[988,507],[359,507]],"score":0.7588},{"poly":[[225,490],[339,490],[339,507],[225,507]],"score":0.8374},{"poly":[[358,474],[521,474],[521,490],[358,490]],"score":0.8161},{"poly":[[223,474],[297,470],[298,488],[223,492]],"score":0.7204},{"poly":[[359,455],[474,455],[474,472],[359,472]],"score":0.6049},{"poly":[[358,442],[998,442],[998,459],[358,459]],"score":0.7955},{"poly":[[226,442],[306,442],[306,459],[226,459]],"score":0.9528},{"poly":[[356,422],[438,426],[437,444],[356,440]],"score":0.7451},{"poly":[[359,412],[978,412],[978,424],[359,424]],"score":0.9905},{"poly":[[358,394],[980,394],[980,411],[358,411]],"score":0.7372},{"poly":[[358,378],[496,378],[496,394],[358,394]],"score":0.7664},{"poly":[[225,378],[296,378],[296,394],[225,394]],"score":0.915},{"poly":[[406,361],[928,361],[928,378],[406,378]],"score":0.6676},{"poly":[[358,361],[402,361],[402,378],[358,378]],"score":0.7784},{"poly":[[356,343],[991,343],[991,365],[356,365]],"score":0.6317},{"poly":[[226,345],[304,345],[304,363],[226,363]],"score":0.7329},{"poly":[[359,330],[815,330],[815,346],[359,346]],"score":0.7809},{"poly":[[358,314],[961,314],[961,330],[358,330]],"score":0.7264},{"poly":[[225,314],[298,314],[298,330],[225,330]],"score":0.9269},{"poly":[[358,280],[570,280],[570,297],[358,297]],"score":0.7954},{"poly":[[225,280],[298,280],[298,299],[225,299]],"score":0.8117},{"poly":[[358,266],[537,266],[537,282],[358,282]],"score":0.7217},{"poly":[[358,251],[981,251],[981,267],[358,267]],"score":0.7465},{"poly":[[226,251],[306,251],[306,267],[226,267]],"score":0.9596},{"poly":[[358,236],[412,236],[412,252],[358,252]],"score":0.7967},{"poly":[[358,219],[973,219],[973,236],[358,236]],"score":0.7517},{"poly":[[793,203],[998,203],[998,219],[793,219]],"score":0.6534},{"poly":[[358,203],[800,203],[800,219],[358,219]],"score":0.732},{"poly":[[228,205],[338,205],[338,216],[228,216]],"score":0.9823},{"poly":[[422,160],[720,160],[720,177],[422,177]],"score":0.8035},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.6937}],"page_no":17,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":626,"y0":1500,"y1":1526},"conf":0.6467,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":624,"y0":1502,"y1":1525},"font_size":0.0,"text":"19"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1016,"y0":48,"y1":84},"conf":0.2257,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":211,"x1":587,"y0":48,"y1":78},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":418,"x1":725,"y0":157,"y1":179},"conf":0.7357,"font_size":0.0,"label":"Table caption","label_id":6,"lines":[{"bbox":{"x0":421,"x1":722,"y0":157,"y1":180},"font_size":-1.729e-24,"text":"Hotpot QA Prompts – Continued from previous page"}],"source":"layout det","text":"Hotpot QA Prompts – Continued from previous page"},{"bbox":{"x0":212,"x1":1018,"y0":175,"y1":358},"conf":0.8907,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":227,"x1":339,"y0":188,"y1":202},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":357,"x1":989,"y0":187,"y1":204},"font_size":0.0,"text":"Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet"},{"bbox":{"x0":357,"x1":979,"y0":203,"y1":220},"font_size":0.0,"text":"mathematician who is best known for his contributions in dimension theory."},{"bbox":{"x0":225,"x1":307,"y0":219,"y1":235},"font_size":0.0,"text":"Thought 2"},{"bbox":{"x0":359,"x1":973,"y0":221,"y1":234},"font_size":0.0,"text":"Pavel Urysohn is a mathematician.I need to search Leonid Levin next and"},{"bbox":{"x0":357,"x1":545,"y0":235,"y1":252},"font_size":0.0,"text":"find its type of work."},{"bbox":{"x0":224,"x1":296,"y0":251,"y1":265},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":527,"y0":253,"y1":266},"font_size":0.0,"text":"Search[Leonid Levin]"},{"bbox":{"x0":226,"x1":339,"y0":267,"y1":281},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":359,"x1":972,"y0":267,"y1":281},"font_size":0.0,"text":"Leonid Anatolievich Levin is a Soviet-American mathematician and computer"},{"bbox":{"x0":357,"x1":446,"y0":281,"y1":299},"font_size":0.0,"text":"scientist."},{"bbox":{"x0":226,"x1":307,"y0":298,"y1":314},"font_size":0.0,"text":"Thought 3"},{"bbox":{"x0":359,"x1":972,"y0":300,"y1":313},"font_size":0.0,"text":"Leonid Levin is a mathematician and computer scientist.So Pavel Urysohn"},{"bbox":{"x0":357,"x1":730,"y0":314,"y1":331},"font_size":0.0,"text":"and Leonid Levin have the same type of work."},{"bbox":{"x0":224,"x1":299,"y0":330,"y1":347},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":356,"x1":452,"y0":329,"y1":349},"font_size":0.0,"text":"Finish[yes]"}],"source":"layout det","text":"<html><body><table><thead><tr><td>Observation 1</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet</td><td></td><td></td></tr></thead><tbody><tr><td>Thought 2</td><td>mathematician who is best known for his contributions in dimension theory. Pavel Urysohn is a mathematician.I need to search Leonid Levin next and</td><td></td><td></td></tr><tr><td></td><td>find its type of work.</td><td></td><td></td></tr><tr><td>Action 2 Observation 2</td><td>Search[Leonid Levin] Leonid Anatolievich Levin is a Soviet-American mathematician and computer</td><td></td><td></td></tr><tr><td></td><td>scientist.</td><td></td><td></td></tr><tr><td>Thought 3</td><td>and Leonid Levin have the same type of work.</td><td></td><td>Leonid Levin is a mathematician and computer scientist.So Pavel Urysohn</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Action 3</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Finish[yes]</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr></table></body></html>"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":212,"x1":1018,"y0":175,"y1":358},"conf":0.8907,"label":"Table","label_id":5},{"bbox":{"x0":418,"x1":725,"y0":157,"y1":179},"conf":0.7357,"label":"Table caption","label_id":6},{"bbox":{"x0":210,"x1":591,"y0":50,"y1":79},"conf":0.6946,"label":"Abandon","label_id":2},{"bbox":{"x0":596,"x1":626,"y0":1500,"y1":1526},"conf":0.6467,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":1016,"y0":48,"y1":84},"conf":0.2257,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[624,1502],[624,1525],[599,1525]],"score":0.8664},{"poly":[[356,327],[449,328],[449,347],[356,345]],"score":0.7246},{"poly":[[225,328],[296,328],[296,346],[225,346]],"score":0.7942},{"poly":[[356,312],[730,312],[730,333],[356,333]],"score":0.667},{"poly":[[833,299],[973,299],[973,315],[833,315]],"score":0.9188},{"poly":[[356,297],[828,297],[828,318],[356,318]],"score":0.6559},{"poly":[[226,299],[306,299],[306,315],[226,315]],"score":0.9429},{"poly":[[361,285],[441,285],[441,297],[361,297]],"score":0.9046},{"poly":[[356,264],[971,266],[971,284],[356,282]],"score":0.697},{"poly":[[225,266],[339,266],[339,282],[225,282]],"score":0.8021},{"poly":[[358,251],[529,251],[529,267],[358,267]],"score":0.7015},{"poly":[[223,251],[297,247],[298,265],[223,269]],"score":0.7406},{"poly":[[356,233],[544,234],[544,253],[356,251]],"score":0.6733},{"poly":[[650,223],[971,223],[971,234],[650,234]],"score":0.9753},{"poly":[[356,218],[642,218],[642,239],[356,239]],"score":0.6446},{"poly":[[226,219],[306,219],[306,236],[226,236]],"score":0.9295},{"poly":[[359,206],[976,206],[976,218],[359,218]],"score":0.8151},{"poly":[[356,185],[990,185],[990,206],[356,206]],"score":0.6185},{"poly":[[225,186],[338,186],[338,203],[225,203]],"score":0.8021},{"poly":[[421,157],[722,158],[722,180],[421,178]],"score":0.7666},{"poly":[[211,50],[587,48],[587,76],[211,78]],"score":0.6611}],"page_no":18,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1500,"y1":1526},"conf":0.6288,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":596,"x1":628,"y0":1497,"y1":1526},"font_size":0.0,"text":"20"}],"source":"layout det","text":""},{"bbox":{"x0":813,"x1":1010,"y0":1442,"y1":1462},"conf":0.5311,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":817,"x1":1004,"y0":1442,"y1":1459},"font_size":0.0,"text":"Continued on next page"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1017,"y0":47,"y1":85},"conf":0.225,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":210,"x1":330,"y0":161,"y1":192},"conf":0.7946,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":211,"x1":328,"y0":161,"y1":188},"font_size":-7.599e-27,"text":"C.2FEVER"}],"source":"layout det","text":"C.2FEVER"},{"bbox":{"x0":213,"x1":1022,"y0":239,"y1":1402},"conf":0.9077,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":522,"x1":621,"y0":239,"y1":254},"font_size":0.0,"text":"FEVER Prompts"},{"bbox":{"x0":226,"x1":295,"y0":262,"y1":279},"font_size":0.0,"text":"Original"},{"bbox":{"x0":359,"x1":973,"y0":263,"y1":279},"font_size":0.0,"text":"Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if"},{"bbox":{"x0":361,"x1":627,"y0":281,"y1":294},"font_size":0.0,"text":"there is NOT ENOUGH INFORMATION."},{"bbox":{"x0":225,"x1":273,"y0":326,"y1":343},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":356,"x1":887,"y0":325,"y1":344},"font_size":0.0,"text":"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company."},{"bbox":{"x0":224,"x1":278,"y0":343,"y1":360},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":360,"x1":428,"y0":344,"y1":358},"font_size":0.0,"text":"SUPPORTS"},{"bbox":{"x0":223,"x1":274,"y0":389,"y1":409},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":359,"x1":752,"y0":392,"y1":407},"font_size":0.0,"text":"Stranger Things is set in Bloomington, Indiana."},{"bbox":{"x0":224,"x1":280,"y0":405,"y1":425},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":359,"x1":422,"y0":407,"y1":421},"font_size":0.0,"text":"REFUTES"},{"bbox":{"x0":223,"x1":274,"y0":452,"y1":472},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":360,"x1":876,"y0":456,"y1":468},"font_size":0.0,"text":"Beautiful reached number two on the Billboard Hot 100 in 2003."},{"bbox":{"x0":225,"x1":278,"y0":472,"y1":485},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":357,"x1":489,"y0":470,"y1":486},"font_size":0.0,"text":"NOT ENOUGH INFO"},{"bbox":{"x0":225,"x1":255,"y0":496,"y1":515},"font_size":0.0,"text":"Act"},{"bbox":{"x0":359,"x1":973,"y0":497,"y1":513},"font_size":0.0,"text":"Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if"},{"bbox":{"x0":360,"x1":627,"y0":515,"y1":527},"font_size":0.0,"text":"there is NOT ENOUGH INFORMATION."},{"bbox":{"x0":226,"x1":272,"y0":561,"y1":575},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":356,"x1":887,"y0":559,"y1":577},"font_size":0.0,"text":"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company."},{"bbox":{"x0":226,"x1":288,"y0":577,"y1":590},"font_size":0.0,"text":"Action2"},{"bbox":{"x0":360,"x1":603,"y0":578,"y1":590},"font_size":0.0,"text":"Search[Nikolaj Coster-Waldau]"},{"bbox":{"x0":226,"x1":338,"y0":594,"y1":606},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":357,"x1":957,"y0":592,"y1":607},"font_size":0.0,"text":"Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and"},{"bbox":{"x0":356,"x1":981,"y0":607,"y1":625},"font_size":0.0,"text":"producer.He graduated from the Danish National School of Performing Arts"},{"bbox":{"x0":359,"x1":956,"y0":624,"y1":640},"font_size":0.0,"text":"in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with"},{"bbox":{"x0":357,"x1":973,"y0":639,"y1":657},"font_size":0.0,"text":"the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy"},{"bbox":{"x0":359,"x1":946,"y0":657,"y1":673},"font_size":0.0,"text":"drama series Game of Thrones, for which he received two Primetime Emmy"},{"bbox":{"x0":357,"x1":945,"y0":673,"y1":688},"font_size":0.0,"text":"Award nominations for Outstanding Supporting Actor in a Drama Series.."},{"bbox":{"x0":359,"x1":949,"y0":688,"y1":704},"font_size":0.0,"text":"Coster-Waldau has appeared in numerous films in his native Denmark and"},{"bbox":{"x0":360,"x1":970,"y0":707,"y1":719},"font_size":0.0,"text":"Scandinavia, including Headhunters (2011) and A Thousand Times Good Night"},{"bbox":{"x0":361,"x1":437,"y0":721,"y1":734},"font_size":0.0,"text":"(2013)."},{"bbox":{"x0":431,"x1":997,"y0":720,"y1":736},"font_size":0.0,"text":"In the U.S, his debut film role was in the war film Black Hawk Down"},{"bbox":{"x0":360,"x1":971,"y0":737,"y1":753},"font_size":0.0,"text":"(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a"},{"bbox":{"x0":359,"x1":999,"y0":753,"y1":768},"font_size":0.0,"text":"detective in the short-lived Fox television series New Amsterdam (2008), and"},{"bbox":{"x0":357,"x1":991,"y0":767,"y1":785},"font_size":0.0,"text":"appeared in the 2009 Fox television film Virtuality, originally intended as"},{"bbox":{"x0":356,"x1":428,"y0":782,"y1":801},"font_size":0.0,"text":"a pilot."},{"bbox":{"x0":226,"x1":296,"y0":800,"y1":813},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":357,"x1":492,"y0":797,"y1":814},"font_size":0.0,"text":"Finish[SUPPORTS]"},{"bbox":{"x0":226,"x1":272,"y0":848,"y1":862},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":359,"x1":752,"y0":847,"y1":863},"font_size":0.0,"text":"Stranger Things is set in Bloomington, Indiana."},{"bbox":{"x0":226,"x1":296,"y0":864,"y1":877},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":359,"x1":554,"y0":864,"y1":880},"font_size":0.0,"text":"Search[Stranger Things]"},{"bbox":{"x0":225,"x1":339,"y0":879,"y1":894},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":359,"x1":947,"y0":880,"y1":896},"font_size":0.0,"text":"Stranger Things is an American science fiction horror drama television"},{"bbox":{"x0":359,"x1":946,"y0":894,"y1":910},"font_size":0.0,"text":"series created by the Duffer Brothers.Set in the 1980s, primarily in"},{"bbox":{"x0":359,"x1":973,"y0":911,"y1":927},"font_size":0.0,"text":"the fictional town of Hawkins, Indiana, the series centers on a number of"},{"bbox":{"x0":357,"x1":997,"y0":928,"y1":944},"font_size":0.0,"text":"mysteries and supernatural events occurring around the town and their impact"},{"bbox":{"x0":357,"x1":739,"y0":944,"y1":960},"font_size":0.0,"text":"on an ensemble of child and adult characters."},{"bbox":{"x0":226,"x1":296,"y0":960,"y1":973},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":485,"y0":961,"y1":973},"font_size":0.0,"text":"Finish[REFUTES]"},{"bbox":{"x0":225,"x1":272,"y0":1008,"y1":1022},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":359,"x1":879,"y0":1007,"y1":1023},"font_size":0.0,"text":"Beautiful reached number two on the Billboard Hot 100 in 2003."},{"bbox":{"x0":225,"x1":296,"y0":1024,"y1":1037},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":360,"x1":502,"y0":1024,"y1":1036},"font_size":0.0,"text":"Search[Beautiful]"},{"bbox":{"x0":225,"x1":339,"y0":1039,"y1":1054},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":357,"x1":994,"y0":1039,"y1":1054},"font_size":0.0,"text":"Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’,"},{"bbox":{"x0":354,"x1":981,"y0":1051,"y1":1072},"font_size":0.0,"text":"’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is"},{"bbox":{"x0":357,"x1":460,"y0":1069,"y1":1087},"font_size":0.0,"text":"Beautiful’]."},{"bbox":{"x0":222,"x1":297,"y0":1083,"y1":1103},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":357,"x1":711,"y0":1085,"y1":1103},"font_size":0.0,"text":"Search[Beautiful (Christina Aguilera song)"},{"bbox":{"x0":225,"x1":339,"y0":1102,"y1":1117},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":359,"x1":998,"y0":1100,"y1":1119},"font_size":0.0,"text":"\"Beautiful\" is a song recorded by American singer Christina Aguilera for her"},{"bbox":{"x0":360,"x1":672,"y0":1117,"y1":1133},"font_size":0.0,"text":"fourth studio album, Stripped (2002)."},{"bbox":{"x0":222,"x1":297,"y0":1132,"y1":1151},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":359,"x1":572,"y0":1134,"y1":1150},"font_size":0.0,"text":"Lookup[Billboard Hot 100]"},{"bbox":{"x0":225,"x1":339,"y0":1150,"y1":1166},"font_size":0.0,"text":"Observation 3"},{"bbox":{"x0":360,"x1":998,"y0":1149,"y1":1167},"font_size":0.0,"text":"(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the"},{"bbox":{"x0":359,"x1":937,"y0":1167,"y1":1183},"font_size":0.0,"text":"United States, where it was certified Gold for 500,000 units shipped."},{"bbox":{"x0":222,"x1":297,"y0":1180,"y1":1200},"font_size":0.0,"text":"Action 4"},{"bbox":{"x0":359,"x1":551,"y0":1184,"y1":1196},"font_size":0.0,"text":"Finish[NOT ENOUGH INFO]"},{"bbox":{"x0":225,"x1":255,"y0":1208,"y1":1226},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":357,"x1":973,"y0":1208,"y1":1224},"font_size":0.0,"text":"Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if"},{"bbox":{"x0":360,"x1":628,"y0":1226,"y1":1238},"font_size":0.0,"text":"there is NOT ENOUGH INFORMATION."},{"bbox":{"x0":225,"x1":273,"y0":1271,"y1":1288},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":357,"x1":888,"y0":1272,"y1":1288},"font_size":0.0,"text":"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company."},{"bbox":{"x0":225,"x1":288,"y0":1288,"y1":1305},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":357,"x1":949,"y0":1288,"y1":1305},"font_size":0.0,"text":"Nikolaj William Coster-Waldau appeared in the 2009 Fox television film"},{"bbox":{"x0":356,"x1":887,"y0":1303,"y1":1322},"font_size":0.0,"text":"Virtuality, so he has worked with the Fox Broadcasting Company."},{"bbox":{"x0":226,"x1":278,"y0":1322,"y1":1335},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":360,"x1":428,"y0":1322,"y1":1334},"font_size":0.0,"text":"SUPPORTS"},{"bbox":{"x0":223,"x1":274,"y0":1365,"y1":1385},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":359,"x1":755,"y0":1369,"y1":1385},"font_size":0.0,"text":"Stranger Things is set in Bloomington, Indiana."},{"bbox":{"x0":224,"x1":289,"y0":1382,"y1":1402},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":357,"x1":932,"y0":1383,"y1":1401},"font_size":0.0,"text":"Stranger Things is in the fictional town of Hawkins, Indiana, not in"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">FEVER Prompts</td></tr></thead><tbody><tr><td>Original</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. SUPPORTS</td></tr><tr><td>Answer</td><td></td></tr><tr><td>Claim Answer</td><td>Stranger Things is set in Bloomington, Indiana. REFUTES</td></tr><tr><td>Claim</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Answer Act</td><td>NOT ENOUGH INFO Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if</td></tr><tr><td>Claim Action2 Observation 1</td><td>there is NOT ENOUGH INFORMATION. Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer.He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and</td></tr><tr><td></td><td>Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot. Finish[SUPPORTS]</td></tr><tr><td>Action 2 Claim</td><td>Stranger Things is set in Bloomington, Indiana. Search[Stranger Things]</td></tr><tr><td>Action 1 Observation 1</td><td>Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers.Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters.</td></tr><tr><td>Action 2</td><td>Finish[REFUTES]</td></tr><tr><td>Claim Action 1</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. Search[Beautiful] Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’,</td></tr><tr><td>Observation 1</td><td>’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is Beautiful’]. Search[Beautiful (Christina Aguilera song)</td></tr><tr><td>Action 2 Observation 2</td><td>\"Beautiful\" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).</td></tr><tr><td>Action 3</td><td>Lookup[Billboard Hot 100] (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the</td></tr><tr><td>Observation 3</td><td>United States, where it was certified Gold for 500,000 units shipped. Finish[NOT ENOUGH INFO]</td></tr><tr><td>Action 4</td><td></td></tr><tr><td>CoT</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.</td></tr><tr><td>Claim</td><td></td></tr><tr><td></td><td>Nikolaj William Coster-Waldau appeared in the 2009 Fox television film</td></tr><tr><td>Thought</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Virtuality, so he has worked with the Fox Broadcasting Company.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>SUPPORTS</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Answer</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Claim</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Stranger Things is set in Bloomington, Indiana.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Thought</td><td>Stranger Things is in the fictional town of Hawkins, Indiana, not in</td></tr></tbody></table></body></html>"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":213,"x1":1022,"y0":239,"y1":1402},"conf":0.9077,"label":"Table","label_id":5},{"bbox":{"x0":210,"x1":330,"y0":161,"y1":192},"conf":0.7946,"label":"Title","label_id":0},{"bbox":{"x0":210,"x1":591,"y0":49,"y1":80},"conf":0.6474,"label":"Abandon","label_id":2},{"bbox":{"x0":595,"x1":627,"y0":1500,"y1":1526},"conf":0.6288,"label":"Abandon","label_id":2},{"bbox":{"x0":813,"x1":1010,"y0":1442,"y1":1462},"conf":0.5311,"label":"Abandon","label_id":2},{"bbox":{"x0":519,"x1":624,"y0":233,"y1":254},"conf":0.2941,"label":"Table caption","label_id":6},{"bbox":{"x0":209,"x1":1017,"y0":47,"y1":85},"conf":0.225,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[600,1497],[628,1502],[624,1526],[596,1520]],"score":0.7255},{"poly":[[817,1442],[1004,1442],[1004,1459],[817,1459]],"score":0.7737},{"poly":[[223,1416],[279,1416],[279,1432],[223,1432]],"score":0.9494},{"poly":[[355,1412],[423,1416],[422,1434],[354,1430]],"score":0.7165},{"poly":[[356,1397],[539,1399],[539,1417],[356,1416]],"score":0.7126},{"poly":[[358,1384],[931,1384],[931,1401],[358,1401]],"score":0.7807},{"poly":[[225,1383],[289,1383],[289,1401],[225,1401]],"score":0.8104},{"poly":[[358,1368],[753,1368],[753,1384],[358,1384]],"score":0.7742},{"poly":[[225,1366],[273,1366],[273,1384],[225,1384]],"score":0.8644},{"poly":[[358,1320],[429,1320],[429,1336],[358,1336]],"score":0.8159},{"poly":[[225,1320],[279,1320],[279,1336],[225,1336]],"score":0.9757},{"poly":[[358,1289],[950,1289],[950,1305],[358,1305]],"score":0.7391},{"poly":[[226,1287],[288,1287],[288,1305],[226,1305]],"score":0.8215},{"poly":[[356,1270],[886,1272],[886,1289],[356,1287]],"score":0.7582},{"poly":[[225,1270],[274,1270],[274,1289],[225,1289]],"score":0.8148},{"poly":[[359,1226],[629,1226],[629,1237],[359,1237]],"score":0.9751},{"poly":[[358,1208],[973,1208],[973,1224],[358,1224]],"score":0.7948},{"poly":[[225,1206],[256,1206],[256,1226],[225,1226]],"score":0.8963},{"poly":[[358,1181],[552,1181],[552,1198],[358,1198]],"score":0.7602},{"poly":[[225,1181],[296,1181],[296,1198],[225,1198]],"score":0.9831},{"poly":[[358,1167],[938,1167],[938,1183],[358,1183]],"score":0.7632},{"poly":[[359,1150],[998,1150],[998,1167],[359,1167]],"score":0.771},{"poly":[[225,1150],[339,1150],[339,1167],[225,1167]],"score":0.82},{"poly":[[358,1134],[572,1134],[572,1150],[358,1150]],"score":0.7878},{"poly":[[225,1134],[298,1134],[298,1150],[225,1150]],"score":0.8914},{"poly":[[357,1115],[668,1114],[669,1135],[358,1137]],"score":0.7066},{"poly":[[354,1097],[1000,1099],[999,1120],[354,1119]],"score":0.7213},{"poly":[[225,1099],[338,1099],[338,1115],[225,1115]],"score":0.6087},{"poly":[[502,1086],[712,1086],[712,1102],[502,1102]],"score":0.7115},{"poly":[[358,1086],[509,1086],[509,1102],[358,1102]],"score":0.7442},{"poly":[[223,1086],[295,1082],[296,1100],[223,1104]],"score":0.722},{"poly":[[356,1068],[461,1068],[461,1084],[356,1084]],"score":0.6109},{"poly":[[905,1056],[980,1056],[980,1068],[905,1068]],"score":0.9424},{"poly":[[672,1054],[906,1054],[906,1071],[672,1071]],"score":0.7514},{"poly":[[356,1051],[674,1053],[673,1071],[356,1069]],"score":0.7115},{"poly":[[893,1038],[996,1040],[996,1056],[893,1054]],"score":0.7754},{"poly":[[600,1041],[670,1041],[670,1053],[600,1053]],"score":0.9552},{"poly":[[356,1035],[602,1036],[602,1058],[356,1056]],"score":0.6562},{"poly":[[225,1038],[339,1038],[339,1054],[225,1054]],"score":0.803},{"poly":[[359,1025],[501,1025],[501,1036],[359,1036]],"score":0.9817},{"poly":[[225,1021],[296,1021],[296,1040],[225,1040]],"score":0.7628},{"poly":[[358,1006],[878,1006],[878,1023],[358,1023]],"score":0.743},{"poly":[[225,1005],[273,1005],[273,1023],[225,1023]],"score":0.8351},{"poly":[[358,959],[487,959],[487,975],[358,975]],"score":0.7992},{"poly":[[225,959],[298,959],[298,975],[225,975]],"score":0.8791},{"poly":[[358,927],[998,927],[998,944],[358,944]],"score":0.7192},{"poly":[[358,911],[975,911],[975,927],[358,927]],"score":0.7265},{"poly":[[359,898],[680,898],[680,909],[359,909]],"score":0.9734},{"poly":[[688,894],[948,894],[948,911],[688,911]],"score":0.7811},{"poly":[[358,879],[948,879],[948,896],[358,896]],"score":0.7398},{"poly":[[225,879],[338,879],[338,896],[225,896]],"score":0.8044},{"poly":[[358,863],[555,863],[555,879],[358,879]],"score":0.787},{"poly":[[223,863],[295,859],[296,878],[223,881]],"score":0.7804},{"poly":[[358,846],[753,846],[753,863],[358,863]],"score":0.7781},{"poly":[[225,845],[274,845],[274,863],[225,863]],"score":0.8037},{"poly":[[358,799],[492,799],[492,815],[358,815]],"score":0.8153},{"poly":[[225,797],[296,797],[296,815],[225,815]],"score":0.8132},{"poly":[[358,784],[429,784],[429,800],[358,800]],"score":0.8157},{"poly":[[356,766],[993,766],[993,787],[356,787]],"score":0.6428},{"poly":[[356,749],[999,749],[999,771],[356,771]],"score":0.6512},{"poly":[[361,736],[973,736],[973,752],[361,752]],"score":0.7562},{"poly":[[431,719],[996,719],[996,736],[431,736]],"score":0.7803},{"poly":[[359,719],[417,719],[417,736],[359,736]],"score":0.8008},{"poly":[[358,705],[973,705],[973,721],[358,721]],"score":0.7955},{"poly":[[358,688],[946,688],[946,705],[358,705]],"score":0.7414},{"poly":[[358,672],[941,672],[941,688],[358,688]],"score":0.7669},{"poly":[[354,652],[948,653],[948,675],[354,673]],"score":0.7167},{"poly":[[597,640],[973,640],[973,657],[597,657]],"score":0.7982},{"poly":[[358,640],[602,640],[602,657],[358,657]],"score":0.7802},{"poly":[[358,624],[958,624],[958,640],[358,640]],"score":0.7519},{"poly":[[451,610],[980,610],[980,622],[451,622]],"score":0.988},{"poly":[[358,609],[439,609],[439,625],[358,625]],"score":0.717},{"poly":[[358,592],[958,592],[958,609],[358,609]],"score":0.7755},{"poly":[[226,594],[338,594],[338,606],[226,606]],"score":0.996},{"poly":[[358,576],[602,576],[602,592],[358,592]],"score":0.7773},{"poly":[[225,574],[289,574],[289,592],[225,592]],"score":0.7983},{"poly":[[358,559],[885,559],[885,576],[358,576]],"score":0.7082},{"poly":[[225,559],[274,559],[274,576],[225,576]],"score":0.9542},{"poly":[[356,511],[630,513],[630,530],[356,528]],"score":0.8056},{"poly":[[358,497],[973,497],[973,513],[358,513]],"score":0.776},{"poly":[[228,498],[253,498],[253,512],[228,512]],"score":0.8112},{"poly":[[359,472],[487,472],[487,483],[359,483]],"score":0.99},{"poly":[[225,470],[279,470],[279,487],[225,487]],"score":0.9835},{"poly":[[358,454],[876,454],[876,470],[358,470]],"score":0.7438},{"poly":[[225,452],[274,452],[274,470],[225,470]],"score":0.8357},{"poly":[[358,406],[422,406],[422,422],[358,422]],"score":0.7911},{"poly":[[225,406],[281,406],[281,422],[225,422]],"score":0.953},{"poly":[[358,391],[753,391],[753,408],[358,408]],"score":0.7871},{"poly":[[225,389],[274,389],[274,408],[225,408]],"score":0.8493},{"poly":[[358,343],[429,343],[429,360],[358,360]],"score":0.8422},{"poly":[[225,342],[279,342],[279,360],[225,360]],"score":0.8354},{"poly":[[354,323],[888,325],[888,347],[354,345]],"score":0.7164},{"poly":[[225,325],[274,325],[274,343],[225,343]],"score":0.8574},{"poly":[[359,280],[629,280],[629,292],[359,292]],"score":0.9749},{"poly":[[358,262],[973,262],[973,279],[358,279]],"score":0.7488},{"poly":[[225,262],[296,262],[296,280],[225,280]],"score":0.8108},{"poly":[[521,233],[622,234],[622,253],[520,251]],"score":0.7794},{"poly":[[212,161],[328,165],[327,188],[211,185]],"score":0.7264},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7161}],"page_no":19,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":625,"y0":1500,"y1":1526},"conf":0.5941,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":624,"y0":1500,"y1":1523},"font_size":0.0,"text":"21"}],"source":"layout det","text":""},{"bbox":{"x0":210,"x1":1016,"y0":48,"y1":84},"conf":0.261,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":434,"x1":708,"y0":157,"y1":180},"conf":0.3537,"font_size":0.0,"label":"Table caption","label_id":6,"lines":[{"bbox":{"x0":439,"x1":703,"y0":160,"y1":177},"font_size":-2.893e+24,"text":"Fever Prompts – Continued from previous page"}],"source":"layout det","text":"Fever Prompts – Continued from previous page"},{"bbox":{"x0":217,"x1":1015,"y0":174,"y1":1248},"conf":0.962,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":227,"x1":271,"y0":205,"y1":218},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":359,"x1":877,"y0":204,"y1":219},"font_size":0.0,"text":"Beautiful reached number two on the Billboard Hot 100 in 2003."},{"bbox":{"x0":225,"x1":288,"y0":217,"y1":236},"font_size":0.0,"text":"Thought"},{"bbox":{"x0":360,"x1":995,"y0":220,"y1":236},"font_size":0.0,"text":"The song peaked at number two on the Billboard Hot 100 in the United States,"},{"bbox":{"x0":359,"x1":623,"y0":234,"y1":250},"font_size":0.0,"text":"but not sure if it was in 2003."},{"bbox":{"x0":225,"x1":278,"y0":249,"y1":267},"font_size":0.0,"text":"Answer"},{"bbox":{"x0":359,"x1":489,"y0":251,"y1":266},"font_size":0.0,"text":"NOT ENOUGH INFO"},{"bbox":{"x0":225,"x1":270,"y0":278,"y1":295},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":360,"x1":973,"y0":278,"y1":293},"font_size":0.0,"text":"Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if"},{"bbox":{"x0":361,"x1":627,"y0":296,"y1":307},"font_size":0.0,"text":"there is NOT ENOUGH INFORMATION."},{"bbox":{"x0":227,"x1":271,"y0":342,"y1":354},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":358,"x1":887,"y0":341,"y1":356},"font_size":0.0,"text":"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company."},{"bbox":{"x0":226,"x1":305,"y0":357,"y1":373},"font_size":0.0,"text":"Thought 1"},{"bbox":{"x0":360,"x1":973,"y0":357,"y1":372},"font_size":0.0,"text":"I need to search Nikolaj Coster-Waldau and find if he has worked with the"},{"bbox":{"x0":358,"x1":570,"y0":373,"y1":389},"font_size":0.0,"text":"Fox Broadcasting Company."},{"bbox":{"x0":226,"x1":295,"y0":390,"y1":402},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":360,"x1":605,"y0":390,"y1":404},"font_size":0.0,"text":"Search[Nikolaj Coster-Waldau]"},{"bbox":{"x0":227,"x1":338,"y0":406,"y1":420},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":359,"x1":956,"y0":407,"y1":418},"font_size":0.0,"text":"Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and"},{"bbox":{"x0":359,"x1":455,"y0":421,"y1":436},"font_size":0.0,"text":"producer."},{"bbox":{"x0":449,"x1":980,"y0":421,"y1":436},"font_size":0.0,"text":"He graduated from the Danish National School of Performing Arts"},{"bbox":{"x0":359,"x1":956,"y0":437,"y1":451},"font_size":0.0,"text":"in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with"},{"bbox":{"x0":358,"x1":972,"y0":451,"y1":469},"font_size":0.0,"text":"the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy"},{"bbox":{"x0":359,"x1":946,"y0":468,"y1":485},"font_size":0.0,"text":"drama series Game of Thrones, for which he received two Primetime Emmy"},{"bbox":{"x0":358,"x1":941,"y0":485,"y1":500},"font_size":0.0,"text":"Award nominations for Outstanding Supporting Actor in a Drama Series.."},{"bbox":{"x0":359,"x1":948,"y0":502,"y1":516},"font_size":0.0,"text":"Coster-Waldau has appeared in numerous films in his native Denmark and"},{"bbox":{"x0":360,"x1":972,"y0":517,"y1":532},"font_size":0.0,"text":"Scandinavia, including Headhunters (2011) and A Thousand Times Good Night"},{"bbox":{"x0":361,"x1":997,"y0":533,"y1":548},"font_size":0.0,"text":"(2013).In the U.S, his debut film role was in the war film Black Hawk Down"},{"bbox":{"x0":361,"x1":973,"y0":550,"y1":564},"font_size":0.0,"text":"(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a"},{"bbox":{"x0":360,"x1":998,"y0":566,"y1":580},"font_size":0.0,"text":"detective in the short-lived Fox television series New Amsterdam (2008), and"},{"bbox":{"x0":359,"x1":989,"y0":581,"y1":597},"font_size":0.0,"text":"appeared in the 2009 Fox television film Virtuality, originally intended as"},{"bbox":{"x0":359,"x1":427,"y0":599,"y1":611},"font_size":0.0,"text":"a pilot."},{"bbox":{"x0":227,"x1":304,"y0":614,"y1":626},"font_size":0.0,"text":"Thought 2"},{"bbox":{"x0":358,"x1":989,"y0":613,"y1":627},"font_size":0.0,"text":"Because he \"appeared in the 2009 Fox television film Virtuality\", he should"},{"bbox":{"x0":358,"x1":748,"y0":627,"y1":644},"font_size":0.0,"text":"have worked with the Fox Broadcasting Company."},{"bbox":{"x0":226,"x1":295,"y0":645,"y1":657},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":495,"y0":645,"y1":660},"font_size":0.0,"text":"Finish[SUPPORTS]"},{"bbox":{"x0":227,"x1":271,"y0":693,"y1":705},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":361,"x1":753,"y0":694,"y1":705},"font_size":0.0,"text":"Stranger Things is set in Bloomington, Indiana."},{"bbox":{"x0":226,"x1":305,"y0":708,"y1":723},"font_size":0.0,"text":"Thought 1"},{"bbox":{"x0":360,"x1":970,"y0":709,"y1":723},"font_size":0.0,"text":"I should search for Stranger Things, and see if it is set in Bloomington,"},{"bbox":{"x0":360,"x1":427,"y0":726,"y1":738},"font_size":0.0,"text":"Indiana."},{"bbox":{"x0":226,"x1":296,"y0":739,"y1":755},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":360,"x1":554,"y0":740,"y1":756},"font_size":0.0,"text":"Search[Stranger Things]"},{"bbox":{"x0":226,"x1":338,"y0":756,"y1":770},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":360,"x1":948,"y0":757,"y1":771},"font_size":0.0,"text":"Stranger Things is an American science fiction horror drama television"},{"bbox":{"x0":359,"x1":948,"y0":771,"y1":788},"font_size":0.0,"text":"series created by the Duffer Brothers.Set in the 1980s, primarily in"},{"bbox":{"x0":359,"x1":973,"y0":787,"y1":805},"font_size":0.0,"text":"the fictional town of Hawkins, Indiana, the series centers on a number of"},{"bbox":{"x0":358,"x1":996,"y0":805,"y1":820},"font_size":0.0,"text":"mysteries and supernatural events occurring around the town and their impact"},{"bbox":{"x0":359,"x1":737,"y0":821,"y1":835},"font_size":0.0,"text":"on an ensemble of child and adult characters."},{"bbox":{"x0":226,"x1":305,"y0":836,"y1":852},"font_size":0.0,"text":"Thought 2"},{"bbox":{"x0":359,"x1":928,"y0":836,"y1":852},"font_size":0.0,"text":"The observation says that it is set in a \"fictional town of Hawkins,"},{"bbox":{"x0":359,"x1":711,"y0":851,"y1":868},"font_size":0.0,"text":"Indiana\", so it is not set in Bloomington."},{"bbox":{"x0":226,"x1":295,"y0":868,"y1":880},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":359,"x1":487,"y0":868,"y1":882},"font_size":0.0,"text":"Finish[REFUTES]"},{"bbox":{"x0":226,"x1":272,"y0":913,"y1":929},"font_size":0.0,"text":"Claim"},{"bbox":{"x0":360,"x1":878,"y0":917,"y1":928},"font_size":0.0,"text":"Beautiful reached number two on the Billboard Hot 100 in 2003."},{"bbox":{"x0":227,"x1":304,"y0":933,"y1":945},"font_size":0.0,"text":"Thought 1"},{"bbox":{"x0":360,"x1":998,"y0":931,"y1":947},"font_size":0.0,"text":"I need to search the song Beautiful and find if it reached number two on the"},{"bbox":{"x0":359,"x1":579,"y0":947,"y1":962},"font_size":0.0,"text":"Billboard Hot 100 in 2003."},{"bbox":{"x0":226,"x1":295,"y0":964,"y1":976},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":361,"x1":502,"y0":965,"y1":976},"font_size":0.0,"text":"Search[Beautiful]"},{"bbox":{"x0":226,"x1":338,"y0":979,"y1":994},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":358,"x1":995,"y0":979,"y1":994},"font_size":0.0,"text":"Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’,"},{"bbox":{"x0":356,"x1":980,"y0":992,"y1":1012},"font_size":0.0,"text":"’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is"},{"bbox":{"x0":360,"x1":462,"y0":1013,"y1":1024},"font_size":0.0,"text":"Beautiful’]."},{"bbox":{"x0":225,"x1":306,"y0":1025,"y1":1043},"font_size":0.0,"text":"Thought 2"},{"bbox":{"x0":358,"x1":981,"y0":1028,"y1":1043},"font_size":0.0,"text":"From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to"},{"bbox":{"x0":359,"x1":480,"y0":1042,"y1":1059},"font_size":0.0,"text":"find the song."},{"bbox":{"x0":227,"x1":296,"y0":1060,"y1":1072},"font_size":0.0,"text":"Action 2"},{"bbox":{"x0":360,"x1":721,"y0":1060,"y1":1075},"font_size":0.0,"text":"Search[Beautiful (Christina Aguilera song)]"},{"bbox":{"x0":225,"x1":338,"y0":1073,"y1":1090},"font_size":0.0,"text":"Observation 2"},{"bbox":{"x0":359,"x1":997,"y0":1073,"y1":1091},"font_size":0.0,"text":"\"Beautiful\" is a song recorded by American singer Christina Aguilera for her"},{"bbox":{"x0":359,"x1":669,"y0":1090,"y1":1107},"font_size":0.0,"text":"fourth studio album, Stripped (2002)."},{"bbox":{"x0":227,"x1":305,"y0":1107,"y1":1123},"font_size":0.0,"text":"Thought 3"},{"bbox":{"x0":360,"x1":918,"y0":1107,"y1":1123},"font_size":0.0,"text":"It does not mention Billboard, so I need to look up \"Billboard Hot"},{"bbox":{"x0":918,"x1":956,"y0":1105,"y1":1121},"font_size":0.0,"text":" ${}^{100}\"$ "},{"bbox":{"x0":956,"x1":981,"y0":1107,"y1":1123},"font_size":0.0,"text":"to"},{"bbox":{"x0":360,"x1":730,"y0":1124,"y1":1138},"font_size":0.0,"text":"find if it reached number two on it in 2003."},{"bbox":{"x0":224,"x1":298,"y0":1136,"y1":1154},"font_size":0.0,"text":"Action 3"},{"bbox":{"x0":360,"x1":571,"y0":1138,"y1":1153},"font_size":0.0,"text":"Lookup[Billboard Hot 100]"},{"bbox":{"x0":226,"x1":339,"y0":1154,"y1":1169},"font_size":0.0,"text":"Observation 3"},{"bbox":{"x0":361,"x1":997,"y0":1155,"y1":1170},"font_size":0.0,"text":"(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the"},{"bbox":{"x0":360,"x1":939,"y0":1171,"y1":1185},"font_size":0.0,"text":"United States, where it was certified Gold for 500,000 units shipped."},{"bbox":{"x0":227,"x1":304,"y0":1188,"y1":1200},"font_size":0.0,"text":"Thought 4"},{"bbox":{"x0":360,"x1":997,"y0":1186,"y1":1202},"font_size":0.0,"text":"It only says the song peaked at number two on the Billboard Hot 100, but not"},{"bbox":{"x0":358,"x1":888,"y0":1201,"y1":1218},"font_size":0.0,"text":"if it was in 2003.I am not sure if this claim is true or not."},{"bbox":{"x0":225,"x1":296,"y0":1218,"y1":1233},"font_size":0.0,"text":"Action 4"},{"bbox":{"x0":359,"x1":554,"y0":1219,"y1":1233},"font_size":0.0,"text":"Finish[NOT ENOUGH INFO]"}],"source":"layout det","text":"<html><body><table><thead><tr><td></td><td></td></tr></thead><tbody><tr><td>Claim Thought</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. The song peaked at number two on the Billboard Hot 100 in the United States, but not sure if it was in 2003.</td></tr><tr><td>Answer</td><td>NOT ENOUGH INFO</td></tr><tr><td>ReAct</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. I need to search Nikolaj Coster-Waldau and find if he has worked with the</td></tr><tr><td>Thought 1 Action 1 Observation 1</td><td>Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy</td></tr><tr><td></td><td>Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013).In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as</td></tr><tr><td>Thought 2</td><td>a pilot. Because he \"appeared in the 2009 Fox television film Virtuality\", he should</td></tr><tr><td>Action 2</td><td>have worked with the Fox Broadcasting Company. Finish[SUPPORTS]</td></tr><tr><td>Claim</td><td>Stranger Things is set in Bloomington, Indiana. I should search for Stranger Things, and see if it is set in Bloomington,</td></tr><tr><td>Thought 1 Action 1</td><td>Indiana. Search[Stranger Things] Stranger Things is an American science fiction horror drama television</td></tr><tr><td>Observation 1</td><td>series created by the Duffer Brothers.Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact</td></tr><tr><td></td><td>on an ensemble of child and adult characters. The observation says that it is set in a \"fictional town of Hawkins,</td></tr><tr><td>Thought 2</td><td>Indiana\", so it is not set in Bloomington. Finish[REFUTES]</td></tr><tr><td>Action 2</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Claim</td><td>I need to search the song Beautiful and find if it reached number two on the</td></tr><tr><td>Thought 1</td><td>Billboard Hot 100 in 2003. Search[Beautiful]</td></tr><tr><td>Action 1 Observation 1</td><td>Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’, ’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is</td></tr><tr><td></td><td>Beautiful’].</td></tr><tr><td>Thought 2</td><td>From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to find the song.</td></tr><tr><td>Action 2</td><td>Search[Beautiful (Christina Aguilera song)] \"Beautiful\" is a song recorded by American singer Christina Aguilera for her</td></tr><tr><td>Observation 2</td><td>fourth studio album, Stripped (2002).</td></tr><tr><td>Thought 3</td><td>It does not mention Billboard, so I need to look up \"Billboard Hot  ${}^{100}\"$ to</td></tr><tr><td></td><td>find if it reached number two on it in 2003.</td></tr><tr><td>Action 3</td><td>Lookup[Billboard Hot 100]</td></tr><tr><td>Observation 3</td><td>(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the</td></tr><tr><td></td><td>United States, where it was certified Gold for 500,000 units shipped.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>It only says the song peaked at number two on the Billboard Hot 100, but not</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Thought 4</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>if it was in 2003.I am not sure if this claim is true or not.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 4</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td></table></body></html>"}],"formula_dets":[{"bbox":{"x0":918,"x1":956,"y0":1105,"y1":1121},"conf":0.6069,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":217,"x1":1015,"y0":174,"y1":1248},"conf":0.962,"label":"Table","label_id":5},{"bbox":{"x0":210,"x1":591,"y0":50,"y1":79},"conf":0.6165,"label":"Abandon","label_id":2},{"bbox":{"x0":595,"x1":625,"y0":1500,"y1":1526},"conf":0.5941,"label":"Abandon","label_id":2},{"bbox":{"x0":434,"x1":708,"y0":157,"y1":180},"conf":0.3537,"label":"Table caption","label_id":6},{"bbox":{"x0":210,"x1":1016,"y0":48,"y1":84},"conf":0.261,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[624,1500],[624,1523],[599,1523]],"score":0.7351},{"poly":[[358,1216],[554,1216],[554,1233],[358,1233]],"score":0.8054},{"poly":[[223,1214],[296,1218],[295,1236],[223,1232]],"score":0.7202},{"poly":[[522,1201],[886,1201],[886,1218],[522,1218]],"score":0.7168},{"poly":[[359,1203],[527,1203],[527,1214],[359,1214]],"score":0.9338},{"poly":[[358,1185],[998,1185],[998,1201],[358,1201]],"score":0.7318},{"poly":[[225,1185],[306,1185],[306,1201],[225,1201]],"score":0.8667},{"poly":[[356,1168],[936,1167],[936,1185],[356,1186]],"score":0.6645},{"poly":[[359,1153],[998,1153],[998,1170],[359,1170]],"score":0.7778},{"poly":[[223,1152],[339,1150],[339,1168],[223,1170]],"score":0.7338},{"poly":[[358,1137],[569,1137],[569,1153],[358,1153]],"score":0.775},{"poly":[[225,1137],[298,1137],[298,1153],[225,1153]],"score":0.8659},{"poly":[[359,1122],[730,1122],[730,1138],[359,1138]],"score":0.7538},{"poly":[[359,1106],[983,1106],[983,1122],[359,1122]],"score":0.712},{"poly":[[225,1106],[306,1106],[306,1122],[225,1122]],"score":0.8746},{"poly":[[357,1087],[668,1086],[669,1107],[358,1109]],"score":0.6749},{"poly":[[225,1074],[339,1074],[339,1091],[225,1091]],"score":0.8055},{"poly":[[358,1069],[996,1071],[996,1092],[358,1091]],"score":0.6697},{"poly":[[499,1058],[718,1058],[718,1074],[499,1074]],"score":0.7069},{"poly":[[358,1058],[506,1058],[506,1074],[358,1074]],"score":0.7196},{"poly":[[225,1058],[296,1058],[296,1074],[225,1074]],"score":0.8731},{"poly":[[356,1039],[481,1041],[480,1059],[356,1058]],"score":0.6917},{"poly":[[358,1026],[983,1026],[983,1043],[358,1043]],"score":0.7632},{"poly":[[224,1027],[305,1023],[306,1041],[225,1045]],"score":0.7607},{"poly":[[358,1010],[461,1010],[461,1026],[358,1026]],"score":0.7836},{"poly":[[358,993],[981,993],[981,1010],[358,1010]],"score":0.7713},{"poly":[[599,980],[672,980],[672,992],[599,992]],"score":0.9317},{"poly":[[359,980],[592,980],[592,992],[359,992]],"score":0.9638},{"poly":[[225,978],[338,978],[338,995],[225,995]],"score":0.7796},{"poly":[[682,977],[994,977],[994,993],[682,993]],"score":0.7927},{"poly":[[359,964],[501,964],[501,975],[359,975]],"score":0.9927},{"poly":[[223,962],[295,958],[296,977],[223,980]],"score":0.7217},{"poly":[[358,945],[579,945],[579,962],[358,962]],"score":0.7966},{"poly":[[354,927],[998,926],[998,947],[354,949]],"score":0.6569},{"poly":[[226,929],[304,929],[304,947],[226,947]],"score":0.7753},{"poly":[[359,916],[876,916],[876,927],[359,927]],"score":0.9559},{"poly":[[225,912],[274,912],[274,931],[225,931]],"score":0.7933},{"poly":[[358,866],[487,866],[487,883],[358,883]],"score":0.795},{"poly":[[223,867],[297,863],[298,881],[223,885]],"score":0.7297},{"poly":[[358,850],[712,851],[712,868],[358,866]],"score":0.7999},{"poly":[[358,835],[926,835],[926,851],[358,851]],"score":0.7369},{"poly":[[226,833],[306,833],[306,851],[226,851]],"score":0.7619},{"poly":[[358,818],[735,818],[735,835],[358,835]],"score":0.7071},{"poly":[[358,804],[998,804],[998,820],[358,820]],"score":0.7549},{"poly":[[358,787],[973,787],[973,804],[358,804]],"score":0.7556},{"poly":[[358,771],[948,771],[948,787],[358,787]],"score":0.733},{"poly":[[356,752],[950,752],[950,774],[356,774]],"score":0.653},{"poly":[[225,754],[338,754],[338,771],[225,771]],"score":0.7999},{"poly":[[358,739],[552,739],[552,756],[358,756]],"score":0.8045},{"poly":[[223,739],[295,736],[296,754],[223,758]],"score":0.8049},{"poly":[[356,721],[429,723],[429,739],[356,737]],"score":0.7976},{"poly":[[358,708],[971,708],[971,724],[358,724]],"score":0.779},{"poly":[[226,706],[304,706],[304,724],[226,724]],"score":0.7346},{"poly":[[358,691],[755,691],[755,708],[358,708]],"score":0.7303},{"poly":[[225,690],[273,690],[273,708],[225,708]],"score":0.8342},{"poly":[[359,645],[494,645],[494,657],[359,657]],"score":0.9694},{"poly":[[225,642],[298,642],[298,658],[225,658]],"score":0.8057},{"poly":[[358,627],[745,627],[745,643],[358,643]],"score":0.7796},{"poly":[[358,610],[991,610],[991,627],[358,627]],"score":0.7089},{"poly":[[226,609],[306,609],[306,627],[226,627]],"score":0.7449},{"poly":[[356,596],[429,596],[429,612],[356,612]],"score":0.765},{"poly":[[356,578],[991,576],[991,597],[356,599]],"score":0.6255},{"poly":[[361,548],[973,548],[973,564],[361,564]],"score":0.7666},{"poly":[[432,535],[996,535],[996,546],[432,546]],"score":0.9753},{"poly":[[359,531],[436,531],[436,548],[359,548]],"score":0.6796},{"poly":[[359,518],[971,518],[971,530],[359,530]],"score":0.9481},{"poly":[[358,498],[946,498],[946,515],[358,515]],"score":0.7352},{"poly":[[358,483],[945,483],[945,500],[358,500]],"score":0.7158},{"poly":[[356,465],[946,469],[946,485],[356,482]],"score":0.7307},{"poly":[[358,452],[975,452],[975,469],[358,469]],"score":0.756},{"poly":[[359,436],[958,436],[958,452],[359,452]],"score":0.7524},{"poly":[[451,422],[980,422],[980,434],[451,434]],"score":0.9903},{"poly":[[356,421],[442,417],[443,434],[356,438]],"score":0.6794},{"poly":[[225,404],[339,404],[339,421],[225,421]],"score":0.794},{"poly":[[356,401],[960,401],[960,422],[356,422]],"score":0.6447},{"poly":[[358,388],[602,388],[602,404],[358,404]],"score":0.7428},{"poly":[[225,388],[296,388],[296,404],[225,404]],"score":0.9031},{"poly":[[356,369],[569,373],[569,390],[356,386]],"score":0.7354},{"poly":[[361,358],[971,358],[971,370],[361,370]],"score":0.9883},{"poly":[[226,355],[306,355],[306,373],[226,373]],"score":0.791},{"poly":[[358,340],[888,340],[888,356],[358,356]],"score":0.7753},{"poly":[[225,338],[274,338],[274,356],[225,356]],"score":0.8278},{"poly":[[359,295],[630,295],[630,307],[359,307]],"score":0.9338},{"poly":[[359,279],[970,279],[970,290],[359,290]],"score":0.9791},{"poly":[[225,276],[271,276],[271,294],[225,294]],"score":0.8576},{"poly":[[358,249],[489,249],[489,266],[358,266]],"score":0.8023},{"poly":[[224,247],[278,251],[277,269],[222,265]],"score":0.8387},{"poly":[[358,233],[622,233],[622,249],[358,249]],"score":0.7725},{"poly":[[358,218],[994,218],[994,234],[358,234]],"score":0.7289},{"poly":[[226,216],[288,216],[288,234],[226,234]],"score":0.8205},{"poly":[[359,205],[875,205],[875,216],[359,216]],"score":0.9397},{"poly":[[226,203],[274,203],[274,219],[226,219]],"score":0.9478},{"poly":[[439,160],[703,160],[703,177],[439,177]],"score":0.7785},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.6909}],"page_no":20,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7279,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1500,"y1":1523},"font_size":0.0,"text":"22"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":86},"conf":0.2839,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":210,"x1":359,"y0":161,"y1":192},"conf":0.8099,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":358,"y0":165,"y1":188},"font_size":-0.008663,"text":"C.3WEBSHOP"}],"source":"layout det","text":"C.3WEBSHOP"},{"bbox":{"x0":208,"x1":1015,"y0":403,"y1":459},"conf":0.6129,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":408,"y1":431},"font_size":-0.008663,"text":"Table 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for"},{"bbox":{"x0":213,"x1":738,"y0":431,"y1":452},"font_size":-0.008663,"text":"Act, and use the prompt on the right (add reasoning) for ReAct."}],"source":"layout det","text":"Table 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for Act, and use the prompt on the right (add reasoning) for ReAct."},{"bbox":{"x0":213,"x1":1028,"y0":476,"y1":1461},"conf":0.516,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":269,"x1":947,"y0":483,"y1":502},"font_size":0.0,"text":"Instruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars"},{"bbox":{"x0":394,"x1":425,"y0":509,"y1":524},"font_size":0.0,"text":"Act"},{"bbox":{"x0":781,"x1":826,"y0":509,"y1":524},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":226,"x1":578,"y0":531,"y1":549},"font_size":0.0,"text":"Aetion: search [3 ounce bright citrus deodorant sensitive skin]"},{"bbox":{"x0":613,"x1":963,"y0":530,"y1":549},"font_size":0.0,"text":"Action: search [3 ounce bright citrus deodorant sensitive skin"},{"bbox":{"x0":226,"x1":308,"y0":549,"y1":563},"font_size":0.0,"text":"Observation:"},{"bbox":{"x0":614,"x1":696,"y0":549,"y1":563},"font_size":0.0,"text":"Observation:"},{"bbox":{"x0":229,"x1":329,"y0":563,"y1":581},"font_size":0.0,"text":"[ Back to Search"},{"bbox":{"x0":621,"x1":717,"y0":563,"y1":581},"font_size":0.0,"text":"Back to Search"},{"bbox":{"x0":226,"x1":371,"y0":580,"y1":597},"font_size":0.0,"text":"Page 1 (Total results: 50)"},{"bbox":{"x0":614,"x1":759,"y0":580,"y1":597},"font_size":0.0,"text":"Page 1 (Total results: 50)"},{"bbox":{"x0":225,"x1":268,"y0":592,"y1":618},"font_size":0.0,"text":"[Next]"},{"bbox":{"x0":614,"x1":658,"y0":592,"y1":618},"font_size":0.0,"text":"[Next]"},{"bbox":{"x0":229,"x1":324,"y0":629,"y1":646},"font_size":0.0,"text":"[B078GWRC1J]"},{"bbox":{"x0":618,"x1":714,"y0":629,"y1":646},"font_size":0.0,"text":"[B078GWRC1J]"},{"bbox":{"x0":227,"x1":578,"y0":647,"y1":662},"font_size":0.0,"text":"Bright Citrus Deodorant by Earth Mama | Natural and Safe for"},{"bbox":{"x0":614,"x1":968,"y0":645,"y1":663},"font_size":0.0,"text":"Bright Citrus Deodorant by Earth Mama | Natural and Safe for"},{"bbox":{"x0":225,"x1":583,"y0":661,"y1":679},"font_size":0.0,"text":"Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic"},{"bbox":{"x0":613,"x1":972,"y0":661,"y1":679},"font_size":0.0,"text":"Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic"},{"bbox":{"x0":226,"x1":337,"y0":679,"y1":694},"font_size":0.0,"text":"Calendula 3-Ounce"},{"bbox":{"x0":615,"x1":726,"y0":679,"y1":694},"font_size":0.0,"text":"Calendula 3-Ounce"},{"bbox":{"x0":226,"x1":267,"y0":695,"y1":709},"font_size":0.0,"text":"$10.99"},{"bbox":{"x0":614,"x1":656,"y0":694,"y1":709},"font_size":0.0,"text":"$10.99"},{"bbox":{"x0":230,"x1":326,"y0":711,"y1":725},"font_size":0.0,"text":"B078GTKVXY"},{"bbox":{"x0":617,"x1":717,"y0":709,"y1":726},"font_size":0.0,"text":"[B078GTKVXY]"},{"bbox":{"x0":227,"x1":577,"y0":727,"y1":742},"font_size":0.0,"text":"Ginger Fresh Deodorant by Earth Mama | Natural and Safe for"},{"bbox":{"x0":615,"x1":966,"y0":727,"y1":742},"font_size":0.0,"text":"Ginger Fresh Deodorant by Earth Mama | Natural and Safe for"},{"bbox":{"x0":225,"x1":583,"y0":741,"y1":759},"font_size":0.0,"text":"Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic"},{"bbox":{"x0":613,"x1":972,"y0":741,"y1":759},"font_size":0.0,"text":"Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic"},{"bbox":{"x0":224,"x1":339,"y0":756,"y1":776},"font_size":0.0,"text":"Calendula 3-Ounce"},{"bbox":{"x0":613,"x1":727,"y0":757,"y1":775},"font_size":0.0,"text":"Calendula 3-Ounce"},{"bbox":{"x0":225,"x1":267,"y0":774,"y1":789},"font_size":0.0,"text":"$10.99"},{"bbox":{"x0":613,"x1":656,"y0":774,"y1":789},"font_size":0.0,"text":"$10.99"},{"bbox":{"x0":228,"x1":325,"y0":790,"y1":807},"font_size":0.0,"text":"[B08KBVJ4XN]"},{"bbox":{"x0":617,"x1":714,"y0":789,"y1":806},"font_size":0.0,"text":"[B08KBVJ4XN]"},{"bbox":{"x0":226,"x1":592,"y0":807,"y1":823},"font_size":0.0,"text":"Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men"},{"bbox":{"x0":614,"x1":981,"y0":805,"y1":824},"font_size":0.0,"text":"Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men"},{"bbox":{"x0":227,"x1":569,"y0":824,"y1":838},"font_size":0.0,"text":", Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar"},{"bbox":{"x0":615,"x1":957,"y0":824,"y1":838},"font_size":0.0,"text":", Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar"},{"bbox":{"x0":224,"x1":574,"y0":837,"y1":857},"font_size":0.0,"text":"& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,"},{"bbox":{"x0":612,"x1":961,"y0":837,"y1":857},"font_size":0.0,"text":"& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,"},{"bbox":{"x0":223,"x1":315,"y0":852,"y1":872},"font_size":0.0,"text":"2.7 oz, 2-Pack)"},{"bbox":{"x0":612,"x1":702,"y0":852,"y1":872},"font_size":0.0,"text":"2.7 oz, 2-Pack)"},{"bbox":{"x0":225,"x1":267,"y0":870,"y1":885},"font_size":0.0,"text":"$15.95"},{"bbox":{"x0":613,"x1":656,"y0":870,"y1":885},"font_size":0.0,"text":"$15.95"},{"bbox":{"x0":224,"x1":401,"y0":901,"y1":921},"font_size":0.0,"text":"Action: click[B078GWRC1J]"},{"bbox":{"x0":615,"x1":994,"y0":903,"y1":917},"font_size":0.0,"text":"Action: think[B078GWRC1J and B078GTKVXY are bright citrus"},{"bbox":{"x0":224,"x1":306,"y0":917,"y1":937},"font_size":0.0,"text":"Observation:"},{"bbox":{"x0":614,"x1":970,"y0":919,"y1":934},"font_size":0.0,"text":"deodorant less then 50 dollars. I can check B078GWRC1J frst."},{"bbox":{"x0":953,"x1":953,"y0":921,"y1":934},"font_size":6.0,"text":"i"},{"bbox":{"x0":228,"x1":324,"y0":932,"y1":955},"font_size":0.0,"text":"[Back to Search]"},{"bbox":{"x0":614,"x1":721,"y0":935,"y1":949},"font_size":0.0,"text":"Observation: OK."},{"bbox":{"x0":228,"x1":264,"y0":951,"y1":970},"font_size":0.0,"text":"[Prev]"},{"bbox":{"x0":224,"x1":576,"y0":967,"y1":987},"font_size":0.0,"text":"scent [assorted scents][bright citrus][calming lavender][ginger"},{"bbox":{"x0":614,"x1":789,"y0":966,"y1":984},"font_size":0.0,"text":"Action: click[B078GWRC1J]"},{"bbox":{"x0":227,"x1":373,"y0":986,"y1":1000},"font_size":0.0,"text":"[fresh] simply non-scents]"},{"bbox":{"x0":612,"x1":696,"y0":981,"y1":1000},"font_size":0.0,"text":"Observation:"},{"bbox":{"x0":226,"x1":546,"y0":1001,"y1":1016},"font_size":0.0,"text":"size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2"},{"bbox":{"x0":619,"x1":714,"y0":999,"y1":1017},"font_size":0.0,"text":"[Back to Search]"},{"bbox":{"x0":226,"x1":268,"y0":1017,"y1":1032},"font_size":0.0,"text":"-pack)]"},{"bbox":{"x0":616,"x1":654,"y0":1012,"y1":1035},"font_size":0.0,"text":"[Prev]"},{"bbox":{"x0":227,"x1":578,"y0":1033,"y1":1048},"font_size":0.0,"text":"Bright Citrus Deodorant by Earth Mama | Natural and Safe for"},{"bbox":{"x0":612,"x1":964,"y0":1031,"y1":1050},"font_size":0.0,"text":"scent [assorted scents][bright citrus][calming lavender][ginger"},{"bbox":{"x0":224,"x1":584,"y0":1046,"y1":1065},"font_size":0.0,"text":"Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic"},{"bbox":{"x0":616,"x1":763,"y0":1050,"y1":1064},"font_size":0.0,"text":"[fresh] simply non-scents]"},{"bbox":{"x0":226,"x1":336,"y0":1065,"y1":1079},"font_size":0.0,"text":"Calendula 3-Ounce"},{"bbox":{"x0":612,"x1":934,"y0":1063,"y1":1081},"font_size":0.0,"text":"size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2"},{"bbox":{"x0":226,"x1":305,"y0":1080,"y1":1095},"font_size":0.0,"text":"Price: $10.99"},{"bbox":{"x0":614,"x1":658,"y0":1079,"y1":1097},"font_size":0.0,"text":"-pack)]"},{"bbox":{"x0":225,"x1":303,"y0":1096,"y1":1113},"font_size":0.0,"text":"Rating: N.A."},{"bbox":{"x0":615,"x1":967,"y0":1097,"y1":1111},"font_size":0.0,"text":"Bright Citrus Deodorant by Earth Mama | Natural and Safe for"},{"bbox":{"x0":226,"x1":303,"y0":1113,"y1":1128},"font_size":0.0,"text":"[Description]"},{"bbox":{"x0":615,"x1":971,"y0":1113,"y1":1128},"font_size":0.0,"text":"Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic"},{"bbox":{"x0":225,"x1":286,"y0":1128,"y1":1145},"font_size":0.0,"text":"[Features]"},{"bbox":{"x0":613,"x1":727,"y0":1126,"y1":1145},"font_size":0.0,"text":"Calendula 3-Ounce"},{"bbox":{"x0":225,"x1":286,"y0":1144,"y1":1161},"font_size":0.0,"text":"[Reviews]"},{"bbox":{"x0":612,"x1":693,"y0":1142,"y1":1162},"font_size":0.0,"text":"Price: $10.99"},{"bbox":{"x0":225,"x1":292,"y0":1160,"y1":1178},"font_size":0.0,"text":"[Buy Now]"},{"bbox":{"x0":613,"x1":690,"y0":1159,"y1":1178},"font_size":0.0,"text":"Rating: N.A."},{"bbox":{"x0":614,"x1":692,"y0":1176,"y1":1193},"font_size":0.0,"text":"[Description]"},{"bbox":{"x0":225,"x1":381,"y0":1191,"y1":1209},"font_size":0.0,"text":"Action: click[bright citrus]"},{"bbox":{"x0":614,"x1":674,"y0":1192,"y1":1210},"font_size":0.0,"text":"[Features]"},{"bbox":{"x0":226,"x1":479,"y0":1210,"y1":1224},"font_size":0.0,"text":"Observation: You have clicked bright citrus."},{"bbox":{"x0":614,"x1":676,"y0":1208,"y1":1225},"font_size":0.0,"text":"[Reviews]"},{"bbox":{"x0":614,"x1":681,"y0":1224,"y1":1241},"font_size":0.0,"text":"[Buy Now]"},{"bbox":{"x0":226,"x1":421,"y0":1241,"y1":1256},"font_size":0.0,"text":"Action: click[3 ounce (pack of 1)]"},{"bbox":{"x0":227,"x1":520,"y0":1257,"y1":1271},"font_size":0.0,"text":"Observation: You have clicked 3 ounce (pack of 1)."},{"bbox":{"x0":615,"x1":949,"y0":1256,"y1":1270},"font_size":0.0,"text":"Action: think[For 3 ounce bottle of bright citrus deodorant"},{"bbox":{"x0":615,"x1":908,"y0":1273,"y1":1288},"font_size":0.0,"text":"for sensitive skin, the item has options ’bright citrus’"},{"bbox":{"x0":226,"x1":368,"y0":1287,"y1":1304},"font_size":0.0,"text":"Action: click[Buy Now]"},{"bbox":{"x0":612,"x1":740,"y0":1287,"y1":1305},"font_size":0.0,"text":"and ’3 ounce (pack of"},{"bbox":{"x0":740,"x1":756,"y0":1289,"y1":1303},"font_size":0.0,"text":" $11'$ "},{"bbox":{"x0":756,"x1":898,"y0":1287,"y1":1305},"font_size":6.0,"text":" and seems good to buy.]"},{"bbox":{"x0":615,"x1":722,"y0":1304,"y1":1318},"font_size":0.0,"text":"Observation: OK."},{"bbox":{"x0":615,"x1":769,"y0":1336,"y1":1350},"font_size":0.0,"text":"Action: click[bright citrus]"},{"bbox":{"x0":614,"x1":869,"y0":1351,"y1":1369},"font_size":0.0,"text":"Observation: You have clicked bright citrus."},{"bbox":{"x0":615,"x1":810,"y0":1384,"y1":1398},"font_size":0.0,"text":"Action: click[3 ounce (pack of 1)]"},{"bbox":{"x0":615,"x1":908,"y0":1400,"y1":1415},"font_size":0.0,"text":"Observation: You have clicked 3 ounce (pack of 1)."},{"bbox":{"x0":614,"x1":755,"y0":1431,"y1":1449},"font_size":0.0,"text":"Action: click[Buy Now]"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Instruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars</td></tr><tr><td>Act</td><td>ReAct</td></tr></thead><tbody><tr><td>Aetion: search [3 ounce bright citrus deodorant sensitive skin]</td><td>Action: search [3 ounce bright citrus deodorant sensitive skin</td></tr><tr><td>Observation:</td><td>Observation:</td></tr><tr><td>[ Back to Search Page 1 (Total results: 50)</td><td>Back to Search Page 1 (Total results: 50)</td></tr><tr><td>[Next]</td><td>[Next]</td></tr><tr><td>[B078GWRC1J]</td><td>[B078GWRC1J]</td></tr><tr><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td></tr><tr><td>Calendula 3-Ounce $10.99</td><td>Calendula 3-Ounce $10.99</td></tr><tr><td>B078GTKVXY Ginger Fresh Deodorant by Earth Mama | Natural and Safe for</td><td>[B078GTKVXY] Ginger Fresh Deodorant by Earth Mama | Natural and Safe for</td></tr><tr><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td></tr><tr><td>$10.99 [B08KBVJ4XN]</td><td>$10.99 [B08KBVJ4XN]</td></tr><tr><td>Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar</td><td>Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar</td></tr><tr><td>& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)</td><td>& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)</td></tr><tr><td>$15.95</td><td>$15.95</td></tr><tr><td>Action: click[B078GWRC1J]</td><td>Action: think[B078GWRC1J and B078GTKVXY are bright citrus deodorant less then 50 dollars. I can check B078GWRC1J frst.</td></tr><tr><td>Observation: [Back to Search]</td><td>Observation: OK.</td></tr><tr><td>[Prev]</td><td></td></tr><tr><td>scent [assorted scents][bright citrus][calming lavender][ginger</td><td>Action: click[B078GWRC1J]</td></tr><tr><td>[fresh] simply non-scents]</td><td>Observation:</td></tr><tr><td>size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)]</td><td>[Back to Search] [Prev]</td></tr><tr><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for</td><td>scent [assorted scents][bright citrus][calming lavender][ginger</td></tr><tr><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td><td>[fresh] simply non-scents]</td></tr><tr><td>Calendula 3-Ounce Price: $10.99</td><td>size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)]</td></tr><tr><td>Rating: N.A.</td><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for</td></tr><tr><td>[Description]</td><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td></tr><tr><td>[Features]</td><td>Calendula 3-Ounce</td></tr><tr><td>[Reviews]</td><td>Price: $10.99</td></tr><tr><td>[Buy Now]</td><td>Rating: N.A.</td></tr><tr><td></td><td>[Description]</td></tr><tr><td>Action: click[bright citrus]</td><td>[Features]</td></tr><tr><td>Observation: You have clicked bright citrus.</td><td>[Reviews]</td></tr><tr><td>Action: click[3 ounce (pack of 1)]</td><td>[Buy Now]</td></tr><tr><td>Observation: You have clicked 3 ounce (pack of 1).</td><td>Action: think[For 3 ounce bottle of bright citrus deodorant</td></tr><tr><td></td><td>for sensitive skin, the item has options ’bright citrus’</td></tr><tr><td>Action: click[Buy Now]</td><td>and ’3 ounce (pack of  $11'$  and seems good to buy.]</td></tr><tr><td></td><td>Observation: OK.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[bright citrus]</td></tr><tr><td></td><td>Observation: You have clicked bright citrus.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[3 ounce (pack of 1)]</td></tr><tr><td></td><td>Observation: You have clicked 3 ounce (pack of 1).</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[Buy Now]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></body></html>"}],"formula_dets":[{"bbox":{"x0":740,"x1":756,"y0":1289,"y1":1303},"conf":0.7866,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":210,"x1":359,"y0":161,"y1":192},"conf":0.8099,"label":"Title","label_id":0},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7279,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1015,"y0":403,"y1":459},"conf":0.6129,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.5439,"label":"Abandon","label_id":2},{"bbox":{"x0":213,"x1":1028,"y0":476,"y1":1461},"conf":0.516,"label":"Table","label_id":5},{"bbox":{"x0":264,"x1":953,"y0":481,"y1":505},"conf":0.3178,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":86},"conf":0.2839,"label":"Abandon","label_id":2},{"bbox":{"x0":216,"x1":598,"y0":471,"y1":1463},"conf":0.2469,"label":"Reference","label_id":11},{"bbox":{"x0":216,"x1":598,"y0":471,"y1":1463},"conf":0.2388,"label":"Table","label_id":5},{"bbox":{"x0":222,"x1":586,"y0":1032,"y1":1081},"conf":0.2059,"label":"Text","label_id":1},{"bbox":{"x0":612,"x1":938,"y0":1065,"y1":1081},"conf":0.2003,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[625,1500],[625,1523],[599,1523]],"score":0.87},{"poly":[[612,1427],[755,1429],[755,1447],[612,1445]],"score":0.8183},{"poly":[[610,1396],[911,1396],[911,1417],[610,1417]],"score":0.7189},{"poly":[[612,1379],[813,1379],[813,1401],[612,1401]],"score":0.7039},{"poly":[[612,1348],[870,1348],[870,1370],[612,1370]],"score":0.7006},{"poly":[[612,1331],[770,1333],[770,1351],[612,1350]],"score":0.7132},{"poly":[[612,1302],[722,1302],[722,1320],[612,1320]],"score":0.8099},{"poly":[[610,1284],[896,1284],[896,1305],[610,1305]],"score":0.7284},{"poly":[[223,1284],[368,1285],[367,1304],[223,1302]],"score":0.7147},{"poly":[[612,1270],[906,1270],[906,1287],[612,1287]],"score":0.8047},{"poly":[[614,1254],[948,1254],[948,1270],[614,1270]],"score":0.9322},{"poly":[[223,1252],[522,1252],[522,1274],[223,1274]],"score":0.7182},{"poly":[[226,1239],[422,1239],[422,1256],[226,1256]],"score":0.9479},{"poly":[[612,1223],[682,1223],[682,1241],[612,1241]],"score":0.9449},{"poly":[[614,1206],[674,1206],[674,1224],[614,1224]],"score":0.8904},{"poly":[[223,1204],[482,1204],[482,1226],[223,1226]],"score":0.7008},{"poly":[[614,1191],[675,1191],[675,1209],[614,1209]],"score":0.7996},{"poly":[[225,1191],[382,1191],[382,1208],[225,1208]],"score":0.9342},{"poly":[[614,1175],[693,1175],[693,1193],[614,1193]],"score":0.8349},{"poly":[[612,1158],[690,1158],[690,1176],[612,1176]],"score":0.7686},{"poly":[[225,1158],[293,1158],[293,1176],[225,1176]],"score":0.8624},{"poly":[[225,1143],[286,1143],[286,1160],[225,1160]],"score":0.9481},{"poly":[[612,1142],[695,1142],[695,1160],[612,1160]],"score":0.711},{"poly":[[612,1125],[727,1127],[727,1145],[612,1143]],"score":0.8202},{"poly":[[225,1127],[286,1127],[286,1145],[225,1145]],"score":0.8811},{"poly":[[612,1109],[971,1110],[971,1129],[612,1127]],"score":0.7459},{"poly":[[225,1110],[303,1110],[303,1129],[225,1129]],"score":0.8525},{"poly":[[612,1092],[968,1092],[968,1114],[612,1114]],"score":0.7001},{"poly":[[225,1094],[301,1094],[301,1112],[225,1112]],"score":0.7964},{"poly":[[611,1080],[661,1075],[662,1093],[613,1098]],"score":0.7625},{"poly":[[225,1077],[304,1077],[304,1096],[225,1096]],"score":0.8131},{"poly":[[612,1061],[936,1061],[936,1082],[612,1082]],"score":0.7131},{"poly":[[225,1063],[338,1063],[338,1079],[225,1079]],"score":0.938},{"poly":[[614,1048],[763,1048],[763,1064],[614,1064]],"score":0.869},{"poly":[[225,1048],[582,1048],[582,1064],[225,1064]],"score":0.8413},{"poly":[[612,1028],[965,1030],[964,1051],[612,1049]],"score":0.7069},{"poly":[[225,1031],[579,1031],[579,1048],[225,1048]],"score":0.8452},{"poly":[[226,1016],[271,1016],[271,1033],[226,1033]],"score":0.8212},{"poly":[[617,1011],[656,1015],[654,1035],[615,1031]],"score":0.7344},{"poly":[[619,998],[713,998],[713,1016],[619,1016]],"score":0.8028},{"poly":[[223,997],[549,997],[549,1018],[223,1018]],"score":0.7411},{"poly":[[226,983],[374,983],[374,1000],[226,1000]],"score":0.8705},{"poly":[[614,982],[695,982],[695,998],[614,998]],"score":0.934},{"poly":[[223,965],[579,967],[579,988],[223,987]],"score":0.7886},{"poly":[[612,964],[790,964],[790,985],[612,985]],"score":0.8454},{"poly":[[228,950],[263,950],[263,970],[228,970]],"score":0.7878},{"poly":[[228,934],[324,934],[324,952],[228,952]],"score":0.8047},{"poly":[[612,932],[723,932],[723,950],[612,950]],"score":0.749},{"poly":[[614,917],[970,917],[970,934],[614,934]],"score":0.7602},{"poly":[[223,916],[308,918],[307,936],[223,934]],"score":0.7923},{"poly":[[614,901],[994,901],[994,917],[614,917]],"score":0.8293},{"poly":[[223,899],[402,899],[402,921],[223,921]],"score":0.8037},{"poly":[[612,868],[659,868],[659,886],[612,886]],"score":0.7867},{"poly":[[223,868],[269,868],[269,886],[223,886]],"score":0.8512},{"poly":[[611,851],[702,853],[702,871],[610,869]],"score":0.7157},{"poly":[[225,853],[314,853],[314,871],[225,871]],"score":0.7457},{"poly":[[614,838],[960,838],[960,855],[614,855]],"score":0.9026},{"poly":[[225,838],[572,838],[572,855],[225,855]],"score":0.941},{"poly":[[615,822],[958,822],[958,838],[615,838]],"score":0.8256},{"poly":[[226,822],[570,822],[570,838],[226,838]],"score":0.8141},{"poly":[[612,805],[980,805],[980,822],[612,822]],"score":0.8288},{"poly":[[223,805],[590,805],[590,822],[223,822]],"score":0.8322},{"poly":[[617,789],[713,789],[713,807],[617,807]],"score":0.8223},{"poly":[[228,789],[326,789],[326,807],[228,807]],"score":0.8028},{"poly":[[614,772],[659,772],[659,789],[614,789]],"score":0.9791},{"poly":[[225,772],[269,772],[269,789],[225,789]],"score":0.9834},{"poly":[[223,757],[338,757],[338,774],[223,774]],"score":0.8475},{"poly":[[612,754],[729,756],[728,774],[612,772]],"score":0.751},{"poly":[[612,738],[971,739],[971,761],[612,759]],"score":0.7423},{"poly":[[221,738],[582,739],[582,761],[221,759]],"score":0.7464},{"poly":[[612,723],[968,723],[968,744],[612,744]],"score":0.7458},{"poly":[[223,723],[580,723],[580,744],[223,744]],"score":0.7448},{"poly":[[615,708],[717,706],[717,724],[616,726]],"score":0.7854},{"poly":[[226,708],[327,706],[328,724],[226,726]],"score":0.7964},{"poly":[[614,693],[660,693],[660,710],[614,710]],"score":0.9554},{"poly":[[225,691],[269,691],[269,710],[225,710]],"score":0.902},{"poly":[[612,675],[729,677],[728,695],[612,693]],"score":0.74},{"poly":[[223,675],[339,677],[339,695],[223,693]],"score":0.7685},{"poly":[[614,662],[970,662],[970,678],[614,678]],"score":0.7982},{"poly":[[225,662],[582,662],[582,678],[225,678]],"score":0.8098},{"poly":[[614,645],[966,645],[966,662],[614,662]],"score":0.8795},{"poly":[[226,645],[579,645],[579,662],[226,662]],"score":0.9154},{"poly":[[617,629],[713,629],[713,647],[617,647]],"score":0.8265},{"poly":[[228,629],[323,629],[323,645],[228,645]],"score":0.9938},{"poly":[[614,592],[659,592],[659,617],[614,617]],"score":0.821},{"poly":[[225,592],[269,592],[269,617],[225,617]],"score":0.8056},{"poly":[[228,579],[371,579],[371,596],[228,596]],"score":0.9354},{"poly":[[612,576],[760,576],[760,597],[612,597]],"score":0.8225},{"poly":[[619,563],[715,563],[715,581],[619,581]],"score":0.772},{"poly":[[231,563],[328,563],[328,581],[231,581]],"score":0.7687},{"poly":[[612,544],[697,546],[697,564],[612,563]],"score":0.7457},{"poly":[[225,546],[309,546],[309,564],[225,564]],"score":0.7012},{"poly":[[614,531],[963,531],[963,548],[614,548]],"score":0.9527},{"poly":[[225,528],[580,528],[580,549],[225,549]],"score":0.8089},{"poly":[[780,508],[827,508],[827,525],[780,525]],"score":0.9663},{"poly":[[394,507],[426,507],[426,525],[394,525]],"score":0.8184},{"poly":[[269,483],[945,483],[945,500],[269,500]],"score":0.9484},{"poly":[[213,431],[738,431],[738,452],[213,452]],"score":0.8387},{"poly":[[213,408],[1011,408],[1011,431],[213,431]],"score":0.7304},{"poly":[[213,165],[358,165],[358,188],[213,188]],"score":0.7668},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7402}],"page_no":21,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7052,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1500,"y1":1523},"font_size":0.0,"text":"23"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":85},"conf":0.2457,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":209,"x1":381,"y0":162,"y1":192},"conf":0.8049,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":378,"y0":163,"y1":188},"font_size":8594000000.0,"text":"C.4ALFWORLD"}],"source":"layout det","text":"C.4ALFWORLD"},{"bbox":{"x0":241,"x1":902,"y0":920,"y1":954},"conf":0.23,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":246,"x1":895,"y0":924,"y1":945},"font_size":8594000000.0,"text":"Table 7: An Act prompt on the ALFWorld clean task. No thoughts are provided."}],"source":"layout det","text":"Table 7: An Act prompt on the ALFWorld clean task. No thoughts are provided."},{"bbox":{"x0":222,"x1":377,"y0":1088,"y1":1103},"conf":0.2654,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":368,"y0":1086,"y1":1102},"font_size":8594000000.0,"text":"> go to fridge 1"}],"source":"layout det","text":"> go to fridge 1"},{"bbox":{"x0":221,"x1":420,"y0":1103,"y1":1119},"conf":0.6152,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":422,"y0":1099,"y1":1120},"font_size":8594000000.0,"text":"The fridge 1 is closed."}],"source":"layout det","text":"The fridge 1 is closed."},{"bbox":{"x0":222,"x1":360,"y0":1120,"y1":1134},"conf":0.3232,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":358,"y0":1117,"y1":1134},"font_size":8594000000.0,"text":"$>$ open fridge 1"}],"source":"layout det","text":"$>$ open fridge 1"},{"bbox":{"x0":220,"x1":911,"y0":1134,"y1":1167},"conf":0.6142,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":910,"y0":1132,"y1":1153},"font_size":8594000000.0,"text":"You open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a"},{"bbox":{"x0":223,"x1":438,"y0":1145,"y1":1170},"font_size":8594000000.0,"text":"potato 3, and a potato 2."}],"source":"layout det","text":"You open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2."},{"bbox":{"x0":221,"x1":409,"y0":1167,"y1":1182},"conf":0.4685,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":409,"y0":1165,"y1":1181},"font_size":8594000000.0,"text":"> go to diningtable 1"}],"source":"layout det","text":"> go to diningtable 1"},{"bbox":{"x0":222,"x1":539,"y0":1215,"y1":1231},"conf":0.2286,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":697,"y0":1214,"y1":1231},"font_size":8594000000.0,"text":"peppershaker 1, a spatula 3, a tomato"}],"source":"layout det","text":"peppershaker 1, a spatula 3, a tomato"},{"bbox":{"x0":221,"x1":530,"y0":1231,"y1":1246},"conf":0.7414,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":521,"y0":1228,"y1":1244},"font_size":8594000000.0,"text":"> take lettuce 1 from diningtable 1"}],"source":"layout det","text":"> take lettuce 1 from diningtable 1"},{"bbox":{"x0":219,"x1":641,"y0":1246,"y1":1261},"conf":0.5232,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":637,"y0":1242,"y1":1262},"font_size":8594000000.0,"text":"You pick up the lettuce 1 from the diningtable 1."}],"source":"layout det","text":"You pick up the lettuce 1 from the diningtable 1."},{"bbox":{"x0":221,"x1":397,"y0":1263,"y1":1277},"conf":0.3922,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":391,"y0":1261,"y1":1277},"font_size":8594000000.0,"text":"> go to sinkbasin 1"}],"source":"layout det","text":"> go to sinkbasin 1"},{"bbox":{"x0":221,"x1":524,"y0":1294,"y1":1309},"conf":0.7368,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":519,"y0":1292,"y1":1308},"font_size":8594000000.0,"text":"> clean lettuce 1 with sinkbasin 1"}],"source":"layout det","text":"> clean lettuce 1 with sinkbasin 1"},{"bbox":{"x0":222,"x1":616,"y0":1310,"y1":1326},"conf":0.8237,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":610,"y0":1308,"y1":1325},"font_size":8594000000.0,"text":"You clean the lettuce 1 using the sinkbasin 1."}],"source":"layout det","text":"You clean the lettuce 1 using the sinkbasin 1."},{"bbox":{"x0":222,"x1":413,"y0":1358,"y1":1374},"conf":0.8007,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":409,"y0":1356,"y1":1373},"font_size":8594000000.0,"text":"> go to diningtable 1"}],"source":"layout det","text":"> go to diningtable 1"},{"bbox":{"x0":223,"x1":926,"y0":1389,"y1":1418},"conf":0.3254,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":920,"y0":1384,"y1":1409},"font_size":8594000000.0,"text":"2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a"},{"bbox":{"x0":225,"x1":544,"y0":1404,"y1":1421},"font_size":8594000000.0,"text":"spatula 3, a tomato 2, and a tomato 1."}],"source":"layout det","text":"2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1."},{"bbox":{"x0":223,"x1":531,"y0":1422,"y1":1437},"conf":0.696,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":524,"y0":1421,"y1":1437},"font_size":8594000000.0,"text":"> put lettuce 1 in/on diningtable 1"}],"source":"layout det","text":"> put lettuce 1 in/on diningtable 1"},{"bbox":{"x0":221,"x1":615,"y0":1438,"y1":1454},"conf":0.8421,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":612,"y0":1432,"y1":1455},"font_size":8594000000.0,"text":"You put the lettuce 1 in/on the diningtable 1."}],"source":"layout det","text":"You put the lettuce 1 in/on the diningtable 1."}],"formula_dets":[{"bbox":{"x0":225,"x1":241,"y0":1123,"y1":1133},"conf":0.5011,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":221,"x1":615,"y0":1438,"y1":1454},"conf":0.8421,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":616,"y0":1310,"y1":1326},"conf":0.8237,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":381,"y0":162,"y1":192},"conf":0.8049,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":413,"y0":1358,"y1":1374},"conf":0.8007,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":530,"y0":1231,"y1":1246},"conf":0.7414,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":524,"y0":1294,"y1":1309},"conf":0.7368,"label":"Text","label_id":1},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7052,"label":"Abandon","label_id":2},{"bbox":{"x0":223,"x1":531,"y0":1422,"y1":1437},"conf":0.696,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.6866,"label":"Abandon","label_id":2},{"bbox":{"x0":221,"x1":420,"y0":1103,"y1":1119},"conf":0.6152,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":911,"y0":1134,"y1":1167},"conf":0.6142,"label":"Text","label_id":1},{"bbox":{"x0":241,"x1":902,"y0":920,"y1":954},"conf":0.6026,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":533,"y0":1407,"y1":1421},"conf":0.5291,"label":"Text","label_id":1},{"bbox":{"x0":219,"x1":641,"y0":1246,"y1":1261},"conf":0.5232,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":409,"y0":1167,"y1":1182},"conf":0.4685,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":397,"y0":1263,"y1":1277},"conf":0.3922,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":926,"y0":1389,"y1":1418},"conf":0.3254,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":360,"y0":1120,"y1":1134},"conf":0.3232,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":377,"y0":1088,"y1":1103},"conf":0.2654,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":85},"conf":0.2457,"label":"Abandon","label_id":2},{"bbox":{"x0":241,"x1":902,"y0":920,"y1":954},"conf":0.23,"label":"Title","label_id":0},{"bbox":{"x0":222,"x1":539,"y0":1215,"y1":1231},"conf":0.2286,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[625,1500],[625,1523],[599,1523]],"score":0.8665},{"poly":[[221,1432],[612,1434],[612,1455],[221,1454]],"score":0.6902},{"poly":[[225,1421],[524,1421],[524,1437],[225,1437]],"score":0.7951},{"poly":[[225,1404],[544,1404],[544,1421],[225,1421]],"score":0.7882},{"poly":[[221,1384],[920,1388],[920,1409],[221,1406]],"score":0.6691},{"poly":[[406,1373],[931,1373],[931,1389],[406,1389]],"score":0.8504},{"poly":[[223,1373],[411,1373],[411,1389],[223,1389]],"score":0.779},{"poly":[[225,1356],[409,1356],[409,1373],[225,1373]],"score":0.7906},{"poly":[[223,1340],[254,1340],[254,1358],[223,1358]],"score":0.8067},{"poly":[[309,1325],[908,1325],[908,1341],[309,1341]],"score":0.7708},{"poly":[[226,1327],[311,1327],[311,1338],[226,1338]],"score":0.8837},{"poly":[[225,1308],[610,1308],[610,1325],[225,1325]],"score":0.7516},{"poly":[[226,1292],[519,1292],[519,1308],[226,1308]],"score":0.7822},{"poly":[[225,1277],[863,1277],[863,1294],[225,1294]],"score":0.7991},{"poly":[[225,1261],[391,1261],[391,1277],[225,1277]],"score":0.8003},{"poly":[[223,1244],[637,1242],[637,1261],[223,1262]],"score":0.7309},{"poly":[[225,1228],[521,1228],[521,1244],[225,1244]],"score":0.6254},{"poly":[[225,1214],[697,1214],[697,1231],[225,1231]],"score":0.79},{"poly":[[225,1198],[916,1198],[916,1214],[225,1214]],"score":0.8174},{"poly":[[223,1180],[893,1180],[893,1201],[223,1201]],"score":0.6841},{"poly":[[225,1165],[409,1165],[409,1181],[225,1181]],"score":0.7647},{"poly":[[223,1148],[437,1145],[438,1166],[223,1170]],"score":0.6384},{"poly":[[614,1132],[910,1132],[910,1153],[614,1153]],"score":0.679},{"poly":[[422,1134],[622,1134],[622,1150],[422,1150]],"score":0.7608},{"poly":[[225,1134],[416,1134],[416,1150],[225,1150]],"score":0.8357},{"poly":[[226,1117],[358,1117],[358,1134],[226,1134]],"score":0.7887},{"poly":[[223,1099],[422,1099],[422,1120],[223,1120]],"score":0.7046},{"poly":[[225,1086],[368,1086],[368,1102],[225,1102]],"score":0.7819},{"poly":[[371,1071],[672,1071],[672,1087],[371,1087]],"score":0.7856},{"poly":[[225,1069],[369,1069],[369,1086],[225,1086]],"score":0.7848},{"poly":[[225,1054],[915,1054],[915,1071],[225,1071]],"score":0.7406},{"poly":[[221,1036],[886,1035],[886,1056],[221,1058]],"score":0.7039},{"poly":[[221,1020],[900,1018],[900,1039],[221,1041]],"score":0.6384},{"poly":[[221,1002],[875,1003],[875,1025],[221,1023]],"score":0.7122},{"poly":[[225,990],[905,990],[905,1006],[225,1006]],"score":0.7883},{"poly":[[511,974],[888,974],[888,990],[511,990]],"score":0.7644},{"poly":[[226,973],[501,973],[501,990],[226,990]],"score":0.722},{"poly":[[246,924],[895,924],[895,945],[246,945]],"score":0.8477},{"poly":[[211,163],[378,165],[377,188],[211,186]],"score":0.7481},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7289}],"page_no":22,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":628,"y0":1499,"y1":1527},"conf":0.7591,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1500,"y1":1523},"font_size":0.0,"text":"24"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":85},"conf":0.3479,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":336,"x1":806,"y0":157,"y1":189},"conf":0.8334,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":343,"x1":798,"y0":162,"y1":183},"font_size":40000000.0,"text":"Table 8: An ReAct prompt on the ALFWorld clean task."}],"source":"layout det","text":"Table 8: An ReAct prompt on the ALFWorld clean task."},{"bbox":{"x0":219,"x1":915,"y0":209,"y1":326},"conf":0.3837,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":228,"x1":890,"y0":214,"y1":231},"font_size":40000000.0,"text":"You are in the middle of a room.Looking quickly around you, you see a cabinet"},{"bbox":{"x0":226,"x1":905,"y0":229,"y1":246},"font_size":40000000.0,"text":"13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet"},{"bbox":{"x0":225,"x1":873,"y0":246,"y1":262},"font_size":40000000.0,"text":"7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet"},{"bbox":{"x0":223,"x1":898,"y0":261,"y1":277},"font_size":40000000.0,"text":"1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a"},{"bbox":{"x0":223,"x1":885,"y0":277,"y1":295},"font_size":40000000.0,"text":"garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a"},{"bbox":{"x0":225,"x1":913,"y0":294,"y1":310},"font_size":40000000.0,"text":"stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1."},{"bbox":{"x0":225,"x1":672,"y0":310,"y1":327},"font_size":40000000.0,"text":"Your task is to:put a clean lettuce in diningtable."}],"source":"layout det","text":"You are in the middle of a room.Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to:put a clean lettuce in diningtable."},{"bbox":{"x0":220,"x1":920,"y0":327,"y1":369},"conf":0.8545,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":916,"y0":322,"y1":345},"font_size":40000000.0,"text":"> think:To solve the task, I need to find and take a lettuce, then clean it with"},{"bbox":{"x0":225,"x1":545,"y0":342,"y1":358},"font_size":40000000.0,"text":"sinkbasin, then put it in diningtable."},{"bbox":{"x0":226,"x1":254,"y0":360,"y1":373},"font_size":40000000.0,"text":"OK."}],"source":"layout det","text":"> think:To solve the task, I need to find and take a lettuce, then clean it with sinkbasin, then put it in diningtable.OK."},{"bbox":{"x0":220,"x1":911,"y0":373,"y1":427},"conf":0.8979,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":910,"y0":384,"y1":409},"font_size":40000000.0,"text":"fridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13).I"},{"bbox":{"x0":225,"x1":901,"y0":375,"y1":391},"font_size":40000000.0,"text":"> think:First I need to find a lettuce.A lettuce is more likely to appear in"},{"bbox":{"x0":225,"x1":605,"y0":404,"y1":421},"font_size":40000000.0,"text":"can check one by one, starting with fridge 1."}],"source":"layout det","text":"fridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13).I> think:First I need to find a lettuce.A lettuce is more likely to appear in can check one by one, starting with fridge 1."},{"bbox":{"x0":222,"x1":258,"y0":422,"y1":437},"conf":0.2619,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":256,"y0":419,"y1":437},"font_size":40000000.0,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":222,"x1":372,"y0":438,"y1":453},"conf":0.8583,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":364,"y0":437,"y1":454},"font_size":40000000.0,"text":"> go to fridge 1"}],"source":"layout det","text":"> go to fridge 1"},{"bbox":{"x0":221,"x1":421,"y0":454,"y1":469},"conf":0.8043,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":419,"y0":450,"y1":467},"font_size":40000000.0,"text":"The fridge 1 is closed."}],"source":"layout det","text":"The fridge 1 is closed."},{"bbox":{"x0":221,"x1":358,"y0":471,"y1":486},"conf":0.8911,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":356,"y0":469,"y1":485},"font_size":40000000.0,"text":"> open fridge 1"}],"source":"layout det","text":"> open fridge 1"},{"bbox":{"x0":220,"x1":911,"y0":485,"y1":516},"conf":0.6376,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":906,"y0":483,"y1":502},"font_size":40000000.0,"text":"You open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a"},{"bbox":{"x0":225,"x1":439,"y0":502,"y1":518},"font_size":40000000.0,"text":"potato 3, and a potato 2."}],"source":"layout det","text":"You open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2."},{"bbox":{"x0":222,"x1":409,"y0":519,"y1":534},"conf":0.8472,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":407,"y0":516,"y1":533},"font_size":40000000.0,"text":"> go to diningtable 1"}],"source":"layout det","text":"> go to diningtable 1"},{"bbox":{"x0":221,"x1":916,"y0":547,"y1":570},"conf":0.2049,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":916,"y0":549,"y1":566},"font_size":40000000.0,"text":"fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a"}],"source":"layout det","text":"fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a"},{"bbox":{"x0":221,"x1":525,"y0":614,"y1":629},"conf":0.8833,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":522,"y0":612,"y1":629},"font_size":40000000.0,"text":"> take lettuce 1 from diningtable 1"}],"source":"layout det","text":"> take lettuce 1 from diningtable 1"},{"bbox":{"x0":222,"x1":642,"y0":630,"y1":646},"conf":0.5463,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":637,"y0":629,"y1":645},"font_size":40000000.0,"text":"You pick up the lettuce 1 from the diningtable 1."}],"source":"layout det","text":"You pick up the lettuce 1 from the diningtable 1."},{"bbox":{"x0":220,"x1":919,"y0":645,"y1":688},"conf":0.7727,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":915,"y0":642,"y1":662},"font_size":40000000.0,"text":"> think:Now I take a lettuce (1).Next, I need to go to sinkbasin (1) and clean"},{"bbox":{"x0":225,"x1":254,"y0":658,"y1":676},"font_size":40000000.0,"text":"it."},{"bbox":{"x0":223,"x1":256,"y0":675,"y1":693},"font_size":40000000.0,"text":"OK."}],"source":"layout det","text":"> think:Now I take a lettuce (1).Next, I need to go to sinkbasin (1) and clean it.OK."},{"bbox":{"x0":222,"x1":392,"y0":693,"y1":709},"conf":0.8656,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":389,"y0":691,"y1":708},"font_size":40000000.0,"text":"> go to sinkbasin 1"}],"source":"layout det","text":"> go to sinkbasin 1"},{"bbox":{"x0":223,"x1":865,"y0":707,"y1":725},"conf":0.4598,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":861,"y0":708,"y1":724},"font_size":40000000.0,"text":"On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3."}],"source":"layout det","text":"On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3."},{"bbox":{"x0":222,"x1":519,"y0":725,"y1":740},"conf":0.8337,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":521,"y0":724,"y1":741},"font_size":40000000.0,"text":"> clean lettuce 1 with sinkbasin 1"}],"source":"layout det","text":"> clean lettuce 1 with sinkbasin 1"},{"bbox":{"x0":222,"x1":615,"y0":741,"y1":756},"conf":0.8551,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":612,"y0":739,"y1":756},"font_size":40000000.0,"text":"You clean the lettuce 1 using the sinkbasin 1."}],"source":"layout det","text":"You clean the lettuce 1 using the sinkbasin 1."},{"bbox":{"x0":223,"x1":906,"y0":758,"y1":777},"conf":0.4445,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":905,"y0":756,"y1":772},"font_size":40000000.0,"text":"> think:Now I clean a lettuce (1).Next, I need to put it in/on diningtable 1."}],"source":"layout det","text":"> think:Now I clean a lettuce (1).Next, I need to put it in/on diningtable 1."},{"bbox":{"x0":222,"x1":255,"y0":773,"y1":787},"conf":0.2385,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":254,"y0":774,"y1":787},"font_size":40000000.0,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":223,"x1":408,"y0":789,"y1":805},"conf":0.8779,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":406,"y0":785,"y1":805},"font_size":40000000.0,"text":"> go to diningtable 1"}],"source":"layout det","text":"> go to diningtable 1"},{"bbox":{"x0":223,"x1":938,"y0":805,"y1":823},"conf":0.2718,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":933,"y0":802,"y1":823},"font_size":40000000.0,"text":"On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork"}],"source":"layout det","text":"On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork"},{"bbox":{"x0":222,"x1":920,"y0":822,"y1":852},"conf":0.4844,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":916,"y0":820,"y1":837},"font_size":40000000.0,"text":"2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a"},{"bbox":{"x0":223,"x1":547,"y0":835,"y1":853},"font_size":40000000.0,"text":"spatula 3, a tomato 2, and a tomato 1."}],"source":"layout det","text":"2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1."},{"bbox":{"x0":223,"x1":527,"y0":853,"y1":867},"conf":0.8172,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":522,"y0":850,"y1":868},"font_size":40000000.0,"text":"> put lettuce 1 in/on diningtable 1"}],"source":"layout det","text":"> put lettuce 1 in/on diningtable 1"},{"bbox":{"x0":221,"x1":615,"y0":869,"y1":885},"conf":0.8725,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":610,"y0":866,"y1":884},"font_size":40000000.0,"text":"You put the lettuce 1 in/on the diningtable 1."}],"source":"layout det","text":"You put the lettuce 1 in/on the diningtable 1."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":220,"x1":911,"y0":373,"y1":427},"conf":0.8979,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":358,"y0":471,"y1":486},"conf":0.8911,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":525,"y0":614,"y1":629},"conf":0.8833,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":408,"y0":789,"y1":805},"conf":0.8779,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":615,"y0":869,"y1":885},"conf":0.8725,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":392,"y0":693,"y1":709},"conf":0.8656,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":372,"y0":438,"y1":453},"conf":0.8583,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":615,"y0":741,"y1":756},"conf":0.8551,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":920,"y0":327,"y1":369},"conf":0.8545,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":409,"y0":519,"y1":534},"conf":0.8472,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":519,"y0":725,"y1":740},"conf":0.8337,"label":"Text","label_id":1},{"bbox":{"x0":336,"x1":806,"y0":157,"y1":189},"conf":0.8334,"label":"Title","label_id":0},{"bbox":{"x0":223,"x1":527,"y0":853,"y1":867},"conf":0.8172,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":421,"y0":454,"y1":469},"conf":0.8043,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.7872,"label":"Abandon","label_id":2},{"bbox":{"x0":220,"x1":919,"y0":645,"y1":688},"conf":0.7727,"label":"Text","label_id":1},{"bbox":{"x0":595,"x1":628,"y0":1499,"y1":1527},"conf":0.7591,"label":"Abandon","label_id":2},{"bbox":{"x0":222,"x1":530,"y0":838,"y1":852},"conf":0.7094,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":911,"y0":485,"y1":516},"conf":0.6376,"label":"Text","label_id":1},{"bbox":{"x0":230,"x1":910,"y0":757,"y1":776},"conf":0.6274,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":914,"y0":278,"y1":327},"conf":0.6141,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":642,"y0":630,"y1":646},"conf":0.5463,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":256,"y0":677,"y1":692},"conf":0.5041,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":920,"y0":822,"y1":852},"conf":0.4844,"label":"Text","label_id":1},{"bbox":{"x0":239,"x1":861,"y0":707,"y1":725},"conf":0.4634,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":865,"y0":707,"y1":725},"conf":0.4598,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":425,"y0":805,"y1":820},"conf":0.4536,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":906,"y0":758,"y1":777},"conf":0.4445,"label":"Text","label_id":1},{"bbox":{"x0":219,"x1":915,"y0":209,"y1":326},"conf":0.3837,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":85},"conf":0.3479,"label":"Abandon","label_id":2},{"bbox":{"x0":222,"x1":254,"y0":677,"y1":692},"conf":0.3208,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":938,"y0":805,"y1":823},"conf":0.2718,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":258,"y0":422,"y1":437},"conf":0.2619,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":911,"y0":210,"y1":231},"conf":0.2472,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":935,"y0":805,"y1":822},"conf":0.2469,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":413,"y0":805,"y1":821},"conf":0.2416,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":255,"y0":773,"y1":787},"conf":0.2385,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":905,"y0":219,"y1":246},"conf":0.2279,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":916,"y0":547,"y1":570},"conf":0.2049,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[625,1500],[625,1523],[599,1523]],"score":0.7881},{"poly":[[406,868],[610,868],[610,884],[406,884]],"score":0.7862},{"poly":[[225,866],[412,866],[412,883],[225,883]],"score":0.6595},{"poly":[[223,851],[522,850],[522,866],[223,868]],"score":0.7474},{"poly":[[223,837],[547,835],[547,851],[223,853]],"score":0.7576},{"poly":[[225,820],[916,820],[916,837],[225,837]],"score":0.7273},{"poly":[[223,802],[933,802],[933,823],[223,823]],"score":0.6208},{"poly":[[221,787],[406,785],[406,804],[221,805]],"score":0.6984},{"poly":[[226,774],[254,774],[254,787],[226,787]],"score":0.9125},{"poly":[[304,756],[905,756],[905,772],[304,772]],"score":0.7711},{"poly":[[223,756],[309,756],[309,772],[223,772]],"score":0.7468},{"poly":[[225,739],[612,739],[612,756],[225,756]],"score":0.7964},{"poly":[[225,724],[521,724],[521,741],[225,741]],"score":0.7765},{"poly":[[225,708],[861,708],[861,724],[225,724]],"score":0.8254},{"poly":[[225,691],[389,691],[389,708],[225,708]],"score":0.7404},{"poly":[[223,675],[256,675],[256,693],[223,693]],"score":0.7813},{"poly":[[225,658],[254,658],[254,676],[225,676]],"score":0.8027},{"poly":[[223,642],[915,644],[915,662],[223,660]],"score":0.6694},{"poly":[[225,629],[637,629],[637,645],[225,645]],"score":0.8223},{"poly":[[225,612],[522,612],[522,629],[225,629]],"score":0.7471},{"poly":[[226,599],[254,599],[254,612],[226,612]],"score":0.9213},{"poly":[[531,581],[738,581],[738,597],[531,597]],"score":0.6893},{"poly":[[301,579],[529,579],[529,596],[301,596]],"score":0.6405},{"poly":[[225,581],[309,581],[309,597],[225,597]],"score":0.7484},{"poly":[[225,566],[698,566],[698,582],[225,582]],"score":0.7826},{"poly":[[225,549],[916,549],[916,566],[225,566]],"score":0.8025},{"poly":[[225,533],[893,533],[893,549],[225,549]],"score":0.7829},{"poly":[[225,516],[407,516],[407,533],[225,533]],"score":0.7478},{"poly":[[225,502],[439,502],[439,518],[225,518]],"score":0.8023},{"poly":[[421,483],[906,483],[906,500],[421,500]],"score":0.706},{"poly":[[225,485],[429,485],[429,502],[225,502]],"score":0.8037},{"poly":[[225,469],[356,469],[356,485],[225,485]],"score":0.8052},{"poly":[[223,450],[419,450],[419,467],[223,467]],"score":0.6033},{"poly":[[225,437],[364,437],[364,454],[225,454]],"score":0.808},{"poly":[[223,419],[256,419],[256,437],[223,437]],"score":0.7429},{"poly":[[225,404],[605,404],[605,421],[225,421]],"score":0.8072},{"poly":[[221,384],[910,388],[910,409],[221,406]],"score":0.6243},{"poly":[[225,375],[901,375],[901,391],[225,391]],"score":0.7556},{"poly":[[226,360],[254,360],[254,373],[226,373]],"score":0.8515},{"poly":[[225,342],[545,342],[545,358],[225,358]],"score":0.7848},{"poly":[[304,322],[916,323],[916,345],[304,343]],"score":0.7104},{"poly":[[225,325],[309,325],[309,342],[225,342]],"score":0.7615},{"poly":[[369,310],[672,310],[672,327],[369,327]],"score":0.7752},{"poly":[[225,310],[373,310],[373,327],[225,327]],"score":0.7856},{"poly":[[225,294],[913,294],[913,310],[225,310]],"score":0.6901},{"poly":[[223,279],[885,277],[885,294],[223,295]],"score":0.77},{"poly":[[223,261],[898,261],[898,277],[223,277]],"score":0.7477},{"poly":[[225,246],[873,246],[873,262],[225,262]],"score":0.7848},{"poly":[[226,229],[905,229],[905,246],[226,246]],"score":0.7524},{"poly":[[509,214],[890,214],[890,231],[509,231]],"score":0.7461},{"poly":[[228,216],[507,216],[507,228],[228,228]],"score":0.9453},{"poly":[[343,162],[798,162],[798,183],[343,183]],"score":0.7885},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7145}],"page_no":23,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7446,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":597,"x1":625,"y0":1500,"y1":1523},"font_size":0.0,"text":"25"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":85},"conf":0.3228,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":317,"x1":824,"y0":156,"y1":190},"conf":0.8107,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":324,"x1":817,"y0":158,"y1":183},"font_size":-7.653e+26,"text":"Table 9: An ReAct-IM prompt on the ALFWorld clean task."}],"source":"layout det","text":"Table 9: An ReAct-IM prompt on the ALFWorld clean task."},{"bbox":{"x0":220,"x1":912,"y0":211,"y1":230},"conf":0.2817,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":227,"x1":890,"y0":214,"y1":231},"font_size":6.0,"text":"You are in the middle of a room.Looking quickly around you, you see a cabinet"}],"source":"layout det","text":"You are in the middle of a room.Looking quickly around you, you see a cabinet"},{"bbox":{"x0":220,"x1":902,"y0":221,"y1":245},"conf":0.264,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":227,"x1":495,"y0":218,"y1":229},"font_size":6.0,"text":"You are in the middle of a room."},{"bbox":{"x0":226,"x1":905,"y0":229,"y1":246},"font_size":6.0,"text":"13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet"}],"source":"layout det","text":"You are in the middle of a room.13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet"},{"bbox":{"x0":223,"x1":888,"y0":247,"y1":262},"conf":0.369,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":871,"y0":244,"y1":261},"font_size":-7.653e+26,"text":"7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet"}],"source":"layout det","text":"7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet"},{"bbox":{"x0":222,"x1":915,"y0":278,"y1":327},"conf":0.526,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":883,"y0":277,"y1":295},"font_size":-7.653e+26,"text":"garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a"},{"bbox":{"x0":225,"x1":913,"y0":294,"y1":310},"font_size":-7.653e+26,"text":"stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1."},{"bbox":{"x0":225,"x1":672,"y0":310,"y1":327},"font_size":-7.653e+26,"text":"Your task is to:put a clean lettuce in diningtable."}],"source":"layout det","text":"garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to:put a clean lettuce in diningtable."},{"bbox":{"x0":219,"x1":920,"y0":327,"y1":366},"conf":0.7131,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":915,"y0":322,"y1":345},"font_size":-7.653e+26,"text":"> think:To solve the task, I need to find and take a lettuce, then clean it with"},{"bbox":{"x0":225,"x1":544,"y0":342,"y1":358},"font_size":-7.653e+26,"text":"sinkbasin, then put it in diningtable."}],"source":"layout det","text":"> think:To solve the task, I need to find and take a lettuce, then clean it with sinkbasin, then put it in diningtable."},{"bbox":{"x0":223,"x1":573,"y0":374,"y1":389},"conf":0.8435,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":570,"y0":373,"y1":389},"font_size":-7.653e+26,"text":"> think:First I need to find a lettuce."}],"source":"layout det","text":"> think:First I need to find a lettuce."},{"bbox":{"x0":222,"x1":366,"y0":406,"y1":421},"conf":0.8848,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":403,"y1":422},"font_size":-7.653e+26,"text":"> go to fridge 1"}],"source":"layout det","text":"> go to fridge 1"},{"bbox":{"x0":222,"x1":423,"y0":422,"y1":438},"conf":0.372,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":421,"y0":421,"y1":437},"font_size":-7.653e+26,"text":"The fridge 1 is closed."}],"source":"layout det","text":"The fridge 1 is closed."},{"bbox":{"x0":222,"x1":571,"y0":438,"y1":453},"conf":0.8967,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":570,"y0":437,"y1":454},"font_size":-7.653e+26,"text":"> think:First I need to find a lettuce."}],"source":"layout det","text":"> think:First I need to find a lettuce."},{"bbox":{"x0":222,"x1":252,"y0":454,"y1":469},"conf":0.3969,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":254,"y0":454,"y1":467},"font_size":-7.653e+26,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":222,"x1":358,"y0":471,"y1":486},"conf":0.8851,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":356,"y0":465,"y1":487},"font_size":-7.653e+26,"text":"> open fridge 1"}],"source":"layout det","text":"> open fridge 1"},{"bbox":{"x0":221,"x1":911,"y0":484,"y1":517},"conf":0.4445,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":908,"y0":485,"y1":502},"font_size":-7.653e+26,"text":"You open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a"},{"bbox":{"x0":225,"x1":439,"y0":502,"y1":518},"font_size":-7.653e+26,"text":"potato 3, and a potato 2."}],"source":"layout det","text":"You open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2."},{"bbox":{"x0":222,"x1":575,"y0":518,"y1":534},"conf":0.6904,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":570,"y0":516,"y1":533},"font_size":-7.653e+26,"text":"> think:First I need to find a lettuce."}],"source":"layout det","text":"> think:First I need to find a lettuce."},{"bbox":{"x0":222,"x1":408,"y0":550,"y1":565},"conf":0.8746,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":406,"y0":546,"y1":566},"font_size":-7.653e+26,"text":"> go to diningtable 1"}],"source":"layout det","text":"> go to diningtable 1"},{"bbox":{"x0":220,"x1":913,"y0":566,"y1":582},"conf":0.2832,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":893,"y0":564,"y1":581},"font_size":-7.653e+26,"text":"On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a"}],"source":"layout det","text":"On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a"},{"bbox":{"x0":221,"x1":699,"y0":598,"y1":614},"conf":0.7055,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":700,"y0":597,"y1":614},"font_size":-7.653e+26,"text":"peppershaker 1, a spatula 3, a tomato 2, and a tomato 1."}],"source":"layout det","text":"peppershaker 1, a spatula 3, a tomato 2, and a tomato 1."},{"bbox":{"x0":220,"x1":575,"y0":614,"y1":629},"conf":0.3159,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":570,"y0":612,"y1":629},"font_size":-7.653e+26,"text":"> think:First I need to find a lettuce."}],"source":"layout det","text":"> think:First I need to find a lettuce."},{"bbox":{"x0":221,"x1":526,"y0":645,"y1":661},"conf":0.877,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":524,"y0":645,"y1":662},"font_size":-7.653e+26,"text":"> take lettuce 1 from diningtable 1"}],"source":"layout det","text":"> take lettuce 1 from diningtable 1"},{"bbox":{"x0":221,"x1":627,"y0":661,"y1":676},"conf":0.4342,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":637,"y0":660,"y1":676},"font_size":-7.653e+26,"text":"You pick up the lettuce 1 from the diningtable 1"}],"source":"layout det","text":"You pick up the lettuce 1 from the diningtable 1"},{"bbox":{"x0":222,"x1":741,"y0":677,"y1":695},"conf":0.8367,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":738,"y0":673,"y1":693},"font_size":-7.653e+26,"text":"> think:I need to clean this lettuce (1) using sinkbasin 1."}],"source":"layout det","text":"> think:I need to clean this lettuce (1) using sinkbasin 1."},{"bbox":{"x0":222,"x1":253,"y0":693,"y1":708},"conf":0.2673,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":256,"y0":693,"y1":706},"font_size":6.0,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":222,"x1":392,"y0":709,"y1":724},"conf":0.8557,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":389,"y0":708,"y1":724},"font_size":-7.653e+26,"text":"> go to sinkbasin 1"}],"source":"layout det","text":"> go to sinkbasin 1"},{"bbox":{"x0":223,"x1":866,"y0":724,"y1":741},"conf":0.7624,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":863,"y0":719,"y1":743},"font_size":-7.653e+26,"text":"On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3."}],"source":"layout det","text":"On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3."},{"bbox":{"x0":222,"x1":742,"y0":741,"y1":761},"conf":0.627,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":738,"y0":741,"y1":757},"font_size":-7.653e+26,"text":"> think:I need to clean this lettuce (1) using sinkbasin 1."}],"source":"layout det","text":"> think:I need to clean this lettuce (1) using sinkbasin 1."},{"bbox":{"x0":221,"x1":518,"y0":772,"y1":788},"conf":0.8474,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":511,"y0":771,"y1":787},"font_size":-7.653e+26,"text":"> clean lettuce 1 with sinkbasin 1"}],"source":"layout det","text":"> clean lettuce 1 with sinkbasin 1"},{"bbox":{"x0":222,"x1":611,"y0":789,"y1":805},"conf":0.4878,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":609,"y0":785,"y1":805},"font_size":-7.653e+26,"text":"You clean the lettuce 1 using the sinkbasin 1."}],"source":"layout det","text":"You clean the lettuce 1 using the sinkbasin 1."},{"bbox":{"x0":217,"x1":808,"y0":804,"y1":825},"conf":0.5034,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":803,"y0":804,"y1":820},"font_size":-7.653e+26,"text":"> think:I need to put this cleaned lettuce (1) in/on diningtable 1."}],"source":"layout det","text":"> think:I need to put this cleaned lettuce (1) in/on diningtable 1."},{"bbox":{"x0":223,"x1":408,"y0":837,"y1":852},"conf":0.8716,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":406,"y0":832,"y1":855},"font_size":-7.653e+26,"text":"> go to diningtable 1"}],"source":"layout det","text":"> go to diningtable 1"},{"bbox":{"x0":221,"x1":926,"y0":852,"y1":894},"conf":0.2028,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":931,"y0":851,"y1":868},"font_size":6.0,"text":"On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork"},{"bbox":{"x0":225,"x1":918,"y0":868,"y1":884},"font_size":6.0,"text":"2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a"},{"bbox":{"x0":227,"x1":545,"y0":887,"y1":898},"font_size":6.0,"text":"spatula 3, a tomato 2, and a tomato 1."}],"source":"layout det","text":"On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork 2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1."},{"bbox":{"x0":223,"x1":808,"y0":901,"y1":921},"conf":0.6908,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":803,"y0":898,"y1":919},"font_size":-7.653e+26,"text":"> think:I need to put this cleaned lettuce (1) in/on diningtable 1."}],"source":"layout det","text":"> think:I need to put this cleaned lettuce (1) in/on diningtable 1."},{"bbox":{"x0":223,"x1":524,"y0":931,"y1":947},"conf":0.8728,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":522,"y0":931,"y1":947},"font_size":-7.653e+26,"text":"> put lettuce 1 in/on diningtable 1"}],"source":"layout det","text":"> put lettuce 1 in/on diningtable 1"},{"bbox":{"x0":221,"x1":614,"y0":948,"y1":965},"conf":0.8566,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":610,"y0":947,"y1":964},"font_size":-7.653e+26,"text":"You put the lettuce 1 in/on the diningtable 1."}],"source":"layout det","text":"You put the lettuce 1 in/on the diningtable 1."},{"bbox":{"x0":210,"x1":417,"y0":1160,"y1":1193},"conf":0.8556,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":413,"y0":1163,"y1":1188},"font_size":-7.653e+26,"text":"DTRAJECTORIES"}],"source":"layout det","text":"DTRAJECTORIES"},{"bbox":{"x0":209,"x1":475,"y0":1308,"y1":1339},"conf":0.8653,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":469,"y0":1312,"y1":1335},"font_size":-7.653e+26,"text":"D.1FEVER TRAJECTORIES"}],"source":"layout det","text":"D.1FEVER TRAJECTORIES"},{"bbox":{"x0":207,"x1":1019,"y0":1415,"y1":1471},"conf":0.8492,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":215,"x1":1011,"y0":1421,"y1":1442},"font_size":-7.653e+26,"text":"We provide some trajectories produced by ReAct, CoTand Acton several randomly selected exam-"},{"bbox":{"x0":213,"x1":793,"y0":1440,"y1":1465},"font_size":-7.653e+26,"text":"ples from the FEVER dev set below. Search results are omitted to space."}],"source":"layout det","text":"We provide some trajectories produced by ReAct, CoTand Acton several randomly selected examples from the FEVER dev set below. Search results are omitted to space."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":222,"x1":571,"y0":438,"y1":453},"conf":0.8967,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":358,"y0":471,"y1":486},"conf":0.8851,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":366,"y0":406,"y1":421},"conf":0.8848,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":526,"y0":645,"y1":661},"conf":0.877,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":408,"y0":550,"y1":565},"conf":0.8746,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":524,"y0":931,"y1":947},"conf":0.8728,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":408,"y0":837,"y1":852},"conf":0.8716,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":475,"y0":1308,"y1":1339},"conf":0.8653,"label":"Title","label_id":0},{"bbox":{"x0":221,"x1":614,"y0":948,"y1":965},"conf":0.8566,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":392,"y0":709,"y1":724},"conf":0.8557,"label":"Text","label_id":1},{"bbox":{"x0":210,"x1":417,"y0":1160,"y1":1193},"conf":0.8556,"label":"Title","label_id":0},{"bbox":{"x0":207,"x1":1019,"y0":1415,"y1":1471},"conf":0.8492,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":518,"y0":772,"y1":788},"conf":0.8474,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":573,"y0":374,"y1":389},"conf":0.8435,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":741,"y0":677,"y1":695},"conf":0.8367,"label":"Text","label_id":1},{"bbox":{"x0":317,"x1":824,"y0":156,"y1":190},"conf":0.8107,"label":"Title","label_id":0},{"bbox":{"x0":222,"x1":422,"y0":422,"y1":438},"conf":0.7723,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":866,"y0":724,"y1":741},"conf":0.7624,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.7584,"label":"Abandon","label_id":2},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7446,"label":"Abandon","label_id":2},{"bbox":{"x0":219,"x1":920,"y0":327,"y1":366},"conf":0.7131,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":699,"y0":598,"y1":614},"conf":0.7055,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":808,"y0":901,"y1":921},"conf":0.6908,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":575,"y0":518,"y1":534},"conf":0.6904,"label":"Text","label_id":1},{"bbox":{"x0":260,"x1":630,"y0":661,"y1":676},"conf":0.6696,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":742,"y0":741,"y1":761},"conf":0.627,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":924,"y0":870,"y1":900},"conf":0.6244,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":915,"y0":278,"y1":327},"conf":0.526,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":440,"y0":503,"y1":518},"conf":0.5086,"label":"Text","label_id":1},{"bbox":{"x0":217,"x1":808,"y0":804,"y1":825},"conf":0.5034,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":611,"y0":789,"y1":805},"conf":0.4878,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":911,"y0":484,"y1":517},"conf":0.4445,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":627,"y0":661,"y1":676},"conf":0.4342,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":545,"y0":343,"y1":358},"conf":0.4183,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":252,"y0":454,"y1":469},"conf":0.3969,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":423,"y0":422,"y1":438},"conf":0.372,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":888,"y0":247,"y1":262},"conf":0.369,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":414,"y0":853,"y1":868},"conf":0.3638,"label":"Text","label_id":1},{"bbox":{"x0":235,"x1":605,"y0":805,"y1":820},"conf":0.3417,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1016,"y0":46,"y1":85},"conf":0.3228,"label":"Abandon","label_id":2},{"bbox":{"x0":220,"x1":575,"y0":614,"y1":629},"conf":0.3159,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":922,"y0":852,"y1":870},"conf":0.289,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":913,"y0":566,"y1":582},"conf":0.2832,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":912,"y0":211,"y1":230},"conf":0.2817,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":253,"y0":693,"y1":708},"conf":0.2673,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":902,"y0":221,"y1":245},"conf":0.264,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":572,"y0":518,"y1":533},"conf":0.2376,"label":"Text","label_id":1},{"bbox":{"x0":219,"x1":926,"y0":854,"y1":890},"conf":0.2152,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":556,"y0":885,"y1":900},"conf":0.2031,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":926,"y0":852,"y1":894},"conf":0.2028,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[597,1500],[625,1500],[625,1523],[597,1523]],"score":0.8622},{"poly":[[213,1442],[793,1440],[793,1464],[213,1465]],"score":0.7244},{"poly":[[215,1421],[1011,1421],[1011,1442],[215,1442]],"score":0.8033},{"poly":[[261,1312],[469,1313],[469,1335],[261,1333]],"score":0.8586},{"poly":[[215,1315],[269,1315],[269,1332],[215,1332]],"score":0.9393},{"poly":[[250,1163],[413,1165],[412,1188],[249,1186]],"score":0.7765},{"poly":[[213,1167],[249,1167],[249,1185],[213,1185]],"score":0.6314},{"poly":[[225,947],[610,947],[610,964],[225,964]],"score":0.7945},{"poly":[[225,931],[522,931],[522,947],[225,947]],"score":0.8108},{"poly":[[226,917],[254,917],[254,931],[226,931]],"score":0.8954},{"poly":[[223,898],[803,898],[803,919],[223,919]],"score":0.6615},{"poly":[[225,884],[544,884],[544,901],[225,901]],"score":0.7991},{"poly":[[225,868],[918,868],[918,884],[225,884]],"score":0.7944},{"poly":[[225,851],[931,851],[931,868],[225,868]],"score":0.8005},{"poly":[[223,832],[406,833],[406,855],[223,853]],"score":0.7286},{"poly":[[225,818],[256,818],[256,837],[225,837]],"score":0.796},{"poly":[[308,804],[803,804],[803,820],[308,820]],"score":0.8144},{"poly":[[225,804],[304,804],[304,820],[225,820]],"score":0.767},{"poly":[[223,785],[609,787],[609,805],[223,804]],"score":0.725},{"poly":[[223,771],[511,771],[511,787],[223,787]],"score":0.784},{"poly":[[226,757],[254,757],[254,771],[226,771]],"score":0.917},{"poly":[[225,741],[738,741],[738,757],[225,757]],"score":0.7688},{"poly":[[223,721],[863,719],[863,741],[223,743]],"score":0.7108},{"poly":[[225,708],[389,708],[389,724],[225,724]],"score":0.8171},{"poly":[[225,693],[256,693],[256,706],[225,706]],"score":0.8708},{"poly":[[223,673],[738,675],[738,693],[223,691]],"score":0.7137},{"poly":[[225,660],[637,660],[637,676],[225,676]],"score":0.7776},{"poly":[[225,645],[524,645],[524,662],[225,662]],"score":0.792},{"poly":[[223,627],[254,627],[254,645],[223,645]],"score":0.7728},{"poly":[[303,612],[570,612],[570,629],[303,629]],"score":0.7788},{"poly":[[225,612],[309,612],[309,629],[225,629]],"score":0.8236},{"poly":[[225,597],[700,597],[700,614],[225,614]],"score":0.7775},{"poly":[[225,581],[915,581],[915,597],[225,597]],"score":0.8049},{"poly":[[225,564],[893,564],[893,581],[225,581]],"score":0.8061},{"poly":[[223,548],[406,546],[406,564],[223,566]],"score":0.6982},{"poly":[[225,535],[254,535],[254,548],[225,548]],"score":0.8578},{"poly":[[308,516],[570,516],[570,533],[308,533]],"score":0.8009},{"poly":[[225,516],[303,516],[303,533],[225,533]],"score":0.7495},{"poly":[[306,502],[439,502],[439,518],[306,518]],"score":0.7734},{"poly":[[225,502],[313,502],[313,518],[225,518]],"score":0.7852},{"poly":[[225,485],[908,485],[908,502],[225,502]],"score":0.8577},{"poly":[[223,469],[356,465],[356,483],[223,487]],"score":0.637},{"poly":[[225,454],[254,454],[254,467],[225,467]],"score":0.8713},{"poly":[[304,439],[570,439],[570,450],[304,450]],"score":0.9588},{"poly":[[225,437],[309,437],[309,454],[225,454]],"score":0.7505},{"poly":[[225,421],[421,421],[421,437],[225,437]],"score":0.8327},{"poly":[[223,404],[362,403],[363,421],[223,422]],"score":0.6728},{"poly":[[226,391],[254,391],[254,404],[226,404]],"score":0.8948},{"poly":[[225,373],[570,373],[570,389],[225,389]],"score":0.78},{"poly":[[226,360],[254,360],[254,373],[226,373]],"score":0.856},{"poly":[[225,342],[544,342],[544,358],[225,358]],"score":0.8146},{"poly":[[304,322],[915,323],[915,345],[304,343]],"score":0.7135},{"poly":[[225,325],[309,325],[309,342],[225,342]],"score":0.7473},{"poly":[[369,310],[672,310],[672,327],[369,327]],"score":0.7898},{"poly":[[225,310],[371,310],[371,327],[225,327]],"score":0.7924},{"poly":[[225,294],[913,294],[913,310],[225,310]],"score":0.7133},{"poly":[[223,279],[883,277],[883,294],[223,295]],"score":0.7717},{"poly":[[225,262],[900,262],[900,279],[225,279]],"score":0.8067},{"poly":[[225,244],[871,244],[871,261],[225,261]],"score":0.7},{"poly":[[226,229],[905,229],[905,246],[226,246]],"score":0.7917},{"poly":[[509,214],[890,214],[890,231],[509,231]],"score":0.7521},{"poly":[[324,158],[817,160],[817,183],[324,181]],"score":0.7151},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7191}],"page_no":24,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":626,"y0":1501,"y1":1526},"conf":0.6372,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1502,"y1":1523},"font_size":0.0,"text":"26"}],"source":"layout det","text":""},{"bbox":{"x0":211,"x1":591,"y0":51,"y1":78},"conf":0.6081,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":587,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":214,"x1":1033,"y0":156,"y1":508},"conf":0.9589,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":486,"x1":659,"y0":161,"y1":175},"font_size":0.0,"text":"Example 2491 (gt: REFUTES)"},{"bbox":{"x0":226,"x1":271,"y0":179,"y1":194},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":226,"x1":278,"y0":194,"y1":208},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":368,"x1":921,"y0":195,"y1":208},"font_size":0.0,"text":"Bermuda Triangle is a loosely-defined region in the Pacific Ocean."},{"bbox":{"x0":226,"x1":312,"y0":210,"y1":224},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":368,"x1":978,"y0":210,"y1":224},"font_size":0.0,"text":"I need to search Bermuda Triangle and find if it is in the Pacific Ocean."},{"bbox":{"x0":226,"x1":305,"y0":227,"y1":240},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":367,"x1":567,"y0":226,"y1":239},"font_size":0.0,"text":"Search[Bermuda Triangle]"},{"bbox":{"x0":224,"x1":347,"y0":239,"y1":258},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":366,"x1":627,"y0":240,"y1":259},"font_size":0.0,"text":"Reportedly haunted locations..."},{"bbox":{"x0":227,"x1":312,"y0":258,"y1":272},"font_size":0.0,"text":"Thought 2:"},{"bbox":{"x0":368,"x1":981,"y0":259,"y1":273},"font_size":0.0,"text":"The observation says that it is in the western part of the North Atlantic"},{"bbox":{"x0":368,"x1":712,"y0":275,"y1":289},"font_size":0.0,"text":"Ocean, so it is not in the Pacific Ocean."},{"bbox":{"x0":226,"x1":304,"y0":291,"y1":304},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":368,"x1":494,"y0":291,"y1":304},"font_size":0.0,"text":"Finish[REFUTES]"},{"bbox":{"x0":226,"x1":345,"y0":306,"y1":320},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":368,"x1":504,"y0":306,"y1":320},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":225,"x1":255,"y0":321,"y1":340},"font_size":0.0,"text":"Act"},{"bbox":{"x0":225,"x1":279,"y0":337,"y1":353},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":368,"x1":920,"y0":339,"y1":352},"font_size":0.0,"text":"Bermuda Triangle is a loosely-defined region in the Pacific Ocean."},{"bbox":{"x0":225,"x1":296,"y0":354,"y1":368},"font_size":0.0,"text":"Action 1"},{"bbox":{"x0":370,"x1":379,"y0":358,"y1":367},"font_size":0.0,"text":":"},{"bbox":{"x0":393,"x1":593,"y0":355,"y1":369},"font_size":0.0,"text":"Search[Bermuda Triangle]"},{"bbox":{"x0":226,"x1":338,"y0":370,"y1":384},"font_size":0.0,"text":"Observation 1"},{"bbox":{"x0":370,"x1":380,"y0":374,"y1":383},"font_size":0.0,"text":":"},{"bbox":{"x0":392,"x1":659,"y0":371,"y1":385},"font_size":0.0,"text":"Reportedly haunted locations:..."},{"bbox":{"x0":224,"x1":305,"y0":383,"y1":402},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":368,"x1":494,"y0":387,"y1":400},"font_size":0.0,"text":"Finish[REFUTES]"},{"bbox":{"x0":226,"x1":345,"y0":402,"y1":416},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":368,"x1":504,"y0":403,"y1":416},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":226,"x1":255,"y0":419,"y1":435},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":226,"x1":278,"y0":435,"y1":449},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":368,"x1":919,"y0":435,"y1":449},"font_size":0.0,"text":"Bermuda Triangle is a loosely-defined region in the Pacific Ocean."},{"bbox":{"x0":224,"x1":296,"y0":449,"y1":468},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":366,"x1":592,"y0":449,"y1":468},"font_size":0.0,"text":"Let’s think step by step."},{"bbox":{"x0":592,"x1":998,"y0":451,"y1":466},"font_size":0.0,"text":"The Bermuda Triangle is a loosely-defined region"},{"bbox":{"x0":368,"x1":571,"y0":468,"y1":481},"font_size":0.0,"text":"in the Atlantic Ocean."},{"bbox":{"x0":565,"x1":938,"y0":468,"y1":481},"font_size":0.0,"text":"The Pacific Ocean is not the Atlantic Ocean."},{"bbox":{"x0":225,"x1":286,"y0":483,"y1":499},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":369,"x1":428,"y0":485,"y1":496},"font_size":0.0,"text":"REFUTES"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Example 2491 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Thought 1:</td><td>I need to search Bermuda Triangle and find if it is in the Pacific Ocean.</td></tr><tr><td>Action 1:</td><td>Search[Bermuda Triangle]</td></tr><tr><td>Observation 1:</td><td>Reportedly haunted locations...</td></tr><tr><td>Thought 2:</td><td>The observation says that it is in the western part of the North Atlantic</td></tr><tr><td>Action 2:</td><td>Ocean, so it is not in the Pacific Ocean.</td></tr><tr><td>Observation 2:</td><td>Finish[REFUTES] Episode finished</td></tr><tr><td colspan=\"2\">Act</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Action 1</td><td>:</td></tr><tr><td>Observation 1</td><td>Search[Bermuda Triangle] : Reportedly haunted locations:...</td></tr><tr><td>Action 2:</td><td></td></tr><tr><td>Observation 2:</td><td>Finish[REFUTES] Episode finished</td></tr><tr><td colspan=\"2\">CoT</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Thought:</td><td>Let’s think step by step. The Bermuda Triangle is a loosely-defined region</td></tr><tr><td>Answer:</td><td>in the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean. REFUTES</td></tr></tbody></table></body></html>"},{"bbox":{"x0":214,"x1":1032,"y0":602,"y1":940},"conf":0.95,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":481,"x1":663,"y0":609,"y1":622},"font_size":0.0,"text":"Example 5908 (gt: SUPPORTS)"},{"bbox":{"x0":225,"x1":272,"y0":626,"y1":643},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":224,"x1":279,"y0":639,"y1":658},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":367,"x1":601,"y0":642,"y1":656},"font_size":0.0,"text":"Princess Mononoke is a film."},{"bbox":{"x0":225,"x1":312,"y0":657,"y1":673},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":367,"x1":869,"y0":659,"y1":672},"font_size":0.0,"text":"I need to search Princess Mononoke and find if it is a film."},{"bbox":{"x0":224,"x1":306,"y0":673,"y1":689},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":366,"x1":577,"y0":674,"y1":688},"font_size":0.0,"text":"Search[Princess Mononoke]"},{"bbox":{"x0":224,"x1":348,"y0":688,"y1":706},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":366,"x1":515,"y0":689,"y1":704},"font_size":0.0,"text":"Princess Mononoke "},{"bbox":{"x0":519,"x1":528,"y0":693,"y1":703},"font_size":6.0,"text":"."},{"bbox":{"x0":528,"x1":536,"y0":693,"y1":703},"font_size":6.0,"text":"."},{"bbox":{"x0":536,"x1":544,"y0":693,"y1":703},"font_size":6.0,"text":"."},{"bbox":{"x0":224,"x1":313,"y0":704,"y1":722},"font_size":0.0,"text":"Thought 2:"},{"bbox":{"x0":367,"x1":894,"y0":707,"y1":720},"font_size":0.0,"text":"From the observation, it says that Princess Mononoke is a film."},{"bbox":{"x0":225,"x1":305,"y0":721,"y1":737},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":367,"x1":501,"y0":723,"y1":735},"font_size":0.0,"text":"Finish[SUPPORTS]"},{"bbox":{"x0":226,"x1":346,"y0":738,"y1":752},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":367,"x1":503,"y0":738,"y1":752},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":225,"x1":255,"y0":753,"y1":771},"font_size":0.0,"text":"Act"},{"bbox":{"x0":224,"x1":280,"y0":769,"y1":785},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":367,"x1":602,"y0":771,"y1":783},"font_size":0.0,"text":"Princess Mononoke is a film."},{"bbox":{"x0":225,"x1":304,"y0":786,"y1":800},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":367,"x1":576,"y0":787,"y1":800},"font_size":0.0,"text":"Search[Princess Mononoke]"},{"bbox":{"x0":226,"x1":346,"y0":803,"y1":816},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":366,"x1":515,"y0":802,"y1":816},"font_size":0.0,"text":"Princess Mononoke"},{"bbox":{"x0":223,"x1":305,"y0":815,"y1":834},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":513,"x1":538,"y0":806,"y1":816},"font_size":0.0,"text":" .."},{"bbox":{"x0":367,"x1":501,"y0":819,"y1":831},"font_size":0.0,"text":"Finish[SUPPORTS]"},{"bbox":{"x0":222,"x1":349,"y0":829,"y1":851},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":366,"x1":503,"y0":834,"y1":848},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":225,"x1":256,"y0":849,"y1":868},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":224,"x1":280,"y0":866,"y1":882},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":367,"x1":603,"y0":868,"y1":880},"font_size":0.0,"text":"Princess Mononoke is a film."},{"bbox":{"x0":224,"x1":296,"y0":880,"y1":901},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":365,"x1":964,"y0":881,"y1":899},"font_size":0.0,"text":"Let’s think step by step.Princess Mononoke is a movie, and movies are"},{"bbox":{"x0":365,"x1":422,"y0":897,"y1":915},"font_size":0.0,"text":"films."},{"bbox":{"x0":226,"x1":286,"y0":916,"y1":929},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":366,"x1":437,"y0":914,"y1":930},"font_size":0.0,"text":"SUPPORTS"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Example 5908 (gt: SUPPORTS)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Princess Mononoke is a film.</td></tr><tr><td>Thought 1:</td><td>I need to search Princess Mononoke and find if it is a film.</td></tr><tr><td>Action 1:</td><td>Search[Princess Mononoke]</td></tr><tr><td>Observation 1:</td><td>Princess Mononoke . . .</td></tr><tr><td>Thought 2:</td><td>From the observation, it says that Princess Mononoke is a film.</td></tr><tr><td>Action 2:</td><td>Finish[SUPPORTS] Episode finished</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td colspan=\"2\">Act Claim:</td></tr><tr><td></td><td>Princess Mononoke is a film.</td></tr><tr><td>Action 1:</td><td>Search[Princess Mononoke]</td></tr><tr><td>Observation 1: Action 2:</td><td>Princess Mononoke  ..</td></tr><tr><td>Observation 2:</td><td>Finish[SUPPORTS]</td></tr><tr><td></td><td>Episode finished</td></tr><tr><td colspan=\"2\">CoT Claim:</td></tr><tr><td>Thought:</td><td>Princess Mononoke is a film. Let’s think step by step.Princess Mononoke is a movie, and movies are</td></tr><tr><td>Answer:</td><td>films.</td></tr><tr><td></td><td>SUPPORTS</td></tr></tbody></table></body></html>"},{"bbox":{"x0":213,"x1":1034,"y0":1036,"y1":1467},"conf":0.9466,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":484,"x1":658,"y0":1040,"y1":1055},"font_size":0.0,"text":"Example 1951 (gt: REFUTES)"},{"bbox":{"x0":225,"x1":272,"y0":1057,"y1":1075},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":226,"x1":279,"y0":1072,"y1":1090},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":368,"x1":746,"y0":1074,"y1":1089},"font_size":0.0,"text":"Soyuz was part of the American space program."},{"bbox":{"x0":227,"x1":312,"y0":1090,"y1":1104},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":367,"x1":938,"y0":1091,"y1":1105},"font_size":0.0,"text":"I need to search Soyuz and find if it was part of the American space"},{"bbox":{"x0":368,"x1":435,"y0":1109,"y1":1120},"font_size":0.0,"text":"program."},{"bbox":{"x0":226,"x1":304,"y0":1122,"y1":1135},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":367,"x1":478,"y0":1123,"y1":1136},"font_size":0.0,"text":"Search[Soyuz]"},{"bbox":{"x0":226,"x1":345,"y0":1138,"y1":1152},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":366,"x1":610,"y0":1138,"y1":1153},"font_size":0.0,"text":"Soyuz is a transliteration..."},{"bbox":{"x0":227,"x1":313,"y0":1154,"y1":1169},"font_size":0.0,"text":"Thought 2:"},{"bbox":{"x0":367,"x1":931,"y0":1153,"y1":1171},"font_size":0.0,"text":"It does not mention the American space program, so I need to search"},{"bbox":{"x0":366,"x1":823,"y0":1165,"y1":1187},"font_size":0.0,"text":"\"American space program\" to find if it was part of it."},{"bbox":{"x0":226,"x1":304,"y0":1186,"y1":1201},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":367,"x1":619,"y0":1185,"y1":1201},"font_size":0.0,"text":"Search[American space program]"},{"bbox":{"x0":226,"x1":345,"y0":1201,"y1":1215},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":367,"x1":526,"y0":1201,"y1":1216},"font_size":0.0,"text":"The space policy..."},{"bbox":{"x0":226,"x1":312,"y0":1217,"y1":1232},"font_size":0.0,"text":"Thought 3:"},{"bbox":{"x0":368,"x1":978,"y0":1217,"y1":1232},"font_size":0.0,"text":"It does not mention Soyuz, so I am not sure if this claim is true or not."},{"bbox":{"x0":225,"x1":305,"y0":1233,"y1":1247},"font_size":0.0,"text":"Action 3:"},{"bbox":{"x0":367,"x1":559,"y0":1234,"y1":1247},"font_size":0.0,"text":"Finish[NOT ENOUGH INFO]"},{"bbox":{"x0":225,"x1":345,"y0":1248,"y1":1263},"font_size":0.0,"text":"Observation 3:"},{"bbox":{"x0":367,"x1":504,"y0":1249,"y1":1263},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":224,"x1":256,"y0":1264,"y1":1284},"font_size":0.0,"text":"Act"},{"bbox":{"x0":226,"x1":278,"y0":1282,"y1":1296},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":367,"x1":745,"y0":1283,"y1":1297},"font_size":0.0,"text":"Soyuz was part of the American space program."},{"bbox":{"x0":226,"x1":305,"y0":1298,"y1":1313},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":366,"x1":478,"y0":1297,"y1":1313},"font_size":0.0,"text":"Search[Soyuz]"},{"bbox":{"x0":226,"x1":346,"y0":1315,"y1":1328},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":366,"x1":607,"y0":1314,"y1":1329},"font_size":0.0,"text":"Soyuz is a transliteration..."},{"bbox":{"x0":226,"x1":304,"y0":1330,"y1":1345},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":366,"x1":560,"y0":1329,"y1":1345},"font_size":0.0,"text":"Finish[NOT ENOUGH INFO]"},{"bbox":{"x0":226,"x1":345,"y0":1346,"y1":1360},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":366,"x1":503,"y0":1346,"y1":1359},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":225,"x1":255,"y0":1360,"y1":1380},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":226,"x1":278,"y0":1378,"y1":1392},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":366,"x1":745,"y0":1378,"y1":1393},"font_size":0.0,"text":"Soyuz was part of the American space program."},{"bbox":{"x0":226,"x1":295,"y0":1392,"y1":1410},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":367,"x1":592,"y0":1394,"y1":1409},"font_size":0.0,"text":"Let’s think step by step."},{"bbox":{"x0":592,"x1":847,"y0":1394,"y1":1409},"font_size":0.0,"text":"Soyuz is a Russian spacecraft."},{"bbox":{"x0":858,"x1":964,"y0":1394,"y1":1408},"font_size":0.0,"text":"The American"},{"bbox":{"x0":366,"x1":923,"y0":1410,"y1":1426},"font_size":0.0,"text":"space program is NASA. NASA and Russia have worked together on the"},{"bbox":{"x0":366,"x1":603,"y0":1426,"y1":1441},"font_size":0.0,"text":"International Space Station."},{"bbox":{"x0":226,"x1":287,"y0":1442,"y1":1457},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":368,"x1":437,"y0":1442,"y1":1457},"font_size":0.0,"text":"SUPPORTS"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Example 1951 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Soyuz was part of the American space program.</td></tr><tr><td>Thought 1:</td><td>I need to search Soyuz and find if it was part of the American space</td></tr><tr><td>Action 1:</td><td>program. Search[Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Thought 2:</td><td>It does not mention the American space program, so I need to search \"American space program\" to find if it was part of it.</td></tr><tr><td>Action 2:</td><td>Search[American space program]</td></tr><tr><td>Observation 2:</td><td>The space policy...</td></tr><tr><td>Thought 3:</td><td>It does not mention Soyuz, so I am not sure if this claim is true or not.</td></tr><tr><td>Action 3: Observation 3:</td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td></td><td>Episode finished</td></tr><tr><td colspan=\"2\">Act Claim:</td></tr><tr><td></td><td>Soyuz was part of the American space program.</td></tr><tr><td>Action 1:</td><td>Search[Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Action 2:</td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td>Observation 2:</td><td>Episode finished</td></tr><tr><td colspan=\"2\">CoT</td></tr><tr><td>Claim: Thought:</td><td>Soyuz was part of the American space program. Let’s think step by step. Soyuz is a Russian spacecraft. The American</td></tr><tr><td></td><td>space program is NASA. NASA and Russia have worked together on the</td></tr><tr><td>Answer:</td><td>International Space Station. SUPPORTS</td></tr></tbody></table></body></html>"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":214,"x1":1033,"y0":156,"y1":508},"conf":0.9589,"label":"Table","label_id":5},{"bbox":{"x0":214,"x1":1032,"y0":602,"y1":940},"conf":0.95,"label":"Table","label_id":5},{"bbox":{"x0":213,"x1":1034,"y0":1036,"y1":1467},"conf":0.9466,"label":"Table","label_id":5},{"bbox":{"x0":596,"x1":626,"y0":1501,"y1":1526},"conf":0.6372,"label":"Abandon","label_id":2},{"bbox":{"x0":211,"x1":591,"y0":51,"y1":78},"conf":0.6081,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[625,1502],[625,1523],[599,1523]],"score":0.8916},{"poly":[[368,1442],[436,1442],[436,1454],[368,1454]],"score":0.9934},{"poly":[[223,1440],[288,1440],[288,1457],[223,1457]],"score":0.9299},{"poly":[[366,1426],[602,1426],[602,1442],[366,1442]],"score":0.806},{"poly":[[366,1409],[923,1409],[923,1426],[366,1426]],"score":0.768},{"poly":[[860,1393],[966,1393],[966,1409],[860,1409]],"score":0.8152},{"poly":[[589,1393],[843,1393],[843,1409],[589,1409]],"score":0.7297},{"poly":[[366,1393],[582,1393],[582,1409],[366,1409]],"score":0.7746},{"poly":[[225,1393],[296,1393],[296,1411],[225,1411]],"score":0.7839},{"poly":[[225,1378],[279,1378],[279,1394],[225,1394]],"score":0.9372},{"poly":[[364,1374],[745,1376],[745,1394],[364,1393]],"score":0.6277},{"poly":[[225,1360],[256,1360],[256,1379],[225,1379]],"score":0.7935},{"poly":[[366,1345],[507,1345],[507,1361],[366,1361]],"score":0.817},{"poly":[[225,1345],[346,1345],[346,1361],[225,1361]],"score":0.8252},{"poly":[[363,1323],[562,1327],[562,1348],[362,1345]],"score":0.7038},{"poly":[[225,1328],[304,1328],[304,1345],[225,1345]],"score":0.8859},{"poly":[[363,1308],[614,1310],[614,1333],[362,1332]],"score":0.606},{"poly":[[223,1310],[348,1312],[347,1330],[223,1328]],"score":0.7538},{"poly":[[366,1297],[479,1297],[479,1313],[366,1313]],"score":0.8326},{"poly":[[225,1297],[304,1297],[304,1313],[225,1313]],"score":0.862},{"poly":[[366,1280],[743,1280],[743,1297],[366,1297]],"score":0.7896},{"poly":[[225,1280],[279,1280],[279,1297],[225,1297]],"score":0.8859},{"poly":[[223,1264],[256,1264],[256,1284],[223,1284]],"score":0.8118},{"poly":[[368,1247],[506,1247],[506,1264],[368,1264]],"score":0.7482},{"poly":[[221,1246],[344,1246],[344,1262],[221,1262]],"score":0.7348},{"poly":[[368,1233],[562,1233],[562,1249],[368,1249]],"score":0.8015},{"poly":[[225,1233],[306,1233],[306,1249],[225,1249]],"score":0.8359},{"poly":[[366,1216],[978,1216],[978,1233],[366,1233]],"score":0.7265},{"poly":[[225,1216],[313,1216],[313,1233],[225,1233]],"score":0.86},{"poly":[[366,1200],[522,1200],[522,1216],[366,1216]],"score":0.7804},{"poly":[[225,1200],[348,1200],[348,1216],[225,1216]],"score":0.9149},{"poly":[[368,1185],[620,1185],[620,1201],[368,1201]],"score":0.77},{"poly":[[225,1185],[304,1185],[304,1201],[225,1201]],"score":0.8255},{"poly":[[366,1167],[823,1167],[823,1188],[366,1188]],"score":0.6502},{"poly":[[366,1153],[931,1153],[931,1170],[366,1170]],"score":0.7811},{"poly":[[225,1152],[314,1152],[314,1170],[225,1170]],"score":0.7405},{"poly":[[366,1137],[604,1137],[604,1153],[366,1153]],"score":0.7424},{"poly":[[225,1137],[348,1137],[348,1153],[225,1153]],"score":0.8135},{"poly":[[366,1119],[476,1119],[476,1135],[366,1135]],"score":0.6979},{"poly":[[225,1120],[306,1120],[306,1137],[225,1137]],"score":0.913},{"poly":[[366,1106],[436,1106],[436,1122],[366,1122]],"score":0.8834},{"poly":[[364,1089],[938,1089],[938,1106],[364,1106]],"score":0.7972},{"poly":[[226,1087],[313,1087],[313,1106],[226,1106]],"score":0.7873},{"poly":[[366,1072],[745,1072],[745,1089],[366,1089]],"score":0.7343},{"poly":[[225,1072],[278,1072],[278,1089],[225,1089]],"score":0.8535},{"poly":[[222,1058],[272,1054],[273,1072],[224,1076]],"score":0.8164},{"poly":[[484,1038],[660,1038],[660,1054],[484,1054]],"score":0.8206},{"poly":[[223,914],[286,914],[286,931],[223,931]],"score":0.9358},{"poly":[[365,910],[438,914],[437,933],[364,929]],"score":0.7886},{"poly":[[366,898],[419,898],[419,914],[366,914]],"score":0.9},{"poly":[[590,883],[965,883],[965,899],[590,899]],"score":0.7729},{"poly":[[364,879],[582,881],[582,899],[364,898]],"score":0.7513},{"poly":[[225,881],[296,881],[296,899],[225,899]],"score":0.8234},{"poly":[[225,866],[279,866],[279,883],[225,883]],"score":0.9631},{"poly":[[364,863],[605,863],[605,884],[364,884]],"score":0.66},{"poly":[[225,848],[256,848],[256,868],[225,868]],"score":0.9318},{"poly":[[366,833],[506,833],[506,850],[366,850]],"score":0.807},{"poly":[[223,832],[346,832],[346,850],[223,850]],"score":0.7114},{"poly":[[366,817],[502,817],[502,833],[366,833]],"score":0.8533},{"poly":[[225,817],[304,817],[304,833],[225,833]],"score":0.8752},{"poly":[[366,800],[514,800],[514,817],[366,817]],"score":0.8014},{"poly":[[225,800],[348,800],[348,817],[225,817]],"score":0.8216},{"poly":[[366,785],[577,785],[577,802],[366,802]],"score":0.8189},{"poly":[[225,784],[304,784],[304,802],[225,802]],"score":0.7139},{"poly":[[366,769],[604,769],[604,785],[366,785]],"score":0.7332},{"poly":[[225,769],[279,769],[279,785],[225,785]],"score":0.9601},{"poly":[[223,752],[256,752],[256,771],[223,771]],"score":0.9192},{"poly":[[366,738],[506,738],[506,754],[366,754]],"score":0.7884},{"poly":[[225,736],[346,736],[346,752],[225,752]],"score":0.823},{"poly":[[366,721],[501,721],[501,738],[366,738]],"score":0.8189},{"poly":[[225,721],[304,721],[304,738],[225,738]],"score":0.9124},{"poly":[[366,705],[895,705],[895,721],[366,721]],"score":0.7827},{"poly":[[223,701],[313,703],[312,721],[223,719]],"score":0.7644},{"poly":[[364,688],[542,690],[542,706],[364,704]],"score":0.7963},{"poly":[[225,688],[346,688],[346,705],[225,705]],"score":0.8151},{"poly":[[366,673],[577,673],[577,690],[366,690]],"score":0.8127},{"poly":[[225,673],[306,673],[306,690],[225,690]],"score":0.8837},{"poly":[[366,657],[870,657],[870,673],[366,673]],"score":0.7518},{"poly":[[225,657],[313,657],[313,673],[225,673]],"score":0.861},{"poly":[[366,640],[602,640],[602,657],[366,657]],"score":0.7777},{"poly":[[224,638],[278,642],[277,660],[222,656]],"score":0.7868},{"poly":[[223,622],[273,622],[273,645],[223,645]],"score":0.7521},{"poly":[[479,602],[664,604],[663,625],[479,624]],"score":0.7428},{"poly":[[368,482],[429,482],[429,498],[368,498]],"score":0.8056},{"poly":[[223,482],[286,482],[286,498],[223,498]],"score":0.9427},{"poly":[[564,469],[936,469],[936,480],[564,480]],"score":0.9839},{"poly":[[368,467],[569,467],[569,483],[368,483]],"score":0.7778},{"poly":[[589,450],[998,450],[998,467],[589,467]],"score":0.7173},{"poly":[[364,449],[585,449],[585,465],[364,465]],"score":0.699},{"poly":[[223,447],[296,451],[295,469],[223,465]],"score":0.7869},{"poly":[[366,434],[920,434],[920,450],[366,450]],"score":0.7694},{"poly":[[225,434],[279,434],[279,450],[225,450]],"score":0.9688},{"poly":[[225,416],[256,416],[256,436],[225,436]],"score":0.8091},{"poly":[[366,401],[506,401],[506,417],[366,417]],"score":0.8389},{"poly":[[225,401],[346,401],[346,417],[225,417]],"score":0.8283},{"poly":[[366,384],[494,384],[494,401],[366,401]],"score":0.8099},{"poly":[[225,384],[304,384],[304,401],[225,401]],"score":0.8295},{"poly":[[391,370],[664,370],[664,386],[391,386]],"score":0.7758},{"poly":[[223,366],[338,368],[337,386],[223,384]],"score":0.7268},{"poly":[[369,358],[379,358],[379,366],[369,366]],"score":0.8785},{"poly":[[391,353],[594,353],[594,370],[391,370]],"score":0.8202},{"poly":[[223,353],[295,349],[296,368],[223,372]],"score":0.7315},{"poly":[[366,337],[920,337],[920,353],[366,353]],"score":0.8067},{"poly":[[225,337],[279,337],[279,353],[225,353]],"score":0.8847},{"poly":[[225,320],[256,320],[256,340],[225,340]],"score":0.8553},{"poly":[[368,307],[504,307],[504,318],[368,318]],"score":0.9497},{"poly":[[223,304],[346,304],[346,322],[223,322]],"score":0.6706},{"poly":[[366,289],[494,289],[494,305],[366,305]],"score":0.7978},{"poly":[[225,289],[306,289],[306,305],[225,305]],"score":0.842},{"poly":[[366,272],[713,272],[713,289],[366,289]],"score":0.7669},{"poly":[[366,256],[980,256],[980,272],[366,272]],"score":0.7738},{"poly":[[226,256],[313,256],[313,274],[226,274]],"score":0.7567},{"poly":[[366,241],[629,241],[629,257],[366,257]],"score":0.7798},{"poly":[[225,241],[348,241],[348,257],[225,257]],"score":0.8239},{"poly":[[366,224],[569,224],[569,241],[366,241]],"score":0.7944},{"poly":[[225,224],[304,224],[304,241],[225,241]],"score":0.891},{"poly":[[366,210],[980,210],[980,226],[366,226]],"score":0.7826},{"poly":[[225,208],[313,208],[313,226],[225,226]],"score":0.7126},{"poly":[[366,193],[918,193],[918,210],[366,210]],"score":0.7805},{"poly":[[226,191],[278,191],[278,210],[226,210]],"score":0.7794},{"poly":[[225,177],[271,177],[271,195],[225,195]],"score":0.8503},{"poly":[[482,157],[662,157],[662,178],[482,178]],"score":0.741},{"poly":[[213,51],[587,49],[587,73],[213,74]],"score":0.725}],"page_no":25,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7241,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":597,"x1":625,"y0":1500,"y1":1523},"font_size":0.0,"text":"27"}],"source":"layout det","text":""},{"bbox":{"x0":208,"x1":1019,"y0":46,"y1":86},"conf":0.411,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":212,"x1":1033,"y0":153,"y1":510},"conf":0.9583,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":484,"x1":660,"y0":160,"y1":174},"font_size":0.0,"text":"Example 3208 (gt: REFUTES)"},{"bbox":{"x0":226,"x1":270,"y0":178,"y1":194},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":225,"x1":278,"y0":192,"y1":210},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":367,"x1":762,"y0":194,"y1":208},"font_size":0.0,"text":"Reign Over Me is an American film made in 2010."},{"bbox":{"x0":226,"x1":312,"y0":210,"y1":224},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":368,"x1":989,"y0":210,"y1":224},"font_size":0.0,"text":"I need to search for Reign Over Me and find if it is an American film made"},{"bbox":{"x0":368,"x1":438,"y0":226,"y1":240},"font_size":0.0,"text":"in 2010."},{"bbox":{"x0":226,"x1":304,"y0":242,"y1":256},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":368,"x1":545,"y0":242,"y1":256},"font_size":0.0,"text":"Search[Reign Over Me]"},{"bbox":{"x0":225,"x1":346,"y0":256,"y1":274},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":365,"x1":510,"y0":256,"y1":275},"font_size":0.0,"text":"Reign Over Me ..."},{"bbox":{"x0":225,"x1":313,"y0":273,"y1":290},"font_size":0.0,"text":"Thought 2:"},{"bbox":{"x0":367,"x1":997,"y0":273,"y1":290},"font_size":0.0,"text":"The observation says that it is an American film made in 2007, so it is not"},{"bbox":{"x0":366,"x1":477,"y0":290,"y1":304},"font_size":0.0,"text":"made in 2010."},{"bbox":{"x0":225,"x1":304,"y0":306,"y1":320},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":367,"x1":493,"y0":306,"y1":320},"font_size":0.0,"text":"Finish[REFUTES]"},{"bbox":{"x0":225,"x1":346,"y0":321,"y1":336},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":367,"x1":504,"y0":322,"y1":337},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":224,"x1":255,"y0":337,"y1":356},"font_size":0.0,"text":"Act"},{"bbox":{"x0":225,"x1":279,"y0":352,"y1":369},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":367,"x1":762,"y0":354,"y1":368},"font_size":0.0,"text":"Reign Over Me is an American film made in 2010."},{"bbox":{"x0":223,"x1":305,"y0":367,"y1":386},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":367,"x1":546,"y0":369,"y1":386},"font_size":0.0,"text":"Search[Reign Over Me]"},{"bbox":{"x0":224,"x1":305,"y0":384,"y1":401},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":366,"x1":502,"y0":385,"y1":401},"font_size":0.0,"text":"Finish[SUPPORTS]"},{"bbox":{"x0":224,"x1":347,"y0":400,"y1":418},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":368,"x1":504,"y0":401,"y1":416},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":225,"x1":255,"y0":417,"y1":436},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":225,"x1":279,"y0":433,"y1":450},"font_size":0.0,"text":"Claim:"},{"bbox":{"x0":367,"x1":762,"y0":435,"y1":449},"font_size":0.0,"text":"Reign Over Me is an American film made in 2010."},{"bbox":{"x0":225,"x1":296,"y0":449,"y1":466},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":367,"x1":947,"y0":450,"y1":467},"font_size":0.0,"text":"Let’s think step by step.First, is Reign Over Me an American film?"},{"bbox":{"x0":948,"x1":987,"y0":450,"y1":467},"font_size":0.0,"text":"Yes,"},{"bbox":{"x0":366,"x1":434,"y0":465,"y1":483},"font_size":0.0,"text":"it is."},{"bbox":{"x0":434,"x1":788,"y0":467,"y1":482},"font_size":0.0,"text":"Second, was it made in 2010?Yes, it was."},{"bbox":{"x0":226,"x1":286,"y0":484,"y1":498},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":367,"x1":437,"y0":480,"y1":498},"font_size":0.0,"text":"SUPPORTS"}],"source":"layout det","text":"<html><body><table><thead><tr><td></td><td>Example 3208 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Thought 1:</td><td>I need to search for Reign Over Me and find if it is an American film made</td></tr><tr><td>Action 1:</td><td>in 2010. Search[Reign Over Me]</td></tr><tr><td>Observation 1:</td><td>Reign Over Me ...</td></tr><tr><td>Thought 2:</td><td>The observation says that it is an American film made in 2007, so it is not</td></tr><tr><td>Action 2:</td><td>made in 2010. Finish[REFUTES]</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td>Act</td><td>Episode finished</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Action 1:</td><td>Search[Reign Over Me]</td></tr><tr><td>Action 2:</td><td>Finish[SUPPORTS] Episode finished</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td>CoT Claim:</td><td></td></tr><tr><td>Thought:</td><td>Reign Over Me is an American film made in 2010. Let’s think step by step.First, is Reign Over Me an American film? Yes,</td></tr><tr><td>Answer:</td><td>it is. Second, was it made in 2010?Yes, it was. SUPPORTS</td></tr></tbody></table></body></html>"},{"bbox":{"x0":209,"x1":515,"y0":777,"y1":808},"conf":0.5248,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":509,"y0":782,"y1":802},"font_size":2.689e-39,"text":"D.2ALFWORLD TRAJECTORIES"}],"source":"layout det","text":"D.2ALFWORLD TRAJECTORIES"},{"bbox":{"x0":208,"x1":1018,"y0":928,"y1":1003},"conf":0.8873,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1011,"y0":929,"y1":955},"font_size":2.689e-39,"text":"We provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,"},{"bbox":{"x0":211,"x1":886,"y0":950,"y1":977},"font_size":2.689e-39,"text":"pick_clean_then_place_in_recep-Knife-None-CounterTop-10/"},{"bbox":{"x0":213,"x1":569,"y0":975,"y1":997},"font_size":2.689e-39,"text":"trial_T20190909_110445_675754."}],"source":"layout det","text":"We provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,pick_clean_then_place_in_recep-Knife-None-CounterTop-10/trial_T20190909_110445_675754."},{"bbox":{"x0":209,"x1":466,"y0":1291,"y1":1322},"conf":0.3677,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":461,"y0":1295,"y1":1317},"font_size":2.689e-39,"text":"D.2.1REACT TRAJECTORY"}],"source":"layout det","text":"D.2.1REACT TRAJECTORY"},{"bbox":{"x0":208,"x1":889,"y0":1438,"y1":1471},"conf":0.8611,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":883,"y0":1440,"y1":1465},"font_size":2.689e-39,"text":"ReAct is able to correctly fnd the knife, clean it, and then place it on a countertop.i"}],"source":"layout det","text":"ReAct is able to correctly fnd the knife, clean it, and then place it on a countertop.i"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":212,"x1":1033,"y0":153,"y1":510},"conf":0.9583,"label":"Table","label_id":5},{"bbox":{"x0":208,"x1":1018,"y0":928,"y1":1003},"conf":0.8873,"label":"Text","label_id":1},{"bbox":{"x0":208,"x1":889,"y0":1438,"y1":1471},"conf":0.8611,"label":"Text","label_id":1},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7241,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":515,"y0":777,"y1":808},"conf":0.5248,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.4991,"label":"Abandon","label_id":2},{"bbox":{"x0":208,"x1":1019,"y0":46,"y1":86},"conf":0.411,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":466,"y0":1291,"y1":1322},"conf":0.3677,"label":"Title","label_id":0},{"bbox":{"x0":480,"x1":664,"y0":157,"y1":176},"conf":0.2693,"label":"Table caption","label_id":6},{"bbox":{"x0":209,"x1":466,"y0":1291,"y1":1322},"conf":0.2217,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[597,1500],[625,1500],[625,1523],[597,1523]],"score":0.8521},{"poly":[[211,1440],[883,1442],[883,1465],[211,1464]],"score":0.7729},{"poly":[[213,1295],[461,1295],[461,1317],[213,1317]],"score":0.8225},{"poly":[[213,975],[569,975],[569,997],[213,997]],"score":0.8482},{"poly":[[211,952],[886,950],[886,975],[211,977]],"score":0.6543},{"poly":[[211,929],[1011,932],[1011,955],[211,952]],"score":0.7153},{"poly":[[213,782],[509,784],[509,802],[213,800]],"score":0.804},{"poly":[[368,482],[436,482],[436,498],[368,498]],"score":0.9294},{"poly":[[223,482],[284,482],[284,498],[223,498]],"score":0.904},{"poly":[[429,464],[790,464],[790,485],[429,485]],"score":0.6734},{"poly":[[368,465],[429,465],[429,482],[368,482]],"score":0.7471},{"poly":[[936,450],[988,450],[988,467],[936,467]],"score":0.7571},{"poly":[[589,450],[943,450],[943,467],[589,467]],"score":0.7751},{"poly":[[368,450],[582,450],[582,467],[368,467]],"score":0.785},{"poly":[[225,450],[296,450],[296,467],[225,467]],"score":0.9253},{"poly":[[366,434],[762,434],[762,450],[366,450]],"score":0.8033},{"poly":[[225,432],[279,432],[279,450],[225,450]],"score":0.7753},{"poly":[[225,416],[256,416],[256,436],[225,436]],"score":0.7472},{"poly":[[366,401],[504,401],[504,417],[366,417]],"score":0.8445},{"poly":[[223,399],[346,399],[346,417],[223,417]],"score":0.6716},{"poly":[[366,384],[501,384],[501,401],[366,401]],"score":0.8507},{"poly":[[225,384],[304,384],[304,401],[225,401]],"score":0.8498},{"poly":[[364,366],[544,368],[544,386],[364,384]],"score":0.7453},{"poly":[[225,368],[304,368],[304,386],[225,386]],"score":0.7591},{"poly":[[225,353],[279,353],[279,370],[225,370]],"score":0.8925},{"poly":[[366,351],[760,350],[760,368],[366,370]],"score":0.7298},{"poly":[[223,337],[254,337],[254,355],[223,355]],"score":0.852},{"poly":[[368,320],[506,320],[506,337],[368,337]],"score":0.8435},{"poly":[[223,320],[344,320],[344,337],[223,337]],"score":0.8178},{"poly":[[225,305],[304,305],[304,322],[225,322]],"score":0.8309},{"poly":[[368,302],[496,302],[496,322],[368,322]],"score":0.6681},{"poly":[[368,289],[476,289],[476,305],[368,305]],"score":0.8266},{"poly":[[366,271],[998,271],[998,292],[366,292]],"score":0.6922},{"poly":[[225,272],[311,272],[311,289],[225,289]],"score":0.9087},{"poly":[[364,254],[481,256],[480,274],[364,272]],"score":0.7295},{"poly":[[225,256],[346,256],[346,274],[225,274]],"score":0.7075},{"poly":[[366,241],[542,241],[542,257],[366,257]],"score":0.8827},{"poly":[[225,241],[303,241],[303,257],[225,257]],"score":0.8793},{"poly":[[368,224],[436,224],[436,241],[368,241]],"score":0.8819},{"poly":[[368,210],[990,210],[990,226],[368,226]],"score":0.797},{"poly":[[225,206],[311,206],[311,224],[225,224]],"score":0.6783},{"poly":[[228,193],[278,193],[278,210],[228,210]],"score":0.848},{"poly":[[364,190],[760,188],[760,210],[364,211]],"score":0.7779},{"poly":[[225,177],[271,177],[271,195],[225,195]],"score":0.8398},{"poly":[[482,157],[662,157],[662,178],[482,178]],"score":0.7625},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7441}],"page_no":26,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7528,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1500,"y1":1523},"font_size":0.0,"text":"28"}],"source":"layout det","text":""},{"bbox":{"x0":485,"x1":660,"y0":156,"y1":179},"conf":0.309,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.3021,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":485,"x1":660,"y0":156,"y1":179},"conf":0.4643,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":487,"x1":654,"y0":160,"y1":177},"font_size":6.0,"text":"ReAct ALFWorld trajectory"}],"source":"layout det","text":"ReAct ALFWorld trajectory"},{"bbox":{"x0":220,"x1":945,"y0":184,"y1":281},"conf":0.9422,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":935,"y0":185,"y1":206},"font_size":-4.065e+19,"text":"You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a"},{"bbox":{"x0":225,"x1":926,"y0":203,"y1":219},"font_size":-4.065e+19,"text":"cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a"},{"bbox":{"x0":226,"x1":925,"y0":219,"y1":236},"font_size":-4.065e+19,"text":"countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a"},{"bbox":{"x0":224,"x1":941,"y0":231,"y1":254},"font_size":-4.065e+19,"text":"fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin"},{"bbox":{"x0":225,"x1":931,"y0":251,"y1":267},"font_size":-4.065e+19,"text":"1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster"},{"bbox":{"x0":226,"x1":241,"y0":267,"y1":280},"font_size":-4.065e+19,"text":"1."}],"source":"layout det","text":"You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1."},{"bbox":{"x0":223,"x1":648,"y0":283,"y1":299},"conf":0.5875,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":645,"y0":282,"y1":299},"font_size":6.0,"text":"Your task is to:put a clean knife in countertop."}],"source":"layout det","text":"Your task is to:put a clean knife in countertop."},{"bbox":{"x0":221,"x1":903,"y0":299,"y1":326},"conf":0.5844,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":539,"y0":308,"y1":333},"font_size":6.0,"text":"sinkbasin, then put it in countertop."},{"bbox":{"x0":226,"x1":898,"y0":299,"y1":315},"font_size":6.0,"text":"> think:To solve the task, I need to find and take a knife, then clean it with"}],"source":"layout det","text":"sinkbasin, then put it in countertop.> think:To solve the task, I need to find and take a knife, then clean it with"},{"bbox":{"x0":220,"x1":940,"y0":345,"y1":381},"conf":0.2701,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":931,"y0":346,"y1":363},"font_size":6.0,"text":"> think:First I need to find a knife.A knife is more likely to appear in cabinet"},{"bbox":{"x0":225,"x1":890,"y0":360,"y1":381},"font_size":6.0,"text":"(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),"}],"source":"layout det","text":"> think:First I need to find a knife.A knife is more likely to appear in cabinet(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),"},{"bbox":{"x0":220,"x1":945,"y0":379,"y1":417},"conf":0.3016,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":941,"y0":378,"y1":394},"font_size":6.0,"text":"sinkbasin (1), stoveburner (1-4), toaster (1).I can check one by one, starting with"},{"bbox":{"x0":225,"x1":313,"y0":394,"y1":411},"font_size":6.0,"text":"cabinet 1."}],"source":"layout det","text":"sinkbasin (1), stoveburner (1-4), toaster (1).I can check one by one, starting with cabinet 1."},{"bbox":{"x0":222,"x1":255,"y0":411,"y1":426},"conf":0.6542,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":254,"y0":411,"y1":424},"font_size":6.0,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":222,"x1":374,"y0":427,"y1":443},"conf":0.8828,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":373,"y0":424,"y1":442},"font_size":6.0,"text":"> go to cabinet 1"}],"source":"layout det","text":"> go to cabinet 1"},{"bbox":{"x0":222,"x1":524,"y0":443,"y1":459},"conf":0.8632,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":522,"y0":441,"y1":459},"font_size":6.0,"text":"On the cabinet 1, you see a bowl 1."}],"source":"layout det","text":"On the cabinet 1, you see a bowl 1."},{"bbox":{"x0":222,"x1":379,"y0":459,"y1":474},"conf":0.8718,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":373,"y0":459,"y1":475},"font_size":6.0,"text":"> go to cabinet 2"}],"source":"layout det","text":"> go to cabinet 2"},{"bbox":{"x0":222,"x1":431,"y0":475,"y1":490},"conf":0.8853,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":429,"y0":474,"y1":490},"font_size":6.0,"text":"The cabinet 2 is closed."}],"source":"layout det","text":"The cabinet 2 is closed."},{"bbox":{"x0":222,"x1":378,"y0":491,"y1":506},"conf":0.8654,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":373,"y0":490,"y1":507},"font_size":6.0,"text":"> go to cabinet 3"}],"source":"layout det","text":"> go to cabinet 3"},{"bbox":{"x0":222,"x1":580,"y0":507,"y1":523},"conf":0.4427,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":579,"y0":503,"y1":523},"font_size":6.0,"text":"On the cabinet 3, you see a glassbottle 1."}],"source":"layout det","text":"On the cabinet 3, you see a glassbottle 1."},{"bbox":{"x0":223,"x1":374,"y0":523,"y1":538},"conf":0.5951,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":371,"y0":521,"y1":538},"font_size":6.0,"text":"> go to cabinet 4"}],"source":"layout det","text":"> go to cabinet 4"},{"bbox":{"x0":222,"x1":513,"y0":538,"y1":553},"conf":0.8999,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":512,"y0":535,"y1":554},"font_size":-4.065e+19,"text":"On the cabinet 4, you see a mug 1."}],"source":"layout det","text":"On the cabinet 4, you see a mug 1."},{"bbox":{"x0":222,"x1":374,"y0":555,"y1":569},"conf":0.8259,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":373,"y0":553,"y1":569},"font_size":6.0,"text":"> go to cabinet 5"}],"source":"layout det","text":"> go to cabinet 5"},{"bbox":{"x0":222,"x1":433,"y0":570,"y1":585},"conf":0.8837,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":429,"y0":569,"y1":586},"font_size":6.0,"text":"The cabinet 5 is closed."}],"source":"layout det","text":"The cabinet 5 is closed."},{"bbox":{"x0":223,"x1":421,"y0":586,"y1":602},"conf":0.8916,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":227,"x1":370,"y0":589,"y1":600},"font_size":6.0,"text":"> go to cabinet 6"}],"source":"layout det","text":"> go to cabinet 6"},{"bbox":{"x0":222,"x1":431,"y0":602,"y1":618},"conf":0.3207,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":427,"y0":599,"y1":615},"font_size":6.0,"text":"The cabinet 6 is closed."}],"source":"layout det","text":"The cabinet 6 is closed."},{"bbox":{"x0":223,"x1":370,"y0":619,"y1":633},"conf":0.7952,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":363,"y0":617,"y1":634},"font_size":6.0,"text":"> go to drawer 1"}],"source":"layout det","text":"> go to drawer 1"},{"bbox":{"x0":222,"x1":423,"y0":634,"y1":650},"conf":0.8856,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":419,"y0":630,"y1":650},"font_size":6.0,"text":"The drawer 1 is closed."}],"source":"layout det","text":"The drawer 1 is closed."},{"bbox":{"x0":222,"x1":367,"y0":650,"y1":665},"conf":0.8822,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":364,"y0":648,"y1":665},"font_size":6.0,"text":"> go to drawer 2"}],"source":"layout det","text":"> go to drawer 2"},{"bbox":{"x0":222,"x1":414,"y0":666,"y1":682},"conf":0.8265,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":419,"y0":665,"y1":681},"font_size":6.0,"text":"The drawer 2 is closed"}],"source":"layout det","text":"The drawer 2 is closed"},{"bbox":{"x0":223,"x1":402,"y0":682,"y1":698},"conf":0.8723,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":363,"y0":681,"y1":698},"font_size":6.0,"text":"> go to drawer 3"}],"source":"layout det","text":"> go to drawer 3"},{"bbox":{"x0":223,"x1":417,"y0":698,"y1":713},"conf":0.8336,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":421,"y0":696,"y1":713},"font_size":6.0,"text":"The drawer 3 is closed."}],"source":"layout det","text":"The drawer 3 is closed."},{"bbox":{"x0":222,"x1":397,"y0":714,"y1":730},"conf":0.8049,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":396,"y0":713,"y1":729},"font_size":6.0,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":222,"x1":929,"y0":729,"y1":760},"conf":0.5059,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":921,"y0":729,"y1":746},"font_size":6.0,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":223,"x1":398,"y0":742,"y1":761},"font_size":6.0,"text":"> go to countertop 2"}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> go to countertop 2"},{"bbox":{"x0":222,"x1":927,"y0":761,"y1":793},"conf":0.8691,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":926,"y0":759,"y1":780},"font_size":6.0,"text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a"},{"bbox":{"x0":225,"x1":531,"y0":777,"y1":794},"font_size":6.0,"text":"plate 2, a potato 3, and a statue 1."}],"source":"layout det","text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1."},{"bbox":{"x0":221,"x1":725,"y0":793,"y1":813},"conf":0.7756,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":722,"y0":792,"y1":808},"font_size":6.0,"text":"> think:Now I find a knife (1).Next, I need to take it."}],"source":"layout det","text":"> think:Now I find a knife (1).Next, I need to take it."},{"bbox":{"x0":221,"x1":500,"y0":825,"y1":840},"conf":0.8637,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":497,"y0":823,"y1":840},"font_size":6.0,"text":"> take knife 1 from countertop 2"}],"source":"layout det","text":"> take knife 1 from countertop 2"},{"bbox":{"x0":222,"x1":615,"y0":841,"y1":857},"conf":0.8867,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":609,"y0":840,"y1":856},"font_size":6.0,"text":"You pick up the knife 1 from the countertop 2."}],"source":"layout det","text":"You pick up the knife 1 from the countertop 2."},{"bbox":{"x0":214,"x1":933,"y0":858,"y1":879},"conf":0.2121,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":930,"y0":853,"y1":873},"font_size":6.0,"text":"> think:Now I take a knife (1).Next, I need to go to sinkbasin (1) and clean it."}],"source":"layout det","text":"> think:Now I take a knife (1).Next, I need to go to sinkbasin (1) and clean it."},{"bbox":{"x0":222,"x1":391,"y0":889,"y1":905},"conf":0.8515,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":389,"y0":888,"y1":904},"font_size":6.0,"text":"> go to sinkbasin 1"}],"source":"layout det","text":"> go to sinkbasin 1"},{"bbox":{"x0":216,"x1":794,"y0":905,"y1":921},"conf":0.5843,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":790,"y0":901,"y1":922},"font_size":6.0,"text":"On the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2."}],"source":"layout det","text":"On the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2."},{"bbox":{"x0":222,"x1":499,"y0":921,"y1":936},"conf":0.8666,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":496,"y0":922,"y1":934},"font_size":6.0,"text":"> clean knife 1 with sinkbasin 1"}],"source":"layout det","text":"> clean knife 1 with sinkbasin 1"},{"bbox":{"x0":222,"x1":599,"y0":937,"y1":953},"conf":0.4304,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":595,"y0":936,"y1":952},"font_size":6.0,"text":"You clean the knife 1 using the sinkbasin 1."}],"source":"layout det","text":"You clean the knife 1 using the sinkbasin 1."},{"bbox":{"x0":218,"x1":881,"y0":954,"y1":975},"conf":0.5389,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":876,"y0":952,"y1":969},"font_size":6.0,"text":"> think:Now I clean a knife (1).Next, I need to put it in/on countertop 1."}],"source":"layout det","text":"> think:Now I clean a knife (1).Next, I need to put it in/on countertop 1."},{"bbox":{"x0":222,"x1":400,"y0":985,"y1":1001},"conf":0.8671,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":396,"y0":983,"y1":1000},"font_size":6.0,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":219,"x1":927,"y0":1000,"y1":1033},"conf":0.3465,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":921,"y0":1000,"y1":1016},"font_size":6.0,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":221,"x1":497,"y0":1011,"y1":1035},"font_size":6.0,"text":"> put knife 1 in/on countertop 1"}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> put knife 1 in/on countertop 1"},{"bbox":{"x0":223,"x1":591,"y0":1033,"y1":1050},"conf":0.7735,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":580,"y0":1030,"y1":1051},"font_size":6.0,"text":"You put the knife 1 in/on the countertop 1"}],"source":"layout det","text":"You put the knife 1 in/on the countertop 1"},{"bbox":{"x0":209,"x1":442,"y0":1265,"y1":1295},"conf":0.7036,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":436,"y0":1267,"y1":1290},"font_size":6.0,"text":"D.2.2ACT TRAJECTORY"}],"source":"layout det","text":"D.2.2ACT TRAJECTORY"},{"bbox":{"x0":208,"x1":1016,"y0":1372,"y1":1471},"conf":0.9465,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1009,"y0":1374,"y1":1399},"font_size":-4.065e+19,"text":"Act is able to correctly fnd the knife, but tries to clean it using a sinkbasin before actually goingi"},{"bbox":{"x0":215,"x1":1008,"y0":1401,"y1":1417},"font_size":-4.065e+19,"text":"to the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same"},{"bbox":{"x0":213,"x1":1011,"y0":1421,"y1":1442},"font_size":-4.065e+19,"text":"sequence of commands. With ReAct, an existing thought would have concretely identifed that thei"},{"bbox":{"x0":215,"x1":875,"y0":1442,"y1":1464},"font_size":-4.065e+19,"text":"knife had been taken, and that the next subgoal was to go to a sinkbasin to clean it."}],"source":"layout det","text":"Act is able to correctly fnd the knife, but tries to clean it using a sinkbasin before actually goingi to the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same sequence of commands. With ReAct, an existing thought would have concretely identifed that thei knife had been taken, and that the next subgoal was to go to a sinkbasin to clean it."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":208,"x1":1016,"y0":1372,"y1":1471},"conf":0.9465,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":945,"y0":184,"y1":281},"conf":0.9422,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":513,"y0":538,"y1":553},"conf":0.8999,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":421,"y0":586,"y1":602},"conf":0.8916,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":615,"y0":841,"y1":857},"conf":0.8867,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":423,"y0":634,"y1":650},"conf":0.8856,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":431,"y0":475,"y1":490},"conf":0.8853,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":433,"y0":570,"y1":585},"conf":0.8837,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":374,"y0":427,"y1":443},"conf":0.8828,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":367,"y0":650,"y1":665},"conf":0.8822,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":402,"y0":682,"y1":698},"conf":0.8723,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":379,"y0":459,"y1":474},"conf":0.8718,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":927,"y0":761,"y1":793},"conf":0.8691,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":400,"y0":985,"y1":1001},"conf":0.8671,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":499,"y0":921,"y1":936},"conf":0.8666,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":378,"y0":491,"y1":506},"conf":0.8654,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":500,"y0":825,"y1":840},"conf":0.8637,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":524,"y0":443,"y1":459},"conf":0.8632,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":391,"y0":889,"y1":905},"conf":0.8515,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":497,"y0":1018,"y1":1032},"conf":0.8378,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":417,"y0":698,"y1":713},"conf":0.8336,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":414,"y0":666,"y1":682},"conf":0.8265,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":374,"y0":555,"y1":569},"conf":0.8259,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":397,"y0":714,"y1":730},"conf":0.8049,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":370,"y0":619,"y1":633},"conf":0.7952,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":81},"conf":0.78,"label":"Abandon","label_id":2},{"bbox":{"x0":221,"x1":725,"y0":793,"y1":813},"conf":0.7756,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":591,"y0":1033,"y1":1050},"conf":0.7735,"label":"Text","label_id":1},{"bbox":{"x0":595,"x1":627,"y0":1499,"y1":1527},"conf":0.7528,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":442,"y0":1265,"y1":1295},"conf":0.7036,"label":"Title","label_id":0},{"bbox":{"x0":222,"x1":255,"y0":411,"y1":426},"conf":0.6542,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":373,"y0":523,"y1":538},"conf":0.6335,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":374,"y0":523,"y1":538},"conf":0.5951,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":648,"y0":283,"y1":299},"conf":0.5875,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":903,"y0":299,"y1":326},"conf":0.5844,"label":"Text","label_id":1},{"bbox":{"x0":216,"x1":794,"y0":905,"y1":921},"conf":0.5843,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":934,"y0":857,"y1":877},"conf":0.5709,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":431,"y0":602,"y1":618},"conf":0.5628,"label":"Text","label_id":1},{"bbox":{"x0":218,"x1":881,"y0":954,"y1":975},"conf":0.5389,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":539,"y0":315,"y1":331},"conf":0.5259,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":933,"y0":857,"y1":876},"conf":0.5119,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":932,"y0":345,"y1":363},"conf":0.5082,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":929,"y0":729,"y1":760},"conf":0.5059,"label":"Text","label_id":1},{"bbox":{"x0":485,"x1":660,"y0":156,"y1":179},"conf":0.4643,"label":"Title","label_id":0},{"bbox":{"x0":235,"x1":599,"y0":936,"y1":952},"conf":0.451,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":580,"y0":507,"y1":523},"conf":0.4427,"label":"Text","label_id":1},{"bbox":{"x0":226,"x1":878,"y0":953,"y1":973},"conf":0.4362,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":599,"y0":937,"y1":953},"conf":0.4304,"label":"Text","label_id":1},{"bbox":{"x0":219,"x1":927,"y0":1000,"y1":1033},"conf":0.3465,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":397,"y0":731,"y1":746},"conf":0.3242,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":431,"y0":602,"y1":618},"conf":0.3207,"label":"Text","label_id":1},{"bbox":{"x0":485,"x1":660,"y0":156,"y1":179},"conf":0.309,"label":"Abandon","label_id":2},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.3021,"label":"Abandon","label_id":2},{"bbox":{"x0":220,"x1":945,"y0":379,"y1":417},"conf":0.3016,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":940,"y0":345,"y1":381},"conf":0.2701,"label":"Text","label_id":1},{"bbox":{"x0":214,"x1":933,"y0":858,"y1":879},"conf":0.2121,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":924,"y0":1000,"y1":1016},"conf":0.2048,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":423,"y0":666,"y1":681},"conf":0.2006,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[625,1500],[625,1523],[599,1523]],"score":0.8491},{"poly":[[215,1442],[875,1442],[875,1464],[215,1464]],"score":0.8032},{"poly":[[213,1421],[1011,1421],[1011,1442],[213,1442]],"score":0.7795},{"poly":[[215,1401],[1008,1401],[1008,1417],[215,1417]],"score":0.9549},{"poly":[[211,1374],[1009,1376],[1009,1399],[211,1398]],"score":0.6912},{"poly":[[213,1267],[436,1269],[436,1290],[213,1289]],"score":0.8512},{"poly":[[392,1033],[580,1033],[580,1049],[392,1049]],"score":0.7826},{"poly":[[223,1030],[392,1030],[392,1051],[223,1051]],"score":0.651},{"poly":[[221,1011],[497,1013],[497,1035],[221,1033]],"score":0.7115},{"poly":[[225,1000],[921,1000],[921,1016],[225,1016]],"score":0.7898},{"poly":[[225,983],[396,983],[396,1000],[225,1000]],"score":0.791},{"poly":[[225,969],[254,969],[254,982],[225,982]],"score":0.808},{"poly":[[303,952],[876,952],[876,969],[303,969]],"score":0.7755},{"poly":[[226,954],[308,954],[308,965],[226,965]],"score":0.9361},{"poly":[[225,936],[595,936],[595,952],[225,952]],"score":0.7727},{"poly":[[226,922],[496,922],[496,934],[226,934]],"score":0.9857},{"poly":[[223,901],[790,901],[790,922],[223,922]],"score":0.674},{"poly":[[225,888],[389,888],[389,904],[225,904]],"score":0.7545},{"poly":[[226,873],[254,873],[254,886],[226,886]],"score":0.9124},{"poly":[[223,853],[930,855],[930,873],[223,871]],"score":0.6963},{"poly":[[223,840],[609,840],[609,856],[223,856]],"score":0.8642},{"poly":[[225,823],[497,823],[497,840],[225,840]],"score":0.7893},{"poly":[[226,810],[254,810],[254,823],[226,823]],"score":0.8654},{"poly":[[516,792],[722,792],[722,808],[516,808]],"score":0.8277},{"poly":[[225,792],[509,792],[509,808],[225,808]],"score":0.7647},{"poly":[[225,777],[531,777],[531,794],[225,794]],"score":0.7906},{"poly":[[223,759],[926,759],[926,780],[223,780]],"score":0.6994},{"poly":[[223,742],[398,744],[397,761],[223,759]],"score":0.7682},{"poly":[[399,729],[921,729],[921,746],[399,746]],"score":0.8205},{"poly":[[225,729],[402,729],[402,746],[225,746]],"score":0.7742},{"poly":[[225,713],[396,713],[396,729],[225,729]],"score":0.7971},{"poly":[[225,696],[421,696],[421,713],[225,713]],"score":0.7794},{"poly":[[225,681],[363,681],[363,698],[225,698]],"score":0.7937},{"poly":[[225,665],[419,665],[419,681],[225,681]],"score":0.8044},{"poly":[[225,648],[364,648],[364,665],[225,665]],"score":0.7483},{"poly":[[223,630],[419,632],[419,650],[223,648]],"score":0.6857},{"poly":[[225,617],[363,617],[363,634],[225,634]],"score":0.7622},{"poly":[[223,599],[427,599],[427,615],[223,615]],"score":0.7421},{"poly":[[225,569],[429,569],[429,586],[225,586]],"score":0.7848},{"poly":[[225,553],[373,553],[373,569],[225,569]],"score":0.7457},{"poly":[[374,538],[512,538],[512,554],[374,554]],"score":0.7851},{"poly":[[225,535],[376,535],[376,551],[225,551]],"score":0.6033},{"poly":[[225,521],[371,521],[371,538],[225,538]],"score":0.7734},{"poly":[[225,507],[374,507],[374,523],[225,523]],"score":0.7963},{"poly":[[372,505],[579,503],[579,521],[373,523]],"score":0.7171},{"poly":[[225,490],[373,490],[373,507],[225,507]],"score":0.7451},{"poly":[[225,474],[429,474],[429,490],[225,490]],"score":0.8067},{"poly":[[225,459],[373,459],[373,475],[225,475]],"score":0.779},{"poly":[[374,442],[522,442],[522,459],[374,459]],"score":0.8613},{"poly":[[223,441],[374,441],[374,457],[223,457]],"score":0.7661},{"poly":[[223,426],[372,424],[373,440],[223,442]],"score":0.7603},{"poly":[[226,411],[254,411],[254,424],[226,424]],"score":0.853},{"poly":[[225,394],[313,394],[313,411],[225,411]],"score":0.8005},{"poly":[[625,378],[941,378],[941,394],[625,394]],"score":0.7867},{"poly":[[223,378],[615,378],[615,394],[223,394]],"score":0.7821},{"poly":[[225,360],[890,360],[890,381],[225,381]],"score":0.6666},{"poly":[[225,346],[931,346],[931,363],[225,363]],"score":0.7742},{"poly":[[226,332],[254,332],[254,345],[226,345]],"score":0.9042},{"poly":[[221,308],[539,312],[539,333],[221,330]],"score":0.6295},{"poly":[[308,299],[898,299],[898,315],[308,315]],"score":0.7834},{"poly":[[226,300],[304,300],[304,312],[226,312]],"score":0.9202},{"poly":[[373,282],[645,282],[645,299],[373,299]],"score":0.7416},{"poly":[[225,282],[376,282],[376,299],[225,299]],"score":0.7677},{"poly":[[226,267],[241,267],[241,280],[226,280]],"score":0.8497},{"poly":[[225,251],[931,251],[931,267],[225,267]],"score":0.635},{"poly":[[225,231],[941,233],[941,254],[224,252]],"score":0.6963},{"poly":[[226,219],[925,219],[925,236],[226,236]],"score":0.7463},{"poly":[[225,203],[926,203],[926,219],[225,219]],"score":0.7504},{"poly":[[507,185],[935,185],[935,206],[507,206]],"score":0.6282},{"poly":[[226,186],[501,186],[501,203],[226,203]],"score":0.7554},{"poly":[[487,160],[654,160],[654,177],[487,177]],"score":0.8601},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7046}],"page_no":27,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1526},"conf":0.7237,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1500,"y1":1521},"font_size":0.0,"text":"29"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.2801,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":493,"x1":651,"y0":157,"y1":179},"conf":0.687,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":492,"x1":645,"y0":157,"y1":178},"font_size":6.0,"text":"Act ALFWorld trajectory"}],"source":"layout det","text":"Act ALFWorld trajectory"},{"bbox":{"x0":220,"x1":943,"y0":184,"y1":282},"conf":0.9365,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":935,"y0":185,"y1":206},"font_size":-4.0910000000000005e+31,"text":"You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a"},{"bbox":{"x0":223,"x1":925,"y0":203,"y1":219},"font_size":-4.0910000000000005e+31,"text":"cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a"},{"bbox":{"x0":226,"x1":925,"y0":219,"y1":236},"font_size":-4.0910000000000005e+31,"text":"countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a"},{"bbox":{"x0":224,"x1":941,"y0":231,"y1":254},"font_size":-4.0910000000000005e+31,"text":"fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin"},{"bbox":{"x0":225,"x1":931,"y0":251,"y1":267},"font_size":-4.0910000000000005e+31,"text":"1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster"},{"bbox":{"x0":226,"x1":241,"y0":267,"y1":280},"font_size":-4.0910000000000005e+31,"text":"1."}],"source":"layout det","text":"You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1."},{"bbox":{"x0":223,"x1":648,"y0":283,"y1":299},"conf":0.5564,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":647,"y0":282,"y1":299},"font_size":6.0,"text":"Your task is to:put a clean knife in countertop."}],"source":"layout det","text":"Your task is to:put a clean knife in countertop."},{"bbox":{"x0":222,"x1":400,"y0":300,"y1":315},"conf":0.7788,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":399,"y0":299,"y1":315},"font_size":6.0,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":223,"x1":928,"y0":314,"y1":346},"conf":0.769,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":921,"y0":312,"y1":332},"font_size":6.0,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":225,"x1":397,"y0":330,"y1":346},"font_size":6.0,"text":"> go to countertop 2"}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> go to countertop 2"},{"bbox":{"x0":221,"x1":929,"y0":347,"y1":378},"conf":0.8826,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":925,"y0":346,"y1":363},"font_size":-4.0910000000000005e+31,"text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a"},{"bbox":{"x0":225,"x1":531,"y0":361,"y1":378},"font_size":-4.0910000000000005e+31,"text":"plate 2, a potato 3, and a statue 1."}],"source":"layout det","text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1."},{"bbox":{"x0":222,"x1":498,"y0":380,"y1":394},"conf":0.8338,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":226,"x1":499,"y0":378,"y1":394},"font_size":6.0,"text":"> take knife 1 from countertop 2"}],"source":"layout det","text":"> take knife 1 from countertop 2"},{"bbox":{"x0":223,"x1":615,"y0":395,"y1":410},"conf":0.8419,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":612,"y0":394,"y1":411},"font_size":6.0,"text":"You pick up the knife 1 from the countertop 2."}],"source":"layout det","text":"You pick up the knife 1 from the countertop 2."},{"bbox":{"x0":222,"x1":401,"y0":412,"y1":427},"conf":0.7196,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":397,"y0":411,"y1":427},"font_size":6.0,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":220,"x1":925,"y0":426,"y1":458},"conf":0.7352,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":921,"y0":426,"y1":442},"font_size":6.0,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":226,"x1":497,"y0":444,"y1":455},"font_size":6.0,"text":"> clean knife 1 with sinkbasin 1"}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1"},{"bbox":{"x0":222,"x1":365,"y0":459,"y1":475},"conf":0.8216,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":363,"y0":459,"y1":475},"font_size":6.0,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":402,"y0":476,"y1":491},"conf":0.8908,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":399,"y0":474,"y1":490},"font_size":-4.0910000000000005e+31,"text":"> go to countertop 3"}],"source":"layout det","text":"> go to countertop 3"},{"bbox":{"x0":220,"x1":944,"y0":507,"y1":537},"conf":0.291,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":941,"y0":507,"y1":523},"font_size":6.0,"text":"1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato"},{"bbox":{"x0":225,"x1":466,"y0":521,"y1":538},"font_size":6.0,"text":"2, a tomato 1, and a vase 2."}],"source":"layout det","text":"1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2."},{"bbox":{"x0":222,"x1":499,"y0":539,"y1":553},"conf":0.8169,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":536,"y1":553},"font_size":6.0,"text":"> take knife 2 from countertop 3"}],"source":"layout det","text":"> take knife 2 from countertop 3"},{"bbox":{"x0":222,"x1":363,"y0":554,"y1":570},"conf":0.8408,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":551,"y1":571},"font_size":6.0,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":401,"y0":571,"y1":586},"conf":0.8976,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":397,"y0":569,"y1":586},"font_size":-4.0910000000000005e+31,"text":"> go to countertop 2"}],"source":"layout det","text":"> go to countertop 2"},{"bbox":{"x0":220,"x1":929,"y0":585,"y1":617},"conf":0.8079,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":926,"y0":582,"y1":604},"font_size":6.0,"text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a"},{"bbox":{"x0":223,"x1":436,"y0":597,"y1":620},"font_size":6.0,"text":"potato 3, and a statue 1."}],"source":"layout det","text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1."},{"bbox":{"x0":222,"x1":499,"y0":618,"y1":634},"conf":0.8281,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":617,"y1":634},"font_size":6.0,"text":"> take knife 1 from countertop 2"}],"source":"layout det","text":"> take knife 1 from countertop 2"},{"bbox":{"x0":222,"x1":362,"y0":634,"y1":650},"conf":0.788,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":361,"y0":634,"y1":650},"font_size":6.0,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":401,"y0":651,"y1":666},"conf":0.8194,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":398,"y0":645,"y1":668},"font_size":6.0,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":221,"x1":926,"y0":665,"y1":682},"conf":0.7038,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":921,"y0":660,"y1":685},"font_size":6.0,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":222,"x1":497,"y0":683,"y1":698},"conf":0.5628,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":681,"y1":698},"font_size":6.0,"text":"> clean knife 1 with sinkbasin 1"}],"source":"layout det","text":"> clean knife 1 with sinkbasin 1"},{"bbox":{"x0":222,"x1":362,"y0":698,"y1":714},"conf":0.8877,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":695,"y1":715},"font_size":-4.0910000000000005e+31,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":401,"y0":715,"y1":730},"conf":0.3104,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":399,"y0":713,"y1":729},"font_size":6.0,"text":"> go to countertop 3"}],"source":"layout det","text":"> go to countertop 3"},{"bbox":{"x0":220,"x1":936,"y0":728,"y1":746},"conf":0.2324,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":933,"y0":726,"y1":747},"font_size":6.0,"text":"On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard"}],"source":"layout det","text":"On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard"},{"bbox":{"x0":221,"x1":944,"y0":747,"y1":777},"conf":0.36,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":941,"y0":742,"y1":764},"font_size":6.0,"text":"1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato"},{"bbox":{"x0":225,"x1":467,"y0":761,"y1":777},"font_size":6.0,"text":"2, a tomato 1, and a vase 2."}],"source":"layout det","text":"1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2."},{"bbox":{"x0":223,"x1":501,"y0":778,"y1":793},"conf":0.482,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":501,"y0":777,"y1":794},"font_size":6.0,"text":"> take knife 2 from countertop 3"}],"source":"layout det","text":"> take knife 2 from countertop 3"},{"bbox":{"x0":222,"x1":363,"y0":793,"y1":809},"conf":0.8751,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":363,"y0":792,"y1":808},"font_size":-4.0910000000000005e+31,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":401,"y0":810,"y1":825},"conf":0.3961,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":398,"y0":803,"y1":827},"font_size":6.0,"text":"> go to countertop 2"}],"source":"layout det","text":"> go to countertop 2"},{"bbox":{"x0":222,"x1":928,"y0":824,"y1":855},"conf":0.7906,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":926,"y0":822,"y1":843},"font_size":6.0,"text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a"},{"bbox":{"x0":223,"x1":436,"y0":836,"y1":860},"font_size":6.0,"text":"potato 3, and a statue 1."}],"source":"layout det","text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1."},{"bbox":{"x0":222,"x1":499,"y0":857,"y1":872},"conf":0.857,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":501,"y0":851,"y1":875},"font_size":6.0,"text":"> take knife 1 from countertop 2"}],"source":"layout det","text":"> take knife 1 from countertop 2"},{"bbox":{"x0":222,"x1":362,"y0":873,"y1":889},"conf":0.8925,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":869,"y1":889},"font_size":-4.0910000000000005e+31,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":400,"y0":889,"y1":905},"conf":0.8692,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":396,"y0":888,"y1":904},"font_size":6.0,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":220,"x1":926,"y0":904,"y1":935},"conf":0.2145,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":921,"y0":904,"y1":921},"font_size":6.0,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":226,"x1":497,"y0":922,"y1":934},"font_size":6.0,"text":"> clean knife 1 with sinkbasin 1"}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1"},{"bbox":{"x0":222,"x1":366,"y0":937,"y1":953},"conf":0.8506,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":934,"y1":954},"font_size":6.0,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":402,"y0":954,"y1":969},"conf":0.8645,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":399,"y0":952,"y1":969},"font_size":6.0,"text":"> go to countertop 3"}],"source":"layout det","text":"> go to countertop 3"},{"bbox":{"x0":218,"x1":944,"y0":968,"y1":1018},"conf":0.2978,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":933,"y0":969,"y1":985},"font_size":6.0,"text":"On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard"},{"bbox":{"x0":223,"x1":943,"y0":982,"y1":1003},"font_size":6.0,"text":"1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato"},{"bbox":{"x0":221,"x1":469,"y0":995,"y1":1020},"font_size":6.0,"text":"2, a tomato 1, and a vase 2."}],"source":"layout det","text":"On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard 1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2."},{"bbox":{"x0":222,"x1":498,"y0":1017,"y1":1032},"conf":0.8137,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":1016,"y1":1033},"font_size":6.0,"text":"> take knife 2 from countertop 3"}],"source":"layout det","text":"> take knife 2 from countertop 3"},{"bbox":{"x0":222,"x1":366,"y0":1033,"y1":1049},"conf":0.8646,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":1030,"y1":1049},"font_size":6.0,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":402,"y0":1050,"y1":1065},"conf":0.8732,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":227,"x1":395,"y0":1051,"y1":1062},"font_size":6.0,"text":"> go to countertop 2"}],"source":"layout det","text":"> go to countertop 2"},{"bbox":{"x0":219,"x1":927,"y0":1064,"y1":1095},"conf":0.8481,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":926,"y0":1061,"y1":1082},"font_size":6.0,"text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a"},{"bbox":{"x0":223,"x1":437,"y0":1077,"y1":1094},"font_size":6.0,"text":"potato 3, and a statue 1."}],"source":"layout det","text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1."},{"bbox":{"x0":222,"x1":500,"y0":1096,"y1":1111},"conf":0.8373,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":499,"y0":1092,"y1":1112},"font_size":6.0,"text":"> take knife 1 from countertop 2"}],"source":"layout det","text":"> take knife 1 from countertop 2"},{"bbox":{"x0":222,"x1":363,"y0":1112,"y1":1128},"conf":0.8698,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":1109,"y1":1129},"font_size":6.0,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":402,"y0":1129,"y1":1144},"conf":0.8764,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":397,"y0":1127,"y1":1143},"font_size":-4.0910000000000005e+31,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":221,"x1":932,"y0":1143,"y1":1188},"conf":0.2724,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":920,"y0":1142,"y1":1160},"font_size":6.0,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":226,"x1":497,"y0":1162,"y1":1173},"font_size":6.0,"text":"> clean knife 1 with sinkbasin 1"},{"bbox":{"x0":223,"x1":359,"y0":1173,"y1":1193},"font_size":6.0,"text":"Nothing happens."}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1 Nothing happens."},{"bbox":{"x0":210,"x1":501,"y0":1296,"y1":1323},"conf":0.7569,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":213,"x1":496,"y0":1297,"y1":1320},"font_size":6.0,"text":"D.2.3REACT-IM TRAJECTORY"}],"source":"layout det","text":"D.2.3REACT-IM TRAJECTORY"},{"bbox":{"x0":209,"x1":1017,"y0":1351,"y1":1470},"conf":0.9627,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1009,"y0":1355,"y1":1376},"font_size":-4.0910000000000005e+31,"text":"ReAct-IM is able to correctly fnd the knife, but is unable to clean it as well. The LLM theni"},{"bbox":{"x0":211,"x1":1009,"y0":1374,"y1":1399},"font_size":-4.0910000000000005e+31,"text":"gets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought"},{"bbox":{"x0":213,"x1":1009,"y0":1399,"y1":1421},"font_size":-4.0910000000000005e+31,"text":"think:I need to find a clean knife., which seems to trick the LLM in believing"},{"bbox":{"x0":213,"x1":1009,"y0":1421,"y1":1442},"font_size":-4.0910000000000005e+31,"text":"that the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured"},{"bbox":{"x0":211,"x1":349,"y0":1440,"y1":1465},"font_size":-4.0910000000000005e+31,"text":"by each thought."}],"source":"layout det","text":"ReAct-IM is able to correctly fnd the knife, but is unable to clean it as well. The LLM theni gets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought think:I need to find a clean knife., which seems to trick the LLM in believing that the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured by each thought."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":209,"x1":1017,"y0":1351,"y1":1470},"conf":0.9627,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":943,"y0":184,"y1":282},"conf":0.9365,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":571,"y1":586},"conf":0.8976,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":362,"y0":873,"y1":889},"conf":0.8925,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":402,"y0":476,"y1":491},"conf":0.8908,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":362,"y0":698,"y1":714},"conf":0.8877,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":929,"y0":347,"y1":378},"conf":0.8826,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":402,"y0":1129,"y1":1144},"conf":0.8764,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":363,"y0":793,"y1":809},"conf":0.8751,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":402,"y0":1050,"y1":1065},"conf":0.8732,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":363,"y0":1112,"y1":1128},"conf":0.8698,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":400,"y0":889,"y1":905},"conf":0.8692,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":366,"y0":1033,"y1":1049},"conf":0.8646,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":402,"y0":954,"y1":969},"conf":0.8645,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":499,"y0":857,"y1":872},"conf":0.857,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":366,"y0":937,"y1":953},"conf":0.8506,"label":"Text","label_id":1},{"bbox":{"x0":219,"x1":927,"y0":1064,"y1":1095},"conf":0.8481,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":615,"y0":395,"y1":410},"conf":0.8419,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":363,"y0":554,"y1":570},"conf":0.8408,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":500,"y0":1096,"y1":1111},"conf":0.8373,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":498,"y0":380,"y1":394},"conf":0.8338,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":499,"y0":618,"y1":634},"conf":0.8281,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":365,"y0":459,"y1":475},"conf":0.8216,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":651,"y1":666},"conf":0.8194,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":499,"y0":539,"y1":553},"conf":0.8169,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":498,"y0":1017,"y1":1032},"conf":0.8137,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":929,"y0":585,"y1":617},"conf":0.8079,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":928,"y0":824,"y1":855},"conf":0.7906,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":362,"y0":634,"y1":650},"conf":0.788,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":49,"y1":80},"conf":0.7866,"label":"Abandon","label_id":2},{"bbox":{"x0":222,"x1":400,"y0":300,"y1":315},"conf":0.7788,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":928,"y0":314,"y1":346},"conf":0.769,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":497,"y0":1160,"y1":1176},"conf":0.7687,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":715,"y1":730},"conf":0.763,"label":"Text","label_id":1},{"bbox":{"x0":210,"x1":501,"y0":1296,"y1":1323},"conf":0.7569,"label":"Title","label_id":0},{"bbox":{"x0":220,"x1":925,"y0":426,"y1":458},"conf":0.7352,"label":"Text","label_id":1},{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1526},"conf":0.7237,"label":"Abandon","label_id":2},{"bbox":{"x0":223,"x1":498,"y0":683,"y1":697},"conf":0.721,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":412,"y1":427},"conf":0.7196,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":363,"y0":1176,"y1":1193},"conf":0.7059,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":926,"y0":665,"y1":682},"conf":0.7038,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":943,"y0":747,"y1":777},"conf":0.7007,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":810,"y1":825},"conf":0.6978,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":906,"y1":921},"conf":0.693,"label":"Text","label_id":1},{"bbox":{"x0":493,"x1":651,"y0":157,"y1":179},"conf":0.687,"label":"Title","label_id":0},{"bbox":{"x0":222,"x1":498,"y0":778,"y1":793},"conf":0.668,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":403,"y0":1065,"y1":1080},"conf":0.664,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":939,"y0":968,"y1":985},"conf":0.5911,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":497,"y0":683,"y1":698},"conf":0.5628,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":648,"y0":283,"y1":299},"conf":0.5564,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":928,"y0":903,"y1":921},"conf":0.5135,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":925,"y0":1143,"y1":1159},"conf":0.502,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":500,"y0":444,"y1":458},"conf":0.483,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":501,"y0":778,"y1":793},"conf":0.482,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":483,"y0":762,"y1":777},"conf":0.4765,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":942,"y0":982,"y1":1014},"conf":0.4421,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":731,"y1":745},"conf":0.4393,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":464,"y0":1002,"y1":1016},"conf":0.4215,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":810,"y1":825},"conf":0.3961,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":439,"y0":603,"y1":617},"conf":0.3652,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":944,"y0":747,"y1":777},"conf":0.36,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":441,"y0":842,"y1":856},"conf":0.3509,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":942,"y0":507,"y1":536},"conf":0.3341,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":497,"y0":922,"y1":936},"conf":0.3269,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":401,"y0":715,"y1":730},"conf":0.3104,"label":"Text","label_id":1},{"bbox":{"x0":218,"x1":944,"y0":968,"y1":1018},"conf":0.2978,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":944,"y0":507,"y1":537},"conf":0.291,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1015,"y0":47,"y1":85},"conf":0.2801,"label":"Abandon","label_id":2},{"bbox":{"x0":221,"x1":932,"y0":1143,"y1":1188},"conf":0.2724,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":497,"y0":443,"y1":458},"conf":0.2394,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":936,"y0":728,"y1":746},"conf":0.2324,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":926,"y0":904,"y1":935},"conf":0.2145,"label":"Text","label_id":1},{"bbox":{"x0":221,"x1":924,"y0":904,"y1":921},"conf":0.2122,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[625,1500],[625,1521],[599,1521]],"score":0.8374},{"poly":[[211,1442],[349,1440],[349,1463],[211,1465]],"score":0.7359},{"poly":[[213,1421],[1009,1421],[1009,1442],[213,1442]],"score":0.7744},{"poly":[[213,1399],[1009,1399],[1009,1421],[213,1421]],"score":0.7342},{"poly":[[211,1376],[1009,1374],[1009,1398],[211,1399]],"score":0.7228},{"poly":[[213,1355],[1009,1355],[1009,1376],[213,1376]],"score":0.7815},{"poly":[[213,1297],[496,1299],[496,1320],[213,1318]],"score":0.7691},{"poly":[[223,1173],[359,1177],[359,1193],[223,1190]],"score":0.7823},{"poly":[[226,1162],[497,1162],[497,1173],[226,1173]],"score":0.9735},{"poly":[[399,1142],[920,1142],[920,1158],[399,1158]],"score":0.7143},{"poly":[[225,1143],[404,1143],[404,1160],[225,1160]],"score":0.7524},{"poly":[[225,1127],[397,1127],[397,1143],[225,1143]],"score":0.7179},{"poly":[[223,1109],[363,1112],[362,1129],[223,1125]],"score":0.7813},{"poly":[[223,1092],[499,1096],[499,1112],[223,1109]],"score":0.7738},{"poly":[[223,1077],[437,1077],[437,1094],[223,1094]],"score":0.685},{"poly":[[223,1061],[926,1061],[926,1082],[223,1082]],"score":0.7024},{"poly":[[223,1030],[363,1031],[362,1049],[223,1048]],"score":0.7266},{"poly":[[225,1016],[499,1016],[499,1033],[225,1033]],"score":0.7996},{"poly":[[221,995],[469,998],[469,1020],[221,1016]],"score":0.643},{"poly":[[223,982],[943,982],[943,1003],[223,1003]],"score":0.6692},{"poly":[[225,969],[933,969],[933,985],[225,985]],"score":0.7918},{"poly":[[225,952],[399,952],[399,969],[225,969]],"score":0.74},{"poly":[[223,934],[363,936],[362,954],[223,952]],"score":0.7299},{"poly":[[226,922],[497,922],[497,934],[226,934]],"score":0.9876},{"poly":[[399,904],[921,904],[921,921],[399,921]],"score":0.8659},{"poly":[[225,904],[402,904],[402,921],[225,921]],"score":0.7679},{"poly":[[225,888],[396,888],[396,904],[225,904]],"score":0.6938},{"poly":[[223,869],[363,871],[362,889],[223,888]],"score":0.7207},{"poly":[[221,853],[500,851],[501,873],[221,875]],"score":0.6984},{"poly":[[223,838],[436,836],[436,858],[223,860]],"score":0.6926},{"poly":[[223,822],[926,822],[926,843],[223,843]],"score":0.6814},{"poly":[[223,805],[397,803],[398,825],[223,827]],"score":0.6831},{"poly":[[225,792],[363,792],[363,808],[225,808]],"score":0.7885},{"poly":[[225,777],[501,777],[501,794],[225,794]],"score":0.7841},{"poly":[[225,761],[467,761],[467,777],[225,777]],"score":0.7918},{"poly":[[223,742],[941,742],[941,764],[223,764]],"score":0.6842},{"poly":[[221,726],[933,726],[933,747],[221,747]],"score":0.656},{"poly":[[225,713],[399,713],[399,729],[225,729]],"score":0.7966},{"poly":[[223,695],[363,696],[362,715],[223,713]],"score":0.7052},{"poly":[[225,681],[499,681],[499,698],[225,698]],"score":0.7913},{"poly":[[225,665],[402,665],[402,681],[225,681]],"score":0.7172},{"poly":[[392,663],[921,660],[921,681],[393,685]],"score":0.6856},{"poly":[[223,647],[397,645],[398,667],[223,668]],"score":0.6984},{"poly":[[225,634],[361,634],[361,650],[225,650]],"score":0.8507},{"poly":[[225,617],[499,617],[499,634],[225,634]],"score":0.7867},{"poly":[[223,599],[436,597],[436,619],[223,620]],"score":0.6651},{"poly":[[223,582],[926,582],[926,604],[223,604]],"score":0.6636},{"poly":[[225,569],[397,569],[397,586],[225,586]],"score":0.7374},{"poly":[[223,551],[363,555],[362,571],[223,567]],"score":0.7814},{"poly":[[225,536],[499,536],[499,553],[225,553]],"score":0.7885},{"poly":[[225,521],[466,521],[466,538],[225,538]],"score":0.7797},{"poly":[[225,507],[941,507],[941,523],[225,523]],"score":0.8029},{"poly":[[223,488],[931,488],[931,505],[223,505]],"score":0.7285},{"poly":[[225,474],[399,474],[399,490],[225,490]],"score":0.7152},{"poly":[[225,459],[363,459],[363,475],[225,475]],"score":0.7676},{"poly":[[226,444],[497,444],[497,455],[226,455]],"score":0.968},{"poly":[[225,426],[921,426],[921,442],[225,442]],"score":0.7651},{"poly":[[225,411],[397,411],[397,427],[225,427]],"score":0.7653},{"poly":[[225,394],[612,394],[612,411],[225,411]],"score":0.8288},{"poly":[[226,378],[499,378],[499,394],[226,394]],"score":0.7948},{"poly":[[225,361],[531,361],[531,378],[225,378]],"score":0.7879},{"poly":[[225,346],[925,346],[925,363],[225,363]],"score":0.7758},{"poly":[[225,330],[397,330],[397,346],[225,346]],"score":0.7507},{"poly":[[401,313],[921,313],[921,330],[401,330]],"score":0.815},{"poly":[[223,312],[403,315],[402,332],[223,328]],"score":0.7121},{"poly":[[225,299],[399,299],[399,315],[225,315]],"score":0.6998},{"poly":[[373,282],[647,282],[647,299],[373,299]],"score":0.7475},{"poly":[[225,282],[373,282],[373,299],[225,299]],"score":0.7357},{"poly":[[226,267],[241,267],[241,280],[226,280]],"score":0.8612},{"poly":[[225,251],[931,251],[931,267],[225,267]],"score":0.63},{"poly":[[225,231],[941,233],[941,254],[224,252]],"score":0.6895},{"poly":[[226,219],[925,219],[925,236],[226,236]],"score":0.7299},{"poly":[[223,203],[925,203],[925,219],[223,219]],"score":0.7345},{"poly":[[507,185],[935,185],[935,206],[507,206]],"score":0.6152},{"poly":[[226,186],[501,186],[501,203],[226,203]],"score":0.7376},{"poly":[[492,157],[645,157],[645,178],[492,178]],"score":0.7059},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.6966}],"page_no":28,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1525},"conf":0.7522,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1502,"y1":1523},"font_size":0.0,"text":"30"}],"source":"layout det","text":""},{"bbox":{"x0":473,"x1":671,"y0":157,"y1":178},"conf":0.3105,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[],"source":"layout det","text":""},{"bbox":{"x0":210,"x1":1014,"y0":47,"y1":85},"conf":0.254,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":473,"x1":671,"y0":157,"y1":178},"conf":0.4992,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":476,"x1":667,"y0":160,"y1":177},"font_size":-1.803e-24,"text":"ReAct-IM ALFWorld trajectory"}],"source":"layout det","text":"ReAct-IM ALFWorld trajectory"},{"bbox":{"x0":221,"x1":942,"y0":184,"y1":281},"conf":0.9256,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":935,"y0":185,"y1":206},"font_size":-1.803e-24,"text":"You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a"},{"bbox":{"x0":225,"x1":926,"y0":203,"y1":219},"font_size":-1.803e-24,"text":"cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a"},{"bbox":{"x0":225,"x1":926,"y0":219,"y1":236},"font_size":-1.803e-24,"text":"countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a"},{"bbox":{"x0":221,"x1":943,"y0":231,"y1":254},"font_size":-1.803e-24,"text":"fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin"},{"bbox":{"x0":225,"x1":931,"y0":251,"y1":267},"font_size":-1.803e-24,"text":"1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster"},{"bbox":{"x0":226,"x1":241,"y0":267,"y1":280},"font_size":-1.803e-24,"text":"1."}],"source":"layout det","text":"You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1."},{"bbox":{"x0":224,"x1":648,"y0":283,"y1":299},"conf":0.4565,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":645,"y0":282,"y1":299},"font_size":-1.803e-24,"text":"Your task is to:put a clean knife in countertop."}],"source":"layout det","text":"Your task is to:put a clean knife in countertop."},{"bbox":{"x0":222,"x1":921,"y0":300,"y1":341},"conf":0.2301,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":306,"x1":916,"y0":299,"y1":315},"font_size":-1.803e-24,"text":"To solve the task, I need to find and take a clean knife, then put it in"},{"bbox":{"x0":223,"x1":321,"y0":312,"y1":332},"font_size":-1.803e-24,"text":"countertop."},{"bbox":{"x0":226,"x1":254,"y0":332,"y1":345},"font_size":-1.803e-24,"text":"OK."}],"source":"layout det","text":"To solve the task, I need to find and take a clean knife, then put it in countertop.OK."},{"bbox":{"x0":223,"x1":554,"y0":346,"y1":362},"conf":0.8237,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":554,"y0":345,"y1":361},"font_size":-1.803e-24,"text":"> think:I need to find a clean knife."}],"source":"layout det","text":"> think:I need to find a clean knife."},{"bbox":{"x0":222,"x1":254,"y0":363,"y1":378},"conf":0.2417,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":256,"y0":363,"y1":376},"font_size":-1.803e-24,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":222,"x1":400,"y0":379,"y1":395},"conf":0.8639,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":397,"y0":378,"y1":394},"font_size":-1.803e-24,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":223,"x1":925,"y0":395,"y1":424},"conf":0.5451,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":923,"y0":389,"y1":413},"font_size":-1.803e-24,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":225,"x1":554,"y0":409,"y1":426},"font_size":-1.803e-24,"text":"> think:I need to find a clean knife."}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> think:I need to find a clean knife."},{"bbox":{"x0":222,"x1":255,"y0":427,"y1":441},"conf":0.6214,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":256,"y0":424,"y1":442},"font_size":-1.803e-24,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":223,"x1":401,"y0":443,"y1":459},"conf":0.8878,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":399,"y0":439,"y1":460},"font_size":-1.803e-24,"text":"> go to countertop 2"}],"source":"layout det","text":"> go to countertop 2"},{"bbox":{"x0":220,"x1":926,"y0":458,"y1":490},"conf":0.5129,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":221,"x1":925,"y0":454,"y1":477},"font_size":-1.803e-24,"text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a"},{"bbox":{"x0":221,"x1":529,"y0":470,"y1":493},"font_size":-1.803e-24,"text":"plate 2, a potato 3, and a statue 1."}],"source":"layout det","text":"On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1."},{"bbox":{"x0":222,"x1":554,"y0":491,"y1":506},"conf":0.7968,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":555,"y0":488,"y1":505},"font_size":-1.803e-24,"text":"> think:I need to find a clean knife."}],"source":"layout det","text":"> think:I need to find a clean knife."},{"bbox":{"x0":223,"x1":251,"y0":507,"y1":522},"conf":0.4387,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":227,"x1":253,"y0":509,"y1":520},"font_size":-1.803e-24,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":223,"x1":500,"y0":522,"y1":538},"conf":0.8844,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":497,"y0":521,"y1":538},"font_size":-1.803e-24,"text":"> take knife 1 from countertop 2"}],"source":"layout det","text":"> take knife 1 from countertop 2"},{"bbox":{"x0":247,"x1":617,"y0":538,"y1":554},"conf":0.6109,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":614,"y0":535,"y1":556},"font_size":-1.803e-24,"text":"u pick up the knife 1 from the countertop 2."}],"source":"layout det","text":"u pick up the knife 1 from the countertop 2."},{"bbox":{"x0":223,"x1":715,"y0":554,"y1":571},"conf":0.7827,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":713,"y0":551,"y1":569},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/on countertop 1."}],"source":"layout det","text":"> think:I need to put this knife (1) in/on countertop 1."},{"bbox":{"x0":223,"x1":252,"y0":571,"y1":585},"conf":0.3109,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":254,"y0":571,"y1":584},"font_size":-1.803e-24,"text":"OK."}],"source":"layout det","text":"OK."},{"bbox":{"x0":222,"x1":399,"y0":587,"y1":602},"conf":0.6437,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":398,"y0":581,"y1":604},"font_size":-1.803e-24,"text":"> go to countertop 1"}],"source":"layout det","text":"> go to countertop 1"},{"bbox":{"x0":222,"x1":927,"y0":601,"y1":637},"conf":0.2029,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":923,"y0":599,"y1":620},"font_size":-1.803e-24,"text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2."},{"bbox":{"x0":225,"x1":715,"y0":617,"y1":634},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/on countertop 1."}],"source":"layout det","text":"On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> think:I need to put this knife (1) in/on countertop 1."},{"bbox":{"x0":222,"x1":499,"y0":649,"y1":665},"conf":0.8525,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":497,"y0":648,"y1":665},"font_size":-1.803e-24,"text":"> put knife 1 in/on countertop 1"}],"source":"layout det","text":"> put knife 1 in/on countertop 1"},{"bbox":{"x0":223,"x1":591,"y0":666,"y1":682},"conf":0.776,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":584,"y0":665,"y1":681},"font_size":-1.803e-24,"text":"You put the knife 1 in/on the countertop 1."}],"source":"layout det","text":"You put the knife 1 in/on the countertop 1."},{"bbox":{"x0":223,"x1":581,"y0":682,"y1":698},"conf":0.3147,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":713,"y0":681,"y1":698},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/"}],"source":"layout det","text":"> think:I need to put this knife (1) in/"},{"bbox":{"x0":222,"x1":498,"y0":714,"y1":730},"conf":0.8211,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":497,"y0":713,"y1":729},"font_size":-1.803e-24,"text":"> put knife 1 in/on countertop 1"}],"source":"layout det","text":"> put knife 1 in/on countertop 1"},{"bbox":{"x0":223,"x1":362,"y0":731,"y1":746},"conf":0.6451,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":364,"y0":729,"y1":746},"font_size":-1.803e-24,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":224,"x1":715,"y0":744,"y1":765},"conf":0.8495,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":712,"y0":744,"y1":761},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/on countertop 1."}],"source":"layout det","text":"> think:I need to put this knife (1) in/on countertop 1."},{"bbox":{"x0":223,"x1":498,"y0":777,"y1":794},"conf":0.8822,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":777,"y1":794},"font_size":-1.803e-24,"text":"> put knife 1 in/on countertop 1"}],"source":"layout det","text":"> put knife 1 in/on countertop 1"},{"bbox":{"x0":223,"x1":364,"y0":794,"y1":808},"conf":0.3678,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":363,"y0":792,"y1":808},"font_size":-1.803e-24,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":224,"x1":715,"y0":808,"y1":826},"conf":0.7637,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":712,"y0":805,"y1":825},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/on countertop 1."}],"source":"layout det","text":"> think:I need to put this knife (1) in/on countertop 1."},{"bbox":{"x0":222,"x1":497,"y0":840,"y1":857},"conf":0.872,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":840,"y1":856},"font_size":-1.803e-24,"text":"> put knife 1 in/on countertop 1"}],"source":"layout det","text":"> put knife 1 in/on countertop 1"},{"bbox":{"x0":223,"x1":372,"y0":857,"y1":873},"conf":0.3463,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":363,"y0":856,"y1":873},"font_size":-1.803e-24,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":223,"x1":715,"y0":871,"y1":894},"conf":0.8228,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":712,"y0":873,"y1":889},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/on countertop 1."}],"source":"layout det","text":"> think:I need to put this knife (1) in/on countertop 1."},{"bbox":{"x0":223,"x1":499,"y0":904,"y1":921},"conf":0.8871,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":904,"y1":921},"font_size":-1.803e-24,"text":"> put knife 1 in/on countertop 1"}],"source":"layout det","text":"> put knife 1 in/on countertop 1"},{"bbox":{"x0":223,"x1":364,"y0":922,"y1":937},"conf":0.8317,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":917,"y1":937},"font_size":-1.803e-24,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":715,"y0":935,"y1":956},"conf":0.7086,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":712,"y0":936,"y1":952},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/on countertop 1."}],"source":"layout det","text":"> think:I need to put this knife (1) in/on countertop 1."},{"bbox":{"x0":223,"x1":497,"y0":968,"y1":984},"conf":0.8934,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":499,"y0":969,"y1":985},"font_size":-1.803e-24,"text":"> put knife 1 in/on countertop 1"}],"source":"layout det","text":"> put knife 1 in/on countertop 1"},{"bbox":{"x0":223,"x1":362,"y0":986,"y1":1000},"conf":0.863,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":223,"x1":363,"y0":982,"y1":1002},"font_size":-1.803e-24,"text":"Nothing happens."}],"source":"layout det","text":"Nothing happens."},{"bbox":{"x0":222,"x1":716,"y0":999,"y1":1036},"conf":0.2422,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":225,"x1":713,"y0":1000,"y1":1016},"font_size":-1.803e-24,"text":"> think:I need to put this knife (1) in/on countertop 1."},{"bbox":{"x0":225,"x1":253,"y0":1018,"y1":1031},"font_size":-1.803e-24,"text":"OK."}],"source":"layout det","text":"> think:I need to put this knife (1) in/on countertop 1.OK."}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":221,"x1":942,"y0":184,"y1":281},"conf":0.9256,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":497,"y0":968,"y1":984},"conf":0.8934,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":401,"y0":443,"y1":459},"conf":0.8878,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":499,"y0":904,"y1":921},"conf":0.8871,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":500,"y0":522,"y1":538},"conf":0.8844,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":498,"y0":777,"y1":794},"conf":0.8822,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":497,"y0":840,"y1":857},"conf":0.872,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":400,"y0":379,"y1":395},"conf":0.8639,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":362,"y0":986,"y1":1000},"conf":0.863,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":499,"y0":649,"y1":665},"conf":0.8525,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":532,"y0":476,"y1":491},"conf":0.8508,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":715,"y0":744,"y1":765},"conf":0.8495,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":364,"y0":922,"y1":937},"conf":0.8317,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":554,"y0":346,"y1":362},"conf":0.8237,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":715,"y0":871,"y1":894},"conf":0.8228,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":498,"y0":714,"y1":730},"conf":0.8211,"label":"Text","label_id":1},{"bbox":{"x0":210,"x1":591,"y0":50,"y1":80},"conf":0.8095,"label":"Abandon","label_id":2},{"bbox":{"x0":222,"x1":554,"y0":491,"y1":506},"conf":0.7968,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":715,"y0":554,"y1":571},"conf":0.7827,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":591,"y0":666,"y1":682},"conf":0.776,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":715,"y0":808,"y1":826},"conf":0.7637,"label":"Text","label_id":1},{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1525},"conf":0.7522,"label":"Abandon","label_id":2},{"bbox":{"x0":224,"x1":921,"y0":601,"y1":618},"conf":0.7126,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":715,"y0":935,"y1":956},"conf":0.7086,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":364,"y0":857,"y1":873},"conf":0.684,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":362,"y0":731,"y1":746},"conf":0.6451,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":399,"y0":587,"y1":602},"conf":0.6437,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":919,"y0":300,"y1":329},"conf":0.6419,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":255,"y0":427,"y1":441},"conf":0.6214,"label":"Text","label_id":1},{"bbox":{"x0":247,"x1":617,"y0":538,"y1":554},"conf":0.6109,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":364,"y0":857,"y1":873},"conf":0.6005,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":925,"y0":395,"y1":424},"conf":0.5451,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":926,"y0":458,"y1":490},"conf":0.5129,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":716,"y0":1000,"y1":1019},"conf":0.5081,"label":"Text","label_id":1},{"bbox":{"x0":473,"x1":671,"y0":157,"y1":178},"conf":0.4992,"label":"Title","label_id":0},{"bbox":{"x0":222,"x1":553,"y0":491,"y1":506},"conf":0.4704,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":648,"y0":283,"y1":299},"conf":0.4565,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":251,"y0":507,"y1":522},"conf":0.4387,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":251,"y0":332,"y1":346},"conf":0.4167,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":713,"y0":618,"y1":636},"conf":0.3976,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":320,"y0":316,"y1":330},"conf":0.3904,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":364,"y0":794,"y1":808},"conf":0.3678,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":372,"y0":857,"y1":873},"conf":0.3463,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":581,"y0":682,"y1":698},"conf":0.3147,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":252,"y0":571,"y1":585},"conf":0.3109,"label":"Text","label_id":1},{"bbox":{"x0":473,"x1":671,"y0":157,"y1":178},"conf":0.3105,"label":"Abandon","label_id":2},{"bbox":{"x0":321,"x1":692,"y0":808,"y1":827},"conf":0.303,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":253,"y0":427,"y1":442},"conf":0.2989,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":403,"y0":460,"y1":475},"conf":0.2753,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":498,"y0":714,"y1":730},"conf":0.269,"label":"Text","label_id":1},{"bbox":{"x0":210,"x1":1014,"y0":47,"y1":85},"conf":0.254,"label":"Abandon","label_id":2},{"bbox":{"x0":222,"x1":716,"y0":999,"y1":1036},"conf":0.2422,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":254,"y0":363,"y1":378},"conf":0.2417,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":921,"y0":300,"y1":341},"conf":0.2301,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":254,"y0":331,"y1":346},"conf":0.209,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":717,"y0":1002,"y1":1024},"conf":0.2034,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":927,"y0":601,"y1":637},"conf":0.2029,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[625,1502],[625,1523],[599,1523]],"score":0.9605},{"poly":[[225,1018],[253,1018],[253,1031],[225,1031]],"score":0.8932},{"poly":[[308,1000],[713,1000],[713,1016],[308,1016]],"score":0.8147},{"poly":[[225,1000],[303,1000],[303,1016],[225,1016]],"score":0.7928},{"poly":[[223,982],[363,983],[362,1002],[223,1000]],"score":0.7564},{"poly":[[225,969],[499,969],[499,985],[225,985]],"score":0.7923},{"poly":[[225,954],[254,954],[254,967],[225,967]],"score":0.8784},{"poly":[[308,936],[712,936],[712,952],[308,952]],"score":0.7902},{"poly":[[225,936],[301,936],[301,952],[225,952]],"score":0.7891},{"poly":[[223,917],[363,919],[362,937],[223,935]],"score":0.7297},{"poly":[[225,904],[499,904],[499,921],[225,921]],"score":0.7967},{"poly":[[226,889],[254,889],[254,903],[226,903]],"score":0.9419},{"poly":[[225,873],[712,873],[712,889],[225,889]],"score":0.7911},{"poly":[[225,856],[363,856],[363,873],[225,873]],"score":0.8178},{"poly":[[225,840],[499,840],[499,856],[225,856]],"score":0.8346},{"poly":[[225,825],[254,825],[254,838],[225,838]],"score":0.9098},{"poly":[[223,805],[712,807],[712,825],[223,823]],"score":0.7214},{"poly":[[225,792],[363,792],[363,808],[225,808]],"score":0.8081},{"poly":[[225,777],[499,777],[499,794],[225,794]],"score":0.7811},{"poly":[[223,759],[254,759],[254,777],[223,777]],"score":0.8206},{"poly":[[306,744],[712,744],[712,761],[306,761]],"score":0.7988},{"poly":[[225,744],[306,744],[306,761],[225,761]],"score":0.8124},{"poly":[[225,729],[364,729],[364,746],[225,746]],"score":0.8022},{"poly":[[225,713],[497,713],[497,729],[225,729]],"score":0.8209},{"poly":[[225,698],[254,698],[254,711],[225,711]],"score":0.8825},{"poly":[[225,681],[713,681],[713,698],[225,698]],"score":0.7974},{"poly":[[225,665],[584,665],[584,681],[225,681]],"score":0.7557},{"poly":[[225,648],[497,648],[497,665],[225,665]],"score":0.7838},{"poly":[[223,632],[256,632],[256,650],[223,650]],"score":0.8238},{"poly":[[225,617],[715,617],[715,634],[225,634]],"score":0.748},{"poly":[[223,599],[923,599],[923,620],[223,620]],"score":0.6696},{"poly":[[223,581],[398,583],[397,604],[223,602]],"score":0.6961},{"poly":[[225,571],[254,571],[254,584],[225,584]],"score":0.8718},{"poly":[[301,551],[713,551],[713,568],[301,568]],"score":0.6625},{"poly":[[225,553],[303,553],[303,569],[225,569]],"score":0.7544},{"poly":[[223,535],[614,535],[614,556],[223,556]],"score":0.6632},{"poly":[[225,521],[497,521],[497,538],[225,538]],"score":0.7841},{"poly":[[225,503],[257,507],[254,525],[222,521]],"score":0.7865},{"poly":[[225,488],[555,488],[555,505],[225,505]],"score":0.7489},{"poly":[[221,472],[529,470],[529,492],[221,493]],"score":0.6973},{"poly":[[221,454],[925,455],[925,477],[221,475]],"score":0.7192},{"poly":[[223,439],[399,439],[399,460],[223,460]],"score":0.6574},{"poly":[[223,424],[256,424],[256,442],[223,442]],"score":0.7414},{"poly":[[306,412],[554,412],[554,424],[306,424]],"score":0.9746},{"poly":[[225,409],[306,409],[306,426],[225,426]],"score":0.7259},{"poly":[[221,391],[923,389],[923,411],[221,413]],"score":0.6845},{"poly":[[225,378],[397,378],[397,394],[225,394]],"score":0.7618},{"poly":[[225,363],[256,363],[256,376],[225,376]],"score":0.8351},{"poly":[[225,345],[554,345],[554,361],[225,361]],"score":0.786},{"poly":[[226,332],[254,332],[254,345],[226,345]],"score":0.9216},{"poly":[[223,312],[321,314],[321,332],[223,330]],"score":0.7069},{"poly":[[306,299],[916,299],[916,315],[306,315]],"score":0.807},{"poly":[[373,282],[645,282],[645,299],[373,299]],"score":0.7328},{"poly":[[225,282],[376,282],[376,299],[225,299]],"score":0.76},{"poly":[[226,267],[241,267],[241,280],[226,280]],"score":0.8855},{"poly":[[225,251],[931,251],[931,267],[225,267]],"score":0.6179},{"poly":[[221,231],[943,233],[943,254],[221,252]],"score":0.6932},{"poly":[[225,219],[926,219],[926,236],[225,236]],"score":0.743},{"poly":[[225,203],[926,203],[926,219],[225,219]],"score":0.7344},{"poly":[[507,185],[935,185],[935,206],[507,206]],"score":0.6328},{"poly":[[225,186],[499,186],[499,203],[225,203]],"score":0.7587},{"poly":[[476,160],[667,160],[667,177],[476,177]],"score":0.8575},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7128}],"page_no":29,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":595,"x1":625,"y0":1499,"y1":1527},"conf":0.666,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":597,"x1":624,"y0":1500,"y1":1523},"font_size":0.0,"text":"31"}],"source":"layout det","text":""},{"bbox":{"x0":209,"x1":1017,"y0":46,"y1":86},"conf":0.2117,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":210,"x1":494,"y0":161,"y1":193},"conf":0.8725,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":491,"y0":163,"y1":188},"font_size":-1.143e+34,"text":"D.3WEBSHOP TRAJECTORIES"}],"source":"layout det","text":"D.3WEBSHOP TRAJECTORIES"},{"bbox":{"x0":209,"x1":1016,"y0":348,"y1":402},"conf":0.6992,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":1011,"y0":351,"y1":375},"font_size":-1.143e+34,"text":"Table 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,"},{"bbox":{"x0":213,"x1":778,"y0":375,"y1":396},"font_size":-1.143e+34,"text":"ReAct uses reasoning to fnd products that satisfy all target attributes.i"}],"source":"layout det","text":"Table 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,ReAct uses reasoning to fnd products that satisfy all target attributes.i"},{"bbox":{"x0":215,"x1":617,"y0":426,"y1":1456},"conf":0.2525,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":225,"x1":614,"y0":427,"y1":442},"font_size":0.0,"text":"Instruction: get me a sixteen pack of apple cinnamon freeze dried ba"},{"bbox":{"x0":403,"x1":434,"y0":452,"y1":468},"font_size":0.0,"text":"Act"},{"bbox":{"x0":223,"x1":597,"y0":472,"y1":493},"font_size":0.0,"text":"Action: search [sixteen pack apple cinnamon freeze dried banana"},{"bbox":{"x0":223,"x1":263,"y0":491,"y1":511},"font_size":0.0,"text":"chips]"},{"bbox":{"x0":224,"x1":308,"y0":506,"y1":525},"font_size":0.0,"text":"Observation:"},{"bbox":{"x0":229,"x1":331,"y0":524,"y1":542},"font_size":0.0,"text":"[Back to Search ]"},{"bbox":{"x0":227,"x1":370,"y0":542,"y1":557},"font_size":0.0,"text":"Page 1 (Total results: 50)"},{"bbox":{"x0":225,"x1":269,"y0":555,"y1":578},"font_size":0.0,"text":"[Next]"},{"bbox":{"x0":229,"x1":315,"y0":588,"y1":606},"font_size":0.0,"text":"[B0061IVFZE]"},{"bbox":{"x0":227,"x1":575,"y0":606,"y1":621},"font_size":0.0,"text":"Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -"},{"bbox":{"x0":225,"x1":345,"y0":622,"y1":637},"font_size":0.0,"text":"Ounce (Pack of 100)"},{"bbox":{"x0":225,"x1":260,"y0":637,"y1":653},"font_size":0.0,"text":"$85.0"},{"bbox":{"x0":225,"x1":319,"y0":651,"y1":673},"font_size":0.0,"text":"[B096H2P6G2]"},{"bbox":{"x0":227,"x1":576,"y0":672,"y1":687},"font_size":0.0,"text":"Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -"},{"bbox":{"x0":224,"x1":310,"y0":686,"y1":704},"font_size":0.0,"text":"(Variety Pack)"},{"bbox":{"x0":225,"x1":269,"y0":702,"y1":718},"font_size":0.0,"text":"$18.99"},{"bbox":{"x0":229,"x1":318,"y0":719,"y1":734},"font_size":0.0,"text":"[B092JLLYK6]"},{"bbox":{"x0":225,"x1":600,"y0":734,"y1":751},"font_size":0.0,"text":"Nature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect"},{"bbox":{"x0":224,"x1":587,"y0":749,"y1":768},"font_size":0.0,"text":"For School Lunches or an On-The-Go Snack - No Sugar Added,"},{"bbox":{"x0":225,"x1":560,"y0":767,"y1":782},"font_size":0.0,"text":"Non GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Pack"},{"bbox":{"x0":447,"x1":447,"y0":770,"y1":783},"font_size":6.0,"text":"i"},{"bbox":{"x0":224,"x1":268,"y0":782,"y1":798},"font_size":0.0,"text":"$12.99"},{"bbox":{"x0":224,"x1":392,"y0":814,"y1":833},"font_size":0.0,"text":"Action: click [B0061IVFZE]"},{"bbox":{"x0":225,"x1":306,"y0":833,"y1":848},"font_size":0.0,"text":"Observation:"},{"bbox":{"x0":227,"x1":324,"y0":845,"y1":866},"font_size":0.0,"text":"[Back to Search]"},{"bbox":{"x0":229,"x1":265,"y0":864,"y1":883},"font_size":0.0,"text":"[Prev]"},{"bbox":{"x0":224,"x1":612,"y0":879,"y1":899},"font_size":0.0,"text":"favor name [asian pear][banana][fuji apple & cinnamon][strawberry"},{"bbox":{"x0":235,"x1":235,"y0":884,"y1":897},"font_size":6.0,"text":"l"},{"bbox":{"x0":224,"x1":589,"y0":897,"y1":913},"font_size":0.0,"text":"banana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,"},{"bbox":{"x0":224,"x1":373,"y0":912,"y1":929},"font_size":0.0,"text":"0.42 Ounce (Pack of 100)"},{"bbox":{"x0":225,"x1":298,"y0":929,"y1":944},"font_size":0.0,"text":"Price: $85.0"},{"bbox":{"x0":224,"x1":301,"y0":944,"y1":962},"font_size":0.0,"text":"Rating: N.A."},{"bbox":{"x0":224,"x1":304,"y0":960,"y1":979},"font_size":0.0,"text":"[Description]"},{"bbox":{"x0":224,"x1":285,"y0":976,"y1":995},"font_size":0.0,"text":"[Features]"},{"bbox":{"x0":225,"x1":285,"y0":990,"y1":1009},"font_size":0.0,"text":"[Reviews]"},{"bbox":{"x0":224,"x1":292,"y0":1009,"y1":1027},"font_size":0.0,"text":"[Buy Now]"},{"bbox":{"x0":224,"x1":369,"y0":1038,"y1":1058},"font_size":0.0,"text":"Action: click[Buy Now]"},{"bbox":{"x0":224,"x1":301,"y0":1433,"y1":1452},"font_size":0.0,"text":"Score: 0.125"}],"source":"layout det","text":"<html><body><table><thead><tr><td>Instruction: get me a sixteen pack of apple cinnamon freeze dried ba Act</td></tr></thead><tbody><tr><td>Action: search [sixteen pack apple cinnamon freeze dried banana</td></tr><tr><td>chips]</td></tr><tr><td>Observation: [Back to Search ]</td></tr><tr><td>Page 1 (Total results: 50)</td></tr><tr><td>[Next]</td></tr><tr><td>[B0061IVFZE]</td></tr><tr><td>Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -</td></tr><tr><td>Ounce (Pack of 100) $85.0</td></tr><tr><td>[B096H2P6G2]</td></tr><tr><td>Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -</td></tr><tr><td>(Variety Pack) $18.99</td></tr><tr><td>[B092JLLYK6]</td></tr><tr><td>Nature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect</td></tr><tr><td>For School Lunches or an On-The-Go Snack - No Sugar Added, Non GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Pack</td></tr><tr><td>$12.99</td></tr><tr><td></td></tr><tr><td>Action: click [B0061IVFZE] Observation:</td></tr><tr><td>[Back to Search]</td></tr><tr><td>[Prev]</td></tr><tr><td>favor name [asian pear][banana][fuji apple & cinnamon][strawberry</td></tr><tr><td>banana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,</td></tr><tr><td>0.42 Ounce (Pack of 100)</td></tr><tr><td>Price: $85.0</td></tr><tr><td>Rating: N.A. [Description]</td></tr><tr><td>[Features]</td></tr><tr><td>[Reviews]</td></tr><tr><td>[Buy Now]</td></tr><tr><td>Action: click[Buy Now]</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td rowspan=\"3\">Score: 0.125</td></tr></tr></tr><tr><td></td></tr></table></body></html>"},{"bbox":{"x0":215,"x1":617,"y0":426,"y1":1456},"conf":0.2061,"font_size":0.0,"label":"Reference","label_id":11,"lines":[{"bbox":{"x0":447,"x1":447,"y0":770,"y1":783},"font_size":6.0,"text":"i"},{"bbox":{"x0":235,"x1":235,"y0":884,"y1":897},"font_size":6.0,"text":"l"}],"source":"layout det","text":"i  \nl"},{"bbox":{"x0":625,"x1":1048,"y0":427,"y1":1463},"conf":0.4654,"font_size":0.0,"label":"Reference","label_id":11,"lines":[{"bbox":{"x0":623,"x1":872,"y0":432,"y1":444},"font_size":6.0,"text":"ana chips, and price lower than 50.00 dollars"},{"bbox":{"x0":810,"x1":856,"y0":450,"y1":467},"font_size":-1.143e+34,"text":"ReAct"},{"bbox":{"x0":630,"x1":1006,"y0":472,"y1":493},"font_size":-1.143e+34,"text":"Action: seareh [sixteen pack apple cinnamon freeze dried banana"},{"bbox":{"x0":630,"x1":671,"y0":488,"y1":512},"font_size":-1.143e+34,"text":"chips]"},{"bbox":{"x0":628,"x1":717,"y0":501,"y1":528},"font_size":-1.143e+34,"text":"Observation:"},{"bbox":{"x0":635,"x1":739,"y0":518,"y1":545},"font_size":-1.143e+34,"text":"[Back to Search]"},{"bbox":{"x0":632,"x1":780,"y0":538,"y1":559},"font_size":-1.143e+34,"text":"Page 1 (Total results: 50)"},{"bbox":{"x0":634,"x1":677,"y0":553,"y1":578},"font_size":-1.143e+34,"text":"[Next]"},{"bbox":{"x0":635,"x1":723,"y0":587,"y1":606},"font_size":-1.143e+34,"text":"[B0061IVFZE]"},{"bbox":{"x0":632,"x1":983,"y0":602,"y1":624},"font_size":-1.143e+34,"text":"Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -"},{"bbox":{"x0":632,"x1":753,"y0":620,"y1":639},"font_size":-1.143e+34,"text":"Ounce (Pack of 100)"},{"bbox":{"x0":632,"x1":672,"y0":635,"y1":653},"font_size":-1.143e+34,"text":"$85.0"},{"bbox":{"x0":634,"x1":728,"y0":650,"y1":673},"font_size":-1.143e+34,"text":"[B096H2P6G2]"},{"bbox":{"x0":634,"x1":980,"y0":670,"y1":686},"font_size":-1.143e+34,"text":"Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids"},{"bbox":{"x0":630,"x1":715,"y0":685,"y1":703},"font_size":-1.143e+34,"text":"(Variety Pack)"},{"bbox":{"x0":632,"x1":679,"y0":701,"y1":718},"font_size":-1.143e+34,"text":"$18.99"},{"bbox":{"x0":635,"x1":727,"y0":716,"y1":734},"font_size":-1.143e+34,"text":"[B092JLLYK6]"},{"bbox":{"x0":634,"x1":1008,"y0":734,"y1":751},"font_size":-1.143e+34,"text":"Nature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect"},{"bbox":{"x0":630,"x1":995,"y0":747,"y1":767},"font_size":-1.143e+34,"text":"For School Lunches or an On-The-Go Snack - No Sugar Added,"},{"bbox":{"x0":630,"x1":966,"y0":766,"y1":783},"font_size":-1.143e+34,"text":"Non GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi"},{"bbox":{"x0":632,"x1":677,"y0":782,"y1":799},"font_size":-1.143e+34,"text":"$12.99"},{"bbox":{"x0":632,"x1":1034,"y0":812,"y1":832},"font_size":-1.143e+34,"text":"Action: think[B0061IVFZE is strawberry banana, not apple cinnamon."},{"bbox":{"x0":629,"x1":973,"y0":827,"y1":850},"font_size":-1.143e+34,"text":"B096H2P6G2 is fruit snacks, not freeze dried banana chips."},{"bbox":{"x0":629,"x1":996,"y0":841,"y1":866},"font_size":-1.143e+34,"text":"B092JLLYK6 is banana crisps, not apple cinnamon. I can check"},{"bbox":{"x0":630,"x1":744,"y0":860,"y1":880},"font_size":-1.143e+34,"text":"B092JLLYK6 frst.i"},{"bbox":{"x0":632,"x1":742,"y0":878,"y1":896},"font_size":-1.143e+34,"text":"Observation: OK."},{"bbox":{"x0":630,"x1":803,"y0":908,"y1":929},"font_size":-1.143e+34,"text":"Action: click[B092JLLYK6]"},{"bbox":{"x0":630,"x1":715,"y0":924,"y1":944},"font_size":-1.143e+34,"text":"Observation:"},{"bbox":{"x0":637,"x1":733,"y0":942,"y1":960},"font_size":-1.143e+34,"text":"[Back to Search]"},{"bbox":{"x0":637,"x1":674,"y0":959,"y1":977},"font_size":-1.143e+34,"text":"[Prev]"},{"bbox":{"x0":632,"x1":1038,"y0":974,"y1":995},"font_size":-1.143e+34,"text":"favor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]l"},{"bbox":{"x0":630,"x1":1021,"y0":990,"y1":1011},"font_size":-1.143e+34,"text":"[strawberry][strawberry banana][sampler variety pack][mega variety"},{"bbox":{"x0":632,"x1":795,"y0":1008,"y1":1025},"font_size":-1.143e+34,"text":"pack][orchard variety pack]"},{"bbox":{"x0":630,"x1":1034,"y0":1021,"y1":1043},"font_size":-1.143e+34,"text":"size [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack"},{"bbox":{"x0":632,"x1":677,"y0":1040,"y1":1056},"font_size":-1.143e+34,"text":"of 16)]"},{"bbox":{"x0":630,"x1":1011,"y0":1053,"y1":1074},"font_size":-1.143e+34,"text":"Nature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect"},{"bbox":{"x0":630,"x1":1026,"y0":1069,"y1":1091},"font_size":-1.143e+34,"text":"For School Lunches or an On-The-Go Snack - No Sugar Added, Non"},{"bbox":{"x0":632,"x1":943,"y0":1087,"y1":1104},"font_size":-1.143e+34,"text":"GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi"},{"bbox":{"x0":632,"x1":712,"y0":1102,"y1":1120},"font_size":-1.143e+34,"text":"Price: $12.99"},{"bbox":{"x0":632,"x1":708,"y0":1119,"y1":1137},"font_size":-1.143e+34,"text":"Rating: N.A."},{"bbox":{"x0":632,"x1":712,"y0":1135,"y1":1153},"font_size":-1.143e+34,"text":"[Description]"},{"bbox":{"x0":630,"x1":696,"y0":1146,"y1":1170},"font_size":-1.143e+34,"text":"[Features]"},{"bbox":{"x0":632,"x1":695,"y0":1167,"y1":1185},"font_size":-1.143e+34,"text":"[Reviews]"},{"bbox":{"x0":632,"x1":698,"y0":1183,"y1":1201},"font_size":-1.143e+34,"text":"[Buy Now]"},{"bbox":{"x0":634,"x1":1031,"y0":1216,"y1":1233},"font_size":-1.143e+34,"text":"Action: think[For sixteen pack of apple cinnamon freeze dried banana"},{"bbox":{"x0":630,"x1":1028,"y0":1231,"y1":1247},"font_size":-1.143e+34,"text":"chips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of"},{"bbox":{"x0":632,"x1":802,"y0":1246,"y1":1267},"font_size":-1.143e+34,"text":"16)’ and seems good to buy.]"},{"bbox":{"x0":630,"x1":742,"y0":1259,"y1":1281},"font_size":-1.143e+34,"text":"Observation: OK."},{"bbox":{"x0":630,"x1":812,"y0":1292,"y1":1313},"font_size":-1.143e+34,"text":"Action: click[apple cinnamon]"},{"bbox":{"x0":630,"x1":910,"y0":1308,"y1":1330},"font_size":-1.143e+34,"text":"Observation: You have clicked apple cinnamon."},{"bbox":{"x0":632,"x1":856,"y0":1340,"y1":1361},"font_size":-1.143e+34,"text":"Action: click[0.53 ounce (pack of 16)]"},{"bbox":{"x0":632,"x1":955,"y0":1356,"y1":1378},"font_size":-1.143e+34,"text":"Observation: You have clicked 0.53 ounce (pack of 16)."},{"bbox":{"x0":632,"x1":775,"y0":1388,"y1":1408},"font_size":-1.143e+34,"text":"Action: click[Buy Now]"},{"bbox":{"x0":630,"x1":697,"y0":1434,"y1":1452},"font_size":-1.143e+34,"text":"Score: 1.0"}],"source":"layout det","text":"ana chips, and price lower than 50.00 dollars  \nReAct  \nAction: seareh [sixteen pack apple cinnamon freeze dried banana  \nchips]  \nObservation:  \n[Back to Search]  \nPage 1 (Total results: 50)  \n[Next]  \n[B0061IVFZE]  \nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -  \nOunce (Pack of 100)  \n $85.0 [B096H2P6G2]  \nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids  \n(Variety Pack)  \n $18.99 [B092JLLYK6]  \nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect  \nFor School Lunches or an On-The-Go Snack - No Sugar Added,  \nNon GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi  \n $12.99 Action: think[B0061IVFZE is strawberry banana, not apple cinnamon.  \nB096H2P6G2 is fruit snacks, not freeze dried banana chips.  \nB092JLLYK6 is banana crisps, not apple cinnamon. I can check  \nB092JLLYK6 frst.i  \nObservation: OK.  \nAction: click[B092JLLYK6]  \nObservation:  \n[Back to Search]  \n[Prev]  \nfavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]l  \n[strawberry][strawberry banana][sampler variety pack][mega variety  \npack][orchard variety pack]  \nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack  \nof 16)]  \nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect  \nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non  \nGMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi  \nPrice: $12.99  \nRating: N.A.  \n[Description]  \n[Features]  \n[Reviews]  \n[Buy Now]  \nAction: think[For sixteen pack of apple cinnamon freeze dried banana  \nchips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of  \n16)’ and seems good to buy.]  \nObservation: OK.  \nAction: click[apple cinnamon]  \nObservation: You have clicked apple cinnamon.  \nAction: click[0.53 ounce (pack of 16)]  \nObservation: You have clicked 0.53 ounce (pack of 16).  \nAction: click[Buy Now]  \nScore: 1.0"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":210,"x1":494,"y0":161,"y1":193},"conf":0.8725,"label":"Title","label_id":0},{"bbox":{"x0":209,"x1":1016,"y0":348,"y1":402},"conf":0.6992,"label":"Text","label_id":1},{"bbox":{"x0":595,"x1":625,"y0":1499,"y1":1527},"conf":0.666,"label":"Abandon","label_id":2},{"bbox":{"x0":221,"x1":372,"y0":1036,"y1":1060},"conf":0.6617,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":592,"y0":48,"y1":81},"conf":0.6491,"label":"Abandon","label_id":2},{"bbox":{"x0":220,"x1":879,"y0":424,"y1":448},"conf":0.639,"label":"Text","label_id":1},{"bbox":{"x0":630,"x1":778,"y0":1388,"y1":1411},"conf":0.5056,"label":"Text","label_id":1},{"bbox":{"x0":636,"x1":736,"y0":525,"y1":541},"conf":0.495,"label":"Text","label_id":1},{"bbox":{"x0":625,"x1":1048,"y0":427,"y1":1463},"conf":0.4654,"label":"Reference","label_id":11},{"bbox":{"x0":630,"x1":811,"y0":1293,"y1":1312},"conf":0.4206,"label":"Text","label_id":1},{"bbox":{"x0":810,"x1":859,"y0":451,"y1":469},"conf":0.3344,"label":"Title","label_id":0},{"bbox":{"x0":401,"x1":435,"y0":451,"y1":468},"conf":0.3241,"label":"Title","label_id":0},{"bbox":{"x0":630,"x1":912,"y0":1312,"y1":1330},"conf":0.3209,"label":"Text","label_id":1},{"bbox":{"x0":220,"x1":305,"y0":1432,"y1":1454},"conf":0.304,"label":"Abandon","label_id":2},{"bbox":{"x0":630,"x1":1040,"y0":976,"y1":1025},"conf":0.2969,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":294,"y0":1009,"y1":1028},"conf":0.2693,"label":"Text","label_id":1},{"bbox":{"x0":223,"x1":304,"y0":946,"y1":961},"conf":0.2624,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":287,"y0":994,"y1":1009},"conf":0.2579,"label":"Text","label_id":1},{"bbox":{"x0":215,"x1":617,"y0":426,"y1":1456},"conf":0.2525,"label":"Table","label_id":5},{"bbox":{"x0":223,"x1":306,"y0":962,"y1":978},"conf":0.2411,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":614,"y0":881,"y1":923},"conf":0.2408,"label":"Text","label_id":1},{"bbox":{"x0":224,"x1":288,"y0":979,"y1":993},"conf":0.2356,"label":"Text","label_id":1},{"bbox":{"x0":631,"x1":804,"y0":907,"y1":929},"conf":0.2322,"label":"Text","label_id":1},{"bbox":{"x0":401,"x1":435,"y0":451,"y1":468},"conf":0.2168,"label":"Text","label_id":1},{"bbox":{"x0":209,"x1":1017,"y0":46,"y1":86},"conf":0.2117,"label":"Abandon","label_id":2},{"bbox":{"x0":631,"x1":1008,"y0":471,"y1":513},"conf":0.208,"label":"Text","label_id":1},{"bbox":{"x0":215,"x1":617,"y0":426,"y1":1456},"conf":0.2061,"label":"Reference","label_id":11}],"ocr_all":false,"ocr_dets":[{"poly":[[597,1500],[624,1500],[624,1523],[597,1523]],"score":0.7709},{"poly":[[630,1434],[697,1434],[697,1452],[630,1452]],"score":0.8877},{"poly":[[223,1434],[303,1434],[303,1452],[223,1452]],"score":0.8771},{"poly":[[632,1388],[775,1389],[775,1408],[632,1406]],"score":0.7729},{"poly":[[632,1356],[955,1356],[955,1378],[632,1378]],"score":0.7403},{"poly":[[632,1340],[856,1340],[856,1361],[632,1361]],"score":0.7661},{"poly":[[630,1308],[910,1308],[910,1330],[630,1330]],"score":0.7502},{"poly":[[630,1292],[812,1292],[812,1313],[630,1313]],"score":0.7453},{"poly":[[631,1259],[742,1262],[741,1281],[630,1277]],"score":0.6611},{"poly":[[632,1246],[802,1246],[802,1267],[632,1267]],"score":0.6631},{"poly":[[630,1231],[1028,1231],[1028,1247],[630,1247]],"score":0.7904},{"poly":[[634,1216],[1031,1216],[1031,1233],[634,1233]],"score":0.7998},{"poly":[[632,1183],[698,1183],[698,1201],[632,1201]],"score":0.9523},{"poly":[[632,1167],[695,1167],[695,1185],[632,1185]],"score":0.9455},{"poly":[[631,1146],[696,1150],[694,1170],[630,1166]],"score":0.735},{"poly":[[632,1135],[712,1135],[712,1153],[632,1153]],"score":0.8318},{"poly":[[632,1119],[708,1119],[708,1137],[632,1137]],"score":0.806},{"poly":[[632,1102],[712,1102],[712,1120],[632,1120]],"score":0.8186},{"poly":[[632,1087],[943,1087],[943,1104],[632,1104]],"score":0.8866},{"poly":[[630,1069],[1026,1069],[1026,1091],[630,1091]],"score":0.6941},{"poly":[[630,1053],[1011,1053],[1011,1074],[630,1074]],"score":0.7072},{"poly":[[632,1040],[677,1040],[677,1056],[632,1056]],"score":0.8567},{"poly":[[223,1036],[368,1038],[367,1056],[223,1054]],"score":0.7814},{"poly":[[630,1021],[1034,1021],[1034,1043],[630,1043]],"score":0.7428},{"poly":[[632,1008],[795,1008],[795,1025],[632,1025]],"score":0.8431},{"poly":[[225,1008],[293,1008],[293,1026],[225,1026]],"score":0.8295},{"poly":[[630,990],[1021,990],[1021,1011],[630,1011]],"score":0.7458},{"poly":[[225,992],[288,992],[288,1010],[225,1010]],"score":0.798},{"poly":[[632,974],[1038,974],[1038,995],[632,995]],"score":0.7814},{"poly":[[225,975],[286,975],[286,993],[225,993]],"score":0.8188},{"poly":[[225,960],[303,960],[303,977],[225,977]],"score":0.9816},{"poly":[[637,959],[674,959],[674,977],[637,977]],"score":0.8295},{"poly":[[225,944],[301,944],[301,962],[225,962]],"score":0.8175},{"poly":[[637,942],[733,942],[733,960],[637,960]],"score":0.841},{"poly":[[225,927],[299,927],[299,944],[225,944]],"score":0.9187},{"poly":[[631,924],[715,926],[715,944],[630,942]],"score":0.7586},{"poly":[[223,909],[374,911],[374,929],[223,927]],"score":0.7579},{"poly":[[630,908],[803,908],[803,929],[630,929]],"score":0.812},{"poly":[[223,894],[592,894],[592,916],[223,916]],"score":0.7055},{"poly":[[223,876],[612,878],[612,899],[223,898]],"score":0.7352},{"poly":[[632,878],[742,878],[742,896],[632,896]],"score":0.7176},{"poly":[[231,863],[264,863],[264,881],[231,881]],"score":0.788},{"poly":[[630,860],[744,861],[743,880],[630,878]],"score":0.7481},{"poly":[[230,846],[324,846],[324,865],[230,865]],"score":0.8617},{"poly":[[629,841],[996,843],[996,866],[629,865]],"score":0.6176},{"poly":[[629,827],[973,828],[973,850],[629,848]],"score":0.7919},{"poly":[[225,830],[306,830],[306,848],[225,848]],"score":0.7698},{"poly":[[632,812],[1034,813],[1034,832],[632,830]],"score":0.7232},{"poly":[[225,812],[392,812],[392,835],[225,835]],"score":0.7624},{"poly":[[632,782],[677,782],[677,799],[632,799]],"score":0.9712},{"poly":[[223,780],[269,780],[269,799],[223,799]],"score":0.8447},{"poly":[[630,766],[966,766],[966,782],[630,782]],"score":0.8073},{"poly":[[225,766],[562,766],[562,782],[225,782]],"score":0.8309},{"poly":[[630,747],[995,749],[994,767],[630,766]],"score":0.7185},{"poly":[[221,747],[590,746],[590,767],[221,769]],"score":0.7352},{"poly":[[634,734],[1008,734],[1008,751],[634,751]],"score":0.8849},{"poly":[[226,734],[600,734],[600,751],[226,751]],"score":0.868},{"poly":[[635,716],[727,716],[727,734],[635,734]],"score":0.8642},{"poly":[[226,716],[316,716],[316,734],[226,734]],"score":0.8514},{"poly":[[632,701],[679,701],[679,718],[632,718]],"score":0.9105},{"poly":[[225,700],[269,700],[269,718],[225,718]],"score":0.7733},{"poly":[[630,685],[715,685],[715,703],[630,703]],"score":0.8049},{"poly":[[225,685],[309,685],[309,703],[225,703]],"score":0.8361},{"poly":[[634,670],[980,670],[980,686],[634,686]],"score":0.9532},{"poly":[[226,670],[574,670],[574,686],[226,686]],"score":0.9495},{"poly":[[226,653],[314,653],[314,672],[226,672]],"score":0.8403},{"poly":[[634,650],[728,650],[728,673],[634,673]],"score":0.7334},{"poly":[[632,635],[672,635],[672,653],[632,653]],"score":0.8525},{"poly":[[225,635],[264,635],[264,653],[225,653]],"score":0.8706},{"poly":[[632,620],[753,620],[753,639],[632,639]],"score":0.8113},{"poly":[[223,619],[346,620],[346,639],[223,637]],"score":0.7931},{"poly":[[632,602],[983,602],[983,624],[632,624]],"score":0.7105},{"poly":[[223,602],[579,602],[579,624],[223,624]],"score":0.7065},{"poly":[[635,587],[723,587],[723,606],[635,606]],"score":0.9153},{"poly":[[228,587],[316,587],[316,606],[228,606]],"score":0.9272},{"poly":[[634,553],[677,553],[677,578],[634,578]],"score":0.8197},{"poly":[[225,553],[269,553],[269,578],[225,578]],"score":0.8047},{"poly":[[632,538],[780,538],[780,559],[632,559]],"score":0.7701},{"poly":[[226,540],[371,540],[371,556],[226,556]],"score":0.9362},{"poly":[[635,522],[738,518],[739,541],[636,545]],"score":0.6869},{"poly":[[228,522],[332,520],[333,541],[228,543]],"score":0.7184},{"poly":[[225,507],[309,507],[309,525],[225,525]],"score":0.8237},{"poly":[[629,501],[717,505],[716,528],[628,524]],"score":0.7231},{"poly":[[632,488],[671,492],[669,512],[630,508]],"score":0.7723},{"poly":[[224,488],[263,492],[261,512],[222,508]],"score":0.8021},{"poly":[[630,472],[1006,472],[1006,493],[630,493]],"score":0.7656},{"poly":[[223,472],[599,472],[599,493],[223,493]],"score":0.7752},{"poly":[[810,450],[856,450],[856,467],[810,467]],"score":0.9696},{"poly":[[406,452],[432,452],[432,465],[406,465]],"score":0.9323},{"poly":[[223,424],[876,422],[876,444],[223,446]],"score":0.7728},{"poly":[[213,375],[778,375],[778,396],[213,396]],"score":0.8483},{"poly":[[213,351],[1011,351],[1011,375],[213,375]],"score":0.7282},{"poly":[[260,163],[491,165],[490,188],[259,186]],"score":0.7451},{"poly":[[215,167],[269,167],[269,185],[215,185]],"score":0.8153},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7377}],"page_no":30,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1526},"conf":0.6686,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1500,"y1":1523},"font_size":-2.585e-29,"text":"32"}],"source":"layout det","text":""},{"bbox":{"x0":211,"x1":590,"y0":50,"y1":79},"conf":0.6523,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":-2.585e-29,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":210,"x1":435,"y0":161,"y1":191},"conf":0.8135,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":432,"y0":165,"y1":186},"font_size":-4.088e-34,"text":"EMORE ANALYSIS"}],"source":"layout det","text":"EMORE ANALYSIS"},{"bbox":{"x0":211,"x1":633,"y0":216,"y1":243},"conf":0.8637,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":215,"x1":629,"y0":218,"y1":238},"font_size":-4.088e-34,"text":"E.1SUCCESS AND FAILURE MODES ANALYSIS"}],"source":"layout det","text":"E.1SUCCESS AND FAILURE MODES ANALYSIS"},{"bbox":{"x0":209,"x1":1017,"y0":259,"y1":312},"conf":0.8854,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":211,"x1":1011,"y0":259,"y1":284},"font_size":-4.088e-34,"text":"We provide some examples corresponding to the success and error mode analysis given in Sec. 3.3."},{"bbox":{"x0":211,"x1":787,"y0":282,"y1":305},"font_size":-4.088e-34,"text":"Search results and non-representative steps are omitted to reduce space."}],"source":"layout det","text":"We provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.Search results and non-representative steps are omitted to reduce space."},{"bbox":{"x0":211,"x1":1032,"y0":332,"y1":717},"conf":0.954,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":507,"x1":637,"y0":334,"y1":352},"font_size":0.0,"text":"Success: True positive"},{"bbox":{"x0":226,"x1":271,"y0":354,"y1":369},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":226,"x1":304,"y0":371,"y1":385},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":367,"x1":1006,"y0":369,"y1":384},"font_size":0.0,"text":"Author David Chanoff has collaborated with a U.S. Navy admiral who served as"},{"bbox":{"x0":369,"x1":863,"y0":386,"y1":400},"font_size":0.0,"text":"the ambassador to the United Kingdom under which President?"},{"bbox":{"x0":227,"x1":311,"y0":402,"y1":416},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":368,"x1":906,"y0":402,"y1":416},"font_size":0.0,"text":"I need to search David Chanoff and find the U.S. Navy admiral he"},{"bbox":{"x0":368,"x1":520,"y0":419,"y1":432},"font_size":0.0,"text":"collaborated with."},{"bbox":{"x0":226,"x1":304,"y0":434,"y1":447},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":368,"x1":544,"y0":434,"y1":448},"font_size":0.0,"text":"Search[David Chanoff]"},{"bbox":{"x0":226,"x1":345,"y0":450,"y1":464},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":367,"x1":652,"y0":449,"y1":465},"font_size":0.0,"text":"David Chanoff is a noted author..."},{"bbox":{"x0":225,"x1":304,"y0":463,"y1":480},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":368,"x1":568,"y0":466,"y1":480},"font_size":0.0,"text":"Search[William J. Crowe]"},{"bbox":{"x0":226,"x1":345,"y0":482,"y1":496},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":367,"x1":553,"y0":481,"y1":497},"font_size":0.0,"text":"William J. Crowe is..."},{"bbox":{"x0":227,"x1":313,"y0":498,"y1":512},"font_size":0.0,"text":"Thought 3:"},{"bbox":{"x0":368,"x1":945,"y0":498,"y1":512},"font_size":0.0,"text":"William J. Crowe served as the ambassador to the United Kingdom under"},{"bbox":{"x0":367,"x1":564,"y0":513,"y1":529},"font_size":0.0,"text":"President Bill Clinton."},{"bbox":{"x0":574,"x1":829,"y0":513,"y1":528},"font_size":0.0,"text":"So the answer is Bill Clinton."},{"bbox":{"x0":226,"x1":304,"y0":530,"y1":544},"font_size":0.0,"text":"Action 3:"},{"bbox":{"x0":367,"x1":536,"y0":528,"y1":544},"font_size":0.0,"text":"Finish[Bill Clinton]"},{"bbox":{"x0":226,"x1":344,"y0":546,"y1":560},"font_size":0.0,"text":"Observation 3:"},{"bbox":{"x0":368,"x1":504,"y0":546,"y1":560},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":224,"x1":255,"y0":561,"y1":580},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":226,"x1":303,"y0":579,"y1":593},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":368,"x1":964,"y0":579,"y1":593},"font_size":0.0,"text":"What was the name of the team that represented an Arizona university in"},{"bbox":{"x0":369,"x1":969,"y0":595,"y1":609},"font_size":0.0,"text":"the conference that covers a broad expanse of the western United States,"},{"bbox":{"x0":369,"x1":755,"y0":611,"y1":625},"font_size":0.0,"text":"including Arizona, California, and New Mexico?"},{"bbox":{"x0":226,"x1":294,"y0":625,"y1":642},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":368,"x1":597,"y0":626,"y1":641},"font_size":0.0,"text":"Let’s think step by step."},{"bbox":{"x0":591,"x1":996,"y0":627,"y1":641},"font_size":0.0,"text":"The name of the team that represented an Arizona"},{"bbox":{"x0":368,"x1":963,"y0":642,"y1":656},"font_size":0.0,"text":"university in the conference that covers a broad expanse of the western"},{"bbox":{"x0":368,"x1":964,"y0":659,"y1":673},"font_size":0.0,"text":"United States, including Arizona, California, and New Mexico is Arizona"},{"bbox":{"x0":367,"x1":512,"y0":674,"y1":689},"font_size":0.0,"text":"State Sun Devils."},{"bbox":{"x0":225,"x1":286,"y0":689,"y1":705},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":367,"x1":570,"y0":691,"y1":704},"font_size":0.0,"text":"Arizona State Sun Devils"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Success: True positive</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Question:</td><td>Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?</td></tr><tr><td>Thought 1:</td><td>I need to search David Chanoff and find the U.S. Navy admiral he</td></tr><tr><td>Action 1:</td><td>collaborated with. Search[David Chanoff]</td></tr><tr><td>Observation 1:</td><td>David Chanoff is a noted author...</td></tr><tr><td>Action 2:</td><td>Search[William J. Crowe]</td></tr><tr><td>Observation 2:</td><td>William J. Crowe is...</td></tr><tr><td>Thought 3:</td><td>William J. Crowe served as the ambassador to the United Kingdom under</td></tr><tr><td>Action 3:</td><td>President Bill Clinton. So the answer is Bill Clinton.</td></tr><tr><td>Observation 3:</td><td>Finish[Bill Clinton] Episode finished</td></tr><tr><td colspan=\"2\"></td></tr><tr><td>CoT Question:</td><td>What was the name of the team that represented an Arizona university in</td></tr><tr><td>Thought:</td><td>the conference that covers a broad expanse of the western United States, including Arizona, California, and New Mexico? Let’s think step by step. The name of the team that represented an Arizona university in the conference that covers a broad expanse of the western</td></tr><tr><td>Answer:</td><td>United States, including Arizona, California, and New Mexico is Arizona State Sun Devils.</td></tr><tr><td></td><td>Arizona State Sun Devils</td></tr></tbody></table></body></html>"},{"bbox":{"x0":215,"x1":1031,"y0":763,"y1":1065},"conf":0.9106,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":505,"x1":638,"y0":767,"y1":783},"font_size":0.0,"text":"Success: False positive"},{"bbox":{"x0":226,"x1":271,"y0":786,"y1":802},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":227,"x1":304,"y0":803,"y1":818},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":366,"x1":855,"y0":801,"y1":816},"font_size":0.0,"text":"Did the board game San Marco or About Time come out first?"},{"bbox":{"x0":227,"x1":312,"y0":819,"y1":832},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":368,"x1":979,"y0":819,"y1":833},"font_size":0.0,"text":"I need to search San Marco and About Time, and find which came out first."},{"bbox":{"x0":225,"x1":304,"y0":833,"y1":848},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":367,"x1":510,"y0":833,"y1":849},"font_size":0.0,"text":"Search[San Marco]"},{"bbox":{"x0":226,"x1":345,"y0":850,"y1":865},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":365,"x1":598,"y0":847,"y1":867},"font_size":0.0,"text":"San Marco is one of the ..."},{"bbox":{"x0":609,"x1":812,"y0":850,"y1":864},"font_size":0.0,"text":"The was built in venice."},{"bbox":{"x0":226,"x1":312,"y0":866,"y1":880},"font_size":0.0,"text":"Thought 2:"},{"bbox":{"x0":367,"x1":661,"y0":865,"y1":882},"font_size":0.0,"text":"San Marco is a sestiere of Venice."},{"bbox":{"x0":661,"x1":946,"y0":867,"y1":880},"font_size":0.0,"text":"I need to search About Time next."},{"bbox":{"x0":225,"x1":304,"y0":881,"y1":896},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":368,"x1":517,"y0":883,"y1":896},"font_size":0.0,"text":"Search[About Time]"},{"bbox":{"x0":225,"x1":346,"y0":897,"y1":913},"font_size":0.0,"text":"Observation 2:"},{"bbox":{"x0":367,"x1":619,"y0":897,"y1":911},"font_size":0.0,"text":"Could not find [About Time]."},{"bbox":{"x0":617,"x1":689,"y0":899,"y1":911},"font_size":0.0,"text":"Similar:"},{"bbox":{"x0":688,"x1":751,"y0":898,"y1":913},"font_size":0.0,"text":"[...]."},{"bbox":{"x0":226,"x1":312,"y0":914,"y1":929},"font_size":0.0,"text":"Thought 3:"},{"bbox":{"x0":367,"x1":598,"y0":914,"y1":929},"font_size":0.0,"text":"About Time is a 2013 film."},{"bbox":{"x0":598,"x1":837,"y0":914,"y1":929},"font_size":0.0,"text":"So San Marco came out first."},{"bbox":{"x0":225,"x1":304,"y0":931,"y1":944},"font_size":0.0,"text":"Action 3:"},{"bbox":{"x0":367,"x1":509,"y0":931,"y1":944},"font_size":0.0,"text":"Finish[San Marco]"},{"bbox":{"x0":225,"x1":345,"y0":945,"y1":960},"font_size":0.0,"text":"Observation 3:"},{"bbox":{"x0":368,"x1":504,"y0":947,"y1":960},"font_size":0.0,"text":"Episode finished"},{"bbox":{"x0":226,"x1":255,"y0":961,"y1":978},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":225,"x1":302,"y0":979,"y1":994},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":366,"x1":872,"y0":977,"y1":995},"font_size":0.0,"text":"Was Vanderbilt University or Emory University founded first?"},{"bbox":{"x0":226,"x1":296,"y0":992,"y1":1011},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":365,"x1":948,"y0":992,"y1":1010},"font_size":0.0,"text":"Let’s think step by step.Vanderbilt University was founded in 1873."},{"bbox":{"x0":960,"x1":1006,"y0":992,"y1":1011},"font_size":0.0,"text":"Emory"},{"bbox":{"x0":367,"x1":635,"y0":1010,"y1":1025},"font_size":0.0,"text":"University was founded in 1836."},{"bbox":{"x0":637,"x1":882,"y0":1007,"y1":1027},"font_size":0.0,"text":"1873 (Vanderbilt University)"},{"bbox":{"x0":882,"x1":939,"y0":1010,"y1":1025},"font_size":0.0,"text":" $\\tt\\char62 \\tt1836$ "},{"bbox":{"x0":939,"x1":999,"y0":1007,"y1":1027},"font_size":6.0,"text":"(Emory"},{"bbox":{"x0":367,"x1":837,"y0":1026,"y1":1041},"font_size":0.0,"text":"University), so Vanderbilt University was founded first."},{"bbox":{"x0":227,"x1":286,"y0":1044,"y1":1056},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":367,"x1":504,"y0":1042,"y1":1058},"font_size":0.0,"text":"Emory University"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Success: False positive</td></tr></thead><tbody><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>Did the board game San Marco or About Time come out first?</td></tr><tr><td>Thought 1:</td><td>I need to search San Marco and About Time, and find which came out first.</td></tr><tr><td>Action 1:</td><td>Search[San Marco]</td></tr><tr><td>Observation 1:</td><td>San Marco is one of the ... The was built in venice.</td></tr><tr><td>Thought 2:</td><td>San Marco is a sestiere of Venice. I need to search About Time next.</td></tr><tr><td>Action 2: Observation 2:</td><td>Search[About Time]</td></tr><tr><td>Thought 3:</td><td>Could not find [About Time]. Similar: [...].</td></tr><tr><td>Action 3:</td><td>About Time is a 2013 film. So San Marco came out first. Finish[San Marco]</td></tr><tr><td>Observation 3:</td><td>Episode finished</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Question:</td><td>Emory (Emory</td></tr><tr><td></td><td>Was Vanderbilt University or Emory University founded first?  $\\tt\\char62 \\tt1836$ </td></tr><tr><td>Thought:</td><td>Let’s think step by step.Vanderbilt University was founded in 1873.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>University was founded in 1836. 1873 (Vanderbilt University)</td></tr><tr><td></td><td></td></tr><tr><td>Answer:</td><td>University), so Vanderbilt University was founded first. Emory University</td></tr></tbody></table></body></html>"},{"bbox":{"x0":215,"x1":1032,"y0":1116,"y1":1464},"conf":0.6107,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":503,"x1":642,"y0":1122,"y1":1136},"font_size":0.0,"text":"Failure: Reasoning error"},{"bbox":{"x0":227,"x1":271,"y0":1139,"y1":1153},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":226,"x1":304,"y0":1151,"y1":1171},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":366,"x1":948,"y0":1152,"y1":1170},"font_size":0.0,"text":"What was the middle name of the actress who was cast alongside Harvey"},{"bbox":{"x0":365,"x1":999,"y0":1168,"y1":1186},"font_size":0.0,"text":"Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura"},{"bbox":{"x0":367,"x1":989,"y0":1187,"y1":1201},"font_size":0.0,"text":"Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in"},{"bbox":{"x0":366,"x1":454,"y0":1202,"y1":1217},"font_size":0.0,"text":"Hairspray?"},{"bbox":{"x0":226,"x1":312,"y0":1217,"y1":1234},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":367,"x1":1001,"y0":1219,"y1":1233},"font_size":0.0,"text":"I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,"},{"bbox":{"x0":365,"x1":974,"y0":1233,"y1":1250},"font_size":0.0,"text":"Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and"},{"bbox":{"x0":367,"x1":964,"y0":1250,"y1":1264},"font_size":0.0,"text":"Marissa Jaret Winokur, find the actress that was cast alongside them in"},{"bbox":{"x0":367,"x1":678,"y0":1267,"y1":1281},"font_size":0.0,"text":"Hairspray, then find its middle name."},{"bbox":{"x0":225,"x1":303,"y0":1283,"y1":1296},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":368,"x1":569,"y0":1283,"y1":1297},"font_size":0.0,"text":"Search[Harvey Fierstein]"},{"bbox":{"x0":226,"x1":346,"y0":1299,"y1":1313},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":367,"x1":586,"y0":1299,"y1":1313},"font_size":0.0,"text":"Harvey Forbes Fierstein .."},{"bbox":{"x0":586,"x1":594,"y0":1301,"y1":1311},"font_size":6.0,"text":"."},{"bbox":{"x0":227,"x1":312,"y0":1315,"y1":1329},"font_size":0.0,"text":"Thought 2:"},{"bbox":{"x0":366,"x1":956,"y0":1314,"y1":1331},"font_size":0.0,"text":"Harvey Fierstein is an actor, playwright, and screenwriter.I need to"},{"bbox":{"x0":367,"x1":576,"y0":1331,"y1":1345},"font_size":0.0,"text":"search Linda Hart next..."},{"bbox":{"x0":225,"x1":303,"y0":1346,"y1":1360},"font_size":0.0,"text":"Action 2:"},{"bbox":{"x0":367,"x1":542,"y0":1345,"y1":1361},"font_size":0.0,"text":"Search[Linda Hart]..."},{"bbox":{"x0":226,"x1":254,"y0":1362,"y1":1378},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":226,"x1":302,"y0":1379,"y1":1393},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":367,"x1":712,"y0":1379,"y1":1393},"font_size":0.0,"text":"Who is older Danny Green or James Worthy?"},{"bbox":{"x0":225,"x1":295,"y0":1392,"y1":1411},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":366,"x1":581,"y0":1392,"y1":1411},"font_size":0.0,"text":"Let’s think step by step."},{"bbox":{"x0":591,"x1":989,"y0":1395,"y1":1409},"font_size":0.0,"text":"Danny Green was born in 1987.James Worthy was"},{"bbox":{"x0":366,"x1":481,"y0":1411,"y1":1424},"font_size":0.0,"text":"born in 1961."},{"bbox":{"x0":486,"x1":990,"y0":1408,"y1":1426},"font_size":0.0,"text":"1987 (Danny Green) > 1961 (James Worthy), so Danny Green is"},{"bbox":{"x0":365,"x1":419,"y0":1424,"y1":1441},"font_size":0.0,"text":"older."},{"bbox":{"x0":225,"x1":287,"y0":1441,"y1":1458},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":364,"x1":463,"y0":1440,"y1":1459},"font_size":0.0,"text":"Danny Green"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Failure: Reasoning error</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Question:</td><td>What was the middle name of the actress who was cast alongside Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in</td></tr><tr><td>Thought 1:</td><td>Hairspray? I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur, find the actress that was cast alongside them in</td></tr><tr><td>Action 1:</td><td>Hairspray, then find its middle name. Search[Harvey Fierstein]</td></tr><tr><td>Observation 1: Thought 2:</td><td>Harvey Forbes Fierstein .. . Harvey Fierstein is an actor, playwright, and screenwriter.I need to</td></tr><tr><td></td><td>search Linda Hart next...</td></tr><tr><td>Action 2:</td><td>Search[Linda Hart]...</td></tr><tr><td>CoT</td><td colspan=\"2\"></td></tr><tr><td>Question: Thought:</td><td>Who is older Danny Green or James Worthy? Let’s think step by step.</td><td></td></tr><tr><td></td><td>born in 1961.</td><td>Danny Green was born in 1987.James Worthy was</td></tr><tr><td>Answer:</td><td>older. Danny Green</td><td>1987 (Danny Green) > 1961 (James Worthy), so Danny Green is</td></tr></tbody></table></body></html>"}],"formula_dets":[{"bbox":{"x0":882,"x1":939,"y0":1010,"y1":1025},"conf":0.536,"label":"print_embedding","label_id":0}],"height":792,"layout_dets":[{"bbox":{"x0":211,"x1":1032,"y0":332,"y1":717},"conf":0.954,"label":"Table","label_id":5},{"bbox":{"x0":215,"x1":1031,"y0":763,"y1":1065},"conf":0.9106,"label":"Table","label_id":5},{"bbox":{"x0":209,"x1":1017,"y0":259,"y1":312},"conf":0.8854,"label":"Text","label_id":1},{"bbox":{"x0":211,"x1":633,"y0":216,"y1":243},"conf":0.8637,"label":"Title","label_id":0},{"bbox":{"x0":210,"x1":435,"y0":161,"y1":191},"conf":0.8135,"label":"Title","label_id":0},{"bbox":{"x0":596,"x1":627,"y0":1500,"y1":1526},"conf":0.6686,"label":"Abandon","label_id":2},{"bbox":{"x0":211,"x1":590,"y0":50,"y1":79},"conf":0.6523,"label":"Abandon","label_id":2},{"bbox":{"x0":215,"x1":1032,"y0":1116,"y1":1464},"conf":0.6107,"label":"Table","label_id":5}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1500],[625,1500],[625,1523],[599,1523]],"score":0.7977},{"poly":[[366,1440],[464,1440],[464,1457],[366,1457]],"score":0.9163},{"poly":[[223,1440],[288,1440],[288,1457],[223,1457]],"score":0.9463},{"poly":[[365,1422],[420,1426],[418,1444],[364,1440]],"score":0.7849},{"poly":[[486,1404],[993,1406],[993,1427],[486,1426]],"score":0.6907},{"poly":[[366,1407],[482,1407],[482,1426],[366,1426]],"score":0.6708},{"poly":[[848,1393],[990,1393],[990,1409],[848,1409]],"score":0.7888},{"poly":[[590,1393],[840,1393],[840,1409],[590,1409]],"score":0.7883},{"poly":[[366,1393],[582,1393],[582,1409],[366,1409]],"score":0.7703},{"poly":[[226,1393],[296,1393],[296,1411],[226,1411]],"score":0.8254},{"poly":[[223,1378],[303,1378],[303,1394],[223,1394]],"score":0.9359},{"poly":[[366,1376],[715,1376],[715,1393],[366,1393]],"score":0.7923},{"poly":[[225,1360],[256,1360],[256,1378],[225,1378]],"score":0.8314},{"poly":[[225,1343],[304,1343],[304,1361],[225,1361]],"score":0.7339},{"poly":[[364,1327],[579,1328],[579,1346],[364,1345]],"score":0.6461},{"poly":[[366,1313],[956,1313],[956,1330],[366,1330]],"score":0.8106},{"poly":[[226,1312],[313,1312],[313,1330],[226,1330]],"score":0.7345},{"poly":[[366,1297],[594,1297],[594,1313],[366,1313]],"score":0.7492},{"poly":[[225,1297],[348,1297],[348,1313],[225,1313]],"score":0.8141},{"poly":[[366,1280],[570,1280],[570,1297],[366,1297]],"score":0.8002},{"poly":[[225,1280],[304,1280],[304,1297],[225,1297]],"score":0.9802},{"poly":[[366,1266],[679,1266],[679,1282],[366,1282]],"score":0.785},{"poly":[[366,1249],[965,1249],[965,1266],[366,1266]],"score":0.7715},{"poly":[[364,1233],[975,1233],[975,1249],[364,1249]],"score":0.7615},{"poly":[[226,1216],[313,1216],[313,1234],[226,1234]],"score":0.7652},{"poly":[[366,1201],[454,1201],[454,1218],[366,1218]],"score":0.8527},{"poly":[[366,1185],[993,1185],[993,1201],[366,1201]],"score":0.7792},{"poly":[[364,1165],[999,1165],[999,1186],[364,1186]],"score":0.65},{"poly":[[366,1152],[945,1152],[945,1168],[366,1168]],"score":0.7841},{"poly":[[225,1153],[304,1153],[304,1170],[225,1170]],"score":0.8635},{"poly":[[225,1137],[271,1137],[271,1155],[225,1155]],"score":0.8158},{"poly":[[499,1117],[642,1119],[642,1137],[499,1135]],"score":0.7713},{"poly":[[366,1041],[506,1041],[506,1058],[366,1058]],"score":0.7857},{"poly":[[223,1041],[286,1041],[286,1058],[223,1058]],"score":0.9685},{"poly":[[366,1025],[835,1025],[835,1041],[366,1041]],"score":0.8213},{"poly":[[635,1005],[1001,1007],[1001,1028],[635,1026]],"score":0.7424},{"poly":[[366,1008],[645,1008],[645,1025],[366,1025]],"score":0.7915},{"poly":[[958,989],[1009,996],[1006,1014],[956,1007]],"score":0.7283},{"poly":[[225,993],[298,993],[298,1010],[225,1010]],"score":0.8754},{"poly":[[364,992],[946,990],[946,1008],[364,1010]],"score":0.7283},{"poly":[[366,977],[871,977],[871,993],[366,993]],"score":0.8026},{"poly":[[223,974],[306,974],[306,997],[223,997]],"score":0.6786},{"poly":[[225,960],[256,960],[256,980],[225,980]],"score":0.7982},{"poly":[[368,947],[504,947],[504,959],[368,959]],"score":0.9851},{"poly":[[225,944],[346,944],[346,962],[225,962]],"score":0.6826},{"poly":[[364,927],[511,927],[511,944],[364,944]],"score":0.6871},{"poly":[[225,929],[306,929],[306,945],[225,945]],"score":0.8629},{"poly":[[595,912],[837,912],[837,929],[595,929]],"score":0.7776},{"poly":[[366,912],[602,912],[602,929],[366,929]],"score":0.7486},{"poly":[[225,912],[313,912],[313,929],[225,929]],"score":0.8332},{"poly":[[698,898],[752,898],[752,914],[698,914]],"score":0.8527},{"poly":[[366,896],[702,896],[702,912],[366,912]],"score":0.7663},{"poly":[[223,894],[346,896],[346,914],[223,912]],"score":0.7456},{"poly":[[368,883],[519,883],[519,894],[368,894]],"score":0.9889},{"poly":[[225,881],[306,881],[306,898],[225,898]],"score":0.8041},{"poly":[[665,865],[943,865],[943,881],[665,881]],"score":0.7564},{"poly":[[366,865],[662,865],[662,881],[366,881]],"score":0.7887},{"poly":[[225,865],[311,865],[311,881],[225,881]],"score":0.9568},{"poly":[[607,851],[810,851],[810,863],[607,863]],"score":0.9354},{"poly":[[364,848],[595,850],[595,866],[364,865]],"score":0.7962},{"poly":[[225,848],[348,848],[348,865],[225,865]],"score":0.8067},{"poly":[[366,833],[511,833],[511,850],[366,850]],"score":0.7954},{"poly":[[225,833],[306,833],[306,850],[225,850]],"score":0.8244},{"poly":[[366,817],[978,817],[978,833],[366,833]],"score":0.7678},{"poly":[[225,817],[313,817],[313,833],[225,833]],"score":0.8828},{"poly":[[225,802],[306,802],[306,818],[225,818]],"score":0.8414},{"poly":[[366,800],[855,800],[855,817],[366,817]],"score":0.8007},{"poly":[[223,782],[273,782],[273,805],[223,805]],"score":0.738},{"poly":[[506,767],[639,767],[639,784],[506,784]],"score":0.9572},{"poly":[[366,688],[572,688],[572,705],[366,705]],"score":0.8046},{"poly":[[224,686],[288,690],[287,708],[222,704]],"score":0.7547},{"poly":[[366,673],[512,673],[512,690],[366,690]],"score":0.8024},{"poly":[[649,657],[965,657],[965,673],[649,673]],"score":0.711},{"poly":[[366,657],[649,657],[649,673],[366,673]],"score":0.7438},{"poly":[[366,642],[965,642],[965,658],[366,658]],"score":0.7753},{"poly":[[589,625],[998,625],[998,642],[589,642]],"score":0.7607},{"poly":[[366,625],[595,625],[595,642],[366,642]],"score":0.7813},{"poly":[[225,624],[296,624],[296,642],[225,642]],"score":0.8109},{"poly":[[366,609],[757,609],[757,625],[366,625]],"score":0.7822},{"poly":[[363,589],[971,591],[971,612],[363,610]],"score":0.6749},{"poly":[[366,578],[965,578],[965,594],[366,594]],"score":0.788},{"poly":[[225,578],[304,578],[304,594],[225,594]],"score":0.8797},{"poly":[[225,559],[256,559],[256,579],[225,579]],"score":0.8578},{"poly":[[366,543],[507,543],[507,559],[366,559]],"score":0.7903},{"poly":[[225,543],[346,543],[346,561],[225,561]],"score":0.7283},{"poly":[[366,528],[536,528],[536,544],[366,544]],"score":0.8153},{"poly":[[225,526],[304,526],[304,544],[225,544]],"score":0.771},{"poly":[[572,513],[830,513],[830,530],[572,530]],"score":0.7594},{"poly":[[366,513],[565,513],[565,530],[366,530]],"score":0.8},{"poly":[[366,497],[948,497],[948,513],[366,513]],"score":0.7989},{"poly":[[223,493],[313,495],[312,513],[223,511]],"score":0.7756},{"poly":[[364,478],[549,480],[549,498],[364,497]],"score":0.7145},{"poly":[[225,480],[348,480],[348,497],[225,497]],"score":0.8463},{"poly":[[225,465],[304,465],[304,482],[225,482]],"score":0.8055},{"poly":[[364,462],[570,464],[570,482],[364,480]],"score":0.6972},{"poly":[[366,449],[655,449],[655,465],[366,465]],"score":0.8168},{"poly":[[225,449],[346,449],[346,465],[225,465]],"score":0.8572},{"poly":[[366,432],[544,432],[544,449],[366,449]],"score":0.8241},{"poly":[[225,432],[304,432],[304,449],[225,449]],"score":0.8488},{"poly":[[366,417],[519,417],[519,434],[366,434]],"score":0.7947},{"poly":[[366,401],[906,401],[906,417],[366,417]],"score":0.8007},{"poly":[[225,401],[313,401],[313,417],[225,417]],"score":0.888},{"poly":[[366,384],[866,384],[866,401],[366,401]],"score":0.7211},{"poly":[[225,370],[304,370],[304,386],[225,386]],"score":0.8939},{"poly":[[364,365],[1008,366],[1008,388],[364,386]],"score":0.6892},{"poly":[[225,353],[273,353],[273,371],[225,371]],"score":0.8225},{"poly":[[507,335],[635,335],[635,351],[507,351]],"score":0.8972},{"poly":[[211,282],[787,284],[787,305],[211,304]],"score":0.7802},{"poly":[[211,259],[1011,261],[1011,284],[211,282]],"score":0.6878},{"poly":[[261,218],[629,219],[629,238],[261,236]],"score":0.7226},{"poly":[[215,219],[269,219],[269,238],[215,238]],"score":0.7704},{"poly":[[248,165],[432,165],[432,186],[248,186]],"score":0.9188},{"poly":[[215,165],[256,165],[256,185],[215,185]],"score":0.7314},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.7268}],"page_no":31,"scale":2.0,"width":612},{"abandon_blocks":[{"bbox":{"x0":596,"x1":626,"y0":1500,"y1":1526},"conf":0.7054,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":599,"x1":625,"y0":1502,"y1":1523},"font_size":0.0,"text":"33"}],"source":"layout det","text":""},{"bbox":{"x0":211,"x1":1017,"y0":48,"y1":84},"conf":0.2422,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":213,"x1":585,"y0":49,"y1":74},"font_size":0.0,"text":"Published as a conference paper at ICLR 2023"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":212,"x1":1029,"y0":158,"y1":324},"conf":0.8554,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":511,"x1":631,"y0":160,"y1":176},"font_size":0.0,"text":"Failure: Search error"},{"bbox":{"x0":226,"x1":271,"y0":176,"y1":191},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":225,"x1":305,"y0":190,"y1":208},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":367,"x1":971,"y0":192,"y1":207},"font_size":0.0,"text":"What does the goddess associated with the goddess frigg consists of what"},{"bbox":{"x0":368,"x1":420,"y0":209,"y1":220},"font_size":0.0,"text":"tales?"},{"bbox":{"x0":225,"x1":312,"y0":220,"y1":240},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":368,"x1":893,"y0":224,"y1":239},"font_size":0.0,"text":"I need to search goddess frigg and find its associated goddess."},{"bbox":{"x0":226,"x1":304,"y0":240,"y1":253},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":367,"x1":544,"y0":239,"y1":255},"font_size":0.0,"text":"Search[goddess frigg]"},{"bbox":{"x0":227,"x1":346,"y0":256,"y1":269},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":366,"x1":621,"y0":252,"y1":272},"font_size":0.0,"text":"Could not find goddess frigg."},{"bbox":{"x0":624,"x1":699,"y0":254,"y1":270},"font_size":0.0,"text":"Similar:"},{"bbox":{"x0":706,"x1":998,"y0":254,"y1":272},"font_size":0.0,"text":"[’Frigg’, ’Frigg and Freyja common"},{"bbox":{"x0":366,"x1":989,"y0":270,"y1":288},"font_size":0.0,"text":"origin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses"},{"bbox":{"x0":367,"x1":880,"y0":287,"y1":304},"font_size":0.0,"text":"and figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’]."},{"bbox":{"x0":227,"x1":311,"y0":303,"y1":318},"font_size":0.0,"text":"Thought 2:"},{"bbox":{"x0":366,"x1":641,"y0":302,"y1":319},"font_size":0.0,"text":"I need to search Frigg instead..."}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Failure: Search error</td></tr></thead><tbody><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>What does the goddess associated with the goddess frigg consists of what tales?</td></tr><tr><td>Thought 1:</td><td>I need to search goddess frigg and find its associated goddess.</td></tr><tr><td>Action 1:</td><td>Search[goddess frigg] Could not find goddess frigg. Similar: [’Frigg’, ’Frigg and Freyja common</td></tr><tr><td>Observation 1:</td><td>origin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses and figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’].</td></tr><tr><td>Thought 2:</td><td>I need to search Frigg instead...</td></tr></tbody></table></body></html>"},{"bbox":{"x0":213,"x1":1013,"y0":362,"y1":485},"conf":0.8932,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":509,"x1":635,"y0":368,"y1":381},"font_size":0.0,"text":"Failure: Hallucination"},{"bbox":{"x0":225,"x1":255,"y0":380,"y1":399},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":226,"x1":303,"y0":397,"y1":413},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":326,"x1":938,"y0":399,"y1":412},"font_size":0.0,"text":"What was the first year a scientific journal published by an organization"},{"bbox":{"x0":326,"x1":621,"y0":415,"y1":428},"font_size":0.0,"text":"located in the Bronx was published?"},{"bbox":{"x0":225,"x1":295,"y0":428,"y1":447},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":324,"x1":955,"y0":429,"y1":446},"font_size":0.0,"text":"Let’s think step by step.The first year a scientific journal published by"},{"bbox":{"x0":325,"x1":829,"y0":446,"y1":459},"font_size":0.0,"text":"an organization located in the Bronx was published was 1916."},{"bbox":{"x0":225,"x1":286,"y0":462,"y1":475},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":325,"x1":485,"y0":460,"y1":476},"font_size":0.0,"text":"1916 (Label:1909)"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\">Failure: Hallucination</td></tr></thead><tbody><tr><td>CoT Question:</td><td>What was the first year a scientific journal published by an organization</td></tr><tr><td></td><td>located in the Bronx was published?</td></tr><tr><td>Thought:</td><td>Let’s think step by step.The first year a scientific journal published by an organization located in the Bronx was published was 1916.</td></tr><tr><td>Answer:</td><td>1916 (Label:1909)</td></tr></tbody></table></body></html>"},{"bbox":{"x0":214,"x1":1032,"y0":519,"y1":858},"conf":0.9599,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":499,"x1":643,"y0":524,"y1":541},"font_size":0.0,"text":"Failure: Label ambiguity"},{"bbox":{"x0":226,"x1":271,"y0":544,"y1":558},"font_size":0.0,"text":"ReAct"},{"bbox":{"x0":225,"x1":304,"y0":558,"y1":575},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":365,"x1":872,"y0":557,"y1":575},"font_size":0.0,"text":"What nationality is the man who developed the dictator game?"},{"bbox":{"x0":225,"x1":312,"y0":573,"y1":591},"font_size":0.0,"text":"Thought 1:"},{"bbox":{"x0":367,"x1":1005,"y0":576,"y1":589},"font_size":0.0,"text":"I need to search dictator game, find the man who developed it, then find his"},{"bbox":{"x0":366,"x1":470,"y0":591,"y1":606},"font_size":0.0,"text":"nationality."},{"bbox":{"x0":226,"x1":303,"y0":608,"y1":621},"font_size":0.0,"text":"Action 1:"},{"bbox":{"x0":367,"x1":544,"y0":609,"y1":622},"font_size":0.0,"text":"Search[dictator game]"},{"bbox":{"x0":226,"x1":347,"y0":624,"y1":637},"font_size":0.0,"text":"Observation 1:"},{"bbox":{"x0":365,"x1":997,"y0":621,"y1":641},"font_size":0.0,"text":"The dictator game is a popular experimental instrument in social psychology"},{"bbox":{"x0":366,"x1":498,"y0":639,"y1":654},"font_size":0.0,"text":"and economics..."},{"bbox":{"x0":225,"x1":303,"y0":653,"y1":669},"font_size":0.0,"text":"Action 3:"},{"bbox":{"x0":365,"x1":537,"y0":654,"y1":671},"font_size":0.0,"text":"Search[Amos Tversky]"},{"bbox":{"x0":225,"x1":347,"y0":670,"y1":687},"font_size":0.0,"text":"Observation 3:"},{"bbox":{"x0":366,"x1":549,"y0":671,"y1":686},"font_size":0.0,"text":"Amos Nathan Tversky..."},{"bbox":{"x0":227,"x1":311,"y0":688,"y1":701},"font_size":0.0,"text":"Thought 4:"},{"bbox":{"x0":367,"x1":997,"y0":689,"y1":702},"font_size":0.0,"text":"Amos Tversky is an Israeli cognitive and mathematical psychologist.So the"},{"bbox":{"x0":367,"x1":761,"y0":704,"y1":717},"font_size":0.0,"text":"man who developed the dictator game is Israeli."},{"bbox":{"x0":224,"x1":304,"y0":718,"y1":735},"font_size":0.0,"text":"Action 4:"},{"bbox":{"x0":366,"x1":709,"y0":720,"y1":733},"font_size":0.0,"text":"Finish[Israeli] (label:Israel-American)"},{"bbox":{"x0":224,"x1":255,"y0":734,"y1":752},"font_size":0.0,"text":"CoT"},{"bbox":{"x0":225,"x1":304,"y0":751,"y1":767},"font_size":0.0,"text":"Question:"},{"bbox":{"x0":366,"x1":980,"y0":751,"y1":767},"font_size":0.0,"text":"Kurt Fuller from Stockton, CA appeared in which American Detective Comedy"},{"bbox":{"x0":367,"x1":771,"y0":768,"y1":781},"font_size":0.0,"text":"Drama television series created by Steve Franks?"},{"bbox":{"x0":226,"x1":296,"y0":783,"y1":799},"font_size":0.0,"text":"Thought:"},{"bbox":{"x0":366,"x1":996,"y0":783,"y1":799},"font_size":0.0,"text":"Let’s think step by step.Kurt Fuller from Stockton, CA appeared in Psych."},{"bbox":{"x0":365,"x1":972,"y0":798,"y1":816},"font_size":0.0,"text":"Psych is an American Detective Comedy Drama television series created by"},{"bbox":{"x0":366,"x1":479,"y0":816,"y1":830},"font_size":0.0,"text":"Steve Franks."},{"bbox":{"x0":225,"x1":286,"y0":831,"y1":847},"font_size":0.0,"text":"Answer:"},{"bbox":{"x0":367,"x1":479,"y0":832,"y1":845},"font_size":0.0,"text":"Psych (label:"},{"bbox":{"x0":491,"x1":859,"y0":832,"y1":845},"font_size":0.0,"text":"Psych is an American detective comedy-drama)"}],"source":"layout det","text":"<html><body><table><thead><tr><td colspan=\"2\"></td><td>Failure: Label ambiguity</td></tr></thead><tbody><tr><td>ReAct</td><td></td><td></td></tr><tr><td>Question:</td><td></td><td>What nationality is the man who developed the dictator game?</td></tr><tr><td>Thought 1:</td><td>nationality.</td><td>I need to search dictator game, find the man who developed it, then find his</td></tr><tr><td>Action 1:</td><td>Search[dictator game]</td><td></td></tr><tr><td>Observation 1:</td><td>and economics...</td><td>The dictator game is a popular experimental instrument in social psychology</td></tr><tr><td>Action 3:</td><td>Search[Amos Tversky]</td><td></td></tr><tr><td>Observation 3:</td><td>Amos Nathan Tversky...</td><td></td></tr><tr><td>Thought 4:</td><td>man who developed the dictator game is Israeli.</td><td>Amos Tversky is an Israeli cognitive and mathematical psychologist.So the</td></tr><tr><td>Action 4:</td><td>Finish[Israeli] (label:Israel-American)</td><td></td></tr><tr><td>CoT</td><td></td><td></td></tr><tr><td>Question:</td><td></td><td>Kurt Fuller from Stockton, CA appeared in which American Detective Comedy</td></tr><tr><td>Thought:</td><td></td><td>Drama television series created by Steve Franks? Let’s think step by step.Kurt Fuller from Stockton, CA appeared in Psych.</td></tr><tr><td>Answer:</td><td>Steve Franks. Psych (label:</td><td>Psych is an American Detective Comedy Drama television series created by Psych is an American detective comedy-drama)</td></tr></tbody></table></body></html>"}],"formula_dets":[],"height":792,"layout_dets":[{"bbox":{"x0":214,"x1":1032,"y0":519,"y1":858},"conf":0.9599,"label":"Table","label_id":5},{"bbox":{"x0":213,"x1":1013,"y0":362,"y1":485},"conf":0.8932,"label":"Table","label_id":5},{"bbox":{"x0":212,"x1":1029,"y0":158,"y1":324},"conf":0.8554,"label":"Table","label_id":5},{"bbox":{"x0":596,"x1":626,"y0":1500,"y1":1526},"conf":0.7054,"label":"Abandon","label_id":2},{"bbox":{"x0":211,"x1":591,"y0":50,"y1":79},"conf":0.6264,"label":"Abandon","label_id":2},{"bbox":{"x0":211,"x1":1017,"y0":48,"y1":84},"conf":0.2422,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[599,1502],[625,1502],[625,1523],[599,1523]],"score":0.9345},{"poly":[[491,830],[861,830],[861,846],[491,846]],"score":0.7117},{"poly":[[366,830],[481,830],[481,846],[366,846]],"score":0.7818},{"poly":[[223,830],[286,830],[286,846],[223,846]],"score":0.9162},{"poly":[[366,813],[477,813],[477,830],[366,830]],"score":0.7661},{"poly":[[727,799],[973,799],[973,815],[727,815]],"score":0.7544},{"poly":[[366,799],[733,799],[733,815],[366,815]],"score":0.7812},{"poly":[[590,782],[996,782],[996,799],[590,799]],"score":0.7668},{"poly":[[366,782],[584,782],[584,799],[366,799]],"score":0.7584},{"poly":[[226,782],[296,782],[296,799],[226,799]],"score":0.9488},{"poly":[[507,766],[772,766],[772,782],[507,782]],"score":0.7734},{"poly":[[366,766],[511,766],[511,782],[366,782]],"score":0.7055},{"poly":[[366,751],[980,751],[980,767],[366,767]],"score":0.7766},{"poly":[[225,751],[304,751],[304,767],[225,767]],"score":0.936},{"poly":[[225,733],[256,733],[256,752],[225,752]],"score":0.738},{"poly":[[577,719],[712,719],[712,731],[577,731]],"score":0.9587},{"poly":[[368,719],[565,719],[565,731],[368,731]],"score":0.9172},{"poly":[[223,718],[303,718],[303,734],[223,734]],"score":0.8517},{"poly":[[366,701],[763,701],[763,718],[366,718]],"score":0.7931},{"poly":[[366,686],[998,686],[998,703],[366,703]],"score":0.7975},{"poly":[[225,683],[313,685],[312,703],[224,701]],"score":0.7589},{"poly":[[366,670],[552,670],[552,686],[366,686]],"score":0.7439},{"poly":[[225,670],[346,670],[346,686],[225,686]],"score":0.7926},{"poly":[[364,652],[539,652],[539,673],[364,673]],"score":0.6796},{"poly":[[223,653],[301,653],[301,670],[223,670]],"score":0.8791},{"poly":[[366,639],[496,639],[496,655],[366,655]],"score":0.7498},{"poly":[[366,622],[996,622],[996,639],[366,639]],"score":0.7236},{"poly":[[225,622],[346,622],[346,639],[225,639]],"score":0.7524},{"poly":[[366,606],[544,606],[544,622],[366,622]],"score":0.8168},{"poly":[[225,606],[306,606],[306,622],[225,622]],"score":0.8877},{"poly":[[366,591],[469,591],[469,607],[366,607]],"score":0.7858},{"poly":[[366,574],[1006,574],[1006,591],[366,591]],"score":0.785},{"poly":[[226,574],[311,574],[311,591],[226,591]],"score":0.8959},{"poly":[[366,558],[871,558],[871,574],[366,574]],"score":0.7163},{"poly":[[225,558],[304,558],[304,576],[225,576]],"score":0.7624},{"poly":[[222,542],[270,537],[272,557],[224,561]],"score":0.723},{"poly":[[499,521],[644,523],[643,541],[499,539]],"score":0.7371},{"poly":[[441,462],[486,462],[486,474],[441,474]],"score":0.8988},{"poly":[[223,460],[286,460],[286,477],[223,477]],"score":0.9024},{"poly":[[326,457],[431,457],[431,474],[326,474]],"score":0.6004},{"poly":[[324,444],[830,444],[830,460],[324,460]],"score":0.7992},{"poly":[[546,424],[958,427],[958,449],[545,445]],"score":0.6464},{"poly":[[324,429],[549,429],[549,446],[324,446]],"score":0.7745},{"poly":[[226,429],[296,429],[296,446],[226,446]],"score":0.9677},{"poly":[[324,412],[622,412],[622,429],[324,429]],"score":0.75},{"poly":[[324,398],[940,398],[940,414],[324,414]],"score":0.7637},{"poly":[[225,398],[304,398],[304,414],[225,414]],"score":0.8905},{"poly":[[225,380],[256,380],[256,399],[225,399]],"score":0.8783},{"poly":[[507,366],[637,366],[637,383],[507,383]],"score":0.92},{"poly":[[366,300],[634,300],[634,317],[366,317]],"score":0.7032},{"poly":[[223,300],[311,299],[311,317],[223,319]],"score":0.7946},{"poly":[[368,289],[487,289],[487,300],[368,300]],"score":0.9569},{"poly":[[801,282],[880,286],[879,304],[800,300]],"score":0.7827},{"poly":[[531,284],[798,285],[798,304],[530,302]],"score":0.6612},{"poly":[[489,285],[539,285],[539,302],[489,302]],"score":0.7409},{"poly":[[529,266],[991,267],[991,289],[529,287]],"score":0.6987},{"poly":[[364,267],[540,267],[540,289],[364,289]],"score":0.6702},{"poly":[[710,254],[998,254],[998,271],[710,271]],"score":0.7554},{"poly":[[625,254],[698,254],[698,271],[625,271]],"score":0.7528},{"poly":[[364,249],[619,253],[618,274],[364,271]],"score":0.669},{"poly":[[226,256],[344,256],[344,267],[226,267]],"score":0.9761},{"poly":[[364,236],[544,238],[544,256],[364,254]],"score":0.6952},{"poly":[[225,238],[304,238],[304,254],[225,254]],"score":0.8974},{"poly":[[366,223],[893,223],[893,239],[366,239]],"score":0.7486},{"poly":[[226,221],[311,221],[311,239],[226,239]],"score":0.7122},{"poly":[[366,206],[421,206],[421,223],[366,223]],"score":0.9419},{"poly":[[366,190],[971,190],[971,206],[366,206]],"score":0.7376},{"poly":[[225,190],[304,190],[304,206],[225,206]],"score":0.9466},{"poly":[[225,175],[273,175],[273,191],[225,191]],"score":0.9069},{"poly":[[509,157],[634,158],[633,177],[509,175]],"score":0.7231},{"poly":[[213,51],[585,49],[585,73],[213,74]],"score":0.6985}],"page_no":32,"scale":2.0,"width":612}],"pages_success_ratio":0.0,"src_path":"oss://glm-data-ocr-data/services/maas/docs/47e1b393-0965-444c-92ba-1d158022748e","text":"REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\nShunyu $\\mathrm{Yao}^{*,1},$  Jeffrey Zhao2, Dian $\\mathrm{Yu}^{2},$  Nan $\\mathrm{Du}^{2},$  Izhak Shafran2, Karthik Narasimhan1, Yuan Cao\n1Department of Computer Science, Princeton University 2Google Research, Brain team\n1{shunyuy,karthikn}@princeton.edu\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\nABSTRACT\nWhile large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specifc actionsi in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verifcation (Fever), ReAct overcomes prevalenti issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces.Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of $34\\%$  and $10\\%$ respectively, while being prompted with only one or two in-context examples.\n1INTRODUCTION\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Consider the example of cooking up a dish in the kitchen. Between any two specifc actions, we mayi reason in language in order to track progress (“now that everything is cut, I should heat up the pot of water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let me use soy sauce and pepper instead”), and to realize when external information is needed (“how do I prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties.\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLMs)have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\n\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task solving trajectories generated by the model (Act, Thought) and the environment (Obs).\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors. These approaches usually convert multi-modal observations into text, use a language model to generate domain-specifc actions or plans, and theni use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring Huang et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been studies on how reasoning and acting can be combined in a synergistic manner for general task solving,and if such a combination can bring systematic benefts compared to reasoning or acting alone.i\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\n\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:question answering (HotPotQA, Yang et al., 2018), fact verifcation (Fever, Thorne et al., 2018),i text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact with, ReAct outperforms vanilla action generation models while being competitive with chain-ofthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct and CoT that allows for the use of both internal knowledge and externally obtained information during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able to outperform imitation or reinforcement learning methods trained with $10^{3}\\sim\\hat{10}^{5}$ task instances,with an absolute improvement of $34\\%$  and $10\\%$  in success rates respectively. We also demonstrate the importance of sparse, versatile reasoning in decision making by showing consistent advantages over controlled baselines with actions only. Besides general applicability and performance boost,the combination of reasoning and acting also contributes to model interpretability, trustworthiness,and diagnosability across all domains, as humans can readily distinguish information from model’s internal knowledge versus external environments, as well as inspect reasoning traces to understand the decision basis of model actions.\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel promptbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a few-shot learning setup over prior approaches that perform either reasoning or action generation in isolation; (3) we present systematic ablations and analysis to understand the importance of acting in reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial fnetuningi experiments showing the potential of ReAct to improve with additional training data. Scaling up ReAct to train and operate on more tasks and combining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models.\n2REAC T: SYNERGIZING REASONING + ACTING\nConsider a general setup of an agent interacting with an environment for task solving. At time step $t,$  an agent receives an observation $o_{t} \\in\\mathcal{O}$  from the environment and takes an action $a_{t}\\in\\mathcal{A}$ following some policy $\\pi(a_{t}|c_{t}),$  where $c_{t}=(o_{1},a_{1},\\cdots,o_{t-1},a_{t-1},o_{t})$  is the context to the agent.Learning a policy is challenging when the mapping $c_{t}\\mapsto a_{t}$ � is highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct fnali action (Act 4) to fnish the QA task as it requires complex reasoning over the trajectory contexti(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\nThe idea of ReAct is simple: we augment the agent’s action space to $\\hat{\\mathcal{A}}=\\mathcal{A}\\cup\\mathcal{L},$  where $\\mathcal{L}$  is the space of language. An action $\\hat{a}_{t}\\in\\mathcal{L}$  in the language space, which we will refer to as a thought or a reasoning trace, does not affect the external environment, thus leading to no observation feedback. Instead, a thought $\\widehat{a}_{t}$  aims to compose useful information by reasoning over the current context $c_{t}$ and update the context $c_{t+1}=(c_{t},\\hat{a}_{t})$  to support future reasoning or acting. As shown in Figure 1,there could be various types of useful thoughts, e.g. decomposing task goals and create action plans(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\nHowever, as the language space $\\mathcal{L}$  is unlimited, learning in this augmented action space is diffculti and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context examples to generate both domain-specifc actions and free-form language thoughts for task solvingi(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need toappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself.\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators just type down their thoughts in language on top of their actions taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail prompt design for each task in Sections 3 and 4. B) General and fexible: Due to the fexible thoughtll space and thought-action occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but not limited to QA, fact verifcation, text game, and webi navigation. C) Performant and robust: ReAct shows strong generalization to new task instances while learning solely from one to six in-context examples, consistently outperforming baselines with only reasoning or acting across different domains. We also show in Section 3 additional beneftsi when fnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.i D) Human aligned and controllable: ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,humans can also control or correct the agent behavior on the go by thought editing, as shown in Figure 5 in Section 4.\n3KNOWLEDGE-INTENSIVE REASONING TASKS\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact verifcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able toi retrieve information to support reasoning, while also use reasoning to target what to retrieve next,demonstrating a synergy of reasoning and acting.\n3.1SETUP\nDomainsWe consider two datasets challenging knowledge retrieval and reasoning: (1) HotPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact verifcationi benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only setup for both tasks, where models only receive the question/claim as input without access to support paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with an external environment to support reasoning.\nAction SpaceWe design a simple Wikipedia web API with three types of actions to support interactive information retrieval: (1) search[entity], which returns the frst 5 sentences fromi the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page containing string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which would fnish the current task with answer. We note that this action space mostly can only retrieve ai small part of a passage based on exact passage name, which is signifcantly weaker than state-of-the-i art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,and force models to retrieve via explicit reasoning in language.\n3.2METHODS\nReAct PromptingFor HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training set and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense thought), where free-form thoughts are used for various purposes. Specifcally, we use a combinationi of thoughts that decompose questions (“I need to search $\\mathbf{X},$  fndi $y,$  then fndi $z^{3}$ ), extract information from Wikipedia observations ( $``{x}$ was started in $1844\",$  “The paragraph does not tell $\\mathbf{x}\"$ ), perform commonsense ( $``x$  is not y, so z must instead be...”) or arithmetic reasoning $\\left(``1844<1989\"\\right) ,$  guide\n\n\n\n<html><body><table><tr><td>Prompt Methoda</td><td>HotpotQA (EM)</td><td>Fever (Acc)</td></tr><tr><td>Standard</td><td>28.7</td><td>57.1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>29.4</td><td>56.3</td></tr><tr><td>CoT-SC (Wang et al., 2022a)</td><td>33.4</td><td>60.4</td></tr><tr><td>Act</td><td>25.7</td><td>58.9</td></tr><tr><td>ReAct</td><td>27.4</td><td>60.9</td></tr><tr><td>CoT-SC ￫ReAct ReAct￫CoT-SC</td><td>34.2</td><td>64.6</td></tr><tr><td></td><td>35.1</td><td>62.0</td></tr><tr><td>Supervised SoTAb</td><td>67.5</td><td>89.5</td></tr></table></body></html>\n\nTable 1: PaLM-540B prompting results on HotpotQA and Fever.\nFigure 2: PaLM-540B prompting results with respect to number of CoT-SC samples used.\na CoT-SC in Wang et al. (2022b).28.9, 33.8 for Standard, CoT,\nb(Zhu et al., 2021; Lewis et al., 2020)\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the fnal answer (“...soi the answer is $\\mathbf{x}^{\\prime\\prime}$ ). See Appendix C for more details.\nBaselinesWe systematically ablate ReAct trajectories to build prompts for multiple baselines (with formats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,2022), which removes actions and observations and serve as a reasoning-only baseline. We also build a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories with decoding temperature 0.7 during inference and adopting the majority answer, which is found to consistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the Internet to answer questions, though it operates on a different task and action space, and uses imitation and reinforcement learning instead of prompting.\nCombining Internal and External KnowledgeAs will be detail in Section 3.3, we observe that the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts or thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide when to switch to the other method based on the following heuristics: A) ReAct $\\rightarrow$ CoT-SC: when ReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for HotpotQA and FEVER respectively as we fnd more steps will not improve ReAct performance3.i B) CoT-SC ￫ReAct: when the majority answer among n CoT-SC samples occurs less than $n/2$ times (i.e. internal knowledge might not support the task confdently), back off to ReAct.i\nFinetuningDue to the challenge of manually annotating reasoning traces and actions at scale,we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories with correct answers generated by ReAct (also for other baselines) to fnetune smaller languagei models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on input questions/claims. More details are in Appendix B.1.\n3.3RESULTS AND OBSERVATIONS\nReAct outperforms Act consistentlyTable 1 shows HotpotQA and Fever results using PaLM540B as the base model with different prompting methods. We note that ReAct is better than Act on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the fnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also confrm the beneft of reasoningiii traces for more informed acting.\n\n\n\n<html><body><table><thead><tr><td></td><td>Type</td><td>Defnition</td><td>ReAct</td><td>CoT</td></tr></thead><tbody><tr><td rowspan=\"2\">Success</td><td>True positive</td><td>Correct reasoning trace and facts</td><td> $94\\%$ </td><td>86%</td></tr><tr><td>False positive</td><td>Hallucinated reasoning trace or facts</td><td>6%</td><td>14%</td></tr><tr><td rowspan=\"4\">Failure</td><td>Reasoning error</td><td>Wrong reasoning trace (including failing to recover from repetitive steps)</td><td>47%</td><td>16%</td></tr><tr><td>Search result error</td><td>Search return empty or does not contain useful information</td><td> $23\\%$ </td><td>-</td></tr><tr><td>Hallucination</td><td>Hallucinated reasoning trace or facts</td><td> $0\\%$ </td><td>56%</td></tr><tr><td>Label ambiguity</td><td>Right prediction but did not match the label precisely</td><td> $29\\%$ </td><td> $28\\%$ </td></tr></tbody></table></body></html>\n\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their percentages in randomly selected examples studied by human.\nReAct vs. CoTOn the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct and CoT respectively (thus 200 examples in total), and manually labeled their success and failure modes in Table 2. Some key observations are as follows:\nA) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than ReAct ( $14\\%$  vs. $6\\%$  in success mode, and make up its major failure mode $(56\\%)$ . In contrast, the problem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the access of an external knowledge base.\nB) While interleaving reasoning, action and observation steps improves ReAct’s groundedness and trustworthiness, such a structural constraint also reduces its fexibility in formulatingl reasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent error pattern specifc to ReAct, in which the model repetitively generates the previous thoughts andi actions, and we categorize it as part of “reasoning error” as the model fails to reason about what the proper next action to take and jump out of the loop4.\nC) For ReAct, successfully retrieving informative knowledge via search is critical. Noninformative search, which counts for $23\\%$  of the error cases, derails the model reasoning and gives it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between factuality and fexibility, which motivates our proposed strategies of combining two methods.l\nWe provide examples for each success and failure modes in Appendix E.1. We also fnd somei HotpotQA questions may contain outdated answer labels, see Figure 4 for example.\nReAct + CoT-SC perform best for prompting LLMsAlso shown in Table 1, the best prompting method on HotpotQA and Fever are ReAct $\\rightarrow$ CoT-SC and $\\mathtt{CoT-SC}\\to\\mathtt{ReAct}$  respectively.Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both signifcantly and consistently outperform CoT-SC across different number of samples, reachingi CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of properly combining model internal knowledge and external knowledge for reasoning tasks.\nReAct performs best for fne-tuningFigure 3 shows the scaling effect of prompting/fnetuningii four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct performs worst among four methods due to the diffculty to learn both reasoning and acting fromi in-context examples. However, when fnetuned with just 3,000 examples, ReAct becomes the besti method among the four, with PaLM-8B fnetuned ReAct outperforming all PaLM-62B promptingi methods, and PaLM-62B fnetuned ReAct outperforming all 540B prompting methods. In contrast,i fnetuning Standard or CoT is signifcantly worse than fnetuning ReAct or Act for both PaLM-iii 8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a more generalizable skill for knowledge reasoning. As all prompting methods are still signifcantlyi far from domain-specifc state-of-the-art approaches (Table 1), we believe fnetuning with moreii human-written data might be a better way to unleash the power of ReAct.\n\nre 3: Scaling results for prompting and fnetuning on HotPotQA with ReAct (ours) and baselinei\n4DECISION MAKING TASKS\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and WebShop, both of which feature complex environments that require agents to act over long horizons with sparse rewards, warranting the need for reasoning to act and explore effectively.\nALFWorldALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of tasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by navigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take paper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy more than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore systematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into ALFWorld is the need to determine likely locations for common household items (e.g. desklamps will likely be on desks, shelfs, or dressers), making this environment a good ft for LLMs to exploit theiri pretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories from the training set for each task type, where each trajectory includes sparse thoughts that (1)decompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via commonsense where to fnd an object and what to do with it. We show prompts used for ALFWorldi in Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games in a task-specifc setup. For robustness, we construct 6 prompts for each task type through eachi permutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using the same trajectories, but without thoughts — since task instances are randomly chosen from the training set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the importance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation learning agent trained on $10^{5}$ expert trajectories for each task type5.\nWebShopCan ReAct also interact with noisy real-world language environments for practical applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping website environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,and options crawled from Amazon), and requires an agent to purchase a product based on a user instruction (e.g. “I am looking for a nightstand with drawers. It should have a nickel fnish, andi priced lower than $\\$140\")$  through web interactions (e.g. search “nightstand drawers”, choose buttons such as “color: modern-nickel-white” or “back to search”). This task is evaluated by average score(percentage of desired attributes covered by the chosen product averaged across all episodes) and success rate (percentage of episodes where the chosen product satisfes all requirements) on 500 testi instructions. We formulate Act prompts with actions to search, choose product, choose options,and buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,and what products options are relevant to the instruction. See Table 6 for an example prompt, and Table 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\n\n\n\n<html><body><table><thead><tr><td>Method</td><td>Pick</td><td>Clean</td><td>Heat</td><td>Cool</td><td>Look</td><td>Pick 2</td><td>All</td></tr></thead><tbody><tr><td>Act (best of 6)</td><td>88</td><td>42</td><td>74</td><td>67</td><td>72</td><td>41</td><td>45</td></tr><tr><td>ReAct (avg)</td><td>65</td><td>39</td><td>83</td><td>76</td><td>55</td><td>24</td><td>57</td></tr><tr><td>ReAct (best of 6)</td><td>92</td><td>58</td><td>96</td><td>86</td><td>78</td><td>41</td><td>71</td></tr><tr><td>ReAct-IM (avg)</td><td>55</td><td>59</td><td>60</td><td>55</td><td>23</td><td>24</td><td>48</td></tr><tr><td>ReAct-IM (best of 6)</td><td>62</td><td>68</td><td>87</td><td>57</td><td>39</td><td>33</td><td>53</td></tr><tr><td>BUTLERg (best of 8)</td><td>33</td><td>26</td><td>70</td><td>76</td><td>17</td><td>12</td><td>2327</td></tr><tr><td>BUTLER (best of 8)</td><td>46</td><td>39</td><td>74</td><td>100</td><td>22</td><td>24</td><td></td></tr></tbody></table></body></html>\n\nTable 3: AlfWorld task-specifc success rates (%). BUTLER andi BUTLERg results are from Table 4 of Shridhar et al. (2020b). All methods use greedy decoding, except that BUTLER uses beam search.\n\n\n<html><body><table><tr><td>Method</td><td>Score</td><td>SR</td></tr><tr><td>Act ReAct</td><td>62.3 66.6</td><td>30.1 40.0</td></tr><tr><td>IL</td><td>59.9</td><td>29.1</td></tr><tr><td>IL+RL</td><td>62.4</td><td>28.7</td></tr><tr><td>Human Expert</td><td>82.1</td><td>59.6</td></tr></table></body></html>\n\nTable 4: Score and success rate (SR) on Webshop. IL/IL+RL taken from Yao et al. (2022).\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)method additionally trained with 10,587 training instructions.\nResultsReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On ALFWorld, the best ReAct trial achieves an average success rate of $71\\%,$  signifcantly outperformingi the best Act $(45\\%)$  and BUTLER $(37\\%)$  trials. In fact, even the worse ReAct trial $$  beats the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across six controlled trials, with relative performance gain ranging from $33\\%$ to $90\\%$  and averaging $62\\%.$ Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals into smaller subgoals, or loses track of the current state of the environment. Example trajectories comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With additional sparse reasoning, ReAct achieves signifcantly better performance, with an absolutei $10\\%$ improvement over the previous best success rate. By checking examples, we fnd that ReAct is morei likely to identify instruction-relevant products and options by reasoning to bridge the gap between noisy observations and actions (e.g. “For ‘space-saving ottoman bench for living room’, the item has options ‘39x18x18inch’ and ‘blue’ and seems good to buy.”). However, existing methods are still far from the performance of expert humans (Table 4), who perform signifcantly more producti explorations and query re-formulations that are still challenging for prompting-based methods.\nOn the value of internal reasoning vs. external feedbackTo our knowledge, ReAct is the frsti demonstration of combined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”. However, IM’s “inner monologue” is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisfed. In contrast, thei reasoning traces in ReAct for decision making is fexible and sparse, allowing diverse reasoningl types (see Section 2) to be induced for different tasks.\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on fve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakesi in identifying when subgoals were fnished, or what the next subgoal should be, due to a lack of high-i level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example trajectory in Appendix D.2.3.\n\n5RELATED WORK\nLanguage model for reasoningPerhaps the most well-known work of using LLMs for reasoning is Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their own “thinking procedure” for problem solving. Several follow-up works have since been performed,including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shotCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,(Madaan $\\&$  Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other work has also been extended to more sophisticated reasoning architecture beyond simple prompting.For example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps of “selection” and “inference”. STaR (Zelikman et al., 2022) bootstraps the reasoning process by fnetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswelli& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated LM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which fnetunes a LM oni intermediate computation steps, also demonstrate improvement on multi-step computation problems.In contrast to these methods, ReAct performs more than just isolated, fxed reasoning, and integratesi model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\nLanguage model for decision makingThe strong capability of LLMs has enabled them to perform tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a policy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for reinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)and Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl et al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly consider the reasoning procedure either, and also relies on expensive datasets and human feedback collections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the decision making process only requires language description of the reasoning procedure.6\nLLMS have also been increasingly employed in interactive and embodied environments for planning and decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which is then reranked by an affordance model grounded on the visual environments for fnal prediction.i Inner Monologue made further improvements by adding the eponymous “inner monologue\", which is implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the frst work that demonstrates such a closed-loop system, which ReAct builds on. However, we arguei that Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4. We also note that leveraging language as semantically-rich inputs in the process of interactive decision making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti et al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and decision making. What is more, progress in LLMs has also inspired the development of versatile and generalist agents like Reed et al. (2022).\n6CONCLUSION\nWe have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experiments on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. Despite the simplicity of our method, complex tasks with large action spaces require more demonstrations to learn well, which unfortunately can easily go beyond the input length limit of in-context learning. We explore the fne-tuning approach on HotpotQAiwith initial promising results, but learning from more high-quality human annotations will be the desiderata to further improve the performance. Scaling up ReAct with multi-task training and combining it with complementary paradigms like reinforcement learning could result in stronger agents that further unlock the potential of LLMs for more applications.\nACKNOWLEDGMENTS\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP Group. This work was supported in part by the National Science Foundation under Grant No.2107048. Any opinions, fndings, and conclusions or recommendations expressed in this material arei those of the author(s) and do not necessarily refect the views of the National Science Foundation.l\nREPRODUCIBILITY STATEMENT\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible model yet. To increase reproducibility, we have included all used prompts in Appendix C, additional experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct prompting code at https://anonymous.4open.science/r/ReAct-2268/.\nETHICS STATEMENT\nReAct prompts large language models to generate more human interpretable, diagnosable, and controllable task-solving trajectories than previous methods. However, hooking up a large language model with an action space to interact with external environments (e.g. the web, physical environments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful actions in an environment. Our experiments minimize such risks by limiting the interactions to specifc websites (Wikipedia or WebShop) that are free of private information, without any dangerousi actions in the action space design (i.e. models cannot really buy products on WebShop the research benchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing more extensive experiments in the future.\nREFERENCES\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathew son, Sonˇa Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,2020. URL https://arxiv.org/abs/2012.05672.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/2204.01691.\nBen Alderson-Day and Charles Fernyhough.Inner speech: development, cognitive functions,phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.\nAlan Baddeley. Working memory. Science, 255(5044):556–559, 1992.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL https://arxiv.org/abs/2208.14271.\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/2205.09712.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/P19-1346.\nCharles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social interaction and the development of social understanding and executive functions, pp. 56–79, 2010.\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,Sonˇa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving.Improving alignment of dialogue agents via targeted human judgements,2022.URL https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems,33:20179–20191, 2020.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b.\nSiddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed latent actions. In CoRL, pp. 1379–1390, 2021. URL https://proceedings.mlr.press/v164/karamcheti22a.html.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering.arXiv preprint arXiv:2203.05115, 2022.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https://arxiv.org/abs/2202.01771.\n\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsychologia, 3(4):387–392, 1965.\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022. URL https://arxiv.org/abs/2209.07686.\nVincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint arXiv:2104.07972, 2021.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:Browser-assisted question-answering with human feedback, 2021. URL https://arxiv.org/abs/2112.09332.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https://arxiv.org/abs/2205.06175.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740–10749, 2020a.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston.Language models that seek for knowledge: Modular search $\\&$  generation for dialogue and prompt completion. arXiv preprint arXiv:2203.13224, 2022a.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3:a deployed conversational agent that continually learns to responsibly engage, 2022b. URL https://arxiv.org/abs/2208.03188.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verifcation. arXiv preprint arXiv:1803.05355, 2018.i\nLev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39–285, 1987.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,2022a. URL https://arxiv.org/abs/2203.11171.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.\n\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.URL https://aclanthology.org/2020.emnlp-main.704.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206,2022.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2022. URL https://arxiv.org/abs/2205.10625.\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information seeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.\n\nAADDITIONAL RESULTS\nA.1GPT-3 EXPERIMENTS\n\n\n<html><body><table><tr><td></td><td>PaLM-540B</td><td>GPT-3</td></tr><tr><td>HotpotQA (exact match)</td><td>29.4</td><td>30.8</td></tr><tr><td>ALFWorld (success rate %)</td><td>70.9</td><td>78.4</td></tr></table></body></html>\n\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).On HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all 134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\nWe run additional GPT-3 (Brown et al., 2020) experiments to confrm ReAct prompting performancei is general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly because it is fnetuned with human instruction following. This indicates ReAct prompting is effectivei across different large language models on different tasks. The code for these experiments are at https://react-lm.github.io/.\nA.2REACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA\n\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is able to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\nDuring trajectory inspection, we also fnd that sometimes ReAct does not agree with dataset labels asi the labels themselves could be outdated. For example, as shown in Figure 4, the question asks about the size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT give wrong answers due to hallucination, Act fails despite the access of real-world web interaction,due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to retrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better incorporation of reasoning abilities might beneft recent Internet-augmented language models (Nakanoi et al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\nA.3HUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD\nWe also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit ReAct’s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act 17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align with these human thought edits and succeed in the task. From a human perspective, solving such a task becomes signifcantly easier, from typing tens of actions to only editing a couple of thoughts,i which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\n\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct trajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\nis diffcult for Act and previous RL methods, as a human cannot change the model parameters, andi changing a few actions might not edit the rest of the model behavior. This paradigm is also more than human dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct thoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the fexible thought space supports, for better task solving. We believe this is an exciting direction forl human alignment and leave more systematic study as future work.\nBEXPERIMENT DETAILS\nB.1HOTPOTQA FINETUNING DETAILS\nFor all fnetuning we use a batch size of 64. On PaLM-8B, we fnetune ReAct and Act methodsii for 4, 000 steps and Standard and CoT methods for 2, 000 steps. On PaLM-62B, we fnetunei ReAct and Act methods for 4, 000 steps and Standard and CoT methods for 1, 000 steps. We fnd ReAct and Act methods generally beneft from more training steps (and more training data),ii while Standard and CoT methods degrade soon after fnetuning.i\nB.2ALFWORLD IM-STYLE DETAILS\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense external feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)decomposing the current goal and (2) the current subgoal that needs to be completed. In particular,ReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the next subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify where items can be within the environment.\n\nCPROMPTS\nC.1HOTPOTQA\nHotpot QA Prompts\n\n\n<html><body><table><thead><tr><td></td><td></td></tr></thead><tbody><tr><td>Original Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Answer</td><td>1,800 to 7,000 ft</td></tr><tr><td>Question</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"</td></tr><tr><td>Answer</td><td>character Milhouse, who Matt Groening named after who? Richard Nixon</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The</td></tr><tr><td></td><td>Saimaa Gesture? The Saimaa Gesture</td></tr><tr><td>Answer</td><td></td></tr><tr><td>Question Answer</td><td>What profession does Nicholas Ray and Elia Kazan have in common? director, screenwriter, actor</td></tr><tr><td></td><td></td></tr><tr><td>Question Answer</td><td>Which magazine was started first Arthur’s Magazine or First for Women? Arthur’s Magazine</td></tr><tr><td></td><td></td></tr><tr><td>Question Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Yes</td></tr><tr><td>Act</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the</td></tr><tr><td>Action 1</td><td>Colorado orogeny extends into? Search[Colorado orogeny]</td></tr><tr><td>Observation 1</td><td>The Colorado orogeny was an episode of mountain building (an orogeny) in</td></tr><tr><td></td><td>Colorado and surrounding areas.</td></tr><tr><td>Action 2</td><td>Lookup[eastern sector] (Result 1 / 1) The eastern sector extends into the High Plains and is called</td></tr><tr><td>Observation 2</td><td>the Central Plains orogeny.</td></tr><tr><td>Action 3</td><td>Search[High Plains]</td></tr><tr><td>Observation 3</td><td>High Plains refers to one of two distinct land regions: Search[High Plains (United States)]</td></tr><tr><td>Action 4 Observation 4</td><td>The High Plains are a subregion of the Great Plains.From east to west, the</td></tr><tr><td></td><td>High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]</td></tr><tr><td>Action 5</td><td>Finish[1,800 to 7,000 ft]</td></tr><tr><td>Question</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?</td></tr><tr><td>Action 1</td><td>Search[Milhouse] Milhouse Mussolini Van Houten is a recurring character in the Fox animated</td></tr><tr><td>Observation 1</td><td>television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.</td></tr><tr><td>Action 2</td><td>Lookup[named after]</td></tr><tr><td>Observation 2</td><td>(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.</td></tr><tr><td>Action 3</td><td>Finish[Richard Nixon]</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The</td></tr><tr><td></td><td>Saimaa Gesture? Search[Adam Clayton Powell]</td></tr><tr><td>Action 1 Observation 1</td><td>Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office</td></tr><tr><td></td><td>Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell</td></tr><tr><td></td><td>(film)’, ’Giancarlo Esposito’]. Search[Adam Clayton Powell (film)]</td></tr><tr><td>Action 2 Observation 2</td><td>Adam Clayton Powell is a 1989 American documentary film directed by</td></tr><tr><td></td><td>Richard Kilberg.The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired</td></tr><tr><td></td><td>as part of the PBS series The American Experience.</td></tr><tr><td>Action 3</td><td>Finish[The Saimaa Gesture]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Question</td><td>What profession does Nicholas Ray and Elia Kazan have in common?</td></tr><tr><td></td><td></td></tr><tr><td>Action 1</td><td>Search[Nicholas Ray]</td></tr><tr><td></td><td></td></tr><tr><td>Observation 1</td><td></td></tr><tr><td></td><td>Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>1979) was an American film director, screenwriter, and actor best known for</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 2</td><td>the 1955 film Rebel Without a Cause.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Search[Elia Kazan]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Observation 2</td><td>Elia Kazan was an American film and theatre director, producer, screenwriter</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>and actor. C o n t i n u e d  o n  n e x t  p a g e</td></tr></table></body></html>\n\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Hotpot QA Prompts – Continued from previous page</td></tr></thead><tbody><tr><td colspan=\"2\"></td></tr><tr><td>Action 3 Question</td><td>Finish[director, screenwriter, actor] Which magazine was started first Arthur’s Magazine or First for Women?</td></tr><tr><td>Action 1</td><td>Search[Arthur’s Magazine]</td></tr><tr><td>Observation 1</td><td>Arthur’s Magazine (1844-1846) was an American literary periodical published</td></tr><tr><td></td><td>in Philadelphia in the 19th century. Search[First for Women]</td></tr><tr><td>Action 2 Observation 2</td><td>First for Women is a woman’s magazine published by Bauer Media Group in the</td></tr><tr><td></td><td>USA.[1] The magazine was started in 1989. Finish[Arthur’s Magazine]</td></tr><tr><td>Action 3</td><td></td></tr><tr><td>Question</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Search[Pavel Urysohn]</td></tr><tr><td>Action 1 Observation 1</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet</td></tr><tr><td></td><td>mathematician who is best known for his contributions in dimension theory.</td></tr><tr><td>Action 2</td><td>Search[Leonid Levin]</td></tr><tr><td>Observation 2</td><td>Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.</td></tr><tr><td>Action 3</td><td>Finish[yes]</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Thought</td><td>Let’s think step by step.The eastern sector of Colorado orogeny extends into the High Plains. High Plains rise in elevation from around 1,800 to</td></tr><tr><td></td><td>7,000 ft, so the answer is 1,800 to 7,000 ft. 1,800 to 7,000 ft</td></tr><tr><td>Answer</td><td>Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"</td></tr><tr><td>Question</td><td>character Milhouse, who Matt Groening named after who?</td></tr><tr><td>Thought</td><td>Let’s think step by step.Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.</td></tr><tr><td>Answer</td><td>Richard Nixon</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td></tr><tr><td>Thought</td><td>Let’s think step by step. Adam Clayton Powell (film) is a documentary about</td></tr><tr><td></td><td>an African-American politician, not Finnish rock groups.So the documentary about Finnish rock groups must instead be The Saimaa Gesture.</td></tr><tr><td>Answer</td><td>The Saimaa Gesture</td></tr><tr><td>Question Thought</td><td>What profession does Nicholas Ray and Elia Kazan have in common? Let’s think step by step.Professions of Nicholas Ray are director, Professions of Elia Kazan are director, producer,</td></tr><tr><td></td><td>screenwriter, and actor. screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.</td></tr><tr><td>Answer</td><td>director, screenwriter, actor</td></tr><tr><td>Question</td><td>Which magazine was started first Arthur’s Magazine or First for Women? Let’s think step by step.Arthur’s Magazine was started in 1844.First</td></tr><tr><td>Thought</td><td>for Women was started in 1989.1844 (Arthur’s Magazine) < 1989 (First for</td></tr><tr><td></td><td>Women), so Arthur’s Magazine was started first. Arthur’s Magazine</td></tr><tr><td>Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work?</td></tr><tr><td>Question Thought</td><td>Let’s think step by step.Pavel Urysohn is a mathematician.Leonid Levin</td></tr><tr><td></td><td>is a mathematician and computer scientist.So Pavel Urysohn and Leonid Levin have the same type of work.</td></tr><tr><td>Answer</td><td>Yes</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Thought 1</td><td>I need to search Colorado orogeny, find the area that the eastern sector</td></tr><tr><td></td><td>of the Colorado orogeny extends into, then find the elevation range of the</td></tr><tr><td></td><td>area. Search[Colorado orogeny]</td></tr><tr><td>Action 1 Observation 1</td><td>The Colorado orogeny was an episode of mountain building (an orogeny) in</td></tr><tr><td></td><td>Colorado and surrounding areas. It does not mention the eastern sector.</td></tr><tr><td>Thought 2</td><td>So I need to look up eastern sector.</td></tr><tr><td>Action 2</td><td>Lookup[eastern sector]</td></tr><tr><td>Observation 2</td><td>(Result 1 / 1) The eastern sector extends into the High Plains and is called</td></tr><tr><td></td><td>the Central Plains orogeny.</td></tr><tr><td></td><td></td></tr><tr><td>Thought 3</td><td>The eastern sector of Colorado orogeny extends into the High Plains.So I</td></tr><tr><td></td><td></td></tr><tr><td></td><td>need to search High Plains and find its elevation range.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 3</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Search[High Plains]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Observation 3 Thought 4 Action 4</td><td>High Plains refers to one of two distinct land regions I need to instead search High Plains (United States). Search[High Plains (United States)]</td></tr></table></body></html>\n\n\n\nObservation 4The High Plains are a subregion of the Great Plains.From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\nThought 5High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\nAction 5Finish[1,800 to 7,000 ft]\nQuestionMusician and satirist Allie Goertz wrote a song about the \"The Simps character Milhouse, who Matt Groening named after who?\nThought 1The question simplifies to \"The Simpsons\" character Milhouse is named after who.I only need to search Milhouse and find who it is named after.\nObservation 1Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\nThought 2The paragraph does not tell who Milhouse is named after, maybe I can look up\"named after\".\nObservation 2(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\nThought 3Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\nstionWhich documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\nught 1I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.\nObservation 1Could not find [Adam Clayton Powell].Similar:[’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr.State Office Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell(film)’, ’Giancarlo Esposito’].\nAction 2Search[Adam Clayton Powell (film)]\nObservation 2Adam Clayton Powell is a 1989 American documentary film directed by\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\nThought 3Adam Clayton Powell (film) is a documentary about an African-American\npolitician, not Finnish rock groups.So the documentary about Finnish rock groups must instead be The Saimaa Gesture.Action 3Finish[The Saimaa Gesture]\nQuestionWhat profession does Nicholas Ray and Elia Kazan have in common?\nught 1I need to search Nicholas Ray and Elia Kazan, find their professions, then\nAction 1Search[Nicholas Ray]\nObservation 1Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16 1979) was an American film director, screenwriter, and actor best known f the 1955 film Rebel Without a Cause.\nThought 2Professions of Nicholas Ray are director, screenwriter, and actor.I nee to search Elia Kazan next and find his professions.\nThought 3Professions of Elia Kazan are director, producer, screenwriter, and actor\nAction 3Finish[director, screenwriter, actor]\nQuestionWere Pavel Urysohn and Leonid Levin known for the same type of work?\n\nHotpot QA Prompts – Continued from previous page\n\n\n<html><body><table><thead><tr><td>Observation 1</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet</td><td></td><td></td></tr></thead><tbody><tr><td>Thought 2</td><td>mathematician who is best known for his contributions in dimension theory. Pavel Urysohn is a mathematician.I need to search Leonid Levin next and</td><td></td><td></td></tr><tr><td></td><td>find its type of work.</td><td></td><td></td></tr><tr><td>Action 2 Observation 2</td><td>Search[Leonid Levin] Leonid Anatolievich Levin is a Soviet-American mathematician and computer</td><td></td><td></td></tr><tr><td></td><td>scientist.</td><td></td><td></td></tr><tr><td>Thought 3</td><td>and Leonid Levin have the same type of work.</td><td></td><td>Leonid Levin is a mathematician and computer scientist.So Pavel Urysohn</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Action 3</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Finish[yes]</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr></table></body></html>\n\n\nC.2FEVER\n\n\n<html><body><table><thead><tr><td colspan=\"2\">FEVER Prompts</td></tr></thead><tbody><tr><td>Original</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. SUPPORTS</td></tr><tr><td>Answer</td><td></td></tr><tr><td>Claim Answer</td><td>Stranger Things is set in Bloomington, Indiana. REFUTES</td></tr><tr><td>Claim</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Answer Act</td><td>NOT ENOUGH INFO Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if</td></tr><tr><td>Claim Action2 Observation 1</td><td>there is NOT ENOUGH INFORMATION. Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer.He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and</td></tr><tr><td></td><td>Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot. Finish[SUPPORTS]</td></tr><tr><td>Action 2 Claim</td><td>Stranger Things is set in Bloomington, Indiana. Search[Stranger Things]</td></tr><tr><td>Action 1 Observation 1</td><td>Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers.Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters.</td></tr><tr><td>Action 2</td><td>Finish[REFUTES]</td></tr><tr><td>Claim Action 1</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. Search[Beautiful] Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’,</td></tr><tr><td>Observation 1</td><td>’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is Beautiful’]. Search[Beautiful (Christina Aguilera song)</td></tr><tr><td>Action 2 Observation 2</td><td>\"Beautiful\" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).</td></tr><tr><td>Action 3</td><td>Lookup[Billboard Hot 100] (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the</td></tr><tr><td>Observation 3</td><td>United States, where it was certified Gold for 500,000 units shipped. Finish[NOT ENOUGH INFO]</td></tr><tr><td>Action 4</td><td></td></tr><tr><td>CoT</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.</td></tr><tr><td>Claim</td><td></td></tr><tr><td></td><td>Nikolaj William Coster-Waldau appeared in the 2009 Fox television film</td></tr><tr><td>Thought</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Virtuality, so he has worked with the Fox Broadcasting Company.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>SUPPORTS</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Answer</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Claim</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Stranger Things is set in Bloomington, Indiana.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Thought</td><td>Stranger Things is in the fictional town of Hawkins, Indiana, not in</td></tr></tbody></table></body></html>\n\n\nFever Prompts – Continued from previous page\n\n\n<html><body><table><thead><tr><td></td><td></td></tr></thead><tbody><tr><td>Claim Thought</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. The song peaked at number two on the Billboard Hot 100 in the United States, but not sure if it was in 2003.</td></tr><tr><td>Answer</td><td>NOT ENOUGH INFO</td></tr><tr><td>ReAct</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. I need to search Nikolaj Coster-Waldau and find if he has worked with the</td></tr><tr><td>Thought 1 Action 1 Observation 1</td><td>Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994).He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy</td></tr><tr><td></td><td>Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013).In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as</td></tr><tr><td>Thought 2</td><td>a pilot. Because he \"appeared in the 2009 Fox television film Virtuality\", he should</td></tr><tr><td>Action 2</td><td>have worked with the Fox Broadcasting Company. Finish[SUPPORTS]</td></tr><tr><td>Claim</td><td>Stranger Things is set in Bloomington, Indiana. I should search for Stranger Things, and see if it is set in Bloomington,</td></tr><tr><td>Thought 1 Action 1</td><td>Indiana. Search[Stranger Things] Stranger Things is an American science fiction horror drama television</td></tr><tr><td>Observation 1</td><td>series created by the Duffer Brothers.Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact</td></tr><tr><td></td><td>on an ensemble of child and adult characters. The observation says that it is set in a \"fictional town of Hawkins,</td></tr><tr><td>Thought 2</td><td>Indiana\", so it is not set in Bloomington. Finish[REFUTES]</td></tr><tr><td>Action 2</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Claim</td><td>I need to search the song Beautiful and find if it reached number two on the</td></tr><tr><td>Thought 1</td><td>Billboard Hot 100 in 2003. Search[Beautiful]</td></tr><tr><td>Action 1 Observation 1</td><td>Could not find [Beautiful].Similar:[’Beautiful’, ’Beautiful, Beautiful’, ’A Beautiful Mind (film)’, ’Beautiful (Christina Aguilera song)’, ’Life Is</td></tr><tr><td></td><td>Beautiful’].</td></tr><tr><td>Thought 2</td><td>From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to find the song.</td></tr><tr><td>Action 2</td><td>Search[Beautiful (Christina Aguilera song)] \"Beautiful\" is a song recorded by American singer Christina Aguilera for her</td></tr><tr><td>Observation 2</td><td>fourth studio album, Stripped (2002).</td></tr><tr><td>Thought 3</td><td>It does not mention Billboard, so I need to look up \"Billboard Hot  ${}^{100}\"$ to</td></tr><tr><td></td><td>find if it reached number two on it in 2003.</td></tr><tr><td>Action 3</td><td>Lookup[Billboard Hot 100]</td></tr><tr><td>Observation 3</td><td>(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the</td></tr><tr><td></td><td>United States, where it was certified Gold for 500,000 units shipped.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>It only says the song peaked at number two on the Billboard Hot 100, but not</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Thought 4</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>if it was in 2003.I am not sure if this claim is true or not.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>Action 4</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td></table></body></html>\n\n\nC.3WEBSHOP\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for Act, and use the prompt on the right (add reasoning) for ReAct.\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Instruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars</td></tr><tr><td>Act</td><td>ReAct</td></tr></thead><tbody><tr><td>Aetion: search [3 ounce bright citrus deodorant sensitive skin]</td><td>Action: search [3 ounce bright citrus deodorant sensitive skin</td></tr><tr><td>Observation:</td><td>Observation:</td></tr><tr><td>[ Back to Search Page 1 (Total results: 50)</td><td>Back to Search Page 1 (Total results: 50)</td></tr><tr><td>[Next]</td><td>[Next]</td></tr><tr><td>[B078GWRC1J]</td><td>[B078GWRC1J]</td></tr><tr><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td></tr><tr><td>Calendula 3-Ounce $10.99</td><td>Calendula 3-Ounce $10.99</td></tr><tr><td>B078GTKVXY Ginger Fresh Deodorant by Earth Mama | Natural and Safe for</td><td>[B078GTKVXY] Ginger Fresh Deodorant by Earth Mama | Natural and Safe for</td></tr><tr><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td></tr><tr><td>$10.99 [B08KBVJ4XN]</td><td>$10.99 [B08KBVJ4XN]</td></tr><tr><td>Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar</td><td>Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar</td></tr><tr><td>& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)</td><td>& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)</td></tr><tr><td>$15.95</td><td>$15.95</td></tr><tr><td>Action: click[B078GWRC1J]</td><td>Action: think[B078GWRC1J and B078GTKVXY are bright citrus deodorant less then 50 dollars. I can check B078GWRC1J frst.</td></tr><tr><td>Observation: [Back to Search]</td><td>Observation: OK.</td></tr><tr><td>[Prev]</td><td></td></tr><tr><td>scent [assorted scents][bright citrus][calming lavender][ginger</td><td>Action: click[B078GWRC1J]</td></tr><tr><td>[fresh] simply non-scents]</td><td>Observation:</td></tr><tr><td>size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)]</td><td>[Back to Search] [Prev]</td></tr><tr><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for</td><td>scent [assorted scents][bright citrus][calming lavender][ginger</td></tr><tr><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td><td>[fresh] simply non-scents]</td></tr><tr><td>Calendula 3-Ounce Price: $10.99</td><td>size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)]</td></tr><tr><td>Rating: N.A.</td><td>Bright Citrus Deodorant by Earth Mama | Natural and Safe for</td></tr><tr><td>[Description]</td><td>Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic</td></tr><tr><td>[Features]</td><td>Calendula 3-Ounce</td></tr><tr><td>[Reviews]</td><td>Price: $10.99</td></tr><tr><td>[Buy Now]</td><td>Rating: N.A.</td></tr><tr><td></td><td>[Description]</td></tr><tr><td>Action: click[bright citrus]</td><td>[Features]</td></tr><tr><td>Observation: You have clicked bright citrus.</td><td>[Reviews]</td></tr><tr><td>Action: click[3 ounce (pack of 1)]</td><td>[Buy Now]</td></tr><tr><td>Observation: You have clicked 3 ounce (pack of 1).</td><td>Action: think[For 3 ounce bottle of bright citrus deodorant</td></tr><tr><td></td><td>for sensitive skin, the item has options ’bright citrus’</td></tr><tr><td>Action: click[Buy Now]</td><td>and ’3 ounce (pack of  $11'$  and seems good to buy.]</td></tr><tr><td></td><td>Observation: OK.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[bright citrus]</td></tr><tr><td></td><td>Observation: You have clicked bright citrus.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[3 ounce (pack of 1)]</td></tr><tr><td></td><td>Observation: You have clicked 3 ounce (pack of 1).</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Action: click[Buy Now]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></body></html>\n\n\nC.4ALFWORLD\nTable 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\n> go to fridge 1\nThe fridge 1 is closed.\n$>$ open fridge 1\nYou open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2.\n> go to diningtable 1\npeppershaker 1, a spatula 3, a tomato\n> take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n> go to sinkbasin 1\n> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> go to diningtable 1\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n> put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\n\nTable 8: An ReAct prompt on the ALFWorld clean task.\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to:put a clean lettuce in diningtable.\n> think:To solve the task, I need to find and take a lettuce, then clean it with sinkbasin, then put it in diningtable.OK.\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13).I> think:First I need to find a lettuce.A lettuce is more likely to appear in can check one by one, starting with fridge 1.\nOK.\n> go to fridge 1\nThe fridge 1 is closed.\n> open fridge 1\nYou open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2.\n> go to diningtable 1\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\n> take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n> think:Now I take a lettuce (1).Next, I need to go to sinkbasin (1) and clean it.OK.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> think:Now I clean a lettuce (1).Next, I need to put it in/on diningtable 1.\nOK.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n> put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\n\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\nYou are in the middle of a room.Looking quickly around you, you see a cabinet\nYou are in the middle of a room.13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to:put a clean lettuce in diningtable.\n> think:To solve the task, I need to find and take a lettuce, then clean it with sinkbasin, then put it in diningtable.\n> think:First I need to find a lettuce.\n> go to fridge 1\nThe fridge 1 is closed.\n> think:First I need to find a lettuce.\nOK.\n> open fridge 1\nYou open the fridge 1.The fridge 1 is open.In it, you see a cup 3, a egg 2, a potato 3, and a potato 2.\n> think:First I need to find a lettuce.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n> think:First I need to find a lettuce.\n> take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1\n> think:I need to clean this lettuce (1) using sinkbasin 1.\nOK.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n> think:I need to clean this lettuce (1) using sinkbasin 1.\n> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> think:I need to put this cleaned lettuce (1) in/on diningtable 1.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork 2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n> think:I need to put this cleaned lettuce (1) in/on diningtable 1.\n> put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\nDTRAJECTORIES\nD.1FEVER TRAJECTORIES\nWe provide some trajectories produced by ReAct, CoTand Acton several randomly selected examples from the FEVER dev set below. Search results are omitted to space.\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Example 2491 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Thought 1:</td><td>I need to search Bermuda Triangle and find if it is in the Pacific Ocean.</td></tr><tr><td>Action 1:</td><td>Search[Bermuda Triangle]</td></tr><tr><td>Observation 1:</td><td>Reportedly haunted locations...</td></tr><tr><td>Thought 2:</td><td>The observation says that it is in the western part of the North Atlantic</td></tr><tr><td>Action 2:</td><td>Ocean, so it is not in the Pacific Ocean.</td></tr><tr><td>Observation 2:</td><td>Finish[REFUTES] Episode finished</td></tr><tr><td colspan=\"2\">Act</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Action 1</td><td>:</td></tr><tr><td>Observation 1</td><td>Search[Bermuda Triangle] : Reportedly haunted locations:...</td></tr><tr><td>Action 2:</td><td></td></tr><tr><td>Observation 2:</td><td>Finish[REFUTES] Episode finished</td></tr><tr><td colspan=\"2\">CoT</td></tr><tr><td>Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Thought:</td><td>Let’s think step by step. The Bermuda Triangle is a loosely-defined region</td></tr><tr><td>Answer:</td><td>in the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean. REFUTES</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Example 5908 (gt: SUPPORTS)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Princess Mononoke is a film.</td></tr><tr><td>Thought 1:</td><td>I need to search Princess Mononoke and find if it is a film.</td></tr><tr><td>Action 1:</td><td>Search[Princess Mononoke]</td></tr><tr><td>Observation 1:</td><td>Princess Mononoke . . .</td></tr><tr><td>Thought 2:</td><td>From the observation, it says that Princess Mononoke is a film.</td></tr><tr><td>Action 2:</td><td>Finish[SUPPORTS] Episode finished</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td colspan=\"2\">Act Claim:</td></tr><tr><td></td><td>Princess Mononoke is a film.</td></tr><tr><td>Action 1:</td><td>Search[Princess Mononoke]</td></tr><tr><td>Observation 1: Action 2:</td><td>Princess Mononoke  ..</td></tr><tr><td>Observation 2:</td><td>Finish[SUPPORTS]</td></tr><tr><td></td><td>Episode finished</td></tr><tr><td colspan=\"2\">CoT Claim:</td></tr><tr><td>Thought:</td><td>Princess Mononoke is a film. Let’s think step by step.Princess Mononoke is a movie, and movies are</td></tr><tr><td>Answer:</td><td>films.</td></tr><tr><td></td><td>SUPPORTS</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Example 1951 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Soyuz was part of the American space program.</td></tr><tr><td>Thought 1:</td><td>I need to search Soyuz and find if it was part of the American space</td></tr><tr><td>Action 1:</td><td>program. Search[Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Thought 2:</td><td>It does not mention the American space program, so I need to search \"American space program\" to find if it was part of it.</td></tr><tr><td>Action 2:</td><td>Search[American space program]</td></tr><tr><td>Observation 2:</td><td>The space policy...</td></tr><tr><td>Thought 3:</td><td>It does not mention Soyuz, so I am not sure if this claim is true or not.</td></tr><tr><td>Action 3: Observation 3:</td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td></td><td>Episode finished</td></tr><tr><td colspan=\"2\">Act Claim:</td></tr><tr><td></td><td>Soyuz was part of the American space program.</td></tr><tr><td>Action 1:</td><td>Search[Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Action 2:</td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td>Observation 2:</td><td>Episode finished</td></tr><tr><td colspan=\"2\">CoT</td></tr><tr><td>Claim: Thought:</td><td>Soyuz was part of the American space program. Let’s think step by step. Soyuz is a Russian spacecraft. The American</td></tr><tr><td></td><td>space program is NASA. NASA and Russia have worked together on the</td></tr><tr><td>Answer:</td><td>International Space Station. SUPPORTS</td></tr></tbody></table></body></html>\n\n\n\n\n<html><body><table><thead><tr><td></td><td>Example 3208 (gt: REFUTES)</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Thought 1:</td><td>I need to search for Reign Over Me and find if it is an American film made</td></tr><tr><td>Action 1:</td><td>in 2010. Search[Reign Over Me]</td></tr><tr><td>Observation 1:</td><td>Reign Over Me ...</td></tr><tr><td>Thought 2:</td><td>The observation says that it is an American film made in 2007, so it is not</td></tr><tr><td>Action 2:</td><td>made in 2010. Finish[REFUTES]</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td>Act</td><td>Episode finished</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Action 1:</td><td>Search[Reign Over Me]</td></tr><tr><td>Action 2:</td><td>Finish[SUPPORTS] Episode finished</td></tr><tr><td colspan=\"2\">Observation 2:</td></tr><tr><td>CoT Claim:</td><td></td></tr><tr><td>Thought:</td><td>Reign Over Me is an American film made in 2010. Let’s think step by step.First, is Reign Over Me an American film? Yes,</td></tr><tr><td>Answer:</td><td>it is. Second, was it made in 2010?Yes, it was. SUPPORTS</td></tr></tbody></table></body></html>\n\nD.2ALFWORLD TRAJECTORIES\nWe provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,pick_clean_then_place_in_recep-Knife-None-CounterTop-10/trial_T20190909_110445_675754.\nD.2.1REACT TRAJECTORY\nReAct is able to correctly fnd the knife, clean it, and then place it on a countertop.i\n\nReAct ALFWorld trajectory\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to:put a clean knife in countertop.\nsinkbasin, then put it in countertop.> think:To solve the task, I need to find and take a knife, then clean it with\n> think:First I need to find a knife.A knife is more likely to appear in cabinet(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\nsinkbasin (1), stoveburner (1-4), toaster (1).I can check one by one, starting with cabinet 1.\nOK.\n> go to cabinet 1\nOn the cabinet 1, you see a bowl 1.\n> go to cabinet 2\nThe cabinet 2 is closed.\n> go to cabinet 3\nOn the cabinet 3, you see a glassbottle 1.\n> go to cabinet 4\nOn the cabinet 4, you see a mug 1.\n> go to cabinet 5\nThe cabinet 5 is closed.\n> go to cabinet 6\nThe cabinet 6 is closed.\n> go to drawer 1\nThe drawer 1 is closed.\n> go to drawer 2\nThe drawer 2 is closed\n> go to drawer 3\nThe drawer 3 is closed.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1.\n> think:Now I find a knife (1).Next, I need to take it.\n> take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n> think:Now I take a knife (1).Next, I need to go to sinkbasin (1) and clean it.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\n> clean knife 1 with sinkbasin 1\nYou clean the knife 1 using the sinkbasin 1.\n> think:Now I clean a knife (1).Next, I need to put it in/on countertop 1.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> put knife 1 in/on countertop 1\nYou put the knife 1 in/on the countertop 1\nD.2.2ACT TRAJECTORY\nAct is able to correctly fnd the knife, but tries to clean it using a sinkbasin before actually goingi to the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same sequence of commands. With ReAct, an existing thought would have concretely identifed that thei knife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\n\nAct ALFWorld trajectory\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to:put a clean knife in countertop.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1.\n> take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1\nNothing happens.\n> go to countertop 3\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2.\n> take knife 2 from countertop 3\nNothing happens.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1.\n> take knife 1 from countertop 2\nNothing happens.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n> clean knife 1 with sinkbasin 1\nNothing happens.\n> go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2.\n> take knife 2 from countertop 3\nNothing happens.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1.\n> take knife 1 from countertop 2\nNothing happens.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1\nNothing happens.\n> go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard 1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato 2, a tomato 1, and a vase 2.\n> take knife 2 from countertop 3\nNothing happens.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a potato 3, and a statue 1.\n> take knife 1 from countertop 2\nNothing happens.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> clean knife 1 with sinkbasin 1 Nothing happens.\nD.2.3REACT-IM TRAJECTORY\nReAct-IM is able to correctly fnd the knife, but is unable to clean it as well. The LLM theni gets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought think:I need to find a clean knife., which seems to trick the LLM in believing that the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured by each thought.\n\nReAct-IM ALFWorld trajectory\nYou are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to:put a clean knife in countertop.\nTo solve the task, I need to find and take a clean knife, then put it in countertop.OK.\n> think:I need to find a clean knife.\nOK.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> think:I need to find a clean knife.\nOK.\n> go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a plate 2, a potato 3, and a statue 1.\n> think:I need to find a clean knife.\nOK.\n> take knife 1 from countertop 2\nu pick up the knife 1 from the countertop 2.\n> think:I need to put this knife (1) in/on countertop 1.\nOK.\n> go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.> think:I need to put this knife (1) in/on countertop 1.\n> put knife 1 in/on countertop 1\nYou put the knife 1 in/on the countertop 1.\n> think:I need to put this knife (1) in/\n> put knife 1 in/on countertop 1\nNothing happens.\n> think:I need to put this knife (1) in/on countertop 1.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think:I need to put this knife (1) in/on countertop 1.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think:I need to put this knife (1) in/on countertop 1.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think:I need to put this knife (1) in/on countertop 1.\n> put knife 1 in/on countertop 1\nNothing happens.\n> think:I need to put this knife (1) in/on countertop 1.OK.\n\nD.3WEBSHOP TRAJECTORIES\nTable 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,ReAct uses reasoning to fnd products that satisfy all target attributes.i\n\n\n<html><body><table><thead><tr><td>Instruction: get me a sixteen pack of apple cinnamon freeze dried ba Act</td></tr></thead><tbody><tr><td>Action: search [sixteen pack apple cinnamon freeze dried banana</td></tr><tr><td>chips]</td></tr><tr><td>Observation: [Back to Search ]</td></tr><tr><td>Page 1 (Total results: 50)</td></tr><tr><td>[Next]</td></tr><tr><td>[B0061IVFZE]</td></tr><tr><td>Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -</td></tr><tr><td>Ounce (Pack of 100) $85.0</td></tr><tr><td>[B096H2P6G2]</td></tr><tr><td>Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -</td></tr><tr><td>(Variety Pack) $18.99</td></tr><tr><td>[B092JLLYK6]</td></tr><tr><td>Nature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect</td></tr><tr><td>For School Lunches or an On-The-Go Snack - No Sugar Added, Non GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Pack</td></tr><tr><td>$12.99</td></tr><tr><td></td></tr><tr><td>Action: click [B0061IVFZE] Observation:</td></tr><tr><td>[Back to Search]</td></tr><tr><td>[Prev]</td></tr><tr><td>favor name [asian pear][banana][fuji apple & cinnamon][strawberry</td></tr><tr><td>banana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,</td></tr><tr><td>0.42 Ounce (Pack of 100)</td></tr><tr><td>Price: $85.0</td></tr><tr><td>Rating: N.A. [Description]</td></tr><tr><td>[Features]</td></tr><tr><td>[Reviews]</td></tr><tr><td>[Buy Now]</td></tr><tr><td>Action: click[Buy Now]</td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td rowspan=\"3\">Score: 0.125</td></tr></tr></tr><tr><td></td></tr></table></body></html>\n\ni  \nl\nana chips, and price lower than 50.00 dollars  \nReAct  \nAction: seareh [sixteen pack apple cinnamon freeze dried banana  \nchips]  \nObservation:  \n[Back to Search]  \nPage 1 (Total results: 50)  \n[Next]  \n[B0061IVFZE]  \nBrothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0.42 -  \nOunce (Pack of 100)  \n $85.0 [B096H2P6G2]  \nMoon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids  \n(Variety Pack)  \n $18.99 [B092JLLYK6]  \nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect  \nFor School Lunches or an On-The-Go Snack - No Sugar Added,  \nNon GMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi  \n $12.99 Action: think[B0061IVFZE is strawberry banana, not apple cinnamon.  \nB096H2P6G2 is fruit snacks, not freeze dried banana chips.  \nB092JLLYK6 is banana crisps, not apple cinnamon. I can check  \nB092JLLYK6 frst.i  \nObservation: OK.  \nAction: click[B092JLLYK6]  \nObservation:  \n[Back to Search]  \n[Prev]  \nfavor name [apple][apple cinnamon][banana][cantaloupe][peach][pear]l  \n[strawberry][strawberry banana][sampler variety pack][mega variety  \npack][orchard variety pack]  \nsize [0.53 ounce (pack of 6)][0.53 ounce (pack of 8)][0.53 ounce (pack  \nof 16)]  \nNature’s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect  \nFor School Lunches or an On-The-Go Snack - No Sugar Added, Non  \nGMO, Gluten Free, Nothing Artifcial (0.53oz) 6-Packi  \nPrice: $12.99  \nRating: N.A.  \n[Description]  \n[Features]  \n[Reviews]  \n[Buy Now]  \nAction: think[For sixteen pack of apple cinnamon freeze dried banana  \nchips, the item has options ’apple cinnamon’ and ’0.53 ounce (pack of  \n16)’ and seems good to buy.]  \nObservation: OK.  \nAction: click[apple cinnamon]  \nObservation: You have clicked apple cinnamon.  \nAction: click[0.53 ounce (pack of 16)]  \nObservation: You have clicked 0.53 ounce (pack of 16).  \nAction: click[Buy Now]  \nScore: 1.0\n\nEMORE ANALYSIS\nE.1SUCCESS AND FAILURE MODES ANALYSIS\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.Search results and non-representative steps are omitted to reduce space.\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Success: True positive</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Question:</td><td>Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?</td></tr><tr><td>Thought 1:</td><td>I need to search David Chanoff and find the U.S. Navy admiral he</td></tr><tr><td>Action 1:</td><td>collaborated with. Search[David Chanoff]</td></tr><tr><td>Observation 1:</td><td>David Chanoff is a noted author...</td></tr><tr><td>Action 2:</td><td>Search[William J. Crowe]</td></tr><tr><td>Observation 2:</td><td>William J. Crowe is...</td></tr><tr><td>Thought 3:</td><td>William J. Crowe served as the ambassador to the United Kingdom under</td></tr><tr><td>Action 3:</td><td>President Bill Clinton. So the answer is Bill Clinton.</td></tr><tr><td>Observation 3:</td><td>Finish[Bill Clinton] Episode finished</td></tr><tr><td colspan=\"2\"></td></tr><tr><td>CoT Question:</td><td>What was the name of the team that represented an Arizona university in</td></tr><tr><td>Thought:</td><td>the conference that covers a broad expanse of the western United States, including Arizona, California, and New Mexico? Let’s think step by step. The name of the team that represented an Arizona university in the conference that covers a broad expanse of the western</td></tr><tr><td>Answer:</td><td>United States, including Arizona, California, and New Mexico is Arizona State Sun Devils.</td></tr><tr><td></td><td>Arizona State Sun Devils</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Success: False positive</td></tr></thead><tbody><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>Did the board game San Marco or About Time come out first?</td></tr><tr><td>Thought 1:</td><td>I need to search San Marco and About Time, and find which came out first.</td></tr><tr><td>Action 1:</td><td>Search[San Marco]</td></tr><tr><td>Observation 1:</td><td>San Marco is one of the ... The was built in venice.</td></tr><tr><td>Thought 2:</td><td>San Marco is a sestiere of Venice. I need to search About Time next.</td></tr><tr><td>Action 2: Observation 2:</td><td>Search[About Time]</td></tr><tr><td>Thought 3:</td><td>Could not find [About Time]. Similar: [...].</td></tr><tr><td>Action 3:</td><td>About Time is a 2013 film. So San Marco came out first. Finish[San Marco]</td></tr><tr><td>Observation 3:</td><td>Episode finished</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Question:</td><td>Emory (Emory</td></tr><tr><td></td><td>Was Vanderbilt University or Emory University founded first?  $\\tt\\char62 \\tt1836$ </td></tr><tr><td>Thought:</td><td>Let’s think step by step.Vanderbilt University was founded in 1873.</td></tr><tr><td></td><td></td></tr><tr><td></td><td>University was founded in 1836. 1873 (Vanderbilt University)</td></tr><tr><td></td><td></td></tr><tr><td>Answer:</td><td>University), so Vanderbilt University was founded first. Emory University</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Failure: Reasoning error</td></tr></thead><tbody><tr><td colspan=\"2\">ReAct</td></tr><tr><td>Question:</td><td>What was the middle name of the actress who was cast alongside Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in</td></tr><tr><td>Thought 1:</td><td>Hairspray? I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur, find the actress that was cast alongside them in</td></tr><tr><td>Action 1:</td><td>Hairspray, then find its middle name. Search[Harvey Fierstein]</td></tr><tr><td>Observation 1: Thought 2:</td><td>Harvey Forbes Fierstein .. . Harvey Fierstein is an actor, playwright, and screenwriter.I need to</td></tr><tr><td></td><td>search Linda Hart next...</td></tr><tr><td>Action 2:</td><td>Search[Linda Hart]...</td></tr><tr><td>CoT</td><td colspan=\"2\"></td></tr><tr><td>Question: Thought:</td><td>Who is older Danny Green or James Worthy? Let’s think step by step.</td><td></td></tr><tr><td></td><td>born in 1961.</td><td>Danny Green was born in 1987.James Worthy was</td></tr><tr><td>Answer:</td><td>older. Danny Green</td><td>1987 (Danny Green) > 1961 (James Worthy), so Danny Green is</td></tr></tbody></table></body></html>\n\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Failure: Search error</td></tr></thead><tbody><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>What does the goddess associated with the goddess frigg consists of what tales?</td></tr><tr><td>Thought 1:</td><td>I need to search goddess frigg and find its associated goddess.</td></tr><tr><td>Action 1:</td><td>Search[goddess frigg] Could not find goddess frigg. Similar: [’Frigg’, ’Frigg and Freyja common</td></tr><tr><td>Observation 1:</td><td>origin hypothesis’, ’Freyja (section Relation to Frigg and other goddesses and figures)’, ’Eir (redirect from Eir (goddess))’, ’Baldr’].</td></tr><tr><td>Thought 2:</td><td>I need to search Frigg instead...</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\">Failure: Hallucination</td></tr></thead><tbody><tr><td>CoT Question:</td><td>What was the first year a scientific journal published by an organization</td></tr><tr><td></td><td>located in the Bronx was published?</td></tr><tr><td>Thought:</td><td>Let’s think step by step.The first year a scientific journal published by an organization located in the Bronx was published was 1916.</td></tr><tr><td>Answer:</td><td>1916 (Label:1909)</td></tr></tbody></table></body></html>\n\n\n\n<html><body><table><thead><tr><td colspan=\"2\"></td><td>Failure: Label ambiguity</td></tr></thead><tbody><tr><td>ReAct</td><td></td><td></td></tr><tr><td>Question:</td><td></td><td>What nationality is the man who developed the dictator game?</td></tr><tr><td>Thought 1:</td><td>nationality.</td><td>I need to search dictator game, find the man who developed it, then find his</td></tr><tr><td>Action 1:</td><td>Search[dictator game]</td><td></td></tr><tr><td>Observation 1:</td><td>and economics...</td><td>The dictator game is a popular experimental instrument in social psychology</td></tr><tr><td>Action 3:</td><td>Search[Amos Tversky]</td><td></td></tr><tr><td>Observation 3:</td><td>Amos Nathan Tversky...</td><td></td></tr><tr><td>Thought 4:</td><td>man who developed the dictator game is Israeli.</td><td>Amos Tversky is an Israeli cognitive and mathematical psychologist.So the</td></tr><tr><td>Action 4:</td><td>Finish[Israeli] (label:Israel-American)</td><td></td></tr><tr><td>CoT</td><td></td><td></td></tr><tr><td>Question:</td><td></td><td>Kurt Fuller from Stockton, CA appeared in which American Detective Comedy</td></tr><tr><td>Thought:</td><td></td><td>Drama television series created by Steve Franks? Let’s think step by step.Kurt Fuller from Stockton, CA appeared in Psych.</td></tr><tr><td>Answer:</td><td>Steve Franks. Psych (label:</td><td>Psych is an American Detective Comedy Drama television series created by Psych is an American detective comedy-drama)</td></tr></tbody></table></body></html>\n\n"}