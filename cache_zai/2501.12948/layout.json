{"code":0,"doc_size":1312189,"doc_type":"pdf","dst_path":"oss://glm-data-ocr-data/services/maas/pa/fae77c5f-572c-4a94-9274-391069cbc16d.tar","markdown":"## DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n\nDeepSeek-AI\n\nresearch@deepseek.com\n\n## Abstract\n\nWe introduce our frst-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.i DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fne-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.i Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n\n![63b92f1f8217d43fc920b955f4d381ff](imgs/63b92f1f8217d43fc920b955f4d381ff.jpg)\n\nFigure 1 | Benchmark performance of DeepSeek-R1.\n\n\n\n## Contents\n\n1Introduction3\n\n1.1Contributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n\n1.2Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n\n## 2Approach\n\n2.1Overview .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\n\n2.2DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .5\n\n2.2.1Reinforcement Learning Algorithm. . . . . . . . . . . . . . . . . . . . . .5\n\n2.2.2Reward Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\n\n2.2.3Training Template. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\n\n2.2.4Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero6\n\n2.3DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .9\n\n2.3.1Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9\n\n2.3.2Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .10\n\n2.3.3Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .10\n\n2.3.4Reinforcement Learning for all Scenarios. . . . . . . . . . . . . . . . . . .11\n\n2.4Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . .11\n\nExperiment11\n\n3.1DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13\n\n3.2Distilled Model Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14\n\nDiscussion14\n\n4.1Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .14\n\n4.2Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15\n\nConclusion, Limitations, and Future Work16\n\nContributions and Acknowledgments20\n\n\n\n## 1.Introduction\n\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artifcial General Intelligence (AGI).i\n\nRecently, post-training has emerged as an important component of the full training pipeline.It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI‚Äôs o1 (OpenAI, 2024b) series models were the frst to introduce inference-time scaling by increasing the length of the Chain-of-i Thought reasoning process. This approach has achieved signifcant improvements in variousi reasoning tasks, such as mathematics, coding, and scientifc reasoning. However, the challengei of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI‚Äôs o1 series models.\n\nIn this paper, we take the frst step toward improving language model reasoning capabilitiesi using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifcally, we use DeepSeek-V3-Base as the base model and employi GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from $15.6\\%$ to $71.0\\%,$ and with majority voting, the score further improves to $86.7\\%,$ matching the performance of OpenAI-o1-0912.\n\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifcally, we begin by collecting thousands of cold-start data to fne-tune theii DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.After fne-tuning with the new data, the checkpoint undergoes an additional RL process, takingi into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\n\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.532B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.\n\n\n\n## 1.1. Contributions\n\n## Post-Training: Large-Scale Reinforcement Learning on the Base Model\n\nWe directly apply RL to the base model without relying on supervised fne-tuning (SFT) asi a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeekR1-Zero demonstrates capabilities such as self-verifcation, refection, and generatingil long CoTs, marking a signifcant milestone for the research community. Notably, it is thei frst open research to validate that reasoning capabilities of LLMs can be incentivizedi purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model‚Äôs reasoning and non-reasoning capabilities. We believe the pipeline will beneft the industry by creatingi better models.\n\n## Distillation: Smaller Models Can Be Powerful Too\n\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefti the research community to distill better smaller models in the future.\n\nUsing the reasoning data generated by DeepSeek-R1, we fne-tuned several dense modelsi that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeekR1-Distill-Qwen-7B achieves $55.5\\%$ on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores $72.6\\%$ on AIME 2024, $94.3\\%$ on MATH-500,and $57.2\\%$ on LiveCodeBench. These results signifcantly outperform previous open-i source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 1.2. Summary of Evaluation Results\n\nReasoning tasks: (1) DeepSeek-R1 achieves a score of $79.8\\%$ Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of $97.3\\%,$ performing on par with OpenAI-o1-1217 and signifcantly outperforming other models. (2)i On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,as it achieves 2,029 Elo rating on Codeforces outperforming $96.3\\%$ human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.\n\nKnowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeekR1 achieves outstanding results, signifcantly outperforming DeepSeek-V3 with scoresi of $90.8\\%$ on MMLU, $84.0\\%$ on MMLU-Pro, and $71.5\\%$ on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n\n\n\nOthers: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of $87.6\\%$ on AlpacaEval 2.0 and a win-rate of $92.3\\%$  on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.\n\n## 2.Approach\n\n## 2.1. Overview\n\nPrevious work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be signifcantlyi improved through large-scale reinforcement learning (RL), even without using supervised fne-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced withi the inclusion of a small amount of cold-start data. In the following sections, we present: (1)DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and(2) DeepSeek-R1, which applies RL starting from a checkpoint fne-tuned with thousands ofi long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.\n\n## 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n\nReinforcement learning has demonstrated signifcant effectiveness in reasoning tasks, as ev-i idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data,focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.\n\n## 2.2.1. Reinforcement Learning Algorithm\n\nGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.Specifcally, for each questioni $q,$  GRPO samples a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:\n\n$$\\mathcal{J}_{\\texttt{GRPO}}(\\theta)=\\mathbb{E}[q\\sim P(\\texttt{Q}),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\texttt{O}|q)]$$\n\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\min\\left(\\frac{\\pi_{\\theta}(o_{i}|q)}{\\pi_{\\theta_{old}}(o_{i}|q)}A_{i},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{i}|q)}{\\pi_{\\theta_{old}}(o_{i}|q)},1-\\varepsilon,1+\\varepsilon\\right)A_{i}\\right)-\\beta\\text{I D}_{KL}\\left(\\pi_{\\theta}||\\pi_{ref}\\right)\\right),$$\n\n(1)\n\n$$\\mathbb{D}_{\\textit{KL}}\\left(\\pi_{\\theta}||\\pi_{\\textit{ref}}\\right)=\\frac{\\pi_{\\textit{ref}}(o_{i}|q)}{\\pi_{\\theta}(o_{i}|q)}-\\log\\frac{\\pi_{\\textit{ref}}(o_{i}|q)}{\\pi_{\\theta}(o_{i}|q)}-1,$$\n\n(2)\n\nwhere ùúÄand $\\beta$ are hyper-parameters, and $A_{i}$ is the advantage, computed using a group of rewards $\\{r_{1},r_{2},\\ldots,r_{G}\\}$  corresponding to the outputs within each group:\n\n$$A_{i}=\\frac{r_{i}-\\mathrm{m}ean(\\{r_{1},r_{2},\\cdots,r_{G}\\})}{\\mathrm{s}td(\\{r_{1},r_{2},\\cdots,r_{G}\\})}.$$\n\n(3)\n\n\n\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specifc reasoningi question during training.\n\n## 2.2.2. Reward Modeling\n\nThe reward is the source of the training signal, which decides the optimization direction of RL.To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:\n\nAccuracy rewards: The accuracy reward model evaluates whether the response is correct.For example, in the case of math problems with deterministic results, the model is required to provide the fnal answer in a specifed format (e.g., within a box), enabling reliableii rule-based verifcation of correctness. Similarly, for LeetCode problems, a compiler can bei used to generate feedback based on predefned test cases.i\n\nFormat rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‚Äò<think>‚Äô and $'<1$ think>‚Äôtags.\n\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,because we fnd that the neural reward model may suffer from reward hacking in the large-scalei reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.\n\n## 2.2.3. Training Template\n\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specifed instructions. As depicted in Table 1, this templatei requires DeepSeek-R1-Zero to frst produce a reasoning process, followed by the fnal answer.ii We intentionally limit our constraints to this structural format, avoiding any content-specifci biases‚Äîsuch as mandating refective reasoning or promoting particular problem-solving strate-l gies‚Äîto ensure that we can accurately observe the model‚Äôs natural progression during the RL process.\n\n## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zer\n\nPerformance of DeepSeek-R1-ZeroFigure 2 depicts the performance trajectory of DeepSeekR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a signifcanti increase, jumping from an initial $15.6\\%$ to an impressive $71.0\\%,$ reaching performance levels comparable to OpenAI-o1-0912. This signifcant improvement highlights the effcacy of our RLii algorithm in optimizing the model‚Äôs performance over time.\n\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI‚Äôs o1-0912 models across a variety of reasoning-related benchmarks. The fndings reveal that RL empowersi\n\n\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500</td><td rowspan=\"2\">GPQA Diamond</td><td rowspan=\"2\">LiveCode Bench</td><td rowspan=\"2\">CodeForces</td></tr><tr><td>pass@1</td><td>cons@64</td></tr></thead><tbody><tr><td>OpenAI-o1-mini</td><td>63.6</td><td>80.0</td><td>pass@1 90.0</td><td>pass@1 60.0</td><td>pass@1 53.8</td><td>rating</td></tr><tr><td>OpenAI-o1-0912</td><td>74.4</td><td>83.3</td><td>94.8</td><td>77.3</td><td>63.4</td><td>1820 1843</td></tr><tr><td>DeepSeek-R1-Zero</td><td>71.0</td><td>86.7</td><td>95.9</td><td>73.3</td><td>50.0</td><td>1444</td></tr></tbody></table></body></html>\n\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.\n\n![3be2fd86f275ae557f221d317f856859](imgs/3be2fd86f275ae557f221d317f856859.jpg)\n\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.\n\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fne-tuning data. This is a noteworthy achievement, as it underscores the model‚Äôs ability toi learn and generalize effectively through RL alone. Additionally, the performance of DeepSeekR1-Zero can be further augmented through the application of majority voting. For example,when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero‚Äôs performance escalates from $71.0\\%$ to $86.7\\%,$ thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n\nSelf-evolution Process of DeepSeek-R1-ZeroThe self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model‚Äôs progression without the infuence of the supervised fne-tuning stage. This approach providesli a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\n\n\n\n![534c529c448865fc55059cce71c7f951](imgs/534c529c448865fc55059cce71c7f951.jpg)\n\nFigure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\n\nment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens,allowing the model to explore and refne its thought processes in greater depth.i\n\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as refection‚Äîwhere the modell revisits and reevaluates its previous steps‚Äîand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model‚Äôs interaction with the reinforcement learning environment. This spontaneous development signifcantly enhances DeepSeek-R1-Zero‚Äôs reasoning capabilities,i enabling it to tackle more challenging tasks with greater effciency and accuracy.i\n\nAha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an ‚Äúaha moment‚Äù. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase,DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model‚Äôs growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n\nThis moment is not only an ‚Äúaha moment‚Äù for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The‚Äúaha moment‚Äù serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artifcial systems, paving the way for more autonomous and adaptive models ini the future.\n\n\n\nQuestion: If then the sum of the real solutions of $\\sqrt{a-\\sqrt{a+x}}=x$ Ô∏Åis equal to $a>1,$ Response: <think>Ô∏Å\n\nTo solve the equation $\\sqrt{a-\\sqrt{a+x}}=x,$ Ô∏Å let‚Äôs start by squaring both ¬∑ ¬∑ ¬∑\n\n$$\\left(\\sqrt{a-\\sqrt{a+x}}\\right)^{2}=x^{2}\\ \\Longrightarrow\\ a-\\sqrt{a+x}=x^{2}.$$\n\nRearrange to isolate the inner square root term:\n\n$$(a-x^{2})^{2}=a+x\\implies a^{2}-2ax^{2}+(x^{2})^{2}=a+x\\implies x^{4}-2ax^{2}-x+(a^{2}-a)=0$$\n\nWait, wait. Wait. That‚Äôs an aha moment I can fag here.l\n\nLet‚Äôs reevaluate this step-by-step to identify if the correct sum can be ¬∑ ¬∑ ¬∑\n\nWe st‚àöarted with the equation:\n\n$$\\sqrt{a-\\sqrt{a+x}}=x$$\n\nFirs‚àöt, let‚Äôs square bot‚àöh sides:\n\n$$a-\\sqrt{a+x}=x^{2}\\implies\\sqrt{a+x}=a-x^{2}$$\n\nNext, I could square both sides again, treating the equation: ¬∑ ¬∑ ¬∑\n\nTable 3 | An interesting ‚Äúaha moment‚Äù of an intermediate version of DeepSeek-R1-Zero. The\n\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,allowing us to witness the power and beauty of reinforcement learning.\n\nDrawback of DeepSeek-R1-ZeroAlthough DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\n\n## 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\n\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\n\n## 2.3.1. Cold Start\n\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fne-tune the model as the initial RL actor. To collect such data, we have explored severali approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with refection and verifcation, gathering DeepSeek-R1-li Zero outputs in a readable format, and refning the results through post-processing by humani annotators.\n\nIn this work, we collect thousands of cold-start data to fne-tune the DeepSeek-V3-Base asi the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start datainclude:\n\nReadability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,we design a readable pattern that includes a summary at the end of each response and flters out responses that are not reader-friendly. Here, we defne the output format asii|special_token|<reasoning_process>|special_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.\n\nPotential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models.\n\n## 2.3.2. Reasoning-oriented Reinforcement Learning\n\nAfter fne-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scalei reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model‚Äôs reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defned problems withi clear solutions. During the training process, we observe that CoT often exhibits language mixing,particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model‚Äôs performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the fnal reward. We then apply RL training on the fne-tuned model until it achieves convergenceii on reasoning tasks.\n\n## 2.3.3. Rejection Sampling and Supervised Fine-Tuning\n\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model‚Äôs capabilities in writing, role-playing, and other general-purpose tasks. Specifcally, wei generate the data and fne-tune the model as described below.i\n\nReasoning dataWe curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage,we only included data that could be evaluated using rule-based rewards. However, in this stage,we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.Additionally, because the model output is sometimes chaotic and diffcult to read, we havei fltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. Fori each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n\n\n\nNon-Reasoning dataFor non-reasoning data, such as writing, factual QA, self-cognition,and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries,such as ‚Äúhello‚Äù we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning.\n\nWe fne-tune DeepSeek-V3-Base for two epochs using the above curated dataset of abouti 800k samples.\n\n## 2.3.4. Reinforcement Learning for all Scenarios\n\nTo further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model‚Äôs helpfulness and harmlessness while simultaneously refning its reasoning capabilities. Specifcally, we train the model using a combinationii of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the fnal summary, ensuring that thei assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\n\n## 2.4. Distillation: Empower Small Models with Reasoning Capability\n\nTo equip more effcient smaller models with reasoning capabilities like DeepSeek-R1, we directlyi fne-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) usingi the 800k samples curated with DeepSeek-R1, as detailed in ¬ß2.3.3. Our fndings indicate thati this straightforward distillation method signifcantly enhances the reasoning abilities of smalleri models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.514B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\n\nFor distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.\n\n## 3.Experiment\n\nBenchmarksWe evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verifed (OpenAI,i2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 ‚Äì 2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifcally, wei adhere to the original confgurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Lii et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the fnal summary to evaluation to avoid the length bias. For distilled models, wei report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\n\nEvaluation PromptsFollowing the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simpleevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, $\\mathrm{C}\\mathrm{+}\\mathrm{+},$ C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,after which the expected ratings and percentages of competitors are calculated. SWE-Bench verifed results are obtained via the agentless framework (Xia et al., 2024). AIDER-relatedi benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.\n\nBaselinesWe conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on offcial reports. For distilled models, we also compare the open-source modeli QwQ-32B-Preview (Qwen, 2024a).\n\nEvaluation SetupWe set the maximum generation length to 32,768 tokens for the models.We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and signifcant variability across different checkpoints. Therefore, wei default to pass@ùëòevaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifcally, we use a sampling temperature of 0.6 and a top-ùëùvalue of 0.95 to generatei $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as\n\n$$\\text{pass@1}=\\frac{1}{k}\\sum_{i=1}^{k}p_{i},$$\n\nwhere $p_{i}$ denotes the correctness of the ùëñ-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n\n\n\n\n\n<html><body><table><tr><td></td><td>Benchmark (Metric)</td><td>Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022</td><td>0513</td><td>V3</td><td>o1-mini </td><td>o1-1217</td><td>R1</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>-</td><td>MoE</td><td>-</td><td>-</td><td>MoE</td></tr><tr><td rowspan=\"2\"></td><td># Activated Params</td><td>-</td><td>-</td><td>37B</td><td>-</td><td>-</td><td>37B</td></tr><tr><td># Total Params</td><td>-</td><td>-</td><td>671B</td><td>-</td><td>-</td><td>671B</td></tr><tr><td rowspan=\"12\">English</td><td>MMLU (Pass@1)</td><td>88.3</td><td>87.2</td><td>88.5</td><td>85.2</td><td>91.8</td><td>90.8</td></tr><tr><td>MMLU-Redux (EM)</td><td>88.9</td><td>88.0</td><td>89.1</td><td>86.7</td><td>-</td><td>92.9</td></tr><tr><td>MMLU-Pro (EM)</td><td>78.0</td><td>72.6</td><td>75.9</td><td>80.3</td><td>-</td><td>84.0</td></tr><tr><td>DROP (3-shot F1)</td><td>88.3</td><td>83.7</td><td>91.6</td><td>83.9</td><td>90.2</td><td>92.2</td></tr><tr><td>IF-Eval (Prompt Strict)</td><td>86.5</td><td>84.3</td><td>86.1</td><td>84.8</td><td>-</td><td>83.3</td></tr><tr><td>GPQA Diamond (Pass@1)</td><td>65.0</td><td>49.9</td><td>59.1</td><td>60.0</td><td>75.7</td><td>71.5</td></tr><tr><td>SimpleQA (Correct)</td><td>28.4</td><td>38.2</td><td>24.9</td><td>7.0</td><td>47.0</td><td>30.1</td></tr><tr><td>FRAMES (Acc.)</td><td>72.5</td><td>80.5</td><td>73.3</td><td>76.9</td><td>-</td><td>82.5</td></tr><tr><td>AlpacaEval2.0 (LC-winrate)</td><td>52.0</td><td>51.1</td><td>70.0</td><td>57.8</td><td>-</td><td>87.6</td></tr><tr><td>ArenaHard (GPT-4-1106)</td><td>85.2</td><td>80.4</td><td>85.5</td><td>92.0</td><td>-</td><td>92.3</td></tr><tr><td>LiveCodeBench (Pass@1-COT)</td><td>38.9</td><td>32.9</td><td>36.2</td><td>53.8</td><td>63.4</td><td>65.9</td></tr><tr><td rowspan=\"5\">Code</td><td>Codeforces (Percentile)</td><td>20.3</td><td>23.6</td><td>58.7</td><td>93.4</td><td>96.6</td><td>96.3</td></tr><tr><td>Codeforces (Rating)</td><td>717</td><td>759</td><td>1134</td><td>1820</td><td>2061</td><td>2029</td></tr><tr><td>SWE Verifed (Resolved)</td><td>50.8</td><td>38.8</td><td>42.0</td><td>41.6</td><td>48.9</td><td>49.2</td></tr><tr><td>Aider-Polyglot (Acc.)</td><td>45.3</td><td>16.0</td><td>49.6</td><td>32.9</td><td>61.7</td><td>53.3</td></tr><tr><td>AIME 2024 (Pass@1)</td><td>16.0</td><td>9.3</td><td>39.2</td><td>63.6</td><td>79.2</td><td>79.8</td></tr><tr><td rowspan=\"3\">Math</td><td>MATH-500 (Pass@1)</td><td>78.3</td><td>74.6</td><td>90.2</td><td>90.0</td><td>96.4</td><td>97.3</td></tr><tr><td>CNMO 2024 (Pass@1)</td><td>13.1</td><td>10.8</td><td>43.2</td><td>67.6</td><td>-</td><td>78.8</td></tr><tr><td>CLUEWSC (EM)</td><td>85.4</td><td>87.9</td><td>90.9</td><td>89.9</td><td></td><td></td></tr><tr><td rowspan=\"3\">ChineseC-Eval (EM)</td><td></td><td>76.7</td><td>76.0</td><td>86.5</td><td>68.9</td><td>-</td><td>92.8 91.8</td></tr><tr><td></td><td></td><td></td><td></td><td>40.3</td><td>-</td><td>63.7</td></tr><tr><td>C-SimpleQA (Correct)</td><td>55.4</td><td>58.7</td><td>68.0</td><td></td><td>-</td><td></td></tr></table></body></html>\n\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\n\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over $70\\%.$\n\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model‚Äôs ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the fnal stages of supervised fne-tuning (SFT) and RLii training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,indicating DeepSeek-R1‚Äôs strengths in writing tasks and open-domain question answering. Its signifcant outperformance of DeepSeek-V3 underscores the generalization benefts of large-scaleii RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates thatDeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\n\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verifed. We believe the engineeringi performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited.\n\n3.2. Distilled Model Evaluation\n\n\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500</td><td rowspan=\"2\">GPQA Diamond pass@1</td><td rowspan=\"2\">LiveCode Bench</td><td rowspan=\"2\">CodeForces</td></tr><tr><td>pass@1</td><td>cons@64</td></tr><tr><td>GPT-4o-0513</td><td>9.3</td><td>13.4</td><td>pass@1 74.6</td><td>49.9</td><td>pass@1 32.9</td><td>rating 759</td></tr><tr><td>Claude-3.5-Sonnet-1022</td><td>16.0</td><td>26.7</td><td>78.3</td><td>65.0</td><td>38.9</td><td>717</td></tr><tr><td>OpenAI-o1-mini</td><td>63.6</td><td>80.0</td><td>90.0</td><td>60.0</td><td>53.8</td><td>1820</td></tr><tr><td>QwQ-32B-Preview</td><td>50.0</td><td>60.0</td><td>90.6</td><td>54.5</td><td>41.9</td><td>1316</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-1.5B</td><td>28.9</td><td>52.7</td><td>83.9</td><td>33.8</td><td>16.9</td><td>954</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-7B</td><td>55.5</td><td>83.3</td><td>92.8</td><td>49.1</td><td>37.6</td><td>1189</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-14B</td><td>69.7</td><td>80.0</td><td>93.9</td><td>59.1</td><td>53.1</td><td>1481</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-32B</td><td>72.6</td><td>83.3</td><td>94.3</td><td>62.1</td><td>57.2</td><td>1691</td></tr><tr><td>DeepSeek-R1-Distill-Llama-8B</td><td>50.4</td><td>80.0</td><td>89.1</td><td>49.0</td><td>39.6</td><td>1205</td></tr><tr><td>DeepSeek-R1-Distill-Llama-70B</td><td>70.0</td><td>86.7</td><td>94.5</td><td>65.2</td><td>57.5</td><td>1633</td></tr></table></body></html>\n\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.\n\nAs shown in Table 5, simply distilling DeepSeek-R1‚Äôs outputs enables the effcient DeepSeek-i R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform nonreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32BPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B signifcantlyi exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields signifcant furtheri gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n\n## 4.Discussion\n\n## 4.1. Distillation v.s. Reinforcement Learning\n\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n\n\n\n\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500 pass@1</td><td rowspan=\"2\">GPQA Diamond pass@1</td><td rowspan=\"2\">LiveCodeBench pass@1</td></tr><tr><td>pass@1</td><td>cons@64</td></tr><tr><td>QwQ-32B-Preview</td><td>50.0</td><td>60.0</td><td>90.6</td><td>54.5</td><td>41.9</td></tr><tr><td>DeepSeek-R1-Zero-Qwen-32B</td><td>47.0</td><td>60.0</td><td>91.6</td><td>55.0</td><td>40.2</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-32B</td><td>72.6</td><td>83.3</td><td>94.3</td><td>62.1</td><td>57.2</td></tr></table></body></html>\n\nTable 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\n\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs signifcantly better thani DeepSeek-R1-Zero-Qwen-32B across all benchmarks.\n\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and largerscale reinforcement learning.\n\n## 4.2. Unsuccessful Attempts\n\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n\nProcess Reward Model (PRM)PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly defne a fne-grain step in general reasoning. Second,ii determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\n\nMonte Carlo Tree Search (MCTS)Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specifc reasoning steps necessary for the search. Fori training, we frst use collected prompts to fnd answers via MCTS guided by a pre-trained valueii model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refning the process.i\n\nHowever, this approach encounters several challenges when scaling up the training. First,unlike chess, where the search space is relatively well-defned, token generation presents aniexponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly infuences the quality of generation since it guides each step of the search process.l Training a fne-grained value model is inherently diffcult, which makes it challenging for theii model to iteratively improve. While AlphaGo‚Äôs core success relied on training a value model to progressively enhance its performance, this principle proves diffcult to replicate in our setupi due to the complexities of token generation.\n\nIn conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a signifcant challenge.i\n\n## 5.Conclusion, Limitations, and Future Work\n\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,leveraging cold-start data alongside iterative RL fne-tuning. Ultimately, DeepSeek-R1 achievesi performance comparable to OpenAI-o1-1217 on a range of tasks.\n\nWe further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fne-tune several smalli dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with $28.9\\%$ on AIME and $83.9\\%$ on MATH. Other dense models also achieve impressive results, signifcantly outperforming other instruction-i tuned models based on the same underlying checkpoints.\n\nure, we plan to invest in research across the following directions for DeepSeek-R1.\n\nGeneral Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output.Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these felds.i\n\nLanguage Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n\nPrompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n\nSoftware Engineering Tasks: Due to the long evaluation times, which impact the eff-i ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve effciency.i\n\n\n\n## References\n\nAI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md.\n\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet.\n\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.URL https://arxiv.org/abs/2107.03374.\n\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\n\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https://arxiv.org/abs/2309.17179.\n\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760.\n\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127.\n\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024.\n\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.\n\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.Livecodebench: Holistic and contamination free evaluation of large language models for code.CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\n\n\n\nS. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.Fact, fetch, and reason: A unifed evaluation of retrieval-augmented generation. CoRR,i abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941.\n\nA. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\n\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,2023.\n\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024.\n\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,I. Sutskever, and K. Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n\nB. Y. Lin. ZeroEval: A Unifed Framework for Evaluating Language Models, July 2024. URLi https://github.com/WildEval/ZeroEval.\n\nMAA.American invitational mathematics examination - aime.In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math-competitions/american-invitational-mathematics-examination-aime.\n\nnAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\n\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/.\n\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing-simpleqa/.\n\nOpenAI. Introducing SWE-bench verifed we‚Äôre releasing a human-validated subset of swe-i bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench-verified/.\n\nQwen. Qwq: Refect deeply on the boundaries of the unknown, 2024a. URL https://qwenlml.github.io/blog/qwq-32b-preview/.\n\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5.\n\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\n\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815,2017a. URL http://arxiv.org/abs/1712.01815.\n\n\n\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354‚Äì359,2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.\n\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14.\n\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\n\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\n\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A labelfree step-by-step verifer for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935,i 2023.\n\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.URL https://doi.org/10.48550/arXiv.2406.01574.\n\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang.Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.\n\nH. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152.\n\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\n\n\n\n## Appendix\n\n## A. Contributions and Acknowledgments\n\nCore Contributors  \nDaya Guo  \nDejian Yang  \nHaowei Zhang  \nJunxiao Song  \nRuoyu Zhang  \nRunxin Xu  \nQihao Zhu  \nShirong Ma  \nPeiyi Wang  \nXiao Bi  \nXiaokang Zhang  \nXingkai Yu  \nYu Wu  \nZ.F. Wu  \nZhibin Gou  \nZhihong Shao  \nZhuoshu Li  \nZiyi Gao  \nContributors  \nAixin Liu  \nBing Xue  \nBingxuan Wang  \nBochao Wu  \nBei Feng  \nChengda Lu  \nChenggang Zhao  \nChengqi Deng  \nChong Ruan  \nDamai Dai  \nDeli Chen  \nDongjie Ji  \nErhang Li  \nFangyun Lin  \nFucong Dai  \nFuli Luo*  \nGuangbo Hao  \nGuanting Chen  \nGuowei Li  \nH. Zhang  \nHanwei Xu  \nHonghui Ding  \nHuazuo Gao  \nHui Qu\n\nHui Li  \nJianzhong Guo  \nJiashi Li  \nJingchang Chen  \nJingyang Yuan  \nJinhao Tu  \nJunjie Qiu  \nJunlong Li  \nJ.L. Cai  \nJiaqi Ni  \nJian Liang  \nJin Chen  \nKai Dong  \nKai Hu*  \nKaichao You  \nKaige Gao  \nKang Guan  \nKexin Huang  \nKuai Yu  \nLean Wang  \nLecong Zhang  \nLiang Zhao  \nLitong Wang  \nLiyue Zhang  \nLei Xu  \nLeyi Xia  \nMingchuan Zhang  \nMinghua Zhang  \nMinghui Tang  \nMingxu Zhou  \nMeng Li  \nMiaojun Wang  \nMingming Li  \nNing Tian  \nPanpan Huang  \nPeng Zhang  \nQiancheng Wang  \nQinyu Chen  \nQiushi Du  \nRuiqi Ge*  \nRuisong Zhang  \nRuizhe Pan  \nRunji Wang  \nR.J. Chen  \nR.L. Jin\n\n\n\nRuyi Chen  \nShanghao Lu  \nShangyan Zhou  \nShanhuang Chen  \nShengfeng Ye  \nShiyu Wang  \nShuiping Yu  \nShunfeng Zhou  \nShuting Pan  \nS.S. Li  \nShuang Zhou  \nShaoqing Wu  \nShengfeng Ye  \nTao Yun  \nTian Pei  \nTianyu Sun  \nT. Wang  \nWangding Zeng  \nWen Liu  \nWenfeng Liang  \nWenjun Gao  \nWenqin Yu*  \nWentao Zhang  \nW.L. Xiao  \nWei An  \nXiaodong Liu  \nXiaohan Wang  \nXiaokang Chen  \nXiaotao Nie  \nXin Cheng  \nXin Liu  \nXin Xie  \nXingchao Liu  \nXinyu Yang  \nXinyuan Li  \nXuecheng Su  \nXuheng Lin  \nX.Q. Li  \nXiangyue Jin  \nXiaojin Shen  \nXiaosha Chen  \nXiaowen Sun  \nXiaoxiang Wang  \nXinnan Song  \nXinyi Zhou  \nXianzu Wang  \nXinxia Shan  \nY.K. Li  \nY.Q. Wang\n\nY.X. Wei  \nYang Zhang  \nYanhong Xu  \nYao Li  \nYao Zhao  \nYaofeng Sun  \nYaohui Wang  \nYi Yu  \nYichao Zhang  \nYifan Shi  \nYiliang Xiong  \nYing He  \nYishi Piao  \nYisong Wang  \nYixuan Tan  \nYiyang Ma*  \nYiyuan Liu  \nYongqiang Guo  \nYuan Ou  \nYuduan Wang  \nYue Gong  \nYuheng Zou  \nYujia He  \nYunfan Xiong  \nYuxiang Luo  \nYuxiang You  \nYuxuan Liu  \nYuyang Zhou  \nY.X. Zhu  \nYanping Huang  \nYaohui Li  \nYi Zheng  \nYuchen Zhu  \nYunxian Ma  \nYing Tang  \nYukun Zha  \nYuting Yan  \nZ.Z. Ren  \nZehui Ren  \nZhangli Sha  \nZhe Fu  \nZhean Xu  \nZhenda Xie  \nZhengyan Zhang  \nZhewen Hao  \nZhicheng Ma  \nZhigang Yan  \nZhiyu Wu  \nZihui Gu\n\n\n\nZiyang Song\n\nWithin each role, authors are listed alphabetically by the frst name. Names marked with *i denote individuals who have departed from our team.\n\n","msg":"","ocr_all":false,"page_count":22,"pages":[{"abandon_blocks":[{"bbox":{"x0":25,"x1":78,"y0":521,"y1":1226},"conf":0.4508,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":28,"x1":69,"y0":851,"y1":1223},"font_size":0.0,"text":".[  1v84921a1052:viXr"},{"bbox":{"x0":32,"x1":69,"y0":530,"y1":865},"font_size":0.0,"text":"25202 naJ c2  ]LC.s"}],"source":"layout det","text":""},{"bbox":{"x0":136,"x1":1049,"y0":64,"y1":134},"conf":0.302,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":154,"x1":206,"y0":77,"y1":114},"font_size":0.0,"text":""},{"bbox":{"x0":204,"x1":388,"y0":72,"y1":116},"font_size":0.0,"text":""}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":135,"x1":1056,"y0":225,"y1":311},"conf":0.9392,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":142,"x1":1047,"y0":226,"y1":261},"font_size":2232000000000.0,"text":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via"},{"bbox":{"x0":413,"x1":777,"y0":265,"y1":307},"font_size":2232000000000.0,"text":"Reinforcement Learning"}],"source":"layout det","text":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"},{"bbox":{"x0":524,"x1":666,"y0":357,"y1":390},"conf":0.8596,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":524,"x1":667,"y0":350,"y1":392},"font_size":2232000000000.0,"text":"DeepSeek-AI"}],"source":"layout det","text":"DeepSeek-AI"},{"bbox":{"x0":468,"x1":722,"y0":406,"y1":434},"conf":0.8705,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":471,"x1":718,"y0":407,"y1":432},"font_size":2232000000000.0,"text":"research@deepseek.com"}],"source":"layout det","text":"research@deepseek.com"},{"bbox":{"x0":531,"x1":659,"y0":498,"y1":534},"conf":0.8843,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":535,"x1":658,"y0":500,"y1":532},"font_size":2232000000000.0,"text":"Abstract"}],"source":"layout det","text":"Abstract"},{"bbox":{"x0":133,"x1":1060,"y0":573,"y1":866},"conf":0.9785,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1055,"y0":570,"y1":602},"font_size":2232000000000.0,"text":"We introduce our frst-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.i"},{"bbox":{"x0":135,"x1":1056,"y0":600,"y1":632},"font_size":2232000000000.0,"text":"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-"},{"bbox":{"x0":138,"x1":1055,"y0":633,"y1":659},"font_size":2232000000000.0,"text":"vised fne-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.i"},{"bbox":{"x0":136,"x1":1053,"y0":658,"y1":691},"font_size":2232000000000.0,"text":"Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing"},{"bbox":{"x0":135,"x1":1053,"y0":686,"y1":719},"font_size":2232000000000.0,"text":"reasoning behaviors. However, it encounters challenges such as poor readability, and language"},{"bbox":{"x0":136,"x1":1053,"y0":717,"y1":747},"font_size":2232000000000.0,"text":"mixing. To address these issues and further enhance reasoning performance, we introduce"},{"bbox":{"x0":140,"x1":1051,"y0":747,"y1":772},"font_size":2232000000000.0,"text":"DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-"},{"bbox":{"x0":135,"x1":1053,"y0":772,"y1":803},"font_size":2232000000000.0,"text":"R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the"},{"bbox":{"x0":138,"x1":1051,"y0":805,"y1":830},"font_size":2232000000000.0,"text":"research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models"},{"bbox":{"x0":138,"x1":946,"y0":833,"y1":858},"font_size":2232000000000.0,"text":"(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."}],"source":"layout det","text":"We introduce our frst-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.i DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fne-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.i Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."},{"bbox":{"x0":142,"x1":1049,"y0":884,"y1":1427},"conf":0.9778,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![63b92f1f8217d43fc920b955f4d381ff](imgs/63b92f1f8217d43fc920b955f4d381ff.jpg)"},{"bbox":{"x0":339,"x1":850,"y0":1440,"y1":1475},"conf":0.8977,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":344,"x1":844,"y0":1440,"y1":1470},"font_size":2232000000000.0,"text":"Figure 1 | Benchmark performance of DeepSeek-R1."}],"source":"layout det","text":"Figure 1 | Benchmark performance of DeepSeek-R1."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1060,"y0":573,"y1":866},"conf":0.9785,"label":"Text","label_id":1},{"bbox":{"x0":142,"x1":1049,"y0":884,"y1":1427},"conf":0.9778,"label":"Figure","label_id":3},{"bbox":{"x0":135,"x1":1056,"y0":225,"y1":311},"conf":0.9392,"label":"Title","label_id":0},{"bbox":{"x0":339,"x1":850,"y0":1440,"y1":1475},"conf":0.8977,"label":"Figure caption","label_id":4},{"bbox":{"x0":531,"x1":659,"y0":498,"y1":534},"conf":0.8843,"label":"Title","label_id":0},{"bbox":{"x0":468,"x1":722,"y0":406,"y1":434},"conf":0.8705,"label":"Text","label_id":1},{"bbox":{"x0":524,"x1":666,"y0":357,"y1":390},"conf":0.8596,"label":"Text","label_id":1},{"bbox":{"x0":25,"x1":78,"y0":521,"y1":1226},"conf":0.4508,"label":"Abandon","label_id":2},{"bbox":{"x0":206,"x1":389,"y0":71,"y1":121},"conf":0.3549,"label":"Abandon","label_id":2},{"bbox":{"x0":136,"x1":1049,"y0":64,"y1":134},"conf":0.302,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[344,1440],[844,1440],[844,1470],[344,1470]],"score":0.8637},{"poly":[[948,1402],[1001,1402],[1001,1419],[948,1419]],"score":0.766},{"poly":[[806,1402],[854,1402],[854,1421],[806,1421]],"score":0.8079},{"poly":[[661,1402],[711,1402],[711,1421],[661,1421]],"score":0.7663},{"poly":[[518,1402],[567,1402],[567,1421],[518,1421]],"score":0.7183},{"poly":[[370,1402],[429,1402],[429,1419],[370,1419]],"score":0.8354},{"poly":[[230,1402],[280,1402],[280,1421],[230,1421]],"score":0.7932},{"poly":[[904,1382],[1047,1382],[1047,1405],[904,1405]],"score":0.8501},{"poly":[[806,1384],[856,1384],[856,1403],[806,1403]],"score":0.9772},{"poly":[[647,1384],[727,1384],[727,1403],[647,1403]],"score":0.9279},{"poly":[[484,1378],[601,1383],[600,1407],[484,1403]],"score":0.7378},{"poly":[[357,1382],[440,1386],[439,1405],[356,1401]],"score":0.775},{"poly":[[214,1384],[296,1384],[296,1403],[214,1403]],"score":0.9254},{"poly":[[167,1282],[186,1282],[186,1298],[167,1298]],"score":0.8561},{"poly":[[961,1196],[989,1196],[989,1217],[961,1217]],"score":0.7633},{"poly":[[168,1189],[186,1189],[186,1209],[168,1209]],"score":0.7412},{"poly":[[985,1175],[1035,1175],[1035,1195],[985,1195]],"score":0.7778},{"poly":[[920,1158],[939,1158],[939,1195],[920,1195]],"score":0.6014},{"poly":[[918,1145],[962,1145],[962,1158],[918,1158]],"score":0.8633},{"poly":[[434,1100],[455,1100],[455,1116],[434,1116]],"score":0.8962},{"poly":[[168,1098],[186,1098],[186,1117],[168,1117]],"score":0.6993},{"poly":[[555,1090],[600,1097],[597,1115],[552,1108]],"score":0.8151},{"poly":[[533,1084],[551,1084],[551,1098],[533,1098]],"score":0.7362},{"poly":[[269,1077],[287,1077],[287,1093],[269,1093]],"score":0.6238},{"poly":[[140,1063],[165,1063],[165,1242],[140,1242]],"score":0.6681},{"poly":[[243,1033],[268,1033],[268,1054],[243,1054]],"score":0.7485},{"poly":[[509,1023],[530,1023],[530,1037],[509,1037]],"score":0.7849},{"poly":[[200,1019],[216,1019],[216,1033],[200,1033]],"score":0.8075},{"poly":[[167,1007],[186,1007],[186,1024],[167,1024]],"score":0.8998},{"poly":[[197,1005],[222,1005],[222,1017],[197,1017]],"score":0.9015},{"poly":[[847,982],[863,982],[863,993],[847,993]],"score":0.8135},{"poly":[[821,970],[844,970],[844,982],[821,982]],"score":0.7761},{"poly":[[867,965],[890,965],[890,979],[867,979]],"score":0.917},{"poly":[[702,956],[743,956],[743,970],[702,970]],"score":0.7669},{"poly":[[388,953],[409,953],[409,968],[388,968]],"score":0.7189},{"poly":[[408,942],[434,942],[434,956],[408,956]],"score":0.8373},{"poly":[[675,937],[697,937],[697,953],[675,953]],"score":0.9162},{"poly":[[342,928],[386,928],[386,944],[342,944]],"score":0.7495},{"poly":[[626,918],[678,925],[675,944],[623,938]],"score":0.613},{"poly":[[158,914],[190,914],[190,935],[158,935]],"score":0.7988},{"poly":[[916,891],[1021,891],[1021,910],[916,910]],"score":0.8976},{"poly":[[743,891],[863,891],[863,910],[743,910]],"score":0.8784},{"poly":[[549,889],[689,889],[689,912],[549,912]],"score":0.7913},{"poly":[[340,889],[496,889],[496,912],[340,912]],"score":0.8044},{"poly":[[177,889],[319,889],[319,912],[177,912]],"score":0.7832},{"poly":[[28,851],[65,851],[69,1222],[32,1223]],"score":0.7188},{"poly":[[138,833],[946,833],[946,858],[138,858]],"score":0.7709},{"poly":[[138,805],[1051,805],[1051,830],[138,830]],"score":0.8163},{"poly":[[135,772],[1053,774],[1053,803],[135,802]],"score":0.7554},{"poly":[[140,747],[1051,747],[1051,772],[140,772]],"score":0.773},{"poly":[[136,717],[1053,717],[1053,747],[136,747]],"score":0.6947},{"poly":[[135,686],[1053,689],[1053,719],[135,716]],"score":0.6932},{"poly":[[137,658],[1053,661],[1053,691],[136,688]],"score":0.7454},{"poly":[[138,633],[1055,633],[1055,658],[138,658]],"score":0.8046},{"poly":[[135,600],[1056,602],[1056,632],[135,630]],"score":0.7814},{"poly":[[135,570],[1055,572],[1054,602],[135,600]],"score":0.7709},{"poly":[[32,530],[69,530],[69,865],[32,865]],"score":0.7118},{"poly":[[535,500],[658,500],[658,532],[535,532]],"score":0.9781},{"poly":[[471,407],[718,407],[718,432],[471,432]],"score":0.8506},{"poly":[[524,355],[666,350],[667,387],[525,392]],"score":0.8638},{"poly":[[413,265],[777,272],[776,307],[413,300]],"score":0.8048},{"poly":[[142,226],[1047,226],[1047,261],[142,261]],"score":0.8507},{"poly":[[154,77],[206,77],[206,114],[154,114]],"score":0.6856},{"poly":[[204,72],[388,72],[388,116],[204,116]],"score":0.8403}],"page_no":0,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1580},"conf":0.683,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1554,"y1":1573},"font_size":0.0,"text":"2"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":135,"x1":254,"y0":165,"y1":200},"conf":0.7956,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":134,"x1":252,"y0":165,"y1":199},"font_size":0.0,"text":"Contents"}],"source":"layout det","text":"Contents"},{"bbox":{"x0":145,"x1":1062,"y0":236,"y1":352},"conf":0.7979,"font_size":0.0,"label":"Dir","label_id":10,"lines":[{"bbox":{"x0":136,"x1":1053,"y0":238,"y1":267},"font_size":0.0,"text":"1Introduction3"},{"bbox":{"x0":170,"x1":1055,"y0":277,"y1":305},"font_size":0.0,"text":"1.1Contributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4"},{"bbox":{"x0":170,"x1":1055,"y0":318,"y1":343},"font_size":0.0,"text":"1.2Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4"}],"source":"layout det","text":"1Introduction3\n\n1.1Contributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n\n1.2Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4"},{"bbox":{"x0":134,"x1":278,"y0":373,"y1":408},"conf":0.3526,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":133,"x1":275,"y0":372,"y1":405},"font_size":0.0,"text":"2Approach"}],"source":"layout det","text":"2Approach"},{"bbox":{"x0":153,"x1":1065,"y0":395,"y1":1264},"conf":0.5895,"font_size":0.0,"label":"Dir","label_id":10,"lines":[{"bbox":{"x0":167,"x1":1053,"y0":410,"y1":441},"font_size":0.0,"text":"2.1Overview .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5"},{"bbox":{"x0":168,"x1":1055,"y0":449,"y1":479},"font_size":0.0,"text":"2.2DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .5"},{"bbox":{"x0":220,"x1":1055,"y0":489,"y1":517},"font_size":10.0,"text":"2.2.1Reinforcement Learning Algorithm. . . . . . . . . . . . . . . . . . . . . .5"},{"bbox":{"x0":220,"x1":1055,"y0":522,"y1":558},"font_size":0.0,"text":"2.2.2Reward Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6"},{"bbox":{"x0":220,"x1":1055,"y0":563,"y1":595},"font_size":10.0,"text":"2.2.3Training Template. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6"},{"bbox":{"x0":220,"x1":1053,"y0":603,"y1":628},"font_size":10.0,"text":"2.2.4Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero6"},{"bbox":{"x0":170,"x1":1053,"y0":640,"y1":669},"font_size":10.0,"text":"2.3DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .9"},{"bbox":{"x0":220,"x1":1053,"y0":677,"y1":706},"font_size":10.0,"text":"2.3.1Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9"},{"bbox":{"x0":220,"x1":1055,"y0":716,"y1":747},"font_size":10.0,"text":"2.3.2Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .10"},{"bbox":{"x0":220,"x1":1055,"y0":753,"y1":782},"font_size":10.0,"text":"2.3.3Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .10"},{"bbox":{"x0":220,"x1":1053,"y0":791,"y1":821},"font_size":10.0,"text":"2.3.4Reinforcement Learning for all Scenarios. . . . . . . . . . . . . . . . . . .11"},{"bbox":{"x0":170,"x1":1053,"y0":830,"y1":860},"font_size":0.0,"text":"2.4Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . .11"},{"bbox":{"x0":136,"x1":1055,"y0":888,"y1":919},"font_size":0.0,"text":"Experiment11"},{"bbox":{"x0":167,"x1":1055,"y0":926,"y1":956},"font_size":0.0,"text":"3.1DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13"},{"bbox":{"x0":170,"x1":1053,"y0":967,"y1":994},"font_size":0.0,"text":"3.2Distilled Model Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14"},{"bbox":{"x0":135,"x1":1055,"y0":1024,"y1":1054},"font_size":0.0,"text":"Discussion14"},{"bbox":{"x0":166,"x1":1056,"y0":1061,"y1":1095},"font_size":0.0,"text":"4.1Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .14"},{"bbox":{"x0":170,"x1":1055,"y0":1100,"y1":1131},"font_size":0.0,"text":"4.2Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15"},{"bbox":{"x0":138,"x1":1055,"y0":1161,"y1":1191},"font_size":0.0,"text":"Conclusion, Limitations, and Future Work16"},{"bbox":{"x0":140,"x1":1055,"y0":1219,"y1":1251},"font_size":0.0,"text":" Contributions and Acknowledgments20"}],"source":"layout det","text":"2.1Overview .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\n\n2.2DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .5\n\n2.2.1Reinforcement Learning Algorithm. . . . . . . . . . . . . . . . . . . . . .5\n\n2.2.2Reward Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\n\n2.2.3Training Template. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\n\n2.2.4Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero6\n\n2.3DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .9\n\n2.3.1Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9\n\n2.3.2Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .10\n\n2.3.3Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .10\n\n2.3.4Reinforcement Learning for all Scenarios. . . . . . . . . . . . . . . . . . .11\n\n2.4Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . .11\n\nExperiment11\n\n3.1DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13\n\n3.2Distilled Model Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14\n\nDiscussion14\n\n4.1Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .14\n\n4.2Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15\n\nConclusion, Limitations, and Future Work16\n\nContributions and Acknowledgments20"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":145,"x1":1062,"y0":236,"y1":352},"conf":0.7979,"label":"Dir","label_id":10},{"bbox":{"x0":135,"x1":254,"y0":165,"y1":200},"conf":0.7956,"label":"Title","label_id":0},{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1580},"conf":0.683,"label":"Abandon","label_id":2},{"bbox":{"x0":153,"x1":1065,"y0":395,"y1":1264},"conf":0.5895,"label":"Dir","label_id":10},{"bbox":{"x0":162,"x1":1063,"y0":416,"y1":870},"conf":0.4757,"label":"Dir","label_id":10},{"bbox":{"x0":134,"x1":278,"y0":373,"y1":408},"conf":0.3526,"label":"Title","label_id":0}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1554],[603,1554],[603,1573],[587,1573]],"score":0.6748},{"poly":[[140,1223],[553,1223],[553,1247],[140,1247]],"score":0.8729},{"poly":[[1021,1219],[1055,1219],[1055,1251],[1021,1251]],"score":0.8448},{"poly":[[138,1163],[594,1163],[594,1188],[138,1188]],"score":0.8275},{"poly":[[1021,1161],[1055,1161],[1055,1191],[1021,1191]],"score":0.9166},{"poly":[[170,1103],[463,1103],[463,1128],[170,1128]],"score":0.8426},{"poly":[[1021,1100],[1055,1100],[1055,1131],[1021,1131]],"score":0.8067},{"poly":[[167,1061],[613,1065],[613,1095],[166,1091]],"score":0.7766},{"poly":[[1019,1061],[1056,1061],[1056,1093],[1019,1093]],"score":0.7484},{"poly":[[135,1026],[285,1026],[285,1051],[135,1051]],"score":0.7997},{"poly":[[1021,1024],[1055,1024],[1055,1054],[1021,1054]],"score":0.9357},{"poly":[[170,967],[487,967],[487,991],[170,991]],"score":0.8572},{"poly":[[1023,967],[1053,967],[1053,991],[1023,991]],"score":0.8847},{"poly":[[167,926],[473,926],[473,956],[167,956]],"score":0.7691},{"poly":[[1021,926],[1055,926],[1055,956],[1021,956]],"score":0.8904},{"poly":[[136,891],[292,891],[292,916],[136,916]],"score":0.8689},{"poly":[[1021,888],[1055,888],[1055,919],[1021,919]],"score":0.8529},{"poly":[[170,833],[838,833],[838,858],[170,858]],"score":0.8668},{"poly":[[1021,830],[1053,830],[1053,860],[1021,860]],"score":0.8461},{"poly":[[284,793],[691,796],[691,821],[283,817]],"score":0.8236},{"poly":[[220,795],[296,795],[296,819],[220,819]],"score":0.8387},{"poly":[[1021,791],[1053,791],[1053,821],[1021,821]],"score":0.8575},{"poly":[[285,758],[766,758],[766,782],[285,782]],"score":0.8292},{"poly":[[220,756],[296,756],[296,781],[220,781]],"score":0.8215},{"poly":[[1019,753],[1055,753],[1055,782],[1019,782]],"score":0.8353},{"poly":[[285,716],[728,717],[728,747],[285,745]],"score":0.7271},{"poly":[[220,717],[278,717],[278,744],[220,744]],"score":0.7906},{"poly":[[1021,716],[1055,716],[1055,746],[1021,746]],"score":0.8237},{"poly":[[286,677],[399,681],[398,705],[285,701]],"score":0.8161},{"poly":[[220,679],[276,679],[276,705],[220,705]],"score":0.8511},{"poly":[[1030,679],[1053,679],[1053,705],[1030,705]],"score":0.8183},{"poly":[[214,642],[757,642],[757,667],[214,667]],"score":0.8172},{"poly":[[170,640],[211,640],[211,667],[170,667]],"score":0.7737},{"poly":[[1030,640],[1053,640],[1053,667],[1030,667]],"score":0.8151},{"poly":[[285,605],[1019,605],[1019,628],[285,628]],"score":0.9871},{"poly":[[1030,605],[1053,605],[1053,628],[1030,628]],"score":0.8208},{"poly":[[220,603],[296,603],[296,628],[220,628]],"score":0.7496},{"poly":[[284,563],[479,565],[478,595],[283,593]],"score":0.779},{"poly":[[220,567],[278,567],[278,591],[220,591]],"score":0.9244},{"poly":[[1031,565],[1055,565],[1055,591],[1031,591]],"score":0.8273},{"poly":[[282,522],[475,528],[474,558],[281,552]],"score":0.7428},{"poly":[[1031,528],[1055,528],[1055,554],[1031,554]],"score":0.837},{"poly":[[220,526],[296,526],[296,553],[220,553]],"score":0.7153},{"poly":[[287,491],[634,491],[634,516],[287,516]],"score":0.8598},{"poly":[[220,489],[278,489],[278,516],[220,516]],"score":0.7993},{"poly":[[1031,489],[1055,489],[1055,516],[1031,516]],"score":0.8589},{"poly":[[168,449],[840,449],[840,479],[168,479]],"score":0.7503},{"poly":[[1031,451],[1055,451],[1055,479],[1031,479]],"score":0.8472},{"poly":[[167,410],[333,410],[333,440],[167,440]],"score":0.7282},{"poly":[[1031,412],[1053,412],[1053,439],[1031,439]],"score":0.9326},{"poly":[[133,374],[274,372],[275,403],[133,405]],"score":0.776},{"poly":[[1031,374],[1055,374],[1055,402],[1031,402]],"score":0.7216},{"poly":[[170,318],[535,318],[535,342],[170,342]],"score":0.8199},{"poly":[[1031,318],[1055,318],[1055,342],[1031,342]],"score":0.797},{"poly":[[170,279],[363,279],[363,303],[170,303]],"score":0.875},{"poly":[[1031,277],[1055,277],[1055,303],[1031,303]],"score":0.7767},{"poly":[[137,238],[303,242],[303,267],[136,263]],"score":0.8156},{"poly":[[1031,240],[1053,240],[1053,267],[1031,267]],"score":0.8942},{"poly":[[135,165],[252,169],[251,199],[134,194]],"score":0.8241}],"page_no":1,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":608,"y0":1551,"y1":1579},"conf":0.6136,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1554,"y1":1573},"font_size":0.0,"text":"3"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":331,"y0":166,"y1":200},"conf":0.8888,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":324,"y0":172,"y1":196},"font_size":0.0,"text":"1. Introduction"}],"source":"layout det","text":"1.Introduction"},{"bbox":{"x0":132,"x1":1057,"y0":219,"y1":308},"conf":0.9539,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":225,"y1":249},"font_size":0.0,"text":"In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and"},{"bbox":{"x0":136,"x1":1051,"y0":247,"y1":281},"font_size":0.0,"text":"evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap"},{"bbox":{"x0":136,"x1":574,"y0":277,"y1":302},"font_size":0.0,"text":"towards Artifcial General Intelligence (AGI).i"}],"source":"layout det","text":"In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artifcial General Intelligence (AGI).i"},{"bbox":{"x0":133,"x1":1060,"y0":313,"y1":674},"conf":0.9785,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":319,"y1":344},"font_size":0.0,"text":"Recently, post-training has emerged as an important component of the full training pipeline."},{"bbox":{"x0":135,"x1":1051,"y0":342,"y1":374},"font_size":0.0,"text":"It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt"},{"bbox":{"x0":138,"x1":1051,"y0":374,"y1":398},"font_size":0.0,"text":"to user preferences, all while requiring relatively minimal computational resources against"},{"bbox":{"x0":135,"x1":1053,"y0":396,"y1":428},"font_size":0.0,"text":"pre-training. In the context of reasoning capabilities, OpenAI‚Äôs o1 (OpenAI, 2024b) series models"},{"bbox":{"x0":138,"x1":1051,"y0":428,"y1":453},"font_size":0.0,"text":"were the frst to introduce inference-time scaling by increasing the length of the Chain-of-i"},{"bbox":{"x0":138,"x1":1051,"y0":453,"y1":482},"font_size":0.0,"text":"Thought reasoning process. This approach has achieved signifcant improvements in variousi"},{"bbox":{"x0":138,"x1":1049,"y0":482,"y1":507},"font_size":0.0,"text":"reasoning tasks, such as mathematics, coding, and scientifc reasoning. However, the challengei"},{"bbox":{"x0":140,"x1":1049,"y0":509,"y1":533},"font_size":0.0,"text":"of effective test-time scaling remains an open question for the research community. Several prior"},{"bbox":{"x0":138,"x1":1049,"y0":537,"y1":561},"font_size":0.0,"text":"works have explored various approaches, including process-based reward models (Lightman"},{"bbox":{"x0":140,"x1":1051,"y0":563,"y1":586},"font_size":0.0,"text":"et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),"},{"bbox":{"x0":140,"x1":1049,"y0":589,"y1":614},"font_size":0.0,"text":"and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh"},{"bbox":{"x0":136,"x1":1051,"y0":612,"y1":646},"font_size":0.0,"text":"et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning"},{"bbox":{"x0":138,"x1":675,"y0":644,"y1":670},"font_size":0.0,"text":"performance comparable to OpenAI‚Äôs o1 series models."}],"source":"layout det","text":"Recently, post-training has emerged as an important component of the full training pipeline.It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI‚Äôs o1 (OpenAI, 2024b) series models were the frst to introduce inference-time scaling by increasing the length of the Chain-of-i Thought reasoning process. This approach has achieved signifcant improvements in variousi reasoning tasks, such as mathematics, coding, and scientifc reasoning. However, the challengei of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI‚Äôs o1 series models."},{"bbox":{"x0":133,"x1":1059,"y0":680,"y1":958},"conf":0.9831,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":684,"y1":709},"font_size":0.0,"text":"In this paper, we take the frst step toward improving language model reasoning capabilitiesi"},{"bbox":{"x0":140,"x1":1049,"y0":712,"y1":737},"font_size":0.0,"text":"using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop"},{"bbox":{"x0":136,"x1":1051,"y0":735,"y1":767},"font_size":0.0,"text":"reasoning capabilities without any supervised data, focusing on their self-evolution through"},{"bbox":{"x0":138,"x1":1049,"y0":767,"y1":791},"font_size":0.0,"text":"a pure RL process. Specifcally, we use DeepSeek-V3-Base as the base model and employi"},{"bbox":{"x0":136,"x1":1053,"y0":788,"y1":823},"font_size":0.0,"text":"GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning."},{"bbox":{"x0":135,"x1":1051,"y0":816,"y1":849},"font_size":0.0,"text":"During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting"},{"bbox":{"x0":138,"x1":1047,"y0":847,"y1":872},"font_size":0.0,"text":"reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance"},{"bbox":{"x0":138,"x1":1049,"y0":873,"y1":898},"font_size":0.0,"text":"on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from $15.6\\%$ to"},{"bbox":{"x0":138,"x1":1049,"y0":899,"y1":926},"font_size":0.0,"text":"$71.0\\%,$ and with majority voting, the score further improves to $86.7\\%,$ matching the performance"},{"bbox":{"x0":140,"x1":330,"y0":928,"y1":953},"font_size":0.0,"text":"of OpenAI-o1-0912."}],"source":"layout det","text":"In this paper, we take the frst step toward improving language model reasoning capabilitiesi using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifcally, we use DeepSeek-V3-Base as the base model and employi GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from $15.6\\%$ to $71.0\\%,$ and with majority voting, the score further improves to $86.7\\%,$ matching the performance of OpenAI-o1-0912."},{"bbox":{"x0":132,"x1":1059,"y0":965,"y1":1271},"conf":0.98,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1051,"y0":965,"y1":998},"font_size":0.0,"text":"However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language"},{"bbox":{"x0":140,"x1":1049,"y0":998,"y1":1023},"font_size":0.0,"text":"mixing. To address these issues and further enhance reasoning performance, we introduce"},{"bbox":{"x0":140,"x1":1049,"y0":1024,"y1":1049},"font_size":0.0,"text":"DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training"},{"bbox":{"x0":136,"x1":1051,"y0":1045,"y1":1079},"font_size":0.0,"text":"pipeline. Specifcally, we begin by collecting thousands of cold-start data to fne-tune theii"},{"bbox":{"x0":136,"x1":1053,"y0":1074,"y1":1105},"font_size":0.0,"text":"DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-"},{"bbox":{"x0":140,"x1":1049,"y0":1105,"y1":1130},"font_size":0.0,"text":"Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection"},{"bbox":{"x0":140,"x1":1049,"y0":1131,"y1":1156},"font_size":0.0,"text":"sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains"},{"bbox":{"x0":140,"x1":1049,"y0":1158,"y1":1182},"font_size":0.0,"text":"such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model."},{"bbox":{"x0":135,"x1":1051,"y0":1182,"y1":1216},"font_size":0.0,"text":"After fne-tuning with the new data, the checkpoint undergoes an additional RL process, takingi"},{"bbox":{"x0":138,"x1":1049,"y0":1214,"y1":1238},"font_size":0.0,"text":"into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to"},{"bbox":{"x0":138,"x1":870,"y0":1238,"y1":1265},"font_size":0.0,"text":"as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217."}],"source":"layout det","text":"However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifcally, we begin by collecting thousands of cold-start data to fne-tune theii DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.After fne-tuning with the new data, the checkpoint undergoes an additional RL process, takingi into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217."},{"bbox":{"x0":132,"x1":1060,"y0":1275,"y1":1475},"conf":0.9774,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":1281,"y1":1305},"font_size":0.0,"text":"We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-"},{"bbox":{"x0":136,"x1":1051,"y0":1302,"y1":1338},"font_size":0.0,"text":"32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying"},{"bbox":{"x0":136,"x1":1053,"y0":1331,"y1":1363},"font_size":0.0,"text":"RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-"},{"bbox":{"x0":138,"x1":1051,"y0":1361,"y1":1391},"font_size":0.0,"text":"cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey"},{"bbox":{"x0":140,"x1":1051,"y0":1389,"y1":1414},"font_size":0.0,"text":"et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source"},{"bbox":{"x0":140,"x1":1051,"y0":1416,"y1":1440},"font_size":0.0,"text":"QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a"},{"bbox":{"x0":138,"x1":755,"y0":1444,"y1":1468},"font_size":0.0,"text":"new record on the reasoning benchmarks among dense models."}],"source":"layout det","text":"We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.532B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models."}],"formula_dets":[{"bbox":{"x0":967,"x1":1025,"y0":873,"y1":896},"conf":0.8045,"label":"print_embedding","label_id":0},{"bbox":{"x0":138,"x1":203,"y0":900,"y1":925},"conf":0.7911,"label":"print_embedding","label_id":0},{"bbox":{"x0":731,"x1":794,"y0":899,"y1":925},"conf":0.7573,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1059,"y0":680,"y1":958},"conf":0.9831,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":965,"y1":1271},"conf":0.98,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":313,"y1":674},"conf":0.9785,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":1275,"y1":1475},"conf":0.9774,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":219,"y1":308},"conf":0.9539,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":331,"y0":166,"y1":200},"conf":0.8888,"label":"Title","label_id":0},{"bbox":{"x0":582,"x1":608,"y0":1551,"y1":1579},"conf":0.6136,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1554],[603,1554],[603,1573],[587,1573]],"score":0.7845},{"poly":[[138,1444],[755,1444],[755,1468],[138,1468]],"score":0.7704},{"poly":[[140,1416],[1051,1416],[1051,1440],[140,1440]],"score":0.7773},{"poly":[[140,1389],[1051,1389],[1051,1414],[140,1414]],"score":0.7522},{"poly":[[138,1361],[1051,1361],[1051,1391],[138,1391]],"score":0.6742},{"poly":[[137,1331],[1053,1333],[1053,1363],[136,1361]],"score":0.7229},{"poly":[[137,1302],[1051,1307],[1051,1338],[136,1333]],"score":0.603},{"poly":[[174,1281],[1051,1281],[1051,1305],[174,1305]],"score":0.7928},{"poly":[[138,1240],[870,1238],[870,1263],[138,1265]],"score":0.8048},{"poly":[[138,1214],[1049,1214],[1049,1238],[138,1238]],"score":0.7695},{"poly":[[135,1182],[1051,1186],[1051,1216],[135,1212]],"score":0.6584},{"poly":[[140,1158],[1049,1158],[1049,1182],[140,1182]],"score":0.7303},{"poly":[[140,1131],[1049,1131],[1049,1156],[140,1156]],"score":0.7578},{"poly":[[140,1105],[1049,1105],[1049,1130],[140,1130]],"score":0.7751},{"poly":[[137,1074],[1053,1075],[1053,1105],[136,1103]],"score":0.7222},{"poly":[[136,1049],[1051,1045],[1051,1075],[137,1079]],"score":0.7059},{"poly":[[140,1024],[1049,1024],[1049,1049],[140,1049]],"score":0.7613},{"poly":[[140,998],[1049,998],[1049,1023],[140,1023]],"score":0.7458},{"poly":[[170,965],[1051,968],[1051,998],[170,995]],"score":0.7261},{"poly":[[140,928],[330,928],[330,953],[140,953]],"score":0.7793},{"poly":[[140,902],[1049,902],[1049,926],[140,926]],"score":0.7589},{"poly":[[138,874],[1049,874],[1049,898],[138,898]],"score":0.7672},{"poly":[[138,847],[1047,847],[1047,872],[138,872]],"score":0.7362},{"poly":[[135,816],[1051,819],[1051,849],[135,845]],"score":0.6631},{"poly":[[137,788],[1053,793],[1053,823],[136,817]],"score":0.6612},{"poly":[[138,767],[1049,767],[1049,791],[138,791]],"score":0.7432},{"poly":[[136,737],[1051,735],[1051,765],[137,767]],"score":0.6998},{"poly":[[140,712],[1049,712],[1049,737],[140,737]],"score":0.7825},{"poly":[[172,684],[1049,684],[1049,709],[172,709]],"score":0.7726},{"poly":[[138,646],[675,644],[675,668],[138,670]],"score":0.8169},{"poly":[[137,612],[1051,616],[1051,646],[136,642]],"score":0.6668},{"poly":[[140,589],[1049,589],[1049,614],[140,614]],"score":0.7281},{"poly":[[140,563],[1051,563],[1051,586],[140,586]],"score":0.8521},{"poly":[[138,537],[1049,537],[1049,561],[138,561]],"score":0.7269},{"poly":[[140,509],[1049,509],[1049,533],[140,533]],"score":0.7274},{"poly":[[138,482],[1049,482],[1049,507],[138,507]],"score":0.7502},{"poly":[[138,453],[1051,453],[1051,482],[138,482]],"score":0.6525},{"poly":[[138,428],[1051,428],[1051,453],[138,453]],"score":0.7663},{"poly":[[135,398],[1053,396],[1053,426],[135,428]],"score":0.7023},{"poly":[[138,374],[1051,374],[1051,398],[138,398]],"score":0.7452},{"poly":[[135,342],[1051,344],[1051,374],[135,372]],"score":0.715},{"poly":[[174,319],[1051,319],[1051,344],[174,344]],"score":0.7962},{"poly":[[136,277],[574,277],[574,300],[136,300]],"score":0.8045},{"poly":[[137,247],[1051,251],[1051,281],[136,277]],"score":0.7331},{"poly":[[138,225],[1049,225],[1049,249],[138,249]],"score":0.7744},{"poly":[[138,172],[324,172],[324,196],[138,196]],"score":0.8621}],"page_no":2,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":608,"y0":1551,"y1":1579},"conf":0.6378,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1556,"y1":1573},"font_size":0.0,"text":"4"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":332,"y0":168,"y1":200},"conf":0.8236,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":328,"y0":172,"y1":198},"font_size":0.0,"text":"1.1. Contributions"}],"source":"layout det","text":"1.1. Contributions"},{"bbox":{"x0":134,"x1":848,"y0":214,"y1":249},"conf":0.8505,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":844,"y0":214,"y1":246},"font_size":0.0,"text":"Post-Training: Large-Scale Reinforcement Learning on the Base Model"}],"source":"layout det","text":"Post-Training: Large-Scale Reinforcement Learning on the Base Model"},{"bbox":{"x0":187,"x1":1059,"y0":253,"y1":472},"conf":0.9731,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":260,"y1":284},"font_size":0.0,"text":"We directly apply RL to the base model without relying on supervised fne-tuning (SFT) asi"},{"bbox":{"x0":191,"x1":1049,"y0":286,"y1":310},"font_size":0.0,"text":"a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for"},{"bbox":{"x0":193,"x1":1051,"y0":312,"y1":337},"font_size":0.0,"text":"solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-"},{"bbox":{"x0":193,"x1":1049,"y0":339,"y1":367},"font_size":0.0,"text":"R1-Zero demonstrates capabilities such as self-verifcation, refection, and generatingil"},{"bbox":{"x0":193,"x1":1047,"y0":368,"y1":391},"font_size":0.0,"text":"long CoTs, marking a signifcant milestone for the research community. Notably, it is thei"},{"bbox":{"x0":193,"x1":1049,"y0":395,"y1":419},"font_size":0.0,"text":"frst open research to validate that reasoning capabilities of LLMs can be incentivizedi"},{"bbox":{"x0":193,"x1":1049,"y0":423,"y1":446},"font_size":0.0,"text":"purely through RL, without the need for SFT. This breakthrough paves the way for future"},{"bbox":{"x0":191,"x1":455,"y0":449,"y1":472},"font_size":0.0,"text":"advancements in this area."}],"source":"layout det","text":"We directly apply RL to the base model without relying on supervised fne-tuning (SFT) asi a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeekR1-Zero demonstrates capabilities such as self-verifcation, refection, and generatingil long CoTs, marking a signifcant milestone for the research community. Notably, it is thei frst open research to validate that reasoning capabilities of LLMs can be incentivizedi purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area."},{"bbox":{"x0":188,"x1":1059,"y0":474,"y1":611},"conf":0.9653,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":1049,"y0":475,"y1":500},"font_size":0.0,"text":"We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL"},{"bbox":{"x0":193,"x1":1051,"y0":503,"y1":528},"font_size":0.0,"text":"stages aimed at discovering improved reasoning patterns and aligning with human pref-"},{"bbox":{"x0":193,"x1":1049,"y0":530,"y1":554},"font_size":0.0,"text":"erences, as well as two SFT stages that serve as the seed for the model‚Äôs reasoning and"},{"bbox":{"x0":190,"x1":1051,"y0":554,"y1":586},"font_size":0.0,"text":"non-reasoning capabilities. We believe the pipeline will beneft the industry by creatingi"},{"bbox":{"x0":193,"x1":335,"y0":584,"y1":607},"font_size":0.0,"text":"better models."}],"source":"layout det","text":"We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model‚Äôs reasoning and non-reasoning capabilities. We believe the pipeline will beneft the industry by creatingi better models."},{"bbox":{"x0":135,"x1":650,"y0":651,"y1":682},"conf":0.9012,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":643,"y0":656,"y1":679},"font_size":0.0,"text":"Distillation: Smaller Models Can Be Powerful Too"}],"source":"layout det","text":"Distillation: Smaller Models Can Be Powerful Too"},{"bbox":{"x0":188,"x1":1058,"y0":690,"y1":802},"conf":0.9606,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1051,"y0":696,"y1":719},"font_size":0.0,"text":"We demonstrate that the reasoning patterns of larger models can be distilled into smaller"},{"bbox":{"x0":191,"x1":1049,"y0":721,"y1":747},"font_size":0.0,"text":"models, resulting in better performance compared to the reasoning patterns discovered"},{"bbox":{"x0":195,"x1":1051,"y0":751,"y1":774},"font_size":0.0,"text":"through RL on small models. The open source DeepSeek-R1, as well as its API, will benefti"},{"bbox":{"x0":195,"x1":860,"y0":777,"y1":802},"font_size":0.0,"text":"the research community to distill better smaller models in the future."}],"source":"layout det","text":"We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefti the research community to distill better smaller models in the future."},{"bbox":{"x0":187,"x1":1061,"y0":804,"y1":1024},"conf":0.9766,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1047,"y0":805,"y1":830},"font_size":0.0,"text":"Using the reasoning data generated by DeepSeek-R1, we fne-tuned several dense modelsi"},{"bbox":{"x0":195,"x1":1051,"y0":833,"y1":856},"font_size":0.0,"text":"that are widely used in the research community. The evaluation results demonstrate that"},{"bbox":{"x0":195,"x1":1049,"y0":860,"y1":882},"font_size":0.0,"text":"the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-"},{"bbox":{"x0":193,"x1":1051,"y0":884,"y1":910},"font_size":0.0,"text":"R1-Distill-Qwen-7B achieves $55.5\\%$ on AIME 2024, surpassing QwQ-32B-Preview. Addi-"},{"bbox":{"x0":195,"x1":1049,"y0":910,"y1":935},"font_size":0.0,"text":"tionally, DeepSeek-R1-Distill-Qwen-32B scores $72.6\\%$ on AIME 2024, $94.3\\%$ on MATH-500,"},{"bbox":{"x0":191,"x1":1051,"y0":938,"y1":965},"font_size":0.0,"text":"and $57.2\\%$ on LiveCodeBench. These results signifcantly outperform previous open-i"},{"bbox":{"x0":193,"x1":1053,"y0":967,"y1":991},"font_size":0.0,"text":"source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,"},{"bbox":{"x0":193,"x1":980,"y0":993,"y1":1016},"font_size":0.0,"text":"32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community."}],"source":"layout det","text":"Using the reasoning data generated by DeepSeek-R1, we fne-tuned several dense modelsi that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeekR1-Distill-Qwen-7B achieves $55.5\\%$ on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores $72.6\\%$ on AIME 2024, $94.3\\%$ on MATH-500,and $57.2\\%$ on LiveCodeBench. These results signifcantly outperform previous open-i source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community."},{"bbox":{"x0":134,"x1":507,"y0":1055,"y1":1090},"conf":0.905,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":502,"y0":1061,"y1":1084},"font_size":0.0,"text":"1.2. Summary of Evaluation Results"}],"source":"layout det","text":"1.2. Summary of Evaluation Results"},{"bbox":{"x0":188,"x1":1059,"y0":1099,"y1":1292},"conf":0.9736,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":168,"x1":1049,"y0":1100,"y1":1130},"font_size":0.0,"text":"Reasoning tasks: (1) DeepSeek-R1 achieves a score of $79.8\\%$ Pass@1 on AIME 2024, slightly"},{"bbox":{"x0":190,"x1":1053,"y0":1128,"y1":1160},"font_size":0.0,"text":"surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of $97.3\\%,$"},{"bbox":{"x0":193,"x1":1049,"y0":1158,"y1":1183},"font_size":0.0,"text":"performing on par with OpenAI-o1-1217 and signifcantly outperforming other models. (2)i"},{"bbox":{"x0":193,"x1":1051,"y0":1188,"y1":1210},"font_size":0.0,"text":"On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,"},{"bbox":{"x0":193,"x1":1049,"y0":1211,"y1":1238},"font_size":0.0,"text":"as it achieves 2,029 Elo rating on Codeforces outperforming $96.3\\%$ human participants in"},{"bbox":{"x0":195,"x1":1049,"y0":1240,"y1":1265},"font_size":0.0,"text":"the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than"},{"bbox":{"x0":193,"x1":805,"y0":1268,"y1":1291},"font_size":0.0,"text":"DeepSeek-V3, which could help developers in real world tasks."}],"source":"layout det","text":"Reasoning tasks: (1) DeepSeek-R1 achieves a score of $79.8\\%$ Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of $97.3\\%,$ performing on par with OpenAI-o1-1217 and signifcantly outperforming other models. (2)i On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,as it achieves 2,029 Elo rating on Codeforces outperforming $96.3\\%$ human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks."},{"bbox":{"x0":187,"x1":1059,"y0":1294,"y1":1515},"conf":0.9745,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":190,"x1":1051,"y0":1293,"y1":1319},"font_size":0.0,"text":"Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-"},{"bbox":{"x0":191,"x1":1049,"y0":1319,"y1":1346},"font_size":0.0,"text":"R1 achieves outstanding results, signifcantly outperforming DeepSeek-V3 with scoresi"},{"bbox":{"x0":193,"x1":1049,"y0":1346,"y1":1372},"font_size":0.0,"text":"of $90.8\\%$ on MMLU, $84.0\\%$ on MMLU-Pro, and $71.5\\%$ on GPQA Diamond. While its"},{"bbox":{"x0":193,"x1":1049,"y0":1377,"y1":1400},"font_size":0.0,"text":"performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1"},{"bbox":{"x0":193,"x1":1049,"y0":1403,"y1":1426},"font_size":0.0,"text":"surpasses other closed-source models, demonstrating its competitive edge in educational"},{"bbox":{"x0":195,"x1":1051,"y0":1430,"y1":1452},"font_size":0.0,"text":"tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,"},{"bbox":{"x0":193,"x1":1049,"y0":1454,"y1":1481},"font_size":0.0,"text":"demonstrating its capability in handling fact-based queries. A similar trend is observed"},{"bbox":{"x0":193,"x1":684,"y0":1486,"y1":1509},"font_size":0.0,"text":"where OpenAI-o1 surpasses 4o on this benchmark."}],"source":"layout det","text":"Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeekR1 achieves outstanding results, signifcantly outperforming DeepSeek-V3 with scoresi of $90.8\\%$ on MMLU, $84.0\\%$ on MMLU-Pro, and $71.5\\%$ on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark."}],"formula_dets":[{"bbox":{"x0":221,"x1":282,"y0":1346,"y1":1370},"conf":0.7922,"label":"print_embedding","label_id":0},{"bbox":{"x0":771,"x1":830,"y0":1211,"y1":1235},"conf":0.7921,"label":"print_embedding","label_id":0},{"bbox":{"x0":985,"x1":1053,"y0":1130,"y1":1154},"conf":0.7821,"label":"print_embedding","label_id":0},{"bbox":{"x0":239,"x1":300,"y0":938,"y1":961},"conf":0.7815,"label":"print_embedding","label_id":0},{"bbox":{"x0":404,"x1":466,"y0":1346,"y1":1370},"conf":0.7669,"label":"print_embedding","label_id":0},{"bbox":{"x0":477,"x1":537,"y0":884,"y1":907},"conf":0.7496,"label":"print_embedding","label_id":0},{"bbox":{"x0":640,"x1":699,"y0":911,"y1":934},"conf":0.7376,"label":"print_embedding","label_id":0},{"bbox":{"x0":674,"x1":736,"y0":1346,"y1":1370},"conf":0.73,"label":"print_embedding","label_id":0},{"bbox":{"x0":700,"x1":759,"y0":1100,"y1":1126},"conf":0.629,"label":"print_embedding","label_id":0},{"bbox":{"x0":847,"x1":906,"y0":910,"y1":934},"conf":0.5717,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":187,"x1":1061,"y0":804,"y1":1024},"conf":0.9766,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1059,"y0":1294,"y1":1515},"conf":0.9745,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1059,"y0":1099,"y1":1292},"conf":0.9736,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1059,"y0":253,"y1":472},"conf":0.9731,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1059,"y0":474,"y1":611},"conf":0.9653,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1058,"y0":690,"y1":802},"conf":0.9606,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":507,"y0":1055,"y1":1090},"conf":0.905,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":650,"y0":651,"y1":682},"conf":0.9012,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":848,"y0":214,"y1":249},"conf":0.8505,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":332,"y0":168,"y1":200},"conf":0.8236,"label":"Title","label_id":0},{"bbox":{"x0":582,"x1":608,"y0":1551,"y1":1579},"conf":0.6378,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1556],[603,1556],[603,1573],[587,1573]],"score":0.8691},{"poly":[[193,1486],[684,1486],[684,1509],[193,1509]],"score":0.8879},{"poly":[[193,1456],[1049,1454],[1049,1479],[193,1481]],"score":0.7938},{"poly":[[195,1430],[1051,1430],[1051,1452],[195,1452]],"score":0.8842},{"poly":[[193,1403],[1049,1403],[1049,1426],[193,1426]],"score":0.8468},{"poly":[[193,1377],[1049,1377],[1049,1400],[193,1400]],"score":0.8356},{"poly":[[193,1349],[1049,1349],[1049,1372],[193,1372]],"score":0.8366},{"poly":[[191,1319],[1049,1321],[1049,1345],[191,1344]],"score":0.7606},{"poly":[[190,1295],[1051,1293],[1051,1317],[190,1319]],"score":0.752},{"poly":[[193,1268],[805,1268],[805,1291],[193,1291]],"score":0.8723},{"poly":[[195,1240],[1049,1240],[1049,1265],[195,1265]],"score":0.7327},{"poly":[[193,1214],[1049,1214],[1049,1238],[193,1238]],"score":0.7467},{"poly":[[193,1188],[1051,1188],[1051,1210],[193,1210]],"score":0.8701},{"poly":[[193,1158],[1049,1158],[1049,1182],[193,1182]],"score":0.7308},{"poly":[[190,1130],[1053,1128],[1053,1158],[190,1160]],"score":0.7013},{"poly":[[168,1102],[1049,1105],[1049,1130],[168,1126]],"score":0.722},{"poly":[[140,1061],[502,1061],[502,1084],[140,1084]],"score":0.9093},{"poly":[[193,993],[980,993],[980,1016],[193,1016]],"score":0.8132},{"poly":[[193,967],[1053,967],[1053,991],[193,991]],"score":0.7401},{"poly":[[191,938],[1051,940],[1051,965],[191,963]],"score":0.7864},{"poly":[[195,912],[1049,912],[1049,935],[195,935]],"score":0.8247},{"poly":[[193,886],[1051,886],[1051,910],[193,910]],"score":0.7322},{"poly":[[195,860],[1049,860],[1049,882],[195,882]],"score":0.8452},{"poly":[[195,833],[1051,833],[1051,856],[195,856]],"score":0.8381},{"poly":[[186,805],[1047,805],[1047,830],[186,830]],"score":0.7576},{"poly":[[195,777],[860,777],[860,802],[195,802]],"score":0.7135},{"poly":[[195,751],[1051,751],[1051,774],[195,774]],"score":0.844},{"poly":[[191,723],[1049,721],[1049,746],[191,747]],"score":0.7634},{"poly":[[170,696],[1051,696],[1051,719],[170,719]],"score":0.843},{"poly":[[140,656],[643,656],[643,679],[140,679]],"score":0.833},{"poly":[[193,584],[335,584],[335,607],[193,607]],"score":0.9007},{"poly":[[190,554],[1051,556],[1051,586],[190,584]],"score":0.6886},{"poly":[[193,530],[1049,530],[1049,554],[193,554]],"score":0.7369},{"poly":[[193,503],[1051,503],[1051,528],[193,528]],"score":0.7606},{"poly":[[188,475],[1049,475],[1049,500],[188,500]],"score":0.7412},{"poly":[[191,449],[455,449],[455,472],[191,472]],"score":0.8401},{"poly":[[193,423],[1049,423],[1049,446],[193,446]],"score":0.8459},{"poly":[[193,395],[1049,395],[1049,419],[193,419]],"score":0.7194},{"poly":[[193,368],[1047,368],[1047,391],[193,391]],"score":0.8293},{"poly":[[193,339],[1049,342],[1049,367],[193,363]],"score":0.7293},{"poly":[[193,312],[1051,312],[1051,337],[193,337]],"score":0.7296},{"poly":[[191,286],[1049,286],[1049,310],[191,310]],"score":0.7786},{"poly":[[172,260],[1049,260],[1049,284],[172,284]],"score":0.7517},{"poly":[[136,216],[844,214],[844,244],[137,246]],"score":0.7395},{"poly":[[137,172],[328,174],[328,198],[136,196]],"score":0.7904}],"page_no":3,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.5571,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":578,"x1":610,"y0":1546,"y1":1580},"font_size":0.0,"text":"5"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":187,"x1":1062,"y0":167,"y1":365},"conf":0.9768,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":168,"x1":1053,"y0":168,"y1":202},"font_size":0.0,"text":"Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,"},{"bbox":{"x0":191,"x1":1049,"y0":198,"y1":226},"font_size":0.0,"text":"general question answering, editing, summarization, and more. It achieves an impressive"},{"bbox":{"x0":195,"x1":1051,"y0":226,"y1":251},"font_size":0.0,"text":"length-controlled win-rate of $87.6\\%$ on AlpacaEval 2.0 and a win-rate of $92.3\\%$  on Are-"},{"bbox":{"x0":193,"x1":1051,"y0":254,"y1":279},"font_size":0.0,"text":"naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries."},{"bbox":{"x0":190,"x1":1051,"y0":277,"y1":309},"font_size":0.0,"text":"Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring"},{"bbox":{"x0":193,"x1":1049,"y0":309,"y1":333},"font_size":0.0,"text":"long-context understanding, substantially outperforming DeepSeek-V3 on long-context"},{"bbox":{"x0":193,"x1":321,"y0":335,"y1":360},"font_size":0.0,"text":"benchmarks."}],"source":"layout det","text":"Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of $87.6\\%$ on AlpacaEval 2.0 and a win-rate of $92.3\\%$  on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks."},{"bbox":{"x0":133,"x1":298,"y0":395,"y1":433},"conf":0.8971,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":292,"y0":398,"y1":428},"font_size":0.0,"text":"2. Approach"}],"source":"layout det","text":"2.Approach"},{"bbox":{"x0":134,"x1":292,"y0":451,"y1":482},"conf":0.8854,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":285,"y0":454,"y1":479},"font_size":0.0,"text":"2.1. Overview"}],"source":"layout det","text":"2.1. Overview"},{"bbox":{"x0":134,"x1":1059,"y0":495,"y1":744},"conf":0.9756,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":500,"y1":524},"font_size":0.0,"text":"Previous work has heavily relied on large amounts of supervised data to enhance model"},{"bbox":{"x0":138,"x1":1049,"y0":528,"y1":553},"font_size":0.0,"text":"performance. In this study, we demonstrate that reasoning capabilities can be signifcantlyi"},{"bbox":{"x0":135,"x1":1051,"y0":551,"y1":582},"font_size":0.0,"text":"improved through large-scale reinforcement learning (RL), even without using supervised"},{"bbox":{"x0":138,"x1":1049,"y0":581,"y1":606},"font_size":0.0,"text":"fne-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced withi"},{"bbox":{"x0":138,"x1":1051,"y0":609,"y1":633},"font_size":0.0,"text":"the inclusion of a small amount of cold-start data. In the following sections, we present: (1)"},{"bbox":{"x0":140,"x1":1049,"y0":635,"y1":660},"font_size":0.0,"text":"DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and"},{"bbox":{"x0":140,"x1":1051,"y0":663,"y1":688},"font_size":0.0,"text":"(2) DeepSeek-R1, which applies RL starting from a checkpoint fne-tuned with thousands ofi"},{"bbox":{"x0":140,"x1":1047,"y0":689,"y1":714},"font_size":0.0,"text":"long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to"},{"bbox":{"x0":140,"x1":340,"y0":717,"y1":740},"font_size":0.0,"text":"small dense models."}],"source":"layout det","text":"Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be signifcantlyi improved through large-scale reinforcement learning (RL), even without using supervised fne-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced withi the inclusion of a small amount of cold-start data. In the following sections, we present: (1)DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and(2) DeepSeek-R1, which applies RL starting from a checkpoint fne-tuned with thousands ofi long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models."},{"bbox":{"x0":134,"x1":831,"y0":774,"y1":809},"conf":0.8849,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":826,"y0":779,"y1":803},"font_size":0.0,"text":"2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model"}],"source":"layout det","text":"2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model"},{"bbox":{"x0":133,"x1":1059,"y0":820,"y1":1018},"conf":0.9739,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":824,"y1":849},"font_size":0.0,"text":"Reinforcement learning has demonstrated signifcant effectiveness in reasoning tasks, as ev-i"},{"bbox":{"x0":140,"x1":1049,"y0":851,"y1":875},"font_size":0.0,"text":"idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works"},{"bbox":{"x0":138,"x1":1049,"y0":879,"y1":903},"font_size":0.0,"text":"heavily depended on supervised data, which are time-intensive to gather. In this section, we"},{"bbox":{"x0":140,"x1":1051,"y0":905,"y1":930},"font_size":0.0,"text":"explore the potential of LLMs to develop reasoning capabilities without any supervised data,"},{"bbox":{"x0":138,"x1":1047,"y0":931,"y1":956},"font_size":0.0,"text":"focusing on their self-evolution through a pure reinforcement learning process. We start with a"},{"bbox":{"x0":136,"x1":1049,"y0":958,"y1":984},"font_size":0.0,"text":"brief overview of our RL algorithm, followed by the presentation of some exciting results, and"},{"bbox":{"x0":138,"x1":697,"y0":988,"y1":1012},"font_size":0.0,"text":"hope this provides the community with valuable insights."}],"source":"layout det","text":"Reinforcement learning has demonstrated signifcant effectiveness in reasoning tasks, as ev-i idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data,focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights."},{"bbox":{"x0":133,"x1":545,"y0":1041,"y1":1075},"conf":0.9066,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":541,"y0":1045,"y1":1068},"font_size":0.0,"text":"2.2.1. Reinforcement Learning Algorithm"}],"source":"layout det","text":"2.2.1. Reinforcement Learning Algorithm"},{"bbox":{"x0":133,"x1":1059,"y0":1085,"y1":1229},"conf":0.9742,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1084,"y1":1119},"font_size":0.0,"text":"Group Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group"},{"bbox":{"x0":140,"x1":1049,"y0":1117,"y1":1140},"font_size":0.0,"text":"Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is"},{"bbox":{"x0":140,"x1":1049,"y0":1144,"y1":1168},"font_size":0.0,"text":"typically the same size as the policy model, and estimates the baseline from group scores instead."},{"bbox":{"x0":140,"x1":1047,"y0":1170,"y1":1197},"font_size":0.0,"text":"Specifcally, for each questioni $q,$  GRPO samples a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old"},{"bbox":{"x0":135,"x1":1016,"y0":1195,"y1":1226},"font_size":0.0,"text":"policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:"}],"source":"layout det","text":"Group Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.Specifcally, for each questioni $q,$  GRPO samples a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:"},{"bbox":{"x0":195,"x1":602,"y0":1233,"y1":1265},"conf":0.8363,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\texttt{GRPO}}(\\theta)=\\mathbb{E}[q\\sim P(\\texttt{Q}),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\texttt{O}|q)]$$"},{"bbox":{"x0":278,"x1":996,"y0":1273,"y1":1336},"conf":0.8818,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\min\\left(\\frac{\\pi_{\\theta}(o_{i}|q)}{\\pi_{\\theta_{old}}(o_{i}|q)}A_{i},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{i}|q)}{\\pi_{\\theta_{old}}(o_{i}|q)},1-\\varepsilon,1+\\varepsilon\\right)A_{i}\\right)-\\beta\\text{I D}_{KL}\\left(\\pi_{\\theta}||\\pi_{ref}\\right)\\right),$$"},{"bbox":{"x0":1019,"x1":1056,"y0":1271,"y1":1303},"conf":0.7972,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1053,"y0":1274,"y1":1305},"font_size":0.0,"text":"(1)"}],"source":"layout det","text":"(1)"},{"bbox":{"x0":376,"x1":813,"y0":1347,"y1":1406},"conf":0.9442,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathbb{D}_{\\textit{KL}}\\left(\\pi_{\\theta}||\\pi_{\\textit{ref}}\\right)=\\frac{\\pi_{\\textit{ref}}(o_{i}|q)}{\\pi_{\\theta}(o_{i}|q)}-\\log\\frac{\\pi_{\\textit{ref}}(o_{i}|q)}{\\pi_{\\theta}(o_{i}|q)}-1,$$"},{"bbox":{"x0":1021,"x1":1055,"y0":1363,"y1":1392},"conf":0.8257,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1019,"x1":1056,"y0":1361,"y1":1393},"font_size":0.0,"text":"(2)"}],"source":"layout det","text":"(2)"},{"bbox":{"x0":133,"x1":1058,"y0":1408,"y1":1468},"conf":0.9349,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":1409,"y1":1438},"font_size":0.0,"text":"where ùúÄand $\\beta$ are hyper-parameters, and $A_{i}$ is the advantage, computed using a group of"},{"bbox":{"x0":136,"x1":831,"y0":1433,"y1":1467},"font_size":0.0,"text":"rewards $\\{r_{1},r_{2},\\ldots,r_{G}\\}$  corresponding to the outputs within each group:"}],"source":"layout det","text":"where ùúÄand $\\beta$ are hyper-parameters, and $A_{i}$ is the advantage, computed using a group of rewards $\\{r_{1},r_{2},\\ldots,r_{G}\\}$  corresponding to the outputs within each group:"},{"bbox":{"x0":447,"x1":741,"y0":1471,"y1":1528},"conf":0.9313,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$A_{i}=\\frac{r_{i}-\\mathrm{m}ean(\\{r_{1},r_{2},\\cdots,r_{G}\\})}{\\mathrm{s}td(\\{r_{1},r_{2},\\cdots,r_{G}\\})}.$$"},{"bbox":{"x0":1020,"x1":1055,"y0":1483,"y1":1514},"conf":0.8293,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1056,"y0":1482,"y1":1516},"font_size":0.0,"text":"(3)"}],"source":"layout det","text":"(3)"}],"formula_dets":[{"bbox":{"x0":376,"x1":813,"y0":1347,"y1":1406},"conf":0.9442,"label":"print_isolated","label_id":1},{"bbox":{"x0":447,"x1":741,"y0":1471,"y1":1528},"conf":0.9313,"label":"print_isolated","label_id":1},{"bbox":{"x0":785,"x1":923,"y0":1172,"y1":1196},"conf":0.8931,"label":"print_embedding","label_id":0},{"bbox":{"x0":278,"x1":996,"y0":1273,"y1":1336},"conf":0.8818,"label":"print_isolated","label_id":1},{"bbox":{"x0":226,"x1":353,"y0":1439,"y1":1462},"conf":0.8812,"label":"print_embedding","label_id":0},{"bbox":{"x0":564,"x1":586,"y0":1415,"y1":1434},"conf":0.847,"label":"print_embedding","label_id":0},{"bbox":{"x0":195,"x1":602,"y0":1233,"y1":1265},"conf":0.8363,"label":"print_isolated","label_id":1},{"bbox":{"x0":911,"x1":971,"y0":226,"y1":249},"conf":0.825,"label":"print_embedding","label_id":0},{"bbox":{"x0":485,"x1":546,"y0":226,"y1":249},"conf":0.8072,"label":"print_embedding","label_id":0},{"bbox":{"x0":608,"x1":631,"y0":1207,"y1":1222},"conf":0.768,"label":"print_embedding","label_id":0},{"bbox":{"x0":205,"x1":244,"y0":1207,"y1":1225},"conf":0.7626,"label":"print_embedding","label_id":0},{"bbox":{"x0":270,"x1":284,"y0":1414,"y1":1435},"conf":0.749,"label":"print_embedding","label_id":0},{"bbox":{"x0":428,"x1":444,"y0":1179,"y1":1197},"conf":0.5493,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":187,"x1":1062,"y0":167,"y1":365},"conf":0.9768,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1059,"y0":495,"y1":744},"conf":0.9756,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":1085,"y1":1229},"conf":0.9742,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":820,"y1":1018},"conf":0.9739,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":1408,"y1":1468},"conf":0.9349,"label":"Text","label_id":1},{"bbox":{"x0":442,"x1":747,"y0":1470,"y1":1532},"conf":0.9344,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":545,"y0":1041,"y1":1075},"conf":0.9066,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":298,"y0":395,"y1":433},"conf":0.8971,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":292,"y0":451,"y1":482},"conf":0.8854,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":831,"y0":774,"y1":809},"conf":0.8849,"label":"Title","label_id":0},{"bbox":{"x0":371,"x1":818,"y0":1344,"y1":1408},"conf":0.872,"label":"Equation","label_id":8},{"bbox":{"x0":1020,"x1":1055,"y0":1483,"y1":1514},"conf":0.8293,"label":"Equation caption","label_id":9},{"bbox":{"x0":187,"x1":1003,"y0":1232,"y1":1344},"conf":0.8259,"label":"Equation","label_id":8},{"bbox":{"x0":1021,"x1":1055,"y0":1363,"y1":1392},"conf":0.8257,"label":"Equation caption","label_id":9},{"bbox":{"x0":1019,"x1":1056,"y0":1271,"y1":1303},"conf":0.7972,"label":"Equation caption","label_id":9},{"bbox":{"x0":273,"x1":1003,"y0":1232,"y1":1341},"conf":0.7906,"label":"Equation","label_id":8},{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.5571,"label":"Abandon","label_id":2},{"bbox":{"x0":188,"x1":607,"y0":1231,"y1":1267},"conf":0.3696,"label":"Equation","label_id":8},{"bbox":{"x0":189,"x1":1007,"y0":1231,"y1":1407},"conf":0.3548,"label":"Equation","label_id":8},{"bbox":{"x0":584,"x1":605,"y0":1552,"y1":1578},"conf":0.2225,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[578,1554],[600,1546],[610,1571],[587,1580]],"score":0.6018},{"poly":[[518,1498],[706,1502],[705,1532],[517,1528]],"score":0.7643},{"poly":[[1021,1482],[1056,1482],[1056,1516],[1021,1516]],"score":0.8376},{"poly":[[137,1433],[831,1437],[831,1467],[136,1463]],"score":0.7085},{"poly":[[138,1409],[1053,1409],[1053,1438],[138,1438]],"score":0.7025},{"poly":[[537,1377],[620,1377],[620,1409],[537,1409]],"score":0.7699},{"poly":[[682,1373],[770,1380],[767,1411],[680,1404]],"score":0.8124},{"poly":[[644,1359],[690,1374],[682,1398],[636,1383]],"score":0.7388},{"poly":[[373,1359],[520,1363],[519,1393],[372,1389]],"score":0.8424},{"poly":[[1019,1361],[1056,1361],[1056,1393],[1019,1393]],"score":0.8971},{"poly":[[789,1363],[815,1363],[815,1389],[789,1389]],"score":0.7332},{"poly":[[679,1347],[775,1347],[775,1377],[679,1377]],"score":0.9053},{"poly":[[528,1347],[624,1347],[624,1377],[528,1377]],"score":0.8297},{"poly":[[388,1305],[491,1305],[491,1337],[388,1337]],"score":0.7197},{"poly":[[558,1303],[664,1301],[665,1333],[559,1335]],"score":0.798},{"poly":[[275,1295],[303,1295],[303,1331],[275,1331]],"score":0.6921},{"poly":[[486,1283],[559,1290],[556,1323],[483,1316]],"score":0.7188},{"poly":[[339,1291],[385,1291],[385,1317],[339,1317]],"score":0.9157},{"poly":[[771,1289],[815,1289],[815,1319],[771,1319]],"score":0.6729},{"poly":[[665,1288],[783,1288],[783,1319],[665,1319]],"score":0.7115},{"poly":[[799,1281],[998,1281],[998,1323],[799,1323]],"score":0.8696},{"poly":[[572,1277],[656,1277],[656,1303],[572,1303]],"score":0.9167},{"poly":[[399,1277],[482,1277],[482,1303],[399,1303]],"score":0.9288},{"poly":[[1021,1274],[1053,1274],[1053,1305],[1021,1305]],"score":0.8079},{"poly":[[278,1275],[300,1275],[300,1303],[278,1303]],"score":0.6498},{"poly":[[294,1265],[335,1265],[335,1345],[294,1345]],"score":0.6275},{"poly":[[193,1235],[604,1235],[604,1265],[193,1265]],"score":0.7635},{"poly":[[135,1196],[1016,1195],[1016,1224],[135,1226]],"score":0.7076},{"poly":[[140,1170],[1047,1170],[1047,1195],[140,1195]],"score":0.7692},{"poly":[[140,1144],[1049,1144],[1049,1168],[140,1168]],"score":0.7302},{"poly":[[140,1117],[1049,1117],[1049,1140],[140,1140]],"score":0.8987},{"poly":[[137,1084],[1051,1089],[1051,1119],[136,1114]],"score":0.6822},{"poly":[[140,1045],[541,1045],[541,1068],[140,1068]],"score":0.9533},{"poly":[[138,988],[697,988],[697,1012],[138,1012]],"score":0.7675},{"poly":[[137,958],[1049,960],[1049,984],[136,982]],"score":0.8195},{"poly":[[138,931],[1047,931],[1047,956],[138,956]],"score":0.7249},{"poly":[[140,905],[1051,905],[1051,930],[140,930]],"score":0.7454},{"poly":[[138,879],[1049,879],[1049,903],[138,903]],"score":0.7747},{"poly":[[140,851],[1049,851],[1049,875],[140,875]],"score":0.7418},{"poly":[[140,824],[1051,824],[1051,849],[140,849]],"score":0.7746},{"poly":[[140,779],[826,779],[826,803],[140,803]],"score":0.7521},{"poly":[[140,717],[340,717],[340,740],[140,740]],"score":0.8657},{"poly":[[140,689],[1047,689],[1047,714],[140,714]],"score":0.8048},{"poly":[[140,663],[1051,663],[1051,688],[140,688]],"score":0.7419},{"poly":[[140,635],[1049,635],[1049,660],[140,660]],"score":0.7629},{"poly":[[138,609],[1051,609],[1051,633],[138,633]],"score":0.7758},{"poly":[[138,581],[1049,581],[1049,605],[138,605]],"score":0.7411},{"poly":[[135,551],[1051,553],[1051,582],[135,581]],"score":0.6878},{"poly":[[138,528],[1049,528],[1049,553],[138,553]],"score":0.7622},{"poly":[[140,500],[1049,500],[1049,524],[140,524]],"score":0.777},{"poly":[[138,454],[285,454],[285,479],[138,479]],"score":0.8256},{"poly":[[136,398],[292,398],[292,428],[136,428]],"score":0.8431},{"poly":[[193,335],[321,335],[321,360],[193,360]],"score":0.8758},{"poly":[[193,309],[1049,309],[1049,333],[193,333]],"score":0.791},{"poly":[[190,277],[1051,279],[1051,309],[190,307]],"score":0.6682},{"poly":[[193,254],[1051,254],[1051,279],[193,279]],"score":0.7225},{"poly":[[195,228],[1051,228],[1051,251],[195,251]],"score":0.8253},{"poly":[[191,202],[1049,198],[1049,223],[191,226]],"score":0.7198},{"poly":[[168,168],[1053,172],[1053,202],[168,198]],"score":0.7091}],"page_no":4,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":154,"x1":1040,"y0":172,"y1":302},"conf":0.8602,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":167,"x1":982,"y0":202,"y1":226},"font_size":0.0,"text":"The assistant frst thinks about the reasoning process in the mind and then provides the user"},{"bbox":{"x0":165,"x1":1024,"y0":177,"y1":200},"font_size":0.0,"text":"A conversation between User and Assistant. The user asks a question, and the Assistant solves it."},{"bbox":{"x0":299,"x1":299,"y0":204,"y1":224},"font_size":9.0,"text":"i"},{"bbox":{"x0":165,"x1":372,"y0":274,"y1":296},"font_size":0.0,"text":"<answer> answer here"},{"bbox":{"x0":394,"x1":702,"y0":274,"y1":296},"font_size":10.0,"text":"answer>. User: prompt. Assistant:"},{"bbox":{"x0":372,"x1":394,"y0":275,"y1":293},"font_size":0.0,"text":" $</$ "},{"bbox":{"x0":167,"x1":259,"y0":249,"y1":272},"font_size":0.0,"text":"<answer>"},{"bbox":{"x0":259,"x1":283,"y0":250,"y1":270},"font_size":0.0,"text":" $</$ "},{"bbox":{"x0":283,"x1":840,"y0":249,"y1":272},"font_size":0.0,"text":"answer> tags, respectively, i.e., <think> reasoning process here"},{"bbox":{"x0":864,"x1":923,"y0":249,"y1":272},"font_size":10.0,"text":"think>"},{"bbox":{"x0":840,"x1":864,"y0":251,"y1":270},"font_size":0.0,"text":" $</$ "},{"bbox":{"x0":167,"x1":877,"y0":225,"y1":249},"font_size":0.0,"text":"with the answer. The reasoning process and answer are enclosed within <think>"},{"bbox":{"x0":902,"x1":1003,"y0":225,"y1":249},"font_size":10.0,"text":"think> and"},{"bbox":{"x0":877,"x1":902,"y0":226,"y1":246},"font_size":0.0,"text":" $</$ "}],"source":"layout det","text":""},{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.6175,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1556,"y1":1573},"font_size":0.0,"text":"6"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":139,"x1":1052,"y0":317,"y1":382},"conf":0.894,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":142,"x1":1047,"y0":319,"y1":353},"font_size":2.016,"text":"Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specifc reasoningi"},{"bbox":{"x0":471,"x1":718,"y0":351,"y1":381},"font_size":2.016,"text":"question during training."}],"source":"layout det","text":"Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specifc reasoningi question during training."},{"bbox":{"x0":133,"x1":385,"y0":415,"y1":452},"conf":0.9059,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":379,"y0":416,"y1":449},"font_size":2.016,"text":"2.2.2. Reward Modeling"}],"source":"layout det","text":"2.2.2. Reward Modeling"},{"bbox":{"x0":133,"x1":1058,"y0":458,"y1":551},"conf":0.9556,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1053,"y0":460,"y1":491},"font_size":2.016,"text":"The reward is the source of the training signal, which decides the optimization direction of RL."},{"bbox":{"x0":140,"x1":1049,"y0":493,"y1":516},"font_size":2.016,"text":"To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two"},{"bbox":{"x0":140,"x1":312,"y0":519,"y1":546},"font_size":2.016,"text":"types of rewards:"}],"source":"layout det","text":"The reward is the source of the training signal, which decides the optimization direction of RL.To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:"},{"bbox":{"x0":188,"x1":1059,"y0":555,"y1":693},"conf":0.9626,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":558,"y1":582},"font_size":2.016,"text":"Accuracy rewards: The accuracy reward model evaluates whether the response is correct."},{"bbox":{"x0":193,"x1":1049,"y0":588,"y1":610},"font_size":2.016,"text":"For example, in the case of math problems with deterministic results, the model is required"},{"bbox":{"x0":195,"x1":1047,"y0":614,"y1":638},"font_size":2.016,"text":"to provide the fnal answer in a specifed format (e.g., within a box), enabling reliableii"},{"bbox":{"x0":193,"x1":1049,"y0":642,"y1":667},"font_size":2.016,"text":"rule-based verifcation of correctness. Similarly, for LeetCode problems, a compiler can bei"},{"bbox":{"x0":195,"x1":748,"y0":670,"y1":693},"font_size":2.016,"text":"used to generate feedback based on predefned test cases.i"}],"source":"layout det","text":"Accuracy rewards: The accuracy reward model evaluates whether the response is correct.For example, in the case of math problems with deterministic results, the model is required to provide the fnal answer in a specifed format (e.g., within a box), enabling reliableii rule-based verifcation of correctness. Similarly, for LeetCode problems, a compiler can bei used to generate feedback based on predefned test cases.i"},{"bbox":{"x0":187,"x1":1059,"y0":695,"y1":778},"conf":0.9503,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":179,"x1":1049,"y0":695,"y1":719},"font_size":2.016,"text":"Format rewards: In addition to the accuracy reward model, we employ a format reward"},{"bbox":{"x0":191,"x1":1051,"y0":721,"y1":747},"font_size":2.016,"text":"model that enforces the model to put its thinking process between ‚Äò<think>‚Äô and $'<1$ think>‚Äô"},{"bbox":{"x0":191,"x1":245,"y0":751,"y1":777},"font_size":2.016,"text":"tags."}],"source":"layout det","text":"Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‚Äò<think>‚Äô and $'<1$ think>‚Äôtags."},{"bbox":{"x0":132,"x1":1059,"y0":785,"y1":904},"conf":0.7942,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":791,"y1":816},"font_size":2.016,"text":"We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,"},{"bbox":{"x0":138,"x1":1049,"y0":816,"y1":842},"font_size":2.016,"text":"because we fnd that the neural reward model may suffer from reward hacking in the large-scalei"},{"bbox":{"x0":136,"x1":1051,"y0":840,"y1":874},"font_size":2.016,"text":"reinforcement learning process, and retraining the reward model needs additional training"},{"bbox":{"x0":136,"x1":686,"y0":866,"y1":900},"font_size":2.016,"text":"resources and it complicates the whole training pipeline."}],"source":"layout det","text":"We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,because we fnd that the neural reward model may suffer from reward hacking in the large-scalei reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline."},{"bbox":{"x0":133,"x1":385,"y0":926,"y1":963},"conf":0.9057,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":376,"y0":931,"y1":954},"font_size":2.016,"text":"2.2.3. Training Template"}],"source":"layout det","text":"2.2.3. Training Template"},{"bbox":{"x0":132,"x1":1060,"y0":972,"y1":1171},"conf":0.9752,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":977,"y1":1002},"font_size":2.016,"text":"To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides"},{"bbox":{"x0":140,"x1":1049,"y0":1005,"y1":1030},"font_size":2.016,"text":"the base model to adhere to our specifed instructions. As depicted in Table 1, this templatei"},{"bbox":{"x0":140,"x1":1051,"y0":1031,"y1":1056},"font_size":2.016,"text":"requires DeepSeek-R1-Zero to frst produce a reasoning process, followed by the fnal answer.ii"},{"bbox":{"x0":138,"x1":1049,"y0":1060,"y1":1084},"font_size":2.016,"text":"We intentionally limit our constraints to this structural format, avoiding any content-specifci"},{"bbox":{"x0":138,"x1":1051,"y0":1088,"y1":1112},"font_size":2.016,"text":"biases‚Äîsuch as mandating refective reasoning or promoting particular problem-solving strate-l"},{"bbox":{"x0":136,"x1":1049,"y0":1110,"y1":1138},"font_size":2.016,"text":"gies‚Äîto ensure that we can accurately observe the model‚Äôs natural progression during the RL"},{"bbox":{"x0":134,"x1":222,"y0":1138,"y1":1167},"font_size":2.016,"text":"process."}],"source":"layout det","text":"To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specifed instructions. As depicted in Table 1, this templatei requires DeepSeek-R1-Zero to frst produce a reasoning process, followed by the fnal answer.ii We intentionally limit our constraints to this structural format, avoiding any content-specifci biases‚Äîsuch as mandating refective reasoning or promoting particular problem-solving strate-l gies‚Äîto ensure that we can accurately observe the model‚Äôs natural progression during the RL process."},{"bbox":{"x0":135,"x1":923,"y0":1194,"y1":1231},"conf":0.8395,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":936,"y0":1198,"y1":1224},"font_size":2.016,"text":"2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zer"}],"source":"layout det","text":"2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zer"},{"bbox":{"x0":132,"x1":1060,"y0":1239,"y1":1439},"conf":0.9758,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1245,"y1":1270},"font_size":2.016,"text":"Performance of DeepSeek-R1-ZeroFigure 2 depicts the performance trajectory of DeepSeek-"},{"bbox":{"x0":140,"x1":1051,"y0":1274,"y1":1298},"font_size":2.016,"text":"R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,"},{"bbox":{"x0":140,"x1":1049,"y0":1300,"y1":1324},"font_size":2.016,"text":"DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the"},{"bbox":{"x0":140,"x1":1049,"y0":1328,"y1":1352},"font_size":2.016,"text":"RL training advances. Notably, the average pass@1 score on AIME 2024 shows a signifcanti"},{"bbox":{"x0":140,"x1":1049,"y0":1351,"y1":1379},"font_size":2.016,"text":"increase, jumping from an initial $15.6\\%$ to an impressive $71.0\\%,$ reaching performance levels"},{"bbox":{"x0":140,"x1":1047,"y0":1382,"y1":1405},"font_size":2.016,"text":"comparable to OpenAI-o1-0912. This signifcant improvement highlights the effcacy of our RLii"},{"bbox":{"x0":140,"x1":721,"y0":1410,"y1":1433},"font_size":2.016,"text":"algorithm in optimizing the model‚Äôs performance over time."}],"source":"layout det","text":"Performance of DeepSeek-R1-ZeroFigure 2 depicts the performance trajectory of DeepSeekR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a signifcanti increase, jumping from an initial $15.6\\%$ to an impressive $71.0\\%,$ reaching performance levels comparable to OpenAI-o1-0912. This signifcant improvement highlights the effcacy of our RLii algorithm in optimizing the model‚Äôs performance over time."},{"bbox":{"x0":132,"x1":1059,"y0":1443,"y1":1507},"conf":0.2268,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1049,"y0":1449,"y1":1472},"font_size":2.016,"text":"Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI‚Äôs o1-0912"},{"bbox":{"x0":140,"x1":1049,"y0":1477,"y1":1500},"font_size":2.016,"text":"models across a variety of reasoning-related benchmarks. The fndings reveal that RL empowersi"}],"source":"layout det","text":"Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI‚Äôs o1-0912 models across a variety of reasoning-related benchmarks. The fndings reveal that RL empowersi"}],"formula_dets":[{"bbox":{"x0":698,"x1":765,"y0":1351,"y1":1376},"conf":0.8222,"label":"print_embedding","label_id":0},{"bbox":{"x0":467,"x1":528,"y0":1351,"y1":1375},"conf":0.7928,"label":"print_embedding","label_id":0},{"bbox":{"x0":372,"x1":394,"y0":275,"y1":293},"conf":0.7909,"label":"print_embedding","label_id":0},{"bbox":{"x0":259,"x1":283,"y0":250,"y1":270},"conf":0.7656,"label":"print_embedding","label_id":0},{"bbox":{"x0":840,"x1":864,"y0":251,"y1":270},"conf":0.7449,"label":"print_embedding","label_id":0},{"bbox":{"x0":877,"x1":902,"y0":226,"y1":246},"conf":0.679,"label":"print_embedding","label_id":0},{"bbox":{"x0":950,"x1":981,"y0":723,"y1":745},"conf":0.6448,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1060,"y0":1239,"y1":1439},"conf":0.9758,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":972,"y1":1171},"conf":0.9752,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1059,"y0":555,"y1":693},"conf":0.9626,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":458,"y1":551},"conf":0.9556,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1059,"y0":695,"y1":778},"conf":0.9503,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":385,"y0":415,"y1":452},"conf":0.9059,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":385,"y0":926,"y1":963},"conf":0.9057,"label":"Title","label_id":0},{"bbox":{"x0":139,"x1":1052,"y0":317,"y1":382},"conf":0.894,"label":"Text","label_id":1},{"bbox":{"x0":154,"x1":1040,"y0":172,"y1":302},"conf":0.8602,"label":"Abandon","label_id":2},{"bbox":{"x0":132,"x1":1058,"y0":1443,"y1":1507},"conf":0.8406,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":923,"y0":1194,"y1":1231},"conf":0.8395,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":1059,"y0":785,"y1":904},"conf":0.7942,"label":"Text","label_id":1},{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.6175,"label":"Abandon","label_id":2},{"bbox":{"x0":132,"x1":1059,"y0":1443,"y1":1507},"conf":0.2268,"label":"Text","label_id":1},{"bbox":{"x0":154,"x1":1040,"y0":172,"y1":302},"conf":0.2092,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1556],[603,1556],[603,1573],[587,1573]],"score":0.8462},{"poly":[[140,1477],[1049,1477],[1049,1500],[140,1500]],"score":0.8489},{"poly":[[175,1449],[1049,1449],[1049,1472],[175,1472]],"score":0.8779},{"poly":[[140,1410],[721,1410],[721,1433],[140,1433]],"score":0.9325},{"poly":[[140,1382],[1047,1382],[1047,1405],[140,1405]],"score":0.9099},{"poly":[[140,1354],[1049,1354],[1049,1379],[140,1379]],"score":0.7645},{"poly":[[140,1328],[1049,1328],[1049,1352],[140,1352]],"score":0.7416},{"poly":[[140,1300],[1049,1300],[1049,1324],[140,1324]],"score":0.7219},{"poly":[[140,1274],[1051,1274],[1051,1298],[140,1298]],"score":0.7481},{"poly":[[140,1245],[1051,1245],[1051,1270],[140,1270]],"score":0.7428},{"poly":[[138,1198],[936,1200],[936,1224],[138,1223]],"score":0.8141},{"poly":[[134,1142],[221,1138],[222,1163],[136,1167]],"score":0.8262},{"poly":[[136,1114],[1049,1110],[1049,1135],[137,1138]],"score":0.7507},{"poly":[[138,1088],[1051,1088],[1051,1112],[138,1112]],"score":0.7191},{"poly":[[138,1060],[1049,1060],[1049,1084],[138,1084]],"score":0.7281},{"poly":[[140,1031],[1051,1031],[1051,1056],[140,1056]],"score":0.7157},{"poly":[[140,1005],[1049,1005],[1049,1030],[140,1030]],"score":0.7536},{"poly":[[140,977],[1049,977],[1049,1002],[140,1002]],"score":0.7523},{"poly":[[138,931],[376,931],[376,954],[138,954]],"score":0.8252},{"poly":[[137,866],[686,870],[686,900],[136,896]],"score":0.6377},{"poly":[[137,840],[1051,844],[1051,874],[136,870]],"score":0.6442},{"poly":[[138,816],[1049,817],[1049,842],[138,840]],"score":0.7822},{"poly":[[140,791],[1051,791],[1051,816],[140,816]],"score":0.7366},{"poly":[[191,751],[245,751],[245,777],[191,777]],"score":0.8302},{"poly":[[191,723],[1051,721],[1051,746],[191,747]],"score":0.7792},{"poly":[[179,695],[1049,695],[1049,719],[179,719]],"score":0.7566},{"poly":[[195,670],[748,670],[748,693],[195,693]],"score":0.8415},{"poly":[[193,642],[1049,642],[1049,667],[193,667]],"score":0.744},{"poly":[[195,614],[1047,614],[1047,637],[195,637]],"score":0.7871},{"poly":[[193,588],[1049,588],[1049,610],[193,610]],"score":0.8863},{"poly":[[170,558],[1049,558],[1049,582],[170,582]],"score":0.7324},{"poly":[[140,521],[312,519],[312,544],[140,546]],"score":0.8192},{"poly":[[140,493],[1049,493],[1049,516],[140,516]],"score":0.8492},{"poly":[[135,460],[1053,461],[1053,491],[135,489]],"score":0.7464},{"poly":[[137,416],[379,419],[379,449],[136,445]],"score":0.7413},{"poly":[[471,351],[718,351],[718,381],[471,381]],"score":0.7308},{"poly":[[142,319],[1047,323],[1047,353],[142,349]],"score":0.7049},{"poly":[[165,274],[702,274],[702,296],[165,296]],"score":0.8516},{"poly":[[167,249],[923,249],[923,272],[167,272]],"score":0.8179},{"poly":[[167,225],[1003,225],[1003,249],[167,249]],"score":0.6845},{"poly":[[167,202],[982,202],[982,226],[167,226]],"score":0.7034},{"poly":[[165,177],[1024,177],[1024,200],[165,200]],"score":0.8243}],"page_no":5,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1551,"y1":1579},"conf":0.622,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":583,"x1":606,"y0":1551,"y1":1575},"font_size":0.0,"text":"7"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":139,"x1":1051,"y0":164,"y1":383},"conf":0.9755,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":699,"x1":777,"y0":173,"y1":205},"font_size":0.0,"text":"GPQA"},{"bbox":{"x0":803,"x1":909,"y0":175,"y1":203},"font_size":0.0,"text":"LiveCode"},{"bbox":{"x0":389,"x1":504,"y0":189,"y1":214},"font_size":0.0,"text":"AIME 2024"},{"bbox":{"x0":552,"x1":671,"y0":187,"y1":217},"font_size":0.0,"text":"MATH-500"},{"bbox":{"x0":921,"x1":1048,"y0":184,"y1":218},"font_size":0.0,"text":"CodeForces"},{"bbox":{"x0":142,"x1":215,"y0":201,"y1":230},"font_size":0.0,"text":"Model"},{"bbox":{"x0":684,"x1":792,"y0":198,"y1":232},"font_size":0.0,"text":"Diamond"},{"bbox":{"x0":820,"x1":892,"y0":202,"y1":230},"font_size":0.0,"text":"Bench"},{"bbox":{"x0":351,"x1":433,"y0":237,"y1":272},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":446,"x1":539,"y0":239,"y1":268},"font_size":0.0,"text":"cons@64"},{"bbox":{"x0":569,"x1":652,"y0":236,"y1":273},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":694,"x1":777,"y0":238,"y1":273},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":813,"x1":896,"y0":237,"y1":273},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":949,"x1":1021,"y0":237,"y1":274},"font_size":0.0,"text":"rating"},{"bbox":{"x0":144,"x1":316,"y0":276,"y1":307},"font_size":0.0,"text":"OpenAI-o1-mini"},{"bbox":{"x0":368,"x1":420,"y0":275,"y1":306},"font_size":0.0,"text":"63.6"},{"bbox":{"x0":468,"x1":519,"y0":275,"y1":306},"font_size":0.0,"text":"80.0"},{"bbox":{"x0":587,"x1":638,"y0":275,"y1":306},"font_size":0.0,"text":"90.0"},{"bbox":{"x0":714,"x1":763,"y0":277,"y1":305},"font_size":0.0,"text":"60.0"},{"bbox":{"x0":832,"x1":881,"y0":276,"y1":304},"font_size":0.0,"text":"53.8"},{"bbox":{"x0":958,"x1":1013,"y0":275,"y1":306},"font_size":0.0,"text":"1820"},{"bbox":{"x0":143,"x1":313,"y0":303,"y1":337},"font_size":0.0,"text":"OpenAI-o1-0912"},{"bbox":{"x0":368,"x1":420,"y0":304,"y1":334},"font_size":0.0,"text":"74.4"},{"bbox":{"x0":468,"x1":519,"y0":304,"y1":334},"font_size":0.0,"text":"83.3"},{"bbox":{"x0":587,"x1":638,"y0":305,"y1":335},"font_size":0.0,"text":"94.8"},{"bbox":{"x0":712,"x1":763,"y0":302,"y1":336},"font_size":0.0,"text":"77.3"},{"bbox":{"x0":832,"x1":882,"y0":305,"y1":335},"font_size":0.0,"text":"63.4"},{"bbox":{"x0":959,"x1":1012,"y0":305,"y1":335},"font_size":0.0,"text":"1843"},{"bbox":{"x0":144,"x1":341,"y0":342,"y1":372},"font_size":0.0,"text":"DeepSeek-R1-Zero"},{"bbox":{"x0":368,"x1":419,"y0":341,"y1":372},"font_size":0.0,"text":"71.0"},{"bbox":{"x0":469,"x1":518,"y0":341,"y1":373},"font_size":0.0,"text":"86.7"},{"bbox":{"x0":588,"x1":637,"y0":342,"y1":373},"font_size":0.0,"text":"95.9"},{"bbox":{"x0":713,"x1":763,"y0":341,"y1":373},"font_size":0.0,"text":"73.3"},{"bbox":{"x0":832,"x1":881,"y0":342,"y1":373},"font_size":0.0,"text":"50.0"},{"bbox":{"x0":959,"x1":1012,"y0":342,"y1":373},"font_size":0.0,"text":"1444"}],"source":"layout det","text":"<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500</td><td rowspan=\"2\">GPQA Diamond</td><td rowspan=\"2\">LiveCode Bench</td><td rowspan=\"2\">CodeForces</td></tr><tr><td>pass@1</td><td>cons@64</td></tr></thead><tbody><tr><td>OpenAI-o1-mini</td><td>63.6</td><td>80.0</td><td>pass@1 90.0</td><td>pass@1 60.0</td><td>pass@1 53.8</td><td>rating</td></tr><tr><td>OpenAI-o1-0912</td><td>74.4</td><td>83.3</td><td>94.8</td><td>77.3</td><td>63.4</td><td>1820 1843</td></tr><tr><td>DeepSeek-R1-Zero</td><td>71.0</td><td>86.7</td><td>95.9</td><td>73.3</td><td>50.0</td><td>1444</td></tr></tbody></table></body></html>"},{"bbox":{"x0":163,"x1":1025,"y0":392,"y1":451},"conf":0.865,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":168,"x1":1021,"y0":396,"y1":421},"font_size":0.0,"text":"Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related"},{"bbox":{"x0":530,"x1":654,"y0":426,"y1":446},"font_size":0.0,"text":"benchmarks."}],"source":"layout det","text":"Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks."},{"bbox":{"x0":251,"x1":939,"y0":475,"y1":913},"conf":0.9799,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![3be2fd86f275ae557f221d317f856859](imgs/3be2fd86f275ae557f221d317f856859.jpg)"},{"bbox":{"x0":134,"x1":1056,"y0":928,"y1":991},"conf":0.9344,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":933,"y1":958},"font_size":0.0,"text":"Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample"},{"bbox":{"x0":183,"x1":1005,"y0":960,"y1":984},"font_size":0.0,"text":"16 responses and calculate the overall average accuracy to ensure a stable evaluation."}],"source":"layout det","text":"Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation."},{"bbox":{"x0":134,"x1":1060,"y0":1026,"y1":1277},"conf":0.9744,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1030,"y1":1054},"font_size":0.0,"text":"DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised"},{"bbox":{"x0":138,"x1":1051,"y0":1054,"y1":1082},"font_size":0.0,"text":"fne-tuning data. This is a noteworthy achievement, as it underscores the model‚Äôs ability toi"},{"bbox":{"x0":138,"x1":1051,"y0":1084,"y1":1109},"font_size":0.0,"text":"learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-"},{"bbox":{"x0":138,"x1":1051,"y0":1110,"y1":1135},"font_size":0.0,"text":"R1-Zero can be further augmented through the application of majority voting. For example,"},{"bbox":{"x0":138,"x1":1053,"y0":1138,"y1":1161},"font_size":0.0,"text":"when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero‚Äôs performance"},{"bbox":{"x0":138,"x1":1051,"y0":1164,"y1":1190},"font_size":0.0,"text":"escalates from $71.0\\%$ to $86.7\\%,$ thereby exceeding the performance of OpenAI-o1-0912. The"},{"bbox":{"x0":140,"x1":1049,"y0":1189,"y1":1214},"font_size":0.0,"text":"ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without"},{"bbox":{"x0":138,"x1":1051,"y0":1219,"y1":1244},"font_size":0.0,"text":"majority voting, highlights its strong foundational capabilities and its potential for further"},{"bbox":{"x0":138,"x1":468,"y0":1247,"y1":1270},"font_size":0.0,"text":"advancements in reasoning tasks."}],"source":"layout det","text":"DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fne-tuning data. This is a noteworthy achievement, as it underscores the model‚Äôs ability toi learn and generalize effectively through RL alone. Additionally, the performance of DeepSeekR1-Zero can be further augmented through the application of majority voting. For example,when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero‚Äôs performance escalates from $71.0\\%$ to $86.7\\%,$ thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks."},{"bbox":{"x0":135,"x1":1057,"y0":1313,"y1":1484},"conf":0.9713,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1316,"y1":1340},"font_size":0.0,"text":"Self-evolution Process of DeepSeek-R1-ZeroThe self-evolution process of DeepSeek-R1-Zero"},{"bbox":{"x0":138,"x1":1051,"y0":1345,"y1":1368},"font_size":0.0,"text":"is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities"},{"bbox":{"x0":138,"x1":1051,"y0":1372,"y1":1396},"font_size":0.0,"text":"autonomously. By initiating RL directly from the base model, we can closely monitor the model‚Äôs"},{"bbox":{"x0":138,"x1":1049,"y0":1400,"y1":1425},"font_size":0.0,"text":"progression without the infuence of the supervised fne-tuning stage. This approach providesli"},{"bbox":{"x0":136,"x1":1051,"y0":1426,"y1":1451},"font_size":0.0,"text":"a clear view of how the model evolves over time, particularly in terms of its ability to handle"},{"bbox":{"x0":138,"x1":385,"y0":1454,"y1":1477},"font_size":0.0,"text":"complex reasoning tasks."}],"source":"layout det","text":"Self-evolution Process of DeepSeek-R1-ZeroThe self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model‚Äôs progression without the infuence of the supervised fne-tuning stage. This approach providesli a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks."},{"bbox":{"x0":167,"x1":1057,"y0":1490,"y1":1526},"conf":0.9066,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1053,"y0":1489,"y1":1521},"font_size":0.0,"text":"As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-"}],"source":"layout det","text":"As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-"}],"formula_dets":[{"bbox":{"x0":377,"x1":444,"y0":1165,"y1":1190},"conf":0.8303,"label":"print_embedding","label_id":0},{"bbox":{"x0":286,"x1":348,"y0":1164,"y1":1188},"conf":0.7939,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":251,"x1":939,"y0":475,"y1":913},"conf":0.9799,"label":"Figure","label_id":3},{"bbox":{"x0":139,"x1":1051,"y0":164,"y1":383},"conf":0.9755,"label":"Table","label_id":5},{"bbox":{"x0":134,"x1":1060,"y0":1026,"y1":1277},"conf":0.9744,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1057,"y0":1313,"y1":1484},"conf":0.9713,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1056,"y0":928,"y1":991},"conf":0.9344,"label":"Figure caption","label_id":4},{"bbox":{"x0":167,"x1":1057,"y0":1490,"y1":1526},"conf":0.9066,"label":"Text","label_id":1},{"bbox":{"x0":163,"x1":1025,"y0":392,"y1":451},"conf":0.865,"label":"Table footnote","label_id":7},{"bbox":{"x0":581,"x1":608,"y0":1551,"y1":1579},"conf":0.622,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[583,1551],[606,1551],[606,1575],[583,1575]],"score":0.8187},{"poly":[[170,1489],[1053,1491],[1053,1521],[170,1519]],"score":0.7366},{"poly":[[138,1454],[385,1454],[385,1477],[138,1477]],"score":0.9848},{"poly":[[136,1426],[1051,1426],[1051,1451],[136,1451]],"score":0.7807},{"poly":[[138,1400],[1049,1400],[1049,1424],[138,1424]],"score":0.7617},{"poly":[[138,1372],[1051,1372],[1051,1396],[138,1396]],"score":0.7428},{"poly":[[138,1345],[1051,1345],[1051,1368],[138,1368]],"score":0.9201},{"poly":[[140,1316],[1049,1316],[1049,1340],[140,1340]],"score":0.8046},{"poly":[[138,1247],[468,1247],[468,1270],[138,1270]],"score":0.919},{"poly":[[138,1219],[1051,1219],[1051,1244],[138,1244]],"score":0.789},{"poly":[[140,1189],[1049,1189],[1049,1214],[140,1214]],"score":0.7052},{"poly":[[138,1165],[1051,1165],[1051,1189],[138,1189]],"score":0.7701},{"poly":[[138,1138],[1053,1138],[1053,1161],[138,1161]],"score":0.9224},{"poly":[[138,1110],[1051,1110],[1051,1135],[138,1135]],"score":0.7712},{"poly":[[138,1084],[1051,1084],[1051,1109],[138,1109]],"score":0.7806},{"poly":[[138,1054],[1051,1056],[1051,1081],[138,1079]],"score":0.7867},{"poly":[[140,1030],[1051,1030],[1051,1054],[140,1054]],"score":0.7865},{"poly":[[183,960],[1005,960],[1005,984],[183,984]],"score":0.7439},{"poly":[[140,933],[1049,933],[1049,958],[140,958]],"score":0.7879},{"poly":[[592,889],[634,889],[634,910],[592,910]],"score":0.7463},{"poly":[[714,874],[753,874],[753,893],[714,893]],"score":0.957},{"poly":[[576,874],[615,874],[615,893],[576,893]],"score":0.923},{"poly":[[438,874],[477,874],[477,893],[438,893]],"score":0.9349},{"poly":[[852,872],[890,872],[890,893],[852,893]],"score":0.9137},{"poly":[[803,844],[918,846],[918,865],[803,863]],"score":0.7927},{"poly":[[808,826],[913,828],[913,847],[808,845]],"score":0.8272},{"poly":[[276,826],[298,826],[298,842],[276,842]],"score":0.8276},{"poly":[[812,814],[913,814],[913,831],[812,831]],"score":0.8104},{"poly":[[810,796],[906,796],[906,814],[810,814]],"score":0.9169},{"poly":[[273,775],[301,775],[301,796],[273,796]],"score":0.7468},{"poly":[[275,730],[298,730],[298,746],[275,746]],"score":0.9089},{"poly":[[275,635],[298,635],[298,651],[275,651]],"score":0.8566},{"poly":[[273,584],[298,584],[298,605],[273,605]],"score":0.8026},{"poly":[[273,537],[300,537],[300,558],[273,558]],"score":0.8059},{"poly":[[273,489],[301,489],[301,510],[273,510]],"score":0.8478},{"poly":[[447,475],[785,475],[785,498],[447,498]],"score":0.8131},{"poly":[[530,426],[654,426],[654,446],[530,446]],"score":0.9606},{"poly":[[168,396],[1021,396],[1021,421],[168,421]],"score":0.838},{"poly":[[959,342],[1012,342],[1012,370],[959,370]],"score":0.8736},{"poly":[[144,340],[340,340],[340,370],[144,370]],"score":0.7798},{"poly":[[829,339],[884,339],[884,372],[829,372]],"score":0.8297},{"poly":[[709,339],[766,339],[766,372],[709,372]],"score":0.8667},{"poly":[[583,339],[640,339],[640,372],[583,372]],"score":0.8321},{"poly":[[464,339],[521,339],[521,372],[464,372]],"score":0.7975},{"poly":[[367,340],[420,340],[420,368],[367,368]],"score":0.8417},{"poly":[[142,303],[314,303],[314,333],[142,333]],"score":0.8162},{"poly":[[959,303],[1012,303],[1012,332],[959,332]],"score":0.9367},{"poly":[[831,303],[883,303],[883,332],[831,332]],"score":0.9388},{"poly":[[711,300],[764,300],[764,335],[711,335]],"score":0.8226},{"poly":[[585,297],[641,303],[638,336],[582,331]],"score":0.8505},{"poly":[[466,300],[519,300],[519,333],[466,333]],"score":0.8867},{"poly":[[367,300],[422,300],[422,333],[367,333]],"score":0.8586},{"poly":[[144,279],[314,279],[314,302],[144,302]],"score":0.9838},{"poly":[[959,277],[1012,277],[1012,303],[959,303]],"score":0.9604},{"poly":[[587,277],[638,277],[638,303],[587,303]],"score":0.9582},{"poly":[[369,277],[420,277],[420,303],[369,303]],"score":0.9327},{"poly":[[831,275],[883,275],[883,303],[831,303]],"score":0.8921},{"poly":[[712,275],[762,275],[762,303],[712,303]],"score":0.8697},{"poly":[[468,275],[518,275],[518,303],[468,303]],"score":0.8907},{"poly":[[952,236],[1020,243],[1017,271],[949,264]],"score":0.7732},{"poly":[[696,243],[775,236],[777,262],[698,269]],"score":0.819},{"poly":[[445,240],[537,240],[537,265],[445,265]],"score":0.8558},{"poly":[[352,241],[431,236],[433,263],[354,267]],"score":0.7773},{"poly":[[815,241],[892,234],[894,262],[817,269]],"score":0.7461},{"poly":[[570,243],[647,236],[649,262],[572,269]],"score":0.7772},{"poly":[[821,202],[891,202],[891,228],[821,228]],"score":0.8864},{"poly":[[687,198],[789,202],[788,232],[685,228]],"score":0.835},{"poly":[[142,202],[216,202],[216,228],[142,228]],"score":0.9563},{"poly":[[390,189],[505,189],[505,214],[390,214]],"score":0.8569},{"poly":[[924,184],[1048,190],[1047,216],[923,210]],"score":0.775},{"poly":[[553,188],[672,188],[672,212],[553,212]],"score":0.8927},{"poly":[[804,172],[910,176],[908,202],[803,198]],"score":0.7869},{"poly":[[700,175],[776,175],[776,202],[700,202]],"score":0.9114}],"page_no":6,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1551,"y1":1579},"conf":0.6407,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1554,"y1":1573},"font_size":0.0,"text":"8"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":253,"x1":938,"y0":163,"y1":597},"conf":0.9752,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![534c529c448865fc55059cce71c7f951](imgs/534c529c448865fc55059cce71c7f951.jpg)"},{"bbox":{"x0":134,"x1":1055,"y0":610,"y1":675},"conf":0.9361,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":142,"x1":1047,"y0":616,"y1":640},"font_size":0.0,"text":"Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL"},{"bbox":{"x0":140,"x1":1046,"y0":642,"y1":668},"font_size":0.0,"text":"process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time."}],"source":"layout det","text":"Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time."},{"bbox":{"x0":134,"x1":1059,"y0":709,"y1":854},"conf":0.9695,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":709,"y1":740},"font_size":0.0,"text":"ment throughout the training process. This improvement is not the result of external adjustments"},{"bbox":{"x0":140,"x1":1049,"y0":740,"y1":765},"font_size":0.0,"text":"but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the"},{"bbox":{"x0":140,"x1":1049,"y0":768,"y1":793},"font_size":0.0,"text":"ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-"},{"bbox":{"x0":140,"x1":1051,"y0":796,"y1":821},"font_size":0.0,"text":"tation. This computation ranges from generating hundreds to thousands of reasoning tokens,"},{"bbox":{"x0":140,"x1":904,"y0":823,"y1":847},"font_size":0.0,"text":"allowing the model to explore and refne its thought processes in greater depth.i"}],"source":"layout det","text":"ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens,allowing the model to explore and refne its thought processes in greater depth.i"},{"bbox":{"x0":134,"x1":1058,"y0":859,"y1":1059},"conf":0.9763,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":860,"y1":886},"font_size":0.0,"text":"One of the most remarkable aspects of this self-evolution is the emergence of sophisticated"},{"bbox":{"x0":140,"x1":1047,"y0":889,"y1":915},"font_size":0.0,"text":"behaviors as the test-time computation increases. Behaviors such as refection‚Äîwhere the modell"},{"bbox":{"x0":138,"x1":1051,"y0":917,"y1":942},"font_size":0.0,"text":"revisits and reevaluates its previous steps‚Äîand the exploration of alternative approaches to"},{"bbox":{"x0":138,"x1":1049,"y0":945,"y1":968},"font_size":0.0,"text":"problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead"},{"bbox":{"x0":138,"x1":1049,"y0":972,"y1":996},"font_size":0.0,"text":"emerge as a result of the model‚Äôs interaction with the reinforcement learning environment. This"},{"bbox":{"x0":140,"x1":1051,"y0":998,"y1":1024},"font_size":0.0,"text":"spontaneous development signifcantly enhances DeepSeek-R1-Zero‚Äôs reasoning capabilities,i"},{"bbox":{"x0":138,"x1":915,"y0":1024,"y1":1051},"font_size":0.0,"text":"enabling it to tackle more challenging tasks with greater effciency and accuracy.i"}],"source":"layout det","text":"One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as refection‚Äîwhere the modell revisits and reevaluates its previous steps‚Äîand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model‚Äôs interaction with the reinforcement learning environment. This spontaneous development signifcantly enhances DeepSeek-R1-Zero‚Äôs reasoning capabilities,i enabling it to tackle more challenging tasks with greater effciency and accuracy.i"},{"bbox":{"x0":135,"x1":1057,"y0":1090,"y1":1290},"conf":0.9736,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1089,"y1":1124},"font_size":0.0,"text":"Aha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during"},{"bbox":{"x0":138,"x1":1051,"y0":1121,"y1":1147},"font_size":0.0,"text":"the training of DeepSeek-R1-Zero is the occurrence of an ‚Äúaha moment‚Äù. This moment, as"},{"bbox":{"x0":138,"x1":1051,"y0":1151,"y1":1174},"font_size":0.0,"text":"illustrated in Table 3, occurs in an intermediate version of the model. During this phase,"},{"bbox":{"x0":138,"x1":1049,"y0":1177,"y1":1202},"font_size":0.0,"text":"DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial"},{"bbox":{"x0":136,"x1":1051,"y0":1202,"y1":1233},"font_size":0.0,"text":"approach. This behavior is not only a testament to the model‚Äôs growing reasoning abilities"},{"bbox":{"x0":138,"x1":1047,"y0":1231,"y1":1254},"font_size":0.0,"text":"but also a captivating example of how reinforcement learning can lead to unexpected and"},{"bbox":{"x0":136,"x1":374,"y0":1258,"y1":1284},"font_size":0.0,"text":"sophisticated outcomes."}],"source":"layout det","text":"Aha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an ‚Äúaha moment‚Äù. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase,DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model‚Äôs growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes."},{"bbox":{"x0":134,"x1":1058,"y0":1296,"y1":1491},"conf":0.9746,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":1296,"y1":1323},"font_size":0.0,"text":"This moment is not only an ‚Äúaha moment‚Äù for the model but also for the researchers"},{"bbox":{"x0":138,"x1":1051,"y0":1326,"y1":1351},"font_size":0.0,"text":"observing its behavior. It underscores the power and beauty of reinforcement learning: rather"},{"bbox":{"x0":140,"x1":1051,"y0":1354,"y1":1379},"font_size":0.0,"text":"than explicitly teaching the model on how to solve a problem, we simply provide it with the"},{"bbox":{"x0":138,"x1":1051,"y0":1382,"y1":1405},"font_size":0.0,"text":"right incentives, and it autonomously develops advanced problem-solving strategies. The"},{"bbox":{"x0":136,"x1":1051,"y0":1405,"y1":1431},"font_size":0.0,"text":"‚Äúaha moment‚Äù serves as a powerful reminder of the potential of RL to unlock new levels of"},{"bbox":{"x0":138,"x1":1049,"y0":1435,"y1":1461},"font_size":0.0,"text":"intelligence in artifcial systems, paving the way for more autonomous and adaptive models ini"},{"bbox":{"x0":138,"x1":243,"y0":1461,"y1":1486},"font_size":0.0,"text":"the future."}],"source":"layout det","text":"This moment is not only an ‚Äúaha moment‚Äù for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The‚Äúaha moment‚Äù serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artifcial systems, paving the way for more autonomous and adaptive models ini the future."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1058,"y0":859,"y1":1059},"conf":0.9763,"label":"Text","label_id":1},{"bbox":{"x0":253,"x1":938,"y0":163,"y1":597},"conf":0.9752,"label":"Figure","label_id":3},{"bbox":{"x0":134,"x1":1058,"y0":1296,"y1":1491},"conf":0.9746,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1057,"y0":1090,"y1":1290},"conf":0.9736,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1059,"y0":709,"y1":854},"conf":0.9695,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1055,"y0":610,"y1":675},"conf":0.9361,"label":"Figure caption","label_id":4},{"bbox":{"x0":581,"x1":608,"y0":1551,"y1":1579},"conf":0.6407,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1554],[603,1554],[603,1573],[587,1573]],"score":0.9407},{"poly":[[138,1461],[243,1461],[243,1486],[138,1486]],"score":0.9043},{"poly":[[138,1435],[1049,1435],[1049,1459],[138,1459]],"score":0.7843},{"poly":[[137,1405],[1051,1407],[1051,1431],[136,1430]],"score":0.7549},{"poly":[[138,1382],[1051,1382],[1051,1405],[138,1405]],"score":0.8546},{"poly":[[140,1354],[1051,1354],[1051,1379],[140,1379]],"score":0.7388},{"poly":[[138,1326],[1051,1326],[1051,1351],[138,1351]],"score":0.7268},{"poly":[[172,1296],[1049,1298],[1049,1323],[172,1321]],"score":0.7723},{"poly":[[136,1260],[374,1258],[374,1282],[137,1284]],"score":0.7567},{"poly":[[138,1231],[1047,1231],[1047,1254],[138,1254]],"score":0.7962},{"poly":[[136,1203],[1051,1202],[1051,1231],[137,1233]],"score":0.7039},{"poly":[[138,1177],[1049,1177],[1049,1202],[138,1202]],"score":0.7347},{"poly":[[138,1151],[1051,1151],[1051,1174],[138,1174]],"score":0.8348},{"poly":[[138,1121],[1051,1123],[1051,1147],[138,1145]],"score":0.7873},{"poly":[[137,1089],[1051,1095],[1051,1124],[136,1119]],"score":0.6334},{"poly":[[138,1024],[915,1026],[914,1051],[138,1049]],"score":0.777},{"poly":[[140,998],[1051,998],[1051,1023],[140,1023]],"score":0.7315},{"poly":[[138,972],[1049,972],[1049,996],[138,996]],"score":0.721},{"poly":[[138,945],[1049,945],[1049,968],[138,968]],"score":0.8516},{"poly":[[138,917],[1051,917],[1051,942],[138,942]],"score":0.7497},{"poly":[[140,889],[1047,889],[1047,912],[140,912]],"score":0.8562},{"poly":[[172,860],[1049,861],[1049,886],[172,884]],"score":0.7847},{"poly":[[140,823],[904,823],[904,847],[140,847]],"score":0.784},{"poly":[[140,796],[1051,796],[1051,821],[140,821]],"score":0.7759},{"poly":[[140,768],[1049,768],[1049,793],[140,793]],"score":0.7705},{"poly":[[140,740],[1049,740],[1049,765],[140,765]],"score":0.8133},{"poly":[[136,710],[1051,709],[1051,738],[137,740]],"score":0.7153},{"poly":[[140,644],[1046,642],[1046,667],[140,668]],"score":0.8172},{"poly":[[142,616],[1047,616],[1047,640],[142,640]],"score":0.769},{"poly":[[603,574],[642,574],[642,593],[603,593]],"score":0.8852},{"poly":[[854,557],[889,552],[892,574],[857,580]],"score":0.7688},{"poly":[[723,556],[760,556],[760,577],[723,577]],"score":0.8724},{"poly":[[594,556],[629,556],[629,577],[594,577]],"score":0.7801},{"poly":[[461,556],[498,556],[498,577],[461,577]],"score":0.8407},{"poly":[[284,486],[307,486],[307,498],[284,498]],"score":0.7147},{"poly":[[285,428],[307,428],[307,442],[285,442]],"score":0.8572},{"poly":[[282,368],[312,368],[312,388],[282,388]],"score":0.8012},{"poly":[[280,312],[314,312],[314,333],[280,333]],"score":0.7721},{"poly":[[273,256],[314,256],[314,275],[273,275]],"score":0.9038},{"poly":[[273,200],[315,200],[315,219],[273,219]],"score":0.8875},{"poly":[[413,172],[833,172],[833,189],[413,189]],"score":0.8285}],"page_no":7,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.6091,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":585,"x1":603,"y0":1554,"y1":1573},"font_size":0.0,"text":"9"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":193,"x1":1000,"y0":172,"y1":255},"conf":0.2532,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":200,"x1":989,"y0":177,"y1":209},"font_size":0.0,"text":"Question: If then the sum of the real solutions of $\\sqrt{a-\\sqrt{a+x}}=x$ Ô∏Åis equal to"},{"bbox":{"x0":324,"x1":379,"y0":183,"y1":205},"font_size":0.0,"text":"$a>1,$"},{"bbox":{"x0":204,"x1":390,"y0":219,"y1":244},"font_size":0.0,"text":"Response: <think>"},{"bbox":{"x0":432,"x1":432,"y0":248,"y1":262},"font_size":0.0,"text":"Ô∏Å"}],"source":"layout det","text":"Question: If then the sum of the real solutions of $\\sqrt{a-\\sqrt{a+x}}=x$ Ô∏Åis equal to $a>1,$ Response: <think>Ô∏Å"},{"bbox":{"x0":200,"x1":866,"y0":244,"y1":280},"conf":0.5202,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":202,"x1":860,"y0":246,"y1":277},"font_size":0.0,"text":"To solve the equation $\\sqrt{a-\\sqrt{a+x}}=x,$ Ô∏Å let‚Äôs start by squaring both ¬∑ ¬∑ ¬∑"}],"source":"layout det","text":"To solve the equation $\\sqrt{a-\\sqrt{a+x}}=x,$ Ô∏Å let‚Äôs start by squaring both ¬∑ ¬∑ ¬∑"},{"bbox":{"x0":204,"x1":590,"y0":277,"y1":323},"conf":0.9016,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\left(\\sqrt{a-\\sqrt{a+x}}\\right)^{2}=x^{2}\\ \\Longrightarrow\\ a-\\sqrt{a+x}=x^{2}.$$"},{"bbox":{"x0":198,"x1":667,"y0":321,"y1":348},"conf":0.6827,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":204,"x1":663,"y0":323,"y1":347},"font_size":0.0,"text":"Rearrange to isolate the inner square root term:"}],"source":"layout det","text":"Rearrange to isolate the inner square root term:"},{"bbox":{"x0":202,"x1":939,"y0":343,"y1":381},"conf":0.7497,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$(a-x^{2})^{2}=a+x\\implies a^{2}-2ax^{2}+(x^{2})^{2}=a+x\\implies x^{4}-2ax^{2}-x+(a^{2}-a)=0$$"},{"bbox":{"x0":198,"x1":739,"y0":400,"y1":430},"conf":0.8475,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":204,"x1":730,"y0":402,"y1":429},"font_size":0.0,"text":"Wait, wait. Wait. That‚Äôs an aha moment I can fag here.l"}],"source":"layout det","text":"Wait, wait. Wait. That‚Äôs an aha moment I can fag here.l"},{"bbox":{"x0":198,"x1":905,"y0":431,"y1":459},"conf":0.7995,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":200,"x1":899,"y0":426,"y1":458},"font_size":0.0,"text":"Let‚Äôs reevaluate this step-by-step to identify if the correct sum can be ¬∑ ¬∑ ¬∑"}],"source":"layout det","text":"Let‚Äôs reevaluate this step-by-step to identify if the correct sum can be ¬∑ ¬∑ ¬∑"},{"bbox":{"x0":198,"x1":496,"y0":458,"y1":486},"conf":0.6268,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":206,"x1":491,"y0":458,"y1":482},"font_size":0.0,"text":"We st‚àöarted with the equation:"}],"source":"layout det","text":"We st‚àöarted with the equation:"},{"bbox":{"x0":205,"x1":351,"y0":484,"y1":514},"conf":0.8674,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\sqrt{a-\\sqrt{a+x}}=x$$"},{"bbox":{"x0":198,"x1":483,"y0":516,"y1":541},"conf":0.6425,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":206,"x1":480,"y0":516,"y1":540},"font_size":0.0,"text":"Firs‚àöt, let‚Äôs square bot‚àöh sides:"}],"source":"layout det","text":"Firs‚àöt, let‚Äôs square bot‚àöh sides:"},{"bbox":{"x0":204,"x1":539,"y0":541,"y1":568},"conf":0.8318,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$a-\\sqrt{a+x}=x^{2}\\implies\\sqrt{a+x}=a-x^{2}$$"},{"bbox":{"x0":200,"x1":818,"y0":570,"y1":601},"conf":0.7695,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":200,"x1":812,"y0":567,"y1":598},"font_size":0.0,"text":"Next, I could square both sides again, treating the equation: ¬∑ ¬∑ ¬∑"}],"source":"layout det","text":"Next, I could square both sides again, treating the equation: ¬∑ ¬∑ ¬∑"},{"bbox":{"x0":144,"x1":1048,"y0":640,"y1":673},"conf":0.7751,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":151,"x1":1040,"y0":646,"y1":670},"font_size":0.0,"text":"Table 3 | An interesting ‚Äúaha moment‚Äù of an intermediate version of DeepSeek-R1-Zero. The"}],"source":"layout det","text":"Table 3 | An interesting ‚Äúaha moment‚Äù of an intermediate version of DeepSeek-R1-Zero. The"},{"bbox":{"x0":145,"x1":1044,"y0":674,"y1":732},"conf":0.8503,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":154,"x1":1035,"y0":674,"y1":698},"font_size":0.0,"text":"model learns to rethink using an anthropomorphic tone. This is also an aha moment for us,"},{"bbox":{"x0":246,"x1":939,"y0":696,"y1":728},"font_size":0.0,"text":"allowing us to witness the power and beauty of reinforcement learning."}],"source":"layout det","text":"model learns to rethink using an anthropomorphic tone. This is also an aha moment for us,allowing us to witness the power and beauty of reinforcement learning."},{"bbox":{"x0":133,"x1":1058,"y0":763,"y1":934},"conf":0.972,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":763,"y1":798},"font_size":0.0,"text":"Drawback of DeepSeek-R1-ZeroAlthough DeepSeek-R1-Zero exhibits strong reasoning"},{"bbox":{"x0":140,"x1":1047,"y0":796,"y1":821},"font_size":0.0,"text":"capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces"},{"bbox":{"x0":138,"x1":1051,"y0":824,"y1":849},"font_size":0.0,"text":"several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,"},{"bbox":{"x0":136,"x1":1051,"y0":845,"y1":879},"font_size":0.0,"text":"and language mixing. To make reasoning processes more readable and share them with the"},{"bbox":{"x0":140,"x1":1049,"y0":879,"y1":903},"font_size":0.0,"text":"open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly"},{"bbox":{"x0":136,"x1":287,"y0":903,"y1":930},"font_size":0.0,"text":"cold-start data."}],"source":"layout det","text":"Drawback of DeepSeek-R1-ZeroAlthough DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data."},{"bbox":{"x0":133,"x1":743,"y0":966,"y1":1002},"conf":0.895,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":142,"x1":737,"y0":972,"y1":995},"font_size":0.0,"text":"2.3. DeepSeek-R1: Reinforcement Learning with Cold Start"}],"source":"layout det","text":"2.3. DeepSeek-R1: Reinforcement Learning with Cold Start"},{"bbox":{"x0":133,"x1":1057,"y0":1010,"y1":1182},"conf":0.9728,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1016,"y1":1040},"font_size":0.0,"text":"Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can"},{"bbox":{"x0":138,"x1":1049,"y0":1044,"y1":1068},"font_size":0.0,"text":"reasoning performance be further improved or convergence accelerated by incorporating a small"},{"bbox":{"x0":140,"x1":1051,"y0":1070,"y1":1095},"font_size":0.0,"text":"amount of high-quality data as a cold start? 2) How can we train a user-friendly model that"},{"bbox":{"x0":136,"x1":1051,"y0":1119,"y1":1153},"font_size":0.0,"text":"general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The"},{"bbox":{"x0":136,"x1":636,"y0":1149,"y1":1177},"font_size":0.0,"text":"pipeline consists of four stages, outlined as follows."}],"source":"layout det","text":"Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows."},{"bbox":{"x0":134,"x1":312,"y0":1208,"y1":1241},"conf":0.8812,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":307,"y0":1212,"y1":1237},"font_size":0.0,"text":"2.3.1. Cold Start"}],"source":"layout det","text":"2.3.1. Cold Start"},{"bbox":{"x0":134,"x1":1058,"y0":1252,"y1":1448},"conf":0.9756,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1254,"y1":1281},"font_size":0.0,"text":"Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from"},{"bbox":{"x0":140,"x1":1049,"y0":1284,"y1":1309},"font_size":0.0,"text":"the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data"},{"bbox":{"x0":138,"x1":1049,"y0":1310,"y1":1336},"font_size":0.0,"text":"to fne-tune the model as the initial RL actor. To collect such data, we have explored severali"},{"bbox":{"x0":140,"x1":1051,"y0":1337,"y1":1366},"font_size":0.0,"text":"approaches: using few-shot prompting with a long CoT as an example, directly prompting"},{"bbox":{"x0":140,"x1":1051,"y0":1366,"y1":1391},"font_size":0.0,"text":"models to generate detailed answers with refection and verifcation, gathering DeepSeek-R1-li"},{"bbox":{"x0":140,"x1":1049,"y0":1393,"y1":1417},"font_size":0.0,"text":"Zero outputs in a readable format, and refning the results through post-processing by humani"},{"bbox":{"x0":138,"x1":248,"y0":1423,"y1":1442},"font_size":0.0,"text":"annotators."}],"source":"layout det","text":"Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fne-tune the model as the initial RL actor. To collect such data, we have explored severali approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with refection and verifcation, gathering DeepSeek-R1-li Zero outputs in a readable format, and refning the results through post-processing by humani annotators."},{"bbox":{"x0":133,"x1":1056,"y0":1455,"y1":1520},"conf":0.8986,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":1458,"y1":1485},"font_size":0.0,"text":"In this work, we collect thousands of cold-start data to fne-tune the DeepSeek-V3-Base asi"},{"bbox":{"x0":138,"x1":1051,"y0":1486,"y1":1516},"font_size":0.0,"text":"the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data"}],"source":"layout det","text":"In this work, we collect thousands of cold-start data to fne-tune the DeepSeek-V3-Base asi the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data"}],"formula_dets":[{"bbox":{"x0":204,"x1":590,"y0":277,"y1":323},"conf":0.9016,"label":"print_isolated","label_id":1},{"bbox":{"x0":736,"x1":880,"y0":177,"y1":207},"conf":0.8963,"label":"print_embedding","label_id":0},{"bbox":{"x0":416,"x1":566,"y0":246,"y1":276},"conf":0.8847,"label":"print_embedding","label_id":0},{"bbox":{"x0":205,"x1":351,"y0":484,"y1":514},"conf":0.8674,"label":"print_isolated","label_id":1},{"bbox":{"x0":204,"x1":539,"y0":541,"y1":568},"conf":0.8318,"label":"print_isolated","label_id":1},{"bbox":{"x0":324,"x1":379,"y0":183,"y1":205},"conf":0.8163,"label":"print_embedding","label_id":0},{"bbox":{"x0":202,"x1":939,"y0":343,"y1":381},"conf":0.7497,"label":"print_isolated","label_id":1}],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1058,"y0":1252,"y1":1448},"conf":0.9756,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":1010,"y1":1182},"conf":0.9728,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":763,"y1":934},"conf":0.972,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":1455,"y1":1520},"conf":0.8986,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":743,"y0":966,"y1":1002},"conf":0.895,"label":"Title","label_id":0},{"bbox":{"x0":198,"x1":543,"y0":542,"y1":569},"conf":0.8854,"label":"Equation","label_id":8},{"bbox":{"x0":134,"x1":312,"y0":1208,"y1":1241},"conf":0.8812,"label":"Title","label_id":0},{"bbox":{"x0":145,"x1":1044,"y0":674,"y1":732},"conf":0.8503,"label":"Text","label_id":1},{"bbox":{"x0":198,"x1":739,"y0":400,"y1":430},"conf":0.8475,"label":"Text","label_id":1},{"bbox":{"x0":199,"x1":355,"y0":486,"y1":514},"conf":0.8179,"label":"Equation","label_id":8},{"bbox":{"x0":199,"x1":594,"y0":279,"y1":322},"conf":0.8092,"label":"Equation","label_id":8},{"bbox":{"x0":198,"x1":905,"y0":431,"y1":459},"conf":0.7995,"label":"Text","label_id":1},{"bbox":{"x0":144,"x1":1048,"y0":640,"y1":673},"conf":0.7751,"label":"Text","label_id":1},{"bbox":{"x0":200,"x1":818,"y0":570,"y1":601},"conf":0.7695,"label":"Text","label_id":1},{"bbox":{"x0":198,"x1":667,"y0":321,"y1":348},"conf":0.6827,"label":"Text","label_id":1},{"bbox":{"x0":199,"x1":944,"y0":347,"y1":379},"conf":0.6703,"label":"Equation","label_id":8},{"bbox":{"x0":198,"x1":483,"y0":516,"y1":541},"conf":0.6425,"label":"Text","label_id":1},{"bbox":{"x0":198,"x1":496,"y0":458,"y1":486},"conf":0.6268,"label":"Text","label_id":1},{"bbox":{"x0":581,"x1":608,"y0":1550,"y1":1579},"conf":0.6091,"label":"Abandon","label_id":2},{"bbox":{"x0":200,"x1":866,"y0":244,"y1":280},"conf":0.5202,"label":"Text","label_id":1},{"bbox":{"x0":195,"x1":998,"y0":173,"y1":212},"conf":0.3979,"label":"Text","label_id":1},{"bbox":{"x0":198,"x1":942,"y0":347,"y1":378},"conf":0.3654,"label":"Equation","label_id":8},{"bbox":{"x0":193,"x1":1000,"y0":172,"y1":255},"conf":0.2532,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[585,1554],[603,1554],[603,1573],[585,1573]],"score":0.7807},{"poly":[[138,1486],[1051,1486],[1051,1516],[138,1516]],"score":0.6583},{"poly":[[170,1458],[1049,1459],[1049,1484],[170,1482]],"score":0.7382},{"poly":[[138,1423],[248,1423],[248,1442],[138,1442]],"score":0.8891},{"poly":[[140,1393],[1049,1393],[1049,1417],[140,1417]],"score":0.7516},{"poly":[[140,1366],[1051,1366],[1051,1391],[140,1391]],"score":0.748},{"poly":[[140,1337],[1051,1337],[1051,1366],[140,1366]],"score":0.6552},{"poly":[[138,1310],[1049,1310],[1049,1335],[138,1335]],"score":0.7166},{"poly":[[140,1284],[1049,1284],[1049,1309],[140,1309]],"score":0.7387},{"poly":[[138,1254],[1049,1256],[1049,1281],[138,1279]],"score":0.7831},{"poly":[[140,1212],[307,1212],[307,1237],[140,1237]],"score":0.7721},{"poly":[[136,1153],[636,1149],[636,1174],[137,1177]],"score":0.7227},{"poly":[[136,1123],[1051,1119],[1051,1149],[137,1153]],"score":0.6552},{"poly":[[140,1070],[1051,1070],[1051,1095],[140,1095]],"score":0.7175},{"poly":[[138,1044],[1049,1044],[1049,1068],[138,1068]],"score":0.7406},{"poly":[[140,1016],[1049,1016],[1049,1040],[140,1040]],"score":0.7259},{"poly":[[142,972],[737,972],[737,995],[142,995]],"score":0.9019},{"poly":[[137,903],[287,905],[287,930],[136,928]],"score":0.8029},{"poly":[[140,879],[1049,879],[1049,903],[140,903]],"score":0.8015},{"poly":[[136,849],[1051,845],[1051,875],[137,879]],"score":0.6577},{"poly":[[138,824],[1051,824],[1051,849],[138,849]],"score":0.7716},{"poly":[[140,796],[1047,796],[1047,821],[140,821]],"score":0.7898},{"poly":[[137,763],[1051,767],[1051,798],[136,795]],"score":0.6454},{"poly":[[246,696],[939,698],[939,728],[246,726]],"score":0.6839},{"poly":[[154,674],[1035,674],[1035,698],[154,698]],"score":0.7807},{"poly":[[151,646],[1040,646],[1040,670],[151,670]],"score":0.7446},{"poly":[[200,568],[812,567],[812,596],[200,598]],"score":0.7694},{"poly":[[200,542],[542,537],[543,567],[201,572]],"score":0.6878},{"poly":[[206,516],[480,516],[480,540],[206,540]],"score":0.8514},{"poly":[[206,489],[353,489],[353,514],[206,514]],"score":0.8242},{"poly":[[206,458],[491,458],[491,482],[206,482]],"score":0.7936},{"poly":[[200,428],[899,426],[899,456],[200,458]],"score":0.7332},{"poly":[[204,402],[730,403],[730,428],[204,426]],"score":0.7811},{"poly":[[204,349],[936,349],[936,374],[204,374]],"score":0.8073},{"poly":[[204,323],[663,323],[663,347],[204,347]],"score":0.75},{"poly":[[324,288],[587,288],[587,312],[324,312]],"score":0.8143},{"poly":[[209,286],[337,286],[337,316],[209,316]],"score":0.7843},{"poly":[[202,247],[860,246],[860,275],[202,277]],"score":0.7483},{"poly":[[204,219],[390,219],[390,244],[204,244]],"score":0.7922},{"poly":[[200,177],[989,179],[989,209],[200,207]],"score":0.6978}],"page_no":8,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1579},"conf":0.7939,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":581,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"10"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":225,"y0":169,"y1":199},"conf":0.8204,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":220,"y0":175,"y1":195},"font_size":0.0,"text":"include:"}],"source":"layout det","text":"include:"},{"bbox":{"x0":186,"x1":1059,"y0":209,"y1":426},"conf":0.9773,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":214,"y1":237},"font_size":0.0,"text":"Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable"},{"bbox":{"x0":193,"x1":1049,"y0":242,"y1":267},"font_size":0.0,"text":"for reading. Responses may mix multiple languages or lack markdown formatting to"},{"bbox":{"x0":193,"x1":1051,"y0":270,"y1":293},"font_size":0.0,"text":"highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,"},{"bbox":{"x0":193,"x1":1049,"y0":296,"y1":321},"font_size":0.0,"text":"we design a readable pattern that includes a summary at the end of each response and"},{"bbox":{"x0":193,"x1":1051,"y0":323,"y1":347},"font_size":0.0,"text":"flters out responses that are not reader-friendly. Here, we defne the output format asii"},{"bbox":{"x0":197,"x1":1051,"y0":347,"y1":377},"font_size":0.0,"text":"|special_token|<reasoning_process>|special_token|<summary>, where the reasoning"},{"bbox":{"x0":191,"x1":1051,"y0":375,"y1":405},"font_size":0.0,"text":"process is the CoT for the query, and the summary is used to summarize the reasoning"},{"bbox":{"x0":191,"x1":268,"y0":403,"y1":428},"font_size":0.0,"text":"results."}],"source":"layout det","text":"Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,we design a readable pattern that includes a summary at the end of each response and flters out responses that are not reader-friendly. Here, we defne the output format asii|special_token|<reasoning_process>|special_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results."},{"bbox":{"x0":186,"x1":1059,"y0":428,"y1":517},"conf":0.9572,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":186,"x1":1049,"y0":430,"y1":454},"font_size":0.0,"text":"Potential: By carefully designing the pattern for cold-start data with human priors, we"},{"bbox":{"x0":193,"x1":1049,"y0":460,"y1":484},"font_size":0.0,"text":"observe better performance against DeepSeek-R1-Zero. We believe the iterative training is"},{"bbox":{"x0":193,"x1":532,"y0":486,"y1":510},"font_size":0.0,"text":"a better way for reasoning models."}],"source":"layout det","text":"Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models."},{"bbox":{"x0":132,"x1":637,"y0":540,"y1":578},"conf":0.9064,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":629,"y0":546,"y1":570},"font_size":0.0,"text":"2.3.2. Reasoning-oriented Reinforcement Learning"}],"source":"layout det","text":"2.3.2. Reasoning-oriented Reinforcement Learning"},{"bbox":{"x0":131,"x1":1059,"y0":584,"y1":948},"conf":0.9835,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":586,"y1":617},"font_size":0.0,"text":"After fne-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scalei"},{"bbox":{"x0":136,"x1":1051,"y0":614,"y1":646},"font_size":0.0,"text":"reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses"},{"bbox":{"x0":140,"x1":1049,"y0":646,"y1":670},"font_size":0.0,"text":"on enhancing the model‚Äôs reasoning capabilities, particularly in reasoning-intensive tasks such"},{"bbox":{"x0":140,"x1":1049,"y0":672,"y1":696},"font_size":0.0,"text":"as coding, mathematics, science, and logic reasoning, which involve well-defned problems withi"},{"bbox":{"x0":136,"x1":1053,"y0":695,"y1":728},"font_size":0.0,"text":"clear solutions. During the training process, we observe that CoT often exhibits language mixing,"},{"bbox":{"x0":140,"x1":1049,"y0":728,"y1":753},"font_size":0.0,"text":"particularly when RL prompts involve multiple languages. To mitigate the issue of language"},{"bbox":{"x0":138,"x1":1049,"y0":754,"y1":779},"font_size":0.0,"text":"mixing, we introduce a language consistency reward during RL training, which is calculated"},{"bbox":{"x0":140,"x1":1047,"y0":781,"y1":805},"font_size":0.0,"text":"as the proportion of target language words in the CoT. Although ablation experiments show"},{"bbox":{"x0":140,"x1":1049,"y0":809,"y1":831},"font_size":0.0,"text":"that such alignment results in a slight degradation in the model‚Äôs performance, this reward"},{"bbox":{"x0":140,"x1":1051,"y0":835,"y1":860},"font_size":0.0,"text":"aligns with human preferences, making it more readable. Finally, we combine the accuracy of"},{"bbox":{"x0":136,"x1":1051,"y0":858,"y1":889},"font_size":0.0,"text":"reasoning tasks and the reward for language consistency by directly summing them to form the"},{"bbox":{"x0":138,"x1":1049,"y0":888,"y1":914},"font_size":0.0,"text":"fnal reward. We then apply RL training on the fne-tuned model until it achieves convergenceii"},{"bbox":{"x0":138,"x1":326,"y0":917,"y1":940},"font_size":0.0,"text":"on reasoning tasks."}],"source":"layout det","text":"After fne-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scalei reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model‚Äôs reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defned problems withi clear solutions. During the training process, we observe that CoT often exhibits language mixing,particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model‚Äôs performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the fnal reward. We then apply RL training on the fne-tuned model until it achieves convergenceii on reasoning tasks."},{"bbox":{"x0":132,"x1":674,"y0":971,"y1":1009},"conf":0.9203,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":668,"y0":972,"y1":1007},"font_size":0.0,"text":"2.3.3. Rejection Sampling and Supervised Fine-Tuning"}],"source":"layout det","text":"2.3.3. Rejection Sampling and Supervised Fine-Tuning"},{"bbox":{"x0":132,"x1":1059,"y0":1016,"y1":1162},"conf":0.9698,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1021,"y1":1045},"font_size":0.0,"text":"When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT"},{"bbox":{"x0":140,"x1":1047,"y0":1047,"y1":1072},"font_size":0.0,"text":"(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which"},{"bbox":{"x0":138,"x1":1047,"y0":1075,"y1":1100},"font_size":0.0,"text":"primarily focuses on reasoning, this stage incorporates data from other domains to enhance the"},{"bbox":{"x0":136,"x1":1051,"y0":1100,"y1":1131},"font_size":0.0,"text":"model‚Äôs capabilities in writing, role-playing, and other general-purpose tasks. Specifcally, wei"},{"bbox":{"x0":138,"x1":737,"y0":1130,"y1":1156},"font_size":0.0,"text":"generate the data and fne-tune the model as described below.i"}],"source":"layout det","text":"When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model‚Äôs capabilities in writing, role-playing, and other general-purpose tasks. Specifcally, wei generate the data and fne-tune the model as described below.i"},{"bbox":{"x0":133,"x1":1060,"y0":1195,"y1":1450},"conf":0.9757,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1202,"y1":1226},"font_size":0.0,"text":"Reasoning dataWe curate reasoning prompts and generate reasoning trajectories by perform-"},{"bbox":{"x0":140,"x1":1051,"y0":1230,"y1":1254},"font_size":0.0,"text":"ing rejection sampling from the checkpoint from the above RL training. In the previous stage,"},{"bbox":{"x0":135,"x1":1053,"y0":1251,"y1":1284},"font_size":0.0,"text":"we only included data that could be evaluated using rule-based rewards. However, in this stage,"},{"bbox":{"x0":138,"x1":1047,"y0":1281,"y1":1305},"font_size":0.0,"text":"we expand the dataset by incorporating additional data, some of which use a generative reward"},{"bbox":{"x0":138,"x1":1051,"y0":1310,"y1":1333},"font_size":0.0,"text":"model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment."},{"bbox":{"x0":140,"x1":1049,"y0":1337,"y1":1361},"font_size":0.0,"text":"Additionally, because the model output is sometimes chaotic and diffcult to read, we havei"},{"bbox":{"x0":136,"x1":1051,"y0":1359,"y1":1391},"font_size":0.0,"text":"fltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. Fori"},{"bbox":{"x0":140,"x1":1049,"y0":1393,"y1":1416},"font_size":0.0,"text":"each prompt, we sample multiple responses and retain only the correct ones. In total, we collect"},{"bbox":{"x0":138,"x1":590,"y0":1417,"y1":1444},"font_size":0.0,"text":"about 600k reasoning related training samples."}],"source":"layout det","text":"Reasoning dataWe curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage,we only included data that could be evaluated using rule-based rewards. However, in this stage,we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.Additionally, because the model output is sometimes chaotic and diffcult to read, we havei fltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. Fori each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":131,"x1":1059,"y0":584,"y1":948},"conf":0.9835,"label":"Text","label_id":1},{"bbox":{"x0":186,"x1":1059,"y0":209,"y1":426},"conf":0.9773,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":1195,"y1":1450},"conf":0.9757,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1016,"y1":1162},"conf":0.9698,"label":"Text","label_id":1},{"bbox":{"x0":186,"x1":1059,"y0":428,"y1":517},"conf":0.9572,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":674,"y0":971,"y1":1009},"conf":0.9203,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":637,"y0":540,"y1":578},"conf":0.9064,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":225,"y0":169,"y1":199},"conf":0.8204,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1579},"conf":0.7939,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[581,1554],[610,1554],[610,1577],[581,1577]],"score":0.8637},{"poly":[[138,1417],[590,1419],[590,1444],[138,1442]],"score":0.7707},{"poly":[[140,1393],[1049,1393],[1049,1416],[140,1416]],"score":0.855},{"poly":[[137,1359],[1051,1361],[1051,1391],[136,1389]],"score":0.6816},{"poly":[[140,1337],[1049,1337],[1049,1361],[140,1361]],"score":0.7395},{"poly":[[138,1310],[1051,1310],[1051,1333],[138,1333]],"score":0.8746},{"poly":[[138,1281],[1047,1281],[1047,1305],[138,1305]],"score":0.7094},{"poly":[[135,1251],[1053,1254],[1053,1284],[135,1281]],"score":0.6634},{"poly":[[140,1230],[1051,1230],[1051,1254],[140,1254]],"score":0.739},{"poly":[[140,1202],[1051,1202],[1051,1226],[140,1226]],"score":0.7436},{"poly":[[138,1131],[737,1130],[737,1154],[138,1156]],"score":0.8055},{"poly":[[137,1100],[1051,1102],[1051,1131],[136,1130]],"score":0.7233},{"poly":[[138,1075],[1047,1075],[1047,1100],[138,1100]],"score":0.7309},{"poly":[[140,1047],[1047,1047],[1047,1072],[140,1072]],"score":0.7799},{"poly":[[138,1021],[1049,1021],[1049,1045],[138,1045]],"score":0.756},{"poly":[[137,972],[668,977],[668,1007],[136,1002]],"score":0.6804},{"poly":[[138,917],[326,917],[326,940],[138,940]],"score":0.8721},{"poly":[[138,888],[1049,889],[1049,914],[138,912]],"score":0.8134},{"poly":[[137,858],[1051,860],[1051,889],[136,888]],"score":0.6842},{"poly":[[140,835],[1051,835],[1051,860],[140,860]],"score":0.7721},{"poly":[[140,809],[1049,809],[1049,831],[140,831]],"score":0.8459},{"poly":[[140,781],[1047,781],[1047,805],[140,805]],"score":0.727},{"poly":[[138,754],[1049,754],[1049,779],[138,779]],"score":0.7478},{"poly":[[140,728],[1049,728],[1049,753],[140,753]],"score":0.7604},{"poly":[[137,695],[1053,698],[1053,728],[136,724]],"score":0.6662},{"poly":[[140,672],[1049,672],[1049,696],[140,696]],"score":0.7287},{"poly":[[140,646],[1049,646],[1049,670],[140,670]],"score":0.7291},{"poly":[[137,614],[1051,616],[1051,646],[136,644]],"score":0.6813},{"poly":[[137,586],[1051,588],[1051,617],[136,616]],"score":0.6903},{"poly":[[140,546],[629,546],[629,570],[140,570]],"score":0.8154},{"poly":[[193,486],[532,486],[532,510],[193,510]],"score":0.7414},{"poly":[[193,460],[1049,460],[1049,484],[193,484]],"score":0.7425},{"poly":[[186,430],[1049,430],[1049,454],[186,454]],"score":0.746},{"poly":[[191,403],[268,403],[268,428],[191,428]],"score":0.7429},{"poly":[[191,375],[1051,375],[1051,405],[191,405]],"score":0.6604},{"poly":[[197,347],[1051,347],[1051,377],[197,377]],"score":0.6587},{"poly":[[193,323],[1051,323],[1051,347],[193,347]],"score":0.762},{"poly":[[193,296],[1049,296],[1049,321],[193,321]],"score":0.7403},{"poly":[[193,270],[1051,270],[1051,293],[193,293]],"score":0.8456},{"poly":[[193,242],[1049,242],[1049,267],[193,267]],"score":0.7463},{"poly":[[170,214],[1049,214],[1049,237],[170,237]],"score":0.8359},{"poly":[[140,175],[220,175],[220,195],[140,195]],"score":0.9728}],"page_no":9,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":612,"y0":1550,"y1":1580},"conf":0.7735,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":581,"x1":608,"y0":1554,"y1":1577},"font_size":0.0,"text":"11"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":1059,"y0":167,"y1":340},"conf":0.9744,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":174,"y1":198},"font_size":0.0,"text":"Non-Reasoning dataFor non-reasoning data, such as writing, factual QA, self-cognition,"},{"bbox":{"x0":140,"x1":1051,"y0":202,"y1":225},"font_size":0.0,"text":"and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of"},{"bbox":{"x0":140,"x1":1049,"y0":228,"y1":253},"font_size":0.0,"text":"DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential"},{"bbox":{"x0":140,"x1":1051,"y0":256,"y1":281},"font_size":0.0,"text":"chain-of-thought before answering the question by prompting. However, for simpler queries,"},{"bbox":{"x0":140,"x1":1051,"y0":282,"y1":307},"font_size":0.0,"text":"such as ‚Äúhello‚Äù we do not provide a CoT in response. In the end, we collected a total of"},{"bbox":{"x0":138,"x1":805,"y0":309,"y1":339},"font_size":0.0,"text":"approximately 200k training samples that are unrelated to reasoning."}],"source":"layout det","text":"Non-Reasoning dataFor non-reasoning data, such as writing, factual QA, self-cognition,and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries,such as ‚Äúhello‚Äù we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning."},{"bbox":{"x0":132,"x1":1059,"y0":344,"y1":407},"conf":0.9213,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":347,"y1":374},"font_size":0.0,"text":"We fne-tune DeepSeek-V3-Base for two epochs using the above curated dataset of abouti"},{"bbox":{"x0":140,"x1":276,"y0":377,"y1":402},"font_size":0.0,"text":"800k samples."}],"source":"layout det","text":"We fne-tune DeepSeek-V3-Base for two epochs using the above curated dataset of abouti 800k samples."},{"bbox":{"x0":132,"x1":605,"y0":431,"y1":468},"conf":0.8976,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":597,"y0":439,"y1":461},"font_size":0.0,"text":"2.3.4. Reinforcement Learning for all Scenarios"}],"source":"layout det","text":"2.3.4. Reinforcement Learning for all Scenarios"},{"bbox":{"x0":131,"x1":1059,"y0":477,"y1":892},"conf":0.9842,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":482,"y1":507},"font_size":0.0,"text":"To further align the model with human preferences, we implement a secondary reinforcement"},{"bbox":{"x0":140,"x1":1051,"y0":510,"y1":535},"font_size":0.0,"text":"learning stage aimed at improving the model‚Äôs helpfulness and harmlessness while simultane-"},{"bbox":{"x0":140,"x1":1049,"y0":537,"y1":561},"font_size":0.0,"text":"ously refning its reasoning capabilities. Specifcally, we train the model using a combinationii"},{"bbox":{"x0":140,"x1":1049,"y0":565,"y1":588},"font_size":0.0,"text":"of reward signals and diverse prompt distributions. For reasoning data, we adhere to the"},{"bbox":{"x0":140,"x1":1049,"y0":591,"y1":616},"font_size":0.0,"text":"methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the"},{"bbox":{"x0":140,"x1":1049,"y0":619,"y1":644},"font_size":0.0,"text":"learning process in math, code, and logical reasoning domains. For general data, we resort to"},{"bbox":{"x0":138,"x1":1049,"y0":644,"y1":670},"font_size":0.0,"text":"reward models to capture human preferences in complex and nuanced scenarios. We build"},{"bbox":{"x0":140,"x1":1051,"y0":672,"y1":698},"font_size":0.0,"text":"upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-"},{"bbox":{"x0":136,"x1":1051,"y0":695,"y1":728},"font_size":0.0,"text":"ing prompts. For helpfulness, we focus exclusively on the fnal summary, ensuring that thei"},{"bbox":{"x0":136,"x1":1049,"y0":726,"y1":753},"font_size":0.0,"text":"assessment emphasizes the utility and relevance of the response to the user while minimizing"},{"bbox":{"x0":140,"x1":1049,"y0":754,"y1":779},"font_size":0.0,"text":"interference with the underlying reasoning process. For harmlessness, we evaluate the entire"},{"bbox":{"x0":140,"x1":1049,"y0":781,"y1":805},"font_size":0.0,"text":"response of the model, including both the reasoning process and the summary, to identify and"},{"bbox":{"x0":140,"x1":1049,"y0":809,"y1":833},"font_size":0.0,"text":"mitigate any potential risks, biases, or harmful content that may arise during the generation"},{"bbox":{"x0":136,"x1":1053,"y0":831,"y1":863},"font_size":0.0,"text":"process. Ultimately, the integration of reward signals and diverse data distributions enables us"},{"bbox":{"x0":138,"x1":996,"y0":863,"y1":886},"font_size":0.0,"text":"to train a model that excels in reasoning while prioritizing helpfulness and harmlessness."}],"source":"layout det","text":"To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model‚Äôs helpfulness and harmlessness while simultaneously refning its reasoning capabilities. Specifcally, we train the model using a combinationii of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the fnal summary, ensuring that thei assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness."},{"bbox":{"x0":132,"x1":834,"y0":923,"y1":960},"conf":0.8805,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":824,"y0":930,"y1":953},"font_size":0.0,"text":"2.4. Distillation: Empower Small Models with Reasoning Capability"}],"source":"layout det","text":"2.4. Distillation: Empower Small Models with Reasoning Capability"},{"bbox":{"x0":133,"x1":1060,"y0":967,"y1":1166},"conf":0.9752,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":974,"y1":998},"font_size":0.0,"text":"To equip more effcient smaller models with reasoning capabilities like DeepSeek-R1, we directlyi"},{"bbox":{"x0":136,"x1":1051,"y0":996,"y1":1028},"font_size":0.0,"text":"fne-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) usingi"},{"bbox":{"x0":140,"x1":1049,"y0":1028,"y1":1051},"font_size":0.0,"text":"the 800k samples curated with DeepSeek-R1, as detailed in ¬ß2.3.3. Our fndings indicate thati"},{"bbox":{"x0":140,"x1":1047,"y0":1054,"y1":1078},"font_size":0.0,"text":"this straightforward distillation method signifcantly enhances the reasoning abilities of smalleri"},{"bbox":{"x0":140,"x1":1051,"y0":1082,"y1":1105},"font_size":0.0,"text":"models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-"},{"bbox":{"x0":140,"x1":1049,"y0":1109,"y1":1133},"font_size":0.0,"text":"14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its"},{"bbox":{"x0":138,"x1":721,"y0":1135,"y1":1161},"font_size":0.0,"text":"reasoning capability is slightly better than that of Llama-3.1."}],"source":"layout det","text":"To equip more effcient smaller models with reasoning capabilities like DeepSeek-R1, we directlyi fne-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) usingi the 800k samples curated with DeepSeek-R1, as detailed in ¬ß2.3.3. Our fndings indicate thati this straightforward distillation method signifcantly enhances the reasoning abilities of smalleri models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.514B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1."},{"bbox":{"x0":132,"x1":1059,"y0":1170,"y1":1290},"conf":0.9306,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":1174,"y1":1200},"font_size":0.0,"text":"For distilled models, we apply only SFT and do not include an RL stage, even though"},{"bbox":{"x0":140,"x1":1049,"y0":1205,"y1":1230},"font_size":0.0,"text":"incorporating RL could substantially boost model performance. Our primary goal here is to"},{"bbox":{"x0":140,"x1":1049,"y0":1231,"y1":1256},"font_size":0.0,"text":"demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL"},{"bbox":{"x0":138,"x1":537,"y0":1258,"y1":1284},"font_size":0.0,"text":"stage to the broader research community."}],"source":"layout det","text":"For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community."},{"bbox":{"x0":133,"x1":320,"y0":1321,"y1":1361},"conf":0.8941,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":134,"x1":314,"y0":1324,"y1":1358},"font_size":0.0,"text":"3. Experiment"}],"source":"layout det","text":"3.Experiment"},{"bbox":{"x0":133,"x1":1060,"y0":1374,"y1":1494},"conf":0.9599,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1381,"y1":1405},"font_size":0.0,"text":"BenchmarksWe evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema"},{"bbox":{"x0":140,"x1":1049,"y0":1409,"y1":1431},"font_size":0.0,"text":"et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,"},{"bbox":{"x0":140,"x1":1051,"y0":1435,"y1":1458},"font_size":0.0,"text":"2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,"},{"bbox":{"x0":140,"x1":1049,"y0":1461,"y1":1486},"font_size":0.0,"text":"2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verifed (OpenAI,i"}],"source":"layout det","text":"BenchmarksWe evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verifed (OpenAI,i"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":131,"x1":1059,"y0":477,"y1":892},"conf":0.9842,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":967,"y1":1166},"conf":0.9752,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":167,"y1":340},"conf":0.9744,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":1374,"y1":1494},"conf":0.9599,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1170,"y1":1290},"conf":0.9306,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":344,"y1":407},"conf":0.9213,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":605,"y0":431,"y1":468},"conf":0.8976,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":320,"y0":1321,"y1":1361},"conf":0.8941,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":834,"y0":923,"y1":960},"conf":0.8805,"label":"Title","label_id":0},{"bbox":{"x0":577,"x1":612,"y0":1550,"y1":1580},"conf":0.7735,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[581,1554],[608,1554],[608,1577],[581,1577]],"score":0.8593},{"poly":[[140,1461],[1049,1461],[1049,1486],[140,1486]],"score":0.7871},{"poly":[[140,1435],[1051,1435],[1051,1458],[140,1458]],"score":0.9014},{"poly":[[140,1409],[1049,1409],[1049,1431],[140,1431]],"score":0.8713},{"poly":[[140,1381],[1049,1381],[1049,1405],[140,1405]],"score":0.7518},{"poly":[[135,1324],[314,1328],[313,1358],[134,1354]],"score":0.8042},{"poly":[[138,1258],[537,1260],[537,1284],[138,1282]],"score":0.7822},{"poly":[[140,1231],[1049,1231],[1049,1256],[140,1256]],"score":0.7653},{"poly":[[140,1205],[1049,1205],[1049,1230],[140,1230]],"score":0.7646},{"poly":[[172,1174],[1049,1175],[1049,1200],[172,1198]],"score":0.7911},{"poly":[[138,1137],[721,1135],[721,1159],[138,1161]],"score":0.8514},{"poly":[[140,1109],[1049,1109],[1049,1133],[140,1133]],"score":0.7231},{"poly":[[140,1082],[1051,1082],[1051,1105],[140,1105]],"score":0.9079},{"poly":[[140,1054],[1047,1054],[1047,1077],[140,1077]],"score":0.8153},{"poly":[[140,1028],[1049,1028],[1049,1051],[140,1051]],"score":0.894},{"poly":[[137,996],[1051,998],[1051,1028],[136,1026]],"score":0.6851},{"poly":[[138,974],[1049,974],[1049,998],[138,998]],"score":0.778},{"poly":[[140,930],[824,930],[824,953],[140,953]],"score":0.8566},{"poly":[[138,863],[996,863],[996,886],[138,886]],"score":0.9265},{"poly":[[136,833],[1053,831],[1053,861],[137,863]],"score":0.6865},{"poly":[[140,809],[1049,809],[1049,833],[140,833]],"score":0.7375},{"poly":[[140,781],[1049,781],[1049,805],[140,805]],"score":0.7304},{"poly":[[140,754],[1049,754],[1049,779],[140,779]],"score":0.7309},{"poly":[[136,726],[1049,728],[1049,753],[136,751]],"score":0.7782},{"poly":[[136,698],[1051,695],[1051,724],[137,728]],"score":0.6542},{"poly":[[140,674],[1051,672],[1051,696],[140,698]],"score":0.7603},{"poly":[[138,646],[1049,644],[1049,668],[138,670]],"score":0.7771},{"poly":[[140,619],[1049,619],[1049,644],[140,644]],"score":0.7212},{"poly":[[140,591],[1049,591],[1049,616],[140,616]],"score":0.7144},{"poly":[[140,565],[1049,565],[1049,588],[140,588]],"score":0.8411},{"poly":[[140,537],[1049,537],[1049,561],[140,561]],"score":0.7284},{"poly":[[140,510],[1051,510],[1051,535],[140,535]],"score":0.7632},{"poly":[[138,482],[1051,482],[1051,507],[138,507]],"score":0.7607},{"poly":[[138,439],[597,439],[597,461],[138,461]],"score":0.943},{"poly":[[140,377],[276,377],[276,402],[140,402]],"score":0.8188},{"poly":[[172,347],[1049,349],[1049,374],[172,372]],"score":0.8309},{"poly":[[138,309],[805,309],[805,339],[138,339]],"score":0.6521},{"poly":[[140,282],[1051,282],[1051,307],[140,307]],"score":0.7399},{"poly":[[140,256],[1051,256],[1051,281],[140,281]],"score":0.7344},{"poly":[[140,228],[1049,228],[1049,253],[140,253]],"score":0.7349},{"poly":[[140,202],[1051,202],[1051,225],[140,225]],"score":0.9055},{"poly":[[138,174],[1049,174],[1049,198],[138,198]],"score":0.7493}],"page_no":10,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":162,"x1":694,"y0":1448,"y1":1582},"conf":0.3475,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"12"},{"bbox":{"x0":167,"x1":689,"y0":1491,"y1":1523},"font_size":0.0,"text":"3https://www.cms.org.cn/Home/comp/comp/cid/12.html"},{"bbox":{"x0":168,"x1":411,"y0":1470,"y1":1498},"font_size":0.0,"text":"2https://codeforces.com"},{"bbox":{"x0":170,"x1":369,"y0":1449,"y1":1472},"font_size":0.0,"text":"1https://aider.chat"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":1059,"y0":164,"y1":418},"conf":0.9745,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":174,"y1":196},"font_size":0.0,"text":"2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 ‚Äì 2025-01), Codeforces 2, Chinese"},{"bbox":{"x0":140,"x1":1051,"y0":200,"y1":225},"font_size":0.0,"text":"National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-"},{"bbox":{"x0":138,"x1":1049,"y0":228,"y1":251},"font_size":0.0,"text":"ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we"},{"bbox":{"x0":140,"x1":1049,"y0":254,"y1":279},"font_size":0.0,"text":"also evaluate our models on open-ended generation tasks using LLMs as judges. Specifcally, wei"},{"bbox":{"x0":140,"x1":1049,"y0":282,"y1":306},"font_size":0.0,"text":"adhere to the original confgurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Lii"},{"bbox":{"x0":140,"x1":1049,"y0":309,"y1":333},"font_size":0.0,"text":"et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we"},{"bbox":{"x0":140,"x1":1047,"y0":337,"y1":360},"font_size":0.0,"text":"only feed the fnal summary to evaluation to avoid the length bias. For distilled models, wei"},{"bbox":{"x0":140,"x1":1049,"y0":363,"y1":388},"font_size":0.0,"text":"report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and"},{"bbox":{"x0":140,"x1":300,"y0":389,"y1":412},"font_size":0.0,"text":"LiveCodeBench."}],"source":"layout det","text":"2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 ‚Äì 2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifcally, wei adhere to the original confgurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Lii et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the fnal summary to evaluation to avoid the length bias. For distilled models, wei report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench."},{"bbox":{"x0":132,"x1":1060,"y0":453,"y1":869},"conf":0.9826,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":461,"y1":484},"font_size":0.0,"text":"Evaluation PromptsFollowing the setup in DeepSeek-V3, standard benchmarks such as"},{"bbox":{"x0":136,"x1":1051,"y0":484,"y1":516},"font_size":0.0,"text":"MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-"},{"bbox":{"x0":140,"x1":1049,"y0":516,"y1":540},"font_size":0.0,"text":"evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a"},{"bbox":{"x0":140,"x1":1049,"y0":542,"y1":567},"font_size":0.0,"text":"zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts"},{"bbox":{"x0":140,"x1":1049,"y0":570,"y1":595},"font_size":0.0,"text":"are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot"},{"bbox":{"x0":140,"x1":1049,"y0":596,"y1":621},"font_size":0.0,"text":"may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation"},{"bbox":{"x0":138,"x1":1049,"y0":623,"y1":649},"font_size":0.0,"text":"protocols with default prompts provided by their creators. For code and math benchmarks, the"},{"bbox":{"x0":140,"x1":1052,"y0":651,"y1":675},"font_size":0.0,"text":"HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, $\\mathrm{C}\\mathrm{+}\\mathrm{+},$"},{"bbox":{"x0":142,"x1":1049,"y0":677,"y1":702},"font_size":0.0,"text":"C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated"},{"bbox":{"x0":140,"x1":1049,"y0":705,"y1":728},"font_size":0.0,"text":"using CoT format, with data collected between August 2024 and January 2025. The Codeforces"},{"bbox":{"x0":140,"x1":1051,"y0":733,"y1":756},"font_size":0.0,"text":"dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,"},{"bbox":{"x0":140,"x1":1049,"y0":760,"y1":784},"font_size":0.0,"text":"after which the expected ratings and percentages of competitors are calculated. SWE-Bench"},{"bbox":{"x0":140,"x1":1049,"y0":788,"y1":810},"font_size":0.0,"text":"verifed results are obtained via the agentless framework (Xia et al., 2024). AIDER-relatedi"},{"bbox":{"x0":138,"x1":1049,"y0":814,"y1":838},"font_size":0.0,"text":"benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum"},{"bbox":{"x0":140,"x1":496,"y0":840,"y1":863},"font_size":0.0,"text":"of 32,768 tokens for each benchmark."}],"source":"layout det","text":"Evaluation PromptsFollowing the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simpleevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, $\\mathrm{C}\\mathrm{+}\\mathrm{+},$ C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,after which the expected ratings and percentages of competitors are calculated. SWE-Bench verifed results are obtained via the agentless framework (Xia et al., 2024). AIDER-relatedi benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark."},{"bbox":{"x0":132,"x1":1060,"y0":905,"y1":1050},"conf":0.9694,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":909,"y1":937},"font_size":0.0,"text":"BaselinesWe conduct comprehensive evaluations against several strong baselines, including"},{"bbox":{"x0":140,"x1":1051,"y0":938,"y1":963},"font_size":0.0,"text":"DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217."},{"bbox":{"x0":136,"x1":1051,"y0":961,"y1":993},"font_size":0.0,"text":"Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-"},{"bbox":{"x0":140,"x1":1049,"y0":993,"y1":1017},"font_size":0.0,"text":"mance based on offcial reports. For distilled models, we also compare the open-source modeli"},{"bbox":{"x0":142,"x1":473,"y0":1019,"y1":1044},"font_size":0.0,"text":"QwQ-32B-Preview (Qwen, 2024a)."}],"source":"layout det","text":"BaselinesWe conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on offcial reports. For distilled models, we also compare the open-source modeli QwQ-32B-Preview (Qwen, 2024a)."},{"bbox":{"x0":133,"x1":1060,"y0":1083,"y1":1278},"conf":0.9712,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1088,"y1":1114},"font_size":0.0,"text":"Evaluation SetupWe set the maximum generation length to 32,768 tokens for the models."},{"bbox":{"x0":140,"x1":1049,"y0":1117,"y1":1142},"font_size":0.0,"text":"We found that using greedy decoding to evaluate long-output reasoning models results in"},{"bbox":{"x0":138,"x1":1049,"y0":1144,"y1":1170},"font_size":0.0,"text":"higher repetition rates and signifcant variability across different checkpoints. Therefore, wei"},{"bbox":{"x0":140,"x1":1051,"y0":1172,"y1":1196},"font_size":0.0,"text":"default to pass@ùëòevaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature."},{"bbox":{"x0":140,"x1":1048,"y0":1198,"y1":1223},"font_size":0.0,"text":"Specifcally, we use a sampling temperature of 0.6 and a top-ùëùvalue of 0.95 to generatei $k$"},{"bbox":{"x0":138,"x1":1049,"y0":1226,"y1":1251},"font_size":0.0,"text":"responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1"},{"bbox":{"x0":138,"x1":337,"y0":1251,"y1":1277},"font_size":0.0,"text":"is then calculated as"}],"source":"layout det","text":"Evaluation SetupWe set the maximum generation length to 32,768 tokens for the models.We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and signifcant variability across different checkpoints. Therefore, wei default to pass@ùëòevaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifcally, we use a sampling temperature of 0.6 and a top-ùëùvalue of 0.95 to generatei $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as"},{"bbox":{"x0":507,"x1":682,"y0":1274,"y1":1342},"conf":0.9365,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\text{pass@1}=\\frac{1}{k}\\sum_{i=1}^{k}p_{i},$$"},{"bbox":{"x0":132,"x1":1058,"y0":1348,"y1":1437},"conf":0.9565,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1352,"y1":1378},"font_size":0.0,"text":"where $p_{i}$ denotes the correctness of the ùëñ-th response. This method provides more reliable"},{"bbox":{"x0":135,"x1":1051,"y0":1375,"y1":1409},"font_size":0.0,"text":"performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang"},{"bbox":{"x0":138,"x1":626,"y0":1405,"y1":1431},"font_size":0.0,"text":"et al., 2022) using 64 samples, denoted as cons@64."}],"source":"layout det","text":"where $p_{i}$ denotes the correctness of the ùëñ-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64."}],"formula_dets":[{"bbox":{"x0":507,"x1":682,"y0":1274,"y1":1342},"conf":0.9365,"label":"print_isolated","label_id":1},{"bbox":{"x0":208,"x1":227,"y0":1361,"y1":1378},"conf":0.6322,"label":"print_embedding","label_id":0},{"bbox":{"x0":1002,"x1":1052,"y0":652,"y1":674},"conf":0.5914,"label":"print_embedding","label_id":0},{"bbox":{"x0":1035,"x1":1048,"y0":1202,"y1":1218},"conf":0.5261,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1060,"y0":453,"y1":869},"conf":0.9826,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":164,"y1":418},"conf":0.9745,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":1083,"y1":1278},"conf":0.9712,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1060,"y0":905,"y1":1050},"conf":0.9694,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1348,"y1":1437},"conf":0.9565,"label":"Text","label_id":1},{"bbox":{"x0":503,"x1":687,"y0":1275,"y1":1344},"conf":0.9261,"label":"Equation","label_id":8},{"bbox":{"x0":165,"x1":695,"y0":1449,"y1":1525},"conf":0.8085,"label":"Abandon","label_id":2},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1581},"conf":0.4615,"label":"Abandon","label_id":2},{"bbox":{"x0":162,"x1":694,"y0":1448,"y1":1582},"conf":0.3475,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.8697},{"poly":[[167,1491],[689,1493],[689,1523],[167,1521]],"score":0.7062},{"poly":[[169,1470],[411,1474],[411,1498],[168,1494]],"score":0.7242},{"poly":[[170,1449],[369,1449],[369,1472],[170,1472]],"score":0.8899},{"poly":[[138,1407],[626,1405],[626,1430],[138,1431]],"score":0.8136},{"poly":[[135,1375],[1051,1377],[1051,1409],[135,1407]],"score":0.651},{"poly":[[140,1352],[1049,1352],[1049,1377],[140,1377]],"score":0.7584},{"poly":[[604,1307],[627,1307],[627,1335],[604,1335]],"score":0.6589},{"poly":[[654,1300],[682,1300],[682,1323],[654,1323]],"score":0.8178},{"poly":[[504,1297],[596,1289],[598,1315],[507,1323]],"score":0.6587},{"poly":[[606,1282],[626,1282],[626,1300],[606,1300]],"score":0.6304},{"poly":[[622,1270],[652,1270],[652,1307],[622,1307]],"score":0.6714},{"poly":[[138,1251],[337,1253],[337,1277],[138,1275]],"score":0.7372},{"poly":[[138,1226],[1049,1226],[1049,1251],[138,1251]],"score":0.794},{"poly":[[140,1198],[1051,1198],[1051,1223],[140,1223]],"score":0.7471},{"poly":[[140,1172],[1051,1172],[1051,1196],[140,1196]],"score":0.757},{"poly":[[138,1144],[1049,1145],[1049,1170],[138,1168]],"score":0.7955},{"poly":[[140,1117],[1049,1117],[1049,1142],[140,1142]],"score":0.7612},{"poly":[[138,1088],[1051,1089],[1051,1114],[138,1112]],"score":0.7931},{"poly":[[142,1019],[473,1019],[473,1044],[142,1044]],"score":0.7876},{"poly":[[140,993],[1049,993],[1049,1017],[140,1017]],"score":0.8065},{"poly":[[137,961],[1051,963],[1051,993],[136,991]],"score":0.7112},{"poly":[[140,938],[1051,938],[1051,963],[140,963]],"score":0.7952},{"poly":[[138,909],[1049,912],[1049,937],[138,933]],"score":0.7491},{"poly":[[140,840],[496,840],[496,863],[140,863]],"score":0.8917},{"poly":[[138,814],[1049,814],[1049,838],[138,838]],"score":0.7641},{"poly":[[140,788],[1049,788],[1049,810],[140,810]],"score":0.8253},{"poly":[[140,760],[1049,760],[1049,784],[140,784]],"score":0.7369},{"poly":[[140,733],[1051,733],[1051,756],[140,756]],"score":0.8237},{"poly":[[140,705],[1049,705],[1049,728],[140,728]],"score":0.8337},{"poly":[[142,677],[1049,677],[1049,702],[142,702]],"score":0.7265},{"poly":[[140,651],[1051,651],[1051,675],[140,675]],"score":0.74},{"poly":[[138,624],[1049,623],[1049,647],[138,649]],"score":0.7831},{"poly":[[140,596],[1049,596],[1049,621],[140,621]],"score":0.7593},{"poly":[[140,570],[1049,570],[1049,595],[140,595]],"score":0.7733},{"poly":[[140,542],[1049,542],[1049,567],[140,567]],"score":0.7547},{"poly":[[140,516],[1049,516],[1049,540],[140,540]],"score":0.7769},{"poly":[[137,484],[1051,488],[1051,516],[136,512]],"score":0.7535},{"poly":[[140,461],[1049,461],[1049,484],[140,484]],"score":0.9526},{"poly":[[140,389],[300,389],[300,412],[140,412]],"score":0.841},{"poly":[[140,363],[1049,363],[1049,388],[140,388]],"score":0.749},{"poly":[[140,337],[1047,337],[1047,360],[140,360]],"score":0.8903},{"poly":[[140,309],[1049,309],[1049,333],[140,333]],"score":0.7547},{"poly":[[140,282],[1049,282],[1049,305],[140,305]],"score":0.8407},{"poly":[[140,254],[1049,254],[1049,279],[140,279]],"score":0.7261},{"poly":[[138,228],[1049,228],[1049,251],[138,251]],"score":0.8345},{"poly":[[140,200],[1051,200],[1051,225],[140,225]],"score":0.7339},{"poly":[[140,174],[1051,174],[1051,196],[140,196]],"score":0.9183}],"page_no":11,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1551,"y1":1579},"conf":0.7923,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":578,"x1":613,"y0":1549,"y1":1579},"font_size":0.0,"text":"13"}],"source":"layout det","text":""},{"bbox":{"x0":133,"x1":442,"y0":168,"y1":203},"conf":0.4796,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":136,"x1":436,"y0":170,"y1":197},"font_size":0.0,"text":"3.1. DeepSeek-R1 Evaluation"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":189,"x1":1000,"y0":222,"y1":870},"conf":0.983,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":482,"x1":988,"y0":232,"y1":255},"font_size":0.0,"text":"Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek"},{"bbox":{"x0":267,"x1":414,"y0":242,"y1":266},"font_size":0.0,"text":"Benchmark (Metric)"},{"bbox":{"x0":480,"x1":587,"y0":254,"y1":276},"font_size":0.0,"text":"Sonnet-1022"},{"bbox":{"x0":598,"x1":643,"y0":254,"y1":278},"font_size":0.0,"text":"0513"},{"bbox":{"x0":684,"x1":718,"y0":253,"y1":279},"font_size":0.0,"text":"V3"},{"bbox":{"x0":748,"x1":824,"y0":255,"y1":277},"font_size":0.0,"text":"o1-mini "},{"bbox":{"x0":821,"x1":894,"y0":254,"y1":277},"font_size":0.0,"text":"o1-1217"},{"bbox":{"x0":925,"x1":957,"y0":253,"y1":278},"font_size":0.0,"text":"R1"},{"bbox":{"x0":266,"x1":372,"y0":287,"y1":311},"font_size":0.0,"text":"Architecture"},{"bbox":{"x0":530,"x1":536,"y0":293,"y1":311},"font_size":8.0,"text":"-"},{"bbox":{"x0":616,"x1":626,"y0":295,"y1":306},"font_size":0.0,"text":"-"},{"bbox":{"x0":679,"x1":723,"y0":289,"y1":308},"font_size":0.0,"text":"MoE"},{"bbox":{"x0":778,"x1":791,"y0":294,"y1":308},"font_size":0.0,"text":"-"},{"bbox":{"x0":852,"x1":865,"y0":294,"y1":307},"font_size":0.0,"text":"-"},{"bbox":{"x0":919,"x1":965,"y0":287,"y1":309},"font_size":0.0,"text":"MoE"},{"bbox":{"x0":264,"x1":426,"y0":309,"y1":332},"font_size":0.0,"text":"# Activated Params"},{"bbox":{"x0":530,"x1":536,"y0":315,"y1":332},"font_size":8.0,"text":"-"},{"bbox":{"x0":681,"x1":720,"y0":309,"y1":332},"font_size":0.0,"text":"37B"},{"bbox":{"x0":778,"x1":791,"y0":316,"y1":329},"font_size":0.0,"text":"-"},{"bbox":{"x0":852,"x1":865,"y0":316,"y1":329},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":961,"y0":309,"y1":332},"font_size":0.0,"text":"37B"},{"bbox":{"x0":618,"x1":623,"y0":319,"y1":325},"font_size":0.0,"text":"-"},{"bbox":{"x0":264,"x1":389,"y0":330,"y1":354},"font_size":0.0,"text":"# Total Params"},{"bbox":{"x0":528,"x1":538,"y0":339,"y1":351},"font_size":0.0,"text":"-"},{"bbox":{"x0":614,"x1":627,"y0":337,"y1":352},"font_size":0.0,"text":"-"},{"bbox":{"x0":679,"x1":724,"y0":331,"y1":354},"font_size":0.0,"text":"671B"},{"bbox":{"x0":778,"x1":791,"y0":336,"y1":351},"font_size":0.0,"text":"-"},{"bbox":{"x0":852,"x1":865,"y0":337,"y1":352},"font_size":0.0,"text":"-"},{"bbox":{"x0":919,"x1":966,"y0":331,"y1":354},"font_size":0.0,"text":"671B"},{"bbox":{"x0":264,"x1":384,"y0":362,"y1":389},"font_size":0.0,"text":"MMLU (Pass@1)"},{"bbox":{"x0":512,"x1":553,"y0":364,"y1":387},"font_size":0.0,"text":"88.3"},{"bbox":{"x0":601,"x1":641,"y0":364,"y1":387},"font_size":0.0,"text":"87.2"},{"bbox":{"x0":681,"x1":721,"y0":364,"y1":387},"font_size":0.0,"text":"88.5"},{"bbox":{"x0":764,"x1":804,"y0":364,"y1":387},"font_size":0.0,"text":"85.2"},{"bbox":{"x0":838,"x1":879,"y0":364,"y1":387},"font_size":0.0,"text":"91.8"},{"bbox":{"x0":922,"x1":962,"y0":364,"y1":387},"font_size":0.0,"text":"90.8"},{"bbox":{"x0":265,"x1":420,"y0":386,"y1":409},"font_size":0.0,"text":"MMLU-Redux (EM)"},{"bbox":{"x0":513,"x1":552,"y0":388,"y1":407},"font_size":0.0,"text":"88.9"},{"bbox":{"x0":601,"x1":641,"y0":387,"y1":409},"font_size":0.0,"text":"88.0"},{"bbox":{"x0":680,"x1":720,"y0":387,"y1":409},"font_size":0.0,"text":"89.1"},{"bbox":{"x0":762,"x1":805,"y0":384,"y1":411},"font_size":0.0,"text":"86.7"},{"bbox":{"x0":850,"x1":867,"y0":392,"y1":410},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":962,"y0":386,"y1":408},"font_size":0.0,"text":"92.9"},{"bbox":{"x0":265,"x1":396,"y0":407,"y1":432},"font_size":0.0,"text":"MMLU-Pro (EM)"},{"bbox":{"x0":512,"x1":553,"y0":408,"y1":431},"font_size":0.0,"text":"78.0"},{"bbox":{"x0":601,"x1":641,"y0":408,"y1":432},"font_size":0.0,"text":"72.6"},{"bbox":{"x0":680,"x1":720,"y0":408,"y1":431},"font_size":0.0,"text":"75.9"},{"bbox":{"x0":764,"x1":804,"y0":408,"y1":431},"font_size":0.0,"text":"80.3"},{"bbox":{"x0":852,"x1":865,"y0":413,"y1":429},"font_size":0.0,"text":"-"},{"bbox":{"x0":923,"x1":961,"y0":410,"y1":430},"font_size":0.0,"text":"84.0"},{"bbox":{"x0":265,"x1":385,"y0":428,"y1":453},"font_size":0.0,"text":"DROP (3-shot F1)"},{"bbox":{"x0":512,"x1":553,"y0":430,"y1":452},"font_size":0.0,"text":"88.3"},{"bbox":{"x0":601,"x1":641,"y0":430,"y1":452},"font_size":0.0,"text":"83.7"},{"bbox":{"x0":680,"x1":721,"y0":430,"y1":452},"font_size":0.0,"text":"91.6"},{"bbox":{"x0":764,"x1":804,"y0":430,"y1":452},"font_size":0.0,"text":"83.9"},{"bbox":{"x0":838,"x1":879,"y0":430,"y1":453},"font_size":0.0,"text":"90.2"},{"bbox":{"x0":922,"x1":962,"y0":430,"y1":452},"font_size":0.0,"text":"92.2"},{"bbox":{"x0":265,"x1":413,"y0":452,"y1":475},"font_size":0.0,"text":"IF-Eval (Prompt Strict)"},{"bbox":{"x0":512,"x1":553,"y0":452,"y1":474},"font_size":0.0,"text":"86.5"},{"bbox":{"x0":601,"x1":641,"y0":452,"y1":474},"font_size":0.0,"text":"84.3"},{"bbox":{"x0":682,"x1":719,"y0":453,"y1":473},"font_size":0.0,"text":"86.1"},{"bbox":{"x0":764,"x1":804,"y0":452,"y1":474},"font_size":0.0,"text":"84.8"},{"bbox":{"x0":852,"x1":866,"y0":458,"y1":472},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":962,"y0":451,"y1":474},"font_size":0.0,"text":"83.3"},{"bbox":{"x0":195,"x1":263,"y0":461,"y1":489},"font_size":0.0,"text":"English"},{"bbox":{"x0":265,"x1":456,"y0":470,"y1":501},"font_size":0.0,"text":"GPQA Diamond (Pass@1)"},{"bbox":{"x0":512,"x1":553,"y0":474,"y1":496},"font_size":0.0,"text":"65.0"},{"bbox":{"x0":602,"x1":639,"y0":475,"y1":495},"font_size":0.0,"text":"49.9"},{"bbox":{"x0":680,"x1":720,"y0":473,"y1":496},"font_size":0.0,"text":"59.1"},{"bbox":{"x0":764,"x1":804,"y0":474,"y1":496},"font_size":0.0,"text":"60.0"},{"bbox":{"x0":838,"x1":879,"y0":473,"y1":496},"font_size":0.0,"text":"75.7"},{"bbox":{"x0":922,"x1":962,"y0":473,"y1":496},"font_size":0.0,"text":"71.5"},{"bbox":{"x0":266,"x1":408,"y0":495,"y1":520},"font_size":0.0,"text":"SimpleQA (Correct)"},{"bbox":{"x0":512,"x1":553,"y0":495,"y1":518},"font_size":0.0,"text":"28.4"},{"bbox":{"x0":601,"x1":641,"y0":495,"y1":519},"font_size":0.0,"text":"38.2"},{"bbox":{"x0":680,"x1":720,"y0":494,"y1":518},"font_size":0.0,"text":"24.9"},{"bbox":{"x0":767,"x1":802,"y0":494,"y1":518},"font_size":0.0,"text":"7.0"},{"bbox":{"x0":838,"x1":878,"y0":496,"y1":519},"font_size":0.0,"text":"47.0"},{"bbox":{"x0":923,"x1":960,"y0":496,"y1":517},"font_size":0.0,"text":"30.1"},{"bbox":{"x0":266,"x1":383,"y0":516,"y1":541},"font_size":0.0,"text":"FRAMES (Acc.)"},{"bbox":{"x0":512,"x1":553,"y0":518,"y1":540},"font_size":0.0,"text":"72.5"},{"bbox":{"x0":601,"x1":641,"y0":518,"y1":540},"font_size":0.0,"text":"80.5"},{"bbox":{"x0":680,"x1":721,"y0":517,"y1":540},"font_size":0.0,"text":"73.3"},{"bbox":{"x0":764,"x1":804,"y0":517,"y1":540},"font_size":0.0,"text":"76.9"},{"bbox":{"x0":850,"x1":867,"y0":524,"y1":542},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":962,"y0":518,"y1":540},"font_size":0.0,"text":"82.5"},{"bbox":{"x0":265,"x1":460,"y0":537,"y1":564},"font_size":0.0,"text":"AlpacaEval2.0 (LC-winrate)"},{"bbox":{"x0":512,"x1":553,"y0":540,"y1":562},"font_size":0.0,"text":"52.0"},{"bbox":{"x0":601,"x1":640,"y0":539,"y1":562},"font_size":0.0,"text":"51.1"},{"bbox":{"x0":680,"x1":721,"y0":539,"y1":562},"font_size":0.0,"text":"70.0"},{"bbox":{"x0":764,"x1":804,"y0":539,"y1":562},"font_size":0.0,"text":"57.8"},{"bbox":{"x0":852,"x1":865,"y0":546,"y1":561},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":962,"y0":540,"y1":562},"font_size":0.0,"text":"87.6"},{"bbox":{"x0":266,"x1":436,"y0":561,"y1":585},"font_size":0.0,"text":"ArenaHard (GPT-4-1106)"},{"bbox":{"x0":512,"x1":554,"y0":561,"y1":584},"font_size":0.0,"text":"85.2"},{"bbox":{"x0":601,"x1":641,"y0":561,"y1":584},"font_size":0.0,"text":"80.4"},{"bbox":{"x0":681,"x1":721,"y0":561,"y1":584},"font_size":0.0,"text":"85.5"},{"bbox":{"x0":764,"x1":805,"y0":561,"y1":584},"font_size":0.0,"text":"92.0"},{"bbox":{"x0":852,"x1":865,"y0":567,"y1":582},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":962,"y0":561,"y1":584},"font_size":0.0,"text":"92.3"},{"bbox":{"x0":266,"x1":478,"y0":593,"y1":618},"font_size":0.0,"text":"LiveCodeBench (Pass@1-COT)"},{"bbox":{"x0":512,"x1":553,"y0":594,"y1":617},"font_size":0.0,"text":"38.9"},{"bbox":{"x0":601,"x1":641,"y0":594,"y1":617},"font_size":0.0,"text":"32.9"},{"bbox":{"x0":681,"x1":721,"y0":594,"y1":617},"font_size":0.0,"text":"36.2"},{"bbox":{"x0":764,"x1":804,"y0":594,"y1":617},"font_size":0.0,"text":"53.8"},{"bbox":{"x0":838,"x1":879,"y0":594,"y1":617},"font_size":0.0,"text":"63.4"},{"bbox":{"x0":922,"x1":962,"y0":594,"y1":617},"font_size":0.0,"text":"65.9"},{"bbox":{"x0":266,"x1":426,"y0":616,"y1":641},"font_size":0.0,"text":"Codeforces (Percentile)"},{"bbox":{"x0":512,"x1":553,"y0":617,"y1":639},"font_size":0.0,"text":"20.3"},{"bbox":{"x0":601,"x1":641,"y0":617,"y1":639},"font_size":0.0,"text":"23.6"},{"bbox":{"x0":679,"x1":722,"y0":614,"y1":641},"font_size":0.0,"text":"58.7"},{"bbox":{"x0":762,"x1":806,"y0":613,"y1":640},"font_size":0.0,"text":"93.4"},{"bbox":{"x0":838,"x1":879,"y0":617,"y1":639},"font_size":0.0,"text":"96.6"},{"bbox":{"x0":922,"x1":962,"y0":617,"y1":639},"font_size":0.0,"text":"96.3"},{"bbox":{"x0":206,"x1":255,"y0":630,"y1":649},"font_size":0.0,"text":"Code"},{"bbox":{"x0":265,"x1":409,"y0":637,"y1":664},"font_size":0.0,"text":"Codeforces (Rating)"},{"bbox":{"x0":514,"x1":550,"y0":639,"y1":659},"font_size":0.0,"text":"717"},{"bbox":{"x0":602,"x1":639,"y0":638,"y1":661},"font_size":0.0,"text":"759"},{"bbox":{"x0":681,"x1":722,"y0":640,"y1":659},"font_size":0.0,"text":"1134"},{"bbox":{"x0":763,"x1":806,"y0":638,"y1":660},"font_size":0.0,"text":"1820"},{"bbox":{"x0":837,"x1":878,"y0":640,"y1":659},"font_size":0.0,"text":"2061"},{"bbox":{"x0":920,"x1":964,"y0":638,"y1":660},"font_size":0.0,"text":"2029"},{"bbox":{"x0":265,"x1":439,"y0":659,"y1":684},"font_size":0.0,"text":"SWE Verifed (Resolved)"},{"bbox":{"x0":355,"x1":355,"y0":665,"y1":683},"font_size":8.0,"text":"i"},{"bbox":{"x0":512,"x1":553,"y0":660,"y1":683},"font_size":0.0,"text":"50.8"},{"bbox":{"x0":601,"x1":641,"y0":660,"y1":683},"font_size":0.0,"text":"38.8"},{"bbox":{"x0":681,"x1":721,"y0":660,"y1":683},"font_size":0.0,"text":"42.0"},{"bbox":{"x0":764,"x1":804,"y0":660,"y1":683},"font_size":0.0,"text":"41.6"},{"bbox":{"x0":838,"x1":878,"y0":660,"y1":683},"font_size":0.0,"text":"48.9"},{"bbox":{"x0":922,"x1":962,"y0":660,"y1":683},"font_size":0.0,"text":"49.2"},{"bbox":{"x0":265,"x1":428,"y0":681,"y1":707},"font_size":0.0,"text":"Aider-Polyglot (Acc.)"},{"bbox":{"x0":512,"x1":553,"y0":683,"y1":705},"font_size":0.0,"text":"45.3"},{"bbox":{"x0":602,"x1":640,"y0":684,"y1":703},"font_size":0.0,"text":"16.0"},{"bbox":{"x0":682,"x1":720,"y0":684,"y1":703},"font_size":0.0,"text":"49.6"},{"bbox":{"x0":764,"x1":804,"y0":682,"y1":704},"font_size":0.0,"text":"32.9"},{"bbox":{"x0":838,"x1":879,"y0":682,"y1":704},"font_size":0.0,"text":"61.7"},{"bbox":{"x0":922,"x1":962,"y0":682,"y1":705},"font_size":0.0,"text":"53.3"},{"bbox":{"x0":266,"x1":412,"y0":713,"y1":739},"font_size":0.0,"text":"AIME 2024 (Pass@1)"},{"bbox":{"x0":514,"x1":552,"y0":715,"y1":735},"font_size":0.0,"text":"16.0"},{"bbox":{"x0":605,"x1":636,"y0":714,"y1":735},"font_size":0.0,"text":"9.3"},{"bbox":{"x0":681,"x1":721,"y0":714,"y1":736},"font_size":0.0,"text":"39.2"},{"bbox":{"x0":764,"x1":804,"y0":714,"y1":736},"font_size":0.0,"text":"63.6"},{"bbox":{"x0":838,"x1":879,"y0":714,"y1":736},"font_size":0.0,"text":"79.2"},{"bbox":{"x0":923,"x1":960,"y0":715,"y1":735},"font_size":0.0,"text":"79.8"},{"bbox":{"x0":206,"x1":255,"y0":738,"y1":757},"font_size":0.0,"text":"Math"},{"bbox":{"x0":264,"x1":412,"y0":735,"y1":760},"font_size":0.0,"text":"MATH-500 (Pass@1)"},{"bbox":{"x0":512,"x1":553,"y0":736,"y1":759},"font_size":0.0,"text":"78.3"},{"bbox":{"x0":601,"x1":641,"y0":736,"y1":759},"font_size":0.0,"text":"74.6"},{"bbox":{"x0":680,"x1":721,"y0":736,"y1":759},"font_size":0.0,"text":"90.2"},{"bbox":{"x0":764,"x1":805,"y0":736,"y1":759},"font_size":0.0,"text":"90.0"},{"bbox":{"x0":837,"x1":879,"y0":736,"y1":760},"font_size":0.0,"text":"96.4"},{"bbox":{"x0":920,"x1":963,"y0":735,"y1":761},"font_size":0.0,"text":"97.3"},{"bbox":{"x0":265,"x1":422,"y0":757,"y1":782},"font_size":0.0,"text":"CNMO 2024 (Pass@1)"},{"bbox":{"x0":513,"x1":551,"y0":760,"y1":780},"font_size":0.0,"text":"13.1"},{"bbox":{"x0":603,"x1":640,"y0":760,"y1":780},"font_size":0.0,"text":"10.8"},{"bbox":{"x0":680,"x1":721,"y0":759,"y1":781},"font_size":0.0,"text":"43.2"},{"bbox":{"x0":764,"x1":805,"y0":758,"y1":781},"font_size":0.0,"text":"67.6"},{"bbox":{"x0":852,"x1":866,"y0":765,"y1":780},"font_size":0.0,"text":"-"},{"bbox":{"x0":923,"x1":961,"y0":761,"y1":780},"font_size":0.0,"text":"78.8"},{"bbox":{"x0":265,"x1":392,"y0":790,"y1":814},"font_size":0.0,"text":"CLUEWSC (EM)"},{"bbox":{"x0":512,"x1":553,"y0":791,"y1":813},"font_size":0.0,"text":"85.4"},{"bbox":{"x0":601,"x1":641,"y0":791,"y1":813},"font_size":0.0,"text":"87.9"},{"bbox":{"x0":680,"x1":721,"y0":791,"y1":813},"font_size":0.0,"text":"90.9"},{"bbox":{"x0":764,"x1":805,"y0":791,"y1":814},"font_size":0.0,"text":"89.9"},{"bbox":{"x0":852,"x1":865,"y0":797,"y1":812},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":962,"y0":791,"y1":813},"font_size":0.0,"text":"92.8"},{"bbox":{"x0":194,"x1":359,"y0":811,"y1":839},"font_size":0.0,"text":"ChineseC-Eval (EM)"},{"bbox":{"x0":512,"x1":553,"y0":813,"y1":836},"font_size":0.0,"text":"76.7"},{"bbox":{"x0":599,"x1":642,"y0":812,"y1":838},"font_size":0.0,"text":"76.0"},{"bbox":{"x0":680,"x1":721,"y0":813,"y1":837},"font_size":0.0,"text":"86.5"},{"bbox":{"x0":764,"x1":805,"y0":813,"y1":837},"font_size":0.0,"text":"68.9"},{"bbox":{"x0":853,"x1":863,"y0":822,"y1":833},"font_size":0.0,"text":"-"},{"bbox":{"x0":922,"x1":962,"y0":814,"y1":837},"font_size":0.0,"text":"91.8"},{"bbox":{"x0":265,"x1":426,"y0":834,"y1":861},"font_size":0.0,"text":"C-SimpleQA (Correct)"},{"bbox":{"x0":512,"x1":554,"y0":835,"y1":859},"font_size":0.0,"text":"55.4"},{"bbox":{"x0":600,"x1":642,"y0":835,"y1":859},"font_size":0.0,"text":"58.7"},{"bbox":{"x0":680,"x1":721,"y0":836,"y1":858},"font_size":0.0,"text":"68.0"},{"bbox":{"x0":764,"x1":805,"y0":835,"y1":858},"font_size":0.0,"text":"40.3"},{"bbox":{"x0":919,"x1":963,"y0":833,"y1":860},"font_size":0.0,"text":"63.7"},{"bbox":{"x0":854,"x1":863,"y0":843,"y1":855},"font_size":0.0,"text":"-"}],"source":"layout det","text":"<html><body><table><tr><td></td><td>Benchmark (Metric)</td><td>Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022</td><td>0513</td><td>V3</td><td>o1-mini </td><td>o1-1217</td><td>R1</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>-</td><td>MoE</td><td>-</td><td>-</td><td>MoE</td></tr><tr><td rowspan=\"2\"></td><td># Activated Params</td><td>-</td><td>-</td><td>37B</td><td>-</td><td>-</td><td>37B</td></tr><tr><td># Total Params</td><td>-</td><td>-</td><td>671B</td><td>-</td><td>-</td><td>671B</td></tr><tr><td rowspan=\"12\">English</td><td>MMLU (Pass@1)</td><td>88.3</td><td>87.2</td><td>88.5</td><td>85.2</td><td>91.8</td><td>90.8</td></tr><tr><td>MMLU-Redux (EM)</td><td>88.9</td><td>88.0</td><td>89.1</td><td>86.7</td><td>-</td><td>92.9</td></tr><tr><td>MMLU-Pro (EM)</td><td>78.0</td><td>72.6</td><td>75.9</td><td>80.3</td><td>-</td><td>84.0</td></tr><tr><td>DROP (3-shot F1)</td><td>88.3</td><td>83.7</td><td>91.6</td><td>83.9</td><td>90.2</td><td>92.2</td></tr><tr><td>IF-Eval (Prompt Strict)</td><td>86.5</td><td>84.3</td><td>86.1</td><td>84.8</td><td>-</td><td>83.3</td></tr><tr><td>GPQA Diamond (Pass@1)</td><td>65.0</td><td>49.9</td><td>59.1</td><td>60.0</td><td>75.7</td><td>71.5</td></tr><tr><td>SimpleQA (Correct)</td><td>28.4</td><td>38.2</td><td>24.9</td><td>7.0</td><td>47.0</td><td>30.1</td></tr><tr><td>FRAMES (Acc.)</td><td>72.5</td><td>80.5</td><td>73.3</td><td>76.9</td><td>-</td><td>82.5</td></tr><tr><td>AlpacaEval2.0 (LC-winrate)</td><td>52.0</td><td>51.1</td><td>70.0</td><td>57.8</td><td>-</td><td>87.6</td></tr><tr><td>ArenaHard (GPT-4-1106)</td><td>85.2</td><td>80.4</td><td>85.5</td><td>92.0</td><td>-</td><td>92.3</td></tr><tr><td>LiveCodeBench (Pass@1-COT)</td><td>38.9</td><td>32.9</td><td>36.2</td><td>53.8</td><td>63.4</td><td>65.9</td></tr><tr><td rowspan=\"5\">Code</td><td>Codeforces (Percentile)</td><td>20.3</td><td>23.6</td><td>58.7</td><td>93.4</td><td>96.6</td><td>96.3</td></tr><tr><td>Codeforces (Rating)</td><td>717</td><td>759</td><td>1134</td><td>1820</td><td>2061</td><td>2029</td></tr><tr><td>SWE Verifed (Resolved)</td><td>50.8</td><td>38.8</td><td>42.0</td><td>41.6</td><td>48.9</td><td>49.2</td></tr><tr><td>Aider-Polyglot (Acc.)</td><td>45.3</td><td>16.0</td><td>49.6</td><td>32.9</td><td>61.7</td><td>53.3</td></tr><tr><td>AIME 2024 (Pass@1)</td><td>16.0</td><td>9.3</td><td>39.2</td><td>63.6</td><td>79.2</td><td>79.8</td></tr><tr><td rowspan=\"3\">Math</td><td>MATH-500 (Pass@1)</td><td>78.3</td><td>74.6</td><td>90.2</td><td>90.0</td><td>96.4</td><td>97.3</td></tr><tr><td>CNMO 2024 (Pass@1)</td><td>13.1</td><td>10.8</td><td>43.2</td><td>67.6</td><td>-</td><td>78.8</td></tr><tr><td>CLUEWSC (EM)</td><td>85.4</td><td>87.9</td><td>90.9</td><td>89.9</td><td></td><td></td></tr><tr><td rowspan=\"3\">ChineseC-Eval (EM)</td><td></td><td>76.7</td><td>76.0</td><td>86.5</td><td>68.9</td><td>-</td><td>92.8 91.8</td></tr><tr><td></td><td></td><td></td><td></td><td>40.3</td><td>-</td><td>63.7</td></tr><tr><td>C-SimpleQA (Correct)</td><td>55.4</td><td>58.7</td><td>68.0</td><td></td><td>-</td><td></td></tr></table></body></html>"},{"bbox":{"x0":213,"x1":974,"y0":881,"y1":917},"conf":0.7962,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":220,"x1":969,"y0":884,"y1":909},"font_size":7.695e-27,"text":"Table 4 | Comparison between DeepSeek-R1 and other representative models."}],"source":"layout det","text":"Table 4 | Comparison between DeepSeek-R1 and other representative models."},{"bbox":{"x0":131,"x1":1061,"y0":931,"y1":1263},"conf":0.9756,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":935,"y1":960},"font_size":7.695e-27,"text":"For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA"},{"bbox":{"x0":140,"x1":1053,"y0":963,"y1":988},"font_size":7.695e-27,"text":"Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-"},{"bbox":{"x0":138,"x1":1051,"y0":989,"y1":1014},"font_size":7.695e-27,"text":"provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-"},{"bbox":{"x0":135,"x1":1051,"y0":1012,"y1":1044},"font_size":7.695e-27,"text":"icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1"},{"bbox":{"x0":138,"x1":1051,"y0":1044,"y1":1068},"font_size":7.695e-27,"text":"excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis"},{"bbox":{"x0":136,"x1":1051,"y0":1067,"y1":1096},"font_size":7.695e-27,"text":"capabilities. This highlights the potential of reasoning models in AI-driven search and data"},{"bbox":{"x0":138,"x1":1051,"y0":1098,"y1":1121},"font_size":7.695e-27,"text":"analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,"},{"bbox":{"x0":138,"x1":1051,"y0":1126,"y1":1151},"font_size":7.695e-27,"text":"demonstrating its capability in handling fact-based queries. A similar trend is observed where"},{"bbox":{"x0":140,"x1":1049,"y0":1151,"y1":1175},"font_size":7.695e-27,"text":"OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than"},{"bbox":{"x0":138,"x1":1051,"y0":1179,"y1":1203},"font_size":7.695e-27,"text":"DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse"},{"bbox":{"x0":138,"x1":1051,"y0":1207,"y1":1231},"font_size":7.695e-27,"text":"answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an"},{"bbox":{"x0":136,"x1":351,"y0":1233,"y1":1257},"font_size":7.695e-27,"text":"accuracy of over $70\\%.$"}],"source":"layout det","text":"For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over $70\\%.$"},{"bbox":{"x0":132,"x1":1058,"y0":1271,"y1":1524},"conf":0.9784,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":1274,"y1":1298},"font_size":7.695e-27,"text":"DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a"},{"bbox":{"x0":138,"x1":1049,"y0":1300,"y1":1324},"font_size":7.695e-27,"text":"model‚Äôs ability to follow format instructions. These improvements can be linked to the inclusion"},{"bbox":{"x0":136,"x1":1051,"y0":1326,"y1":1354},"font_size":7.695e-27,"text":"of instruction-following data during the fnal stages of supervised fne-tuning (SFT) and RLii"},{"bbox":{"x0":138,"x1":1051,"y0":1356,"y1":1381},"font_size":7.695e-27,"text":"training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,"},{"bbox":{"x0":138,"x1":1051,"y0":1384,"y1":1407},"font_size":7.695e-27,"text":"indicating DeepSeek-R1‚Äôs strengths in writing tasks and open-domain question answering. Its"},{"bbox":{"x0":138,"x1":1049,"y0":1410,"y1":1436},"font_size":7.695e-27,"text":"signifcant outperformance of DeepSeek-V3 underscores the generalization benefts of large-scaleii"},{"bbox":{"x0":136,"x1":1051,"y0":1435,"y1":1461},"font_size":7.695e-27,"text":"RL, which not only boosts reasoning capabilities but also improves performance across diverse"},{"bbox":{"x0":136,"x1":1051,"y0":1461,"y1":1488},"font_size":7.695e-27,"text":"domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an"},{"bbox":{"x0":136,"x1":1051,"y0":1489,"y1":1516},"font_size":7.695e-27,"text":"average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that"}],"source":"layout det","text":"DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model‚Äôs ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the fnal stages of supervised fne-tuning (SFT) and RLii training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,indicating DeepSeek-R1‚Äôs strengths in writing tasks and open-domain question answering. Its signifcant outperformance of DeepSeek-V3 underscores the generalization benefts of large-scaleii RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that"}],"formula_dets":[{"bbox":{"x0":303,"x1":351,"y0":1234,"y1":1257},"conf":0.7948,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":189,"x1":1000,"y0":222,"y1":870},"conf":0.983,"label":"Table","label_id":5},{"bbox":{"x0":132,"x1":1058,"y0":1271,"y1":1524},"conf":0.9784,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1061,"y0":931,"y1":1263},"conf":0.9756,"label":"Text","label_id":1},{"bbox":{"x0":213,"x1":974,"y0":881,"y1":917},"conf":0.7962,"label":"Table footnote","label_id":7},{"bbox":{"x0":578,"x1":613,"y0":1551,"y1":1579},"conf":0.7923,"label":"Abandon","label_id":2},{"bbox":{"x0":133,"x1":442,"y0":168,"y1":203},"conf":0.4796,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[578,1549],[613,1549],[613,1579],[578,1579]],"score":0.7721},{"poly":[[136,1491],[1051,1489],[1051,1514],[137,1516]],"score":0.777},{"poly":[[137,1461],[1051,1463],[1051,1488],[136,1486]],"score":0.7922},{"poly":[[137,1435],[1051,1437],[1051,1461],[136,1459]],"score":0.7793},{"poly":[[138,1410],[1049,1410],[1049,1435],[138,1435]],"score":0.7539},{"poly":[[138,1384],[1051,1384],[1051,1407],[138,1407]],"score":0.9217},{"poly":[[138,1356],[1051,1356],[1051,1381],[138,1381]],"score":0.7679},{"poly":[[136,1328],[1051,1326],[1051,1351],[136,1352]],"score":0.7902},{"poly":[[138,1300],[1049,1300],[1049,1324],[138,1324]],"score":0.7316},{"poly":[[174,1274],[1051,1274],[1051,1298],[174,1298]],"score":0.7686},{"poly":[[136,1233],[349,1233],[349,1256],[136,1256]],"score":0.7438},{"poly":[[138,1207],[1051,1207],[1051,1231],[138,1231]],"score":0.7566},{"poly":[[138,1179],[1051,1179],[1051,1203],[138,1203]],"score":0.7403},{"poly":[[140,1151],[1049,1151],[1049,1175],[140,1175]],"score":0.7177},{"poly":[[138,1126],[1051,1126],[1051,1151],[138,1151]],"score":0.7738},{"poly":[[138,1098],[1051,1098],[1051,1121],[138,1121]],"score":0.8887},{"poly":[[136,1067],[1051,1067],[1051,1096],[136,1096]],"score":0.647},{"poly":[[138,1044],[1051,1044],[1051,1068],[138,1068]],"score":0.7544},{"poly":[[135,1014],[1051,1012],[1051,1042],[135,1044]],"score":0.6991},{"poly":[[138,989],[1051,989],[1051,1014],[138,1014]],"score":0.7629},{"poly":[[140,963],[1053,963],[1053,988],[140,988]],"score":0.791},{"poly":[[172,935],[1049,935],[1049,960],[172,960]],"score":0.7624},{"poly":[[220,884],[969,884],[969,909],[220,909]],"score":0.7403},{"poly":[[266,835],[427,835],[427,860],[266,860]],"score":0.7843},{"poly":[[920,833],[962,833],[962,860],[920,860]],"score":0.9534},{"poly":[[762,833],[806,833],[806,860],[762,860]],"score":0.9649},{"poly":[[677,833],[723,833],[723,860],[677,860]],"score":0.818},{"poly":[[597,833],[643,833],[643,860],[597,860]],"score":0.9046},{"poly":[[510,833],[555,833],[555,860],[510,860]],"score":0.9649},{"poly":[[193,810],[362,812],[361,837],[193,835]],"score":0.8326},{"poly":[[920,808],[965,813],[962,839],[917,834]],"score":0.8439},{"poly":[[762,810],[806,810],[806,838],[762,838]],"score":0.8645},{"poly":[[677,810],[723,810],[723,838],[677,838]],"score":0.8156},{"poly":[[600,806],[644,811],[641,839],[596,834]],"score":0.7986},{"poly":[[510,810],[555,810],[555,838],[510,838]],"score":0.817},{"poly":[[920,789],[964,789],[964,816],[920,816]],"score":0.9603},{"poly":[[762,789],[806,789],[806,816],[762,816]],"score":0.9593},{"poly":[[677,789],[723,789],[723,816],[677,816]],"score":0.8977},{"poly":[[510,789],[555,789],[555,816],[510,816]],"score":0.9685},{"poly":[[263,786],[394,790],[393,816],[262,812]],"score":0.7562},{"poly":[[599,788],[643,788],[643,816],[599,816]],"score":0.864},{"poly":[[262,756],[424,758],[423,782],[262,781]],"score":0.8309},{"poly":[[920,756],[964,756],[964,782],[920,782]],"score":0.8604},{"poly":[[762,756],[806,756],[806,784],[762,784]],"score":0.8183},{"poly":[[677,756],[723,756],[723,782],[677,782]],"score":0.8934},{"poly":[[599,756],[643,756],[643,782],[599,782]],"score":0.8687},{"poly":[[510,756],[555,756],[555,782],[510,782]],"score":0.8537},{"poly":[[261,731],[413,735],[412,762],[260,758]],"score":0.6947},{"poly":[[202,735],[259,735],[259,760],[202,760]],"score":0.9007},{"poly":[[920,733],[964,733],[964,761],[920,761]],"score":0.8328},{"poly":[[837,733],[881,733],[881,761],[837,761]],"score":0.8448},{"poly":[[762,733],[806,733],[806,761],[762,761]],"score":0.8075},{"poly":[[677,733],[723,733],[723,761],[677,761]],"score":0.8137},{"poly":[[599,733],[643,733],[643,761],[599,761]],"score":0.8574},{"poly":[[510,733],[555,733],[555,761],[510,761]],"score":0.8578},{"poly":[[264,712],[411,714],[411,739],[264,737]],"score":0.8403},{"poly":[[918,712],[964,712],[964,739],[918,739]],"score":0.9304},{"poly":[[837,712],[881,712],[881,740],[837,740]],"score":0.8794},{"poly":[[762,712],[806,712],[806,740],[762,740]],"score":0.8237},{"poly":[[677,712],[723,712],[723,739],[677,739]],"score":0.9441},{"poly":[[601,712],[640,712],[640,740],[601,740]],"score":0.8404},{"poly":[[510,712],[555,712],[555,739],[510,739]],"score":0.9321},{"poly":[[266,681],[429,681],[429,705],[266,705]],"score":0.7726},{"poly":[[920,679],[964,679],[964,707],[920,707]],"score":0.8814},{"poly":[[837,679],[881,679],[881,705],[837,705]],"score":0.9027},{"poly":[[762,679],[806,679],[806,705],[762,705]],"score":0.8858},{"poly":[[679,679],[723,679],[723,705],[679,705]],"score":0.874},{"poly":[[599,679],[643,679],[643,707],[599,707]],"score":0.7899},{"poly":[[510,679],[555,679],[555,705],[510,705]],"score":0.9032},{"poly":[[764,661],[805,661],[805,682],[764,682]],"score":0.9615},{"poly":[[681,661],[721,661],[721,682],[681,682]],"score":0.9612},{"poly":[[918,658],[964,658],[964,684],[918,684]],"score":0.8561},{"poly":[[837,658],[881,658],[881,684],[837,684]],"score":0.8741},{"poly":[[599,658],[643,658],[643,684],[599,684]],"score":0.8788},{"poly":[[510,658],[555,658],[555,684],[510,684]],"score":0.8549},{"poly":[[264,656],[440,660],[439,684],[264,680]],"score":0.7526},{"poly":[[760,637],[806,637],[806,663],[760,663]],"score":0.9284},{"poly":[[677,637],[725,637],[725,663],[677,663]],"score":0.9106},{"poly":[[263,635],[411,639],[411,663],[262,659]],"score":0.7536},{"poly":[[918,635],[966,635],[966,663],[918,663]],"score":0.8203},{"poly":[[835,635],[881,635],[881,663],[835,663]],"score":0.8357},{"poly":[[599,635],[642,635],[642,663],[599,663]],"score":0.8427},{"poly":[[510,635],[553,635],[553,663],[510,663]],"score":0.8335},{"poly":[[204,626],[257,626],[257,651],[204,651]],"score":0.9264},{"poly":[[262,614],[427,616],[427,640],[262,638]],"score":0.8156},{"poly":[[920,614],[964,614],[964,640],[920,640]],"score":0.9527},{"poly":[[837,614],[881,614],[881,640],[837,640]],"score":0.9482},{"poly":[[762,614],[806,614],[806,640],[762,640]],"score":0.9432},{"poly":[[679,614],[721,614],[721,640],[679,640]],"score":0.9821},{"poly":[[599,614],[642,614],[642,640],[599,640]],"score":0.9499},{"poly":[[510,614],[555,614],[555,640],[510,640]],"score":0.9212},{"poly":[[262,593],[475,593],[475,616],[262,616]],"score":0.812},{"poly":[[920,593],[964,593],[964,619],[920,619]],"score":0.8736},{"poly":[[837,593],[881,593],[881,619],[837,619]],"score":0.8931},{"poly":[[762,593],[806,593],[806,619],[762,619]],"score":0.8846},{"poly":[[509,593],[555,593],[555,619],[509,619]],"score":0.8143},{"poly":[[677,591],[723,591],[723,619],[677,619]],"score":0.7907},{"poly":[[599,591],[643,591],[643,619],[599,619]],"score":0.859},{"poly":[[266,561],[440,561],[440,586],[266,586]],"score":0.7372},{"poly":[[920,560],[964,560],[964,588],[920,588]],"score":0.833},{"poly":[[677,560],[723,560],[723,586],[677,586]],"score":0.9103},{"poly":[[599,560],[643,560],[643,588],[599,588]],"score":0.8516},{"poly":[[510,560],[555,560],[555,586],[510,586]],"score":0.9731},{"poly":[[762,558],[806,558],[806,586],[762,586]],"score":0.8632},{"poly":[[851,544],[867,544],[867,561],[851,561]],"score":0.7042},{"poly":[[266,539],[459,539],[459,563],[266,563]],"score":0.7742},{"poly":[[920,537],[964,537],[964,563],[920,563]],"score":0.8774},{"poly":[[762,537],[806,537],[806,565],[762,565]],"score":0.8258},{"poly":[[679,537],[721,537],[721,563],[679,563]],"score":0.9305},{"poly":[[599,537],[642,537],[642,565],[599,565]],"score":0.8647},{"poly":[[510,537],[555,537],[555,563],[510,563]],"score":0.9267},{"poly":[[920,516],[964,516],[964,542],[920,542]],"score":0.9349},{"poly":[[599,516],[643,516],[643,542],[599,542]],"score":0.856},{"poly":[[264,516],[383,516],[383,540],[264,540]],"score":0.7931},{"poly":[[762,514],[806,514],[806,542],[762,542]],"score":0.7975},{"poly":[[679,514],[721,514],[721,542],[679,542]],"score":0.8848},{"poly":[[510,514],[555,514],[555,542],[510,542]],"score":0.7869},{"poly":[[264,495],[408,495],[408,519],[264,519]],"score":0.7645},{"poly":[[920,493],[964,493],[964,519],[920,519]],"score":0.8503},{"poly":[[837,490],[882,496],[879,522],[834,517]],"score":0.7906},{"poly":[[679,493],[721,493],[721,519],[679,519]],"score":0.9529},{"poly":[[599,493],[642,493],[642,519],[599,519]],"score":0.9458},{"poly":[[510,493],[555,493],[555,519],[510,519]],"score":0.8861},{"poly":[[767,488],[805,493],[802,523],[763,518]],"score":0.7552},{"poly":[[597,472],[643,472],[643,498],[597,498]],"score":0.8092},{"poly":[[510,472],[555,472],[555,498],[510,498]],"score":0.9019},{"poly":[[264,472],[455,472],[455,496],[264,496]],"score":0.7963},{"poly":[[920,470],[966,470],[966,498],[920,498]],"score":0.8354},{"poly":[[837,470],[881,470],[881,498],[837,498]],"score":0.8996},{"poly":[[762,470],[806,470],[806,498],[762,498]],"score":0.816},{"poly":[[679,470],[721,470],[721,498],[679,498]],"score":0.8496},{"poly":[[195,461],[262,461],[262,486],[195,486]],"score":0.867},{"poly":[[263,449],[415,451],[415,475],[262,474]],"score":0.8202},{"poly":[[920,449],[964,449],[964,475],[920,475]],"score":0.9617},{"poly":[[762,449],[806,449],[806,475],[762,475]],"score":0.942},{"poly":[[677,449],[721,449],[721,475],[677,475]],"score":0.8745},{"poly":[[599,449],[642,449],[642,475],[599,475]],"score":0.9531},{"poly":[[510,449],[555,449],[555,475],[510,475]],"score":0.9216},{"poly":[[920,428],[964,428],[964,454],[920,454]],"score":0.8698},{"poly":[[677,428],[723,428],[723,454],[677,454]],"score":0.8512},{"poly":[[510,428],[555,428],[555,454],[510,454]],"score":0.8973},{"poly":[[263,424],[389,430],[387,455],[262,449]],"score":0.7628},{"poly":[[837,426],[881,426],[881,454],[837,454]],"score":0.8583},{"poly":[[762,426],[806,426],[806,454],[762,454]],"score":0.7724},{"poly":[[599,426],[642,426],[642,454],[599,454]],"score":0.8489},{"poly":[[922,409],[964,409],[964,430],[922,430]],"score":0.9257},{"poly":[[263,403],[397,409],[396,434],[262,428]],"score":0.7575},{"poly":[[763,403],[807,408],[804,436],[760,431]],"score":0.8054},{"poly":[[679,405],[721,405],[721,432],[679,432]],"score":0.9208},{"poly":[[600,403],[644,408],[641,436],[597,431]],"score":0.8142},{"poly":[[511,403],[556,408],[552,436],[508,431]],"score":0.806},{"poly":[[262,384],[420,386],[420,411],[262,409]],"score":0.7891},{"poly":[[920,384],[964,384],[964,410],[920,410]],"score":0.9407},{"poly":[[762,384],[806,384],[806,410],[762,410]],"score":0.9586},{"poly":[[677,384],[721,384],[721,410],[677,410]],"score":0.9174},{"poly":[[599,384],[643,384],[643,410],[599,410]],"score":0.9218},{"poly":[[510,384],[555,384],[555,410],[510,410]],"score":0.9453},{"poly":[[918,363],[964,363],[964,389],[918,389]],"score":0.8365},{"poly":[[264,363],[385,363],[385,388],[264,388]],"score":0.7853},{"poly":[[837,361],[881,361],[881,389],[837,389]],"score":0.7966},{"poly":[[762,361],[806,361],[806,389],[762,389]],"score":0.9234},{"poly":[[677,361],[723,361],[723,389],[677,389]],"score":0.8779},{"poly":[[599,361],[642,361],[642,389],[599,389]],"score":0.9034},{"poly":[[510,361],[555,361],[555,389],[510,389]],"score":0.8566},{"poly":[[264,330],[388,330],[388,354],[264,354]],"score":0.8329},{"poly":[[918,328],[968,328],[968,356],[918,356]],"score":0.7955},{"poly":[[675,328],[725,328],[725,356],[675,356]],"score":0.785},{"poly":[[773,318],[786,312],[793,325],[780,332]],"score":0.6204},{"poly":[[613,314],[627,314],[627,330],[613,330]],"score":0.6694},{"poly":[[530,310],[541,322],[532,332],[520,320]],"score":0.751},{"poly":[[261,307],[425,307],[425,330],[261,330]],"score":0.69},{"poly":[[920,307],[964,307],[964,333],[920,333]],"score":0.9277},{"poly":[[679,307],[723,307],[723,333],[679,333]],"score":0.9162},{"poly":[[263,282],[374,286],[373,311],[262,307]],"score":0.7227},{"poly":[[916,284],[966,284],[966,310],[916,310]],"score":0.8665},{"poly":[[675,284],[725,284],[725,312],[675,312]],"score":0.8519},{"poly":[[748,254],[893,254],[893,277],[748,277]],"score":0.9847},{"poly":[[477,253],[643,253],[643,275],[477,275]],"score":0.7876},{"poly":[[923,253],[961,253],[961,281],[923,281]],"score":0.8075},{"poly":[[684,254],[718,254],[718,277],[684,277]],"score":0.9223},{"poly":[[264,240],[417,244],[416,269],[264,265]],"score":0.7473},{"poly":[[482,232],[987,232],[987,256],[482,256]],"score":0.7468},{"poly":[[137,170],[436,172],[436,197],[136,195]],"score":0.8594}],"page_no":12,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8001,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":611,"y0":1552,"y1":1575},"font_size":0.0,"text":"14"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":1057,"y0":166,"y1":231},"conf":0.9396,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":167,"y1":200},"font_size":0.0,"text":"DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying"},{"bbox":{"x0":138,"x1":484,"y0":200,"y1":223},"font_size":0.0,"text":"its robustness across multiple tasks."}],"source":"layout det","text":"DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks."},{"bbox":{"x0":133,"x1":1059,"y0":236,"y1":435},"conf":0.9736,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":240,"y1":263},"font_size":0.0,"text":"On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,"},{"bbox":{"x0":138,"x1":1051,"y0":268,"y1":291},"font_size":0.0,"text":"surpassing other models by a large margin. A similar trend is observed on coding algorithm"},{"bbox":{"x0":140,"x1":1049,"y0":295,"y1":318},"font_size":0.0,"text":"tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these"},{"bbox":{"x0":138,"x1":1049,"y0":321,"y1":346},"font_size":0.0,"text":"benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1"},{"bbox":{"x0":136,"x1":1049,"y0":346,"y1":374},"font_size":0.0,"text":"on Aider but achieves comparable performance on SWE Verifed. We believe the engineeringi"},{"bbox":{"x0":138,"x1":1051,"y0":375,"y1":400},"font_size":0.0,"text":"performance of DeepSeek-R1 will improve in the next version, as the amount of related RL"},{"bbox":{"x0":140,"x1":572,"y0":403,"y1":426},"font_size":0.0,"text":"training data currently remains very limited."}],"source":"layout det","text":"On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verifed. We believe the engineeringi performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited."},{"bbox":{"x0":133,"x1":465,"y0":464,"y1":499},"conf":0.8572,"font_size":0.0,"label":"Table caption","label_id":6,"lines":[{"bbox":{"x0":136,"x1":459,"y0":467,"y1":493},"font_size":0.0,"text":"3.2. Distilled Model Evaluation"}],"source":"layout det","text":"3.2. Distilled Model Evaluation"},{"bbox":{"x0":135,"x1":1053,"y0":515,"y1":874},"conf":0.9736,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":746,"x1":810,"y0":529,"y1":552},"font_size":0.0,"text":"GPQA"},{"bbox":{"x0":835,"x1":926,"y0":528,"y1":553},"font_size":0.0,"text":"LiveCode"},{"bbox":{"x0":476,"x1":574,"y0":544,"y1":563},"font_size":0.0,"text":"AIME 2024"},{"bbox":{"x0":619,"x1":719,"y0":543,"y1":564},"font_size":0.0,"text":"MATH-500"},{"bbox":{"x0":941,"x1":1046,"y0":538,"y1":566},"font_size":0.0,"text":"CodeForces"},{"bbox":{"x0":143,"x1":206,"y0":553,"y1":574},"font_size":0.0,"text":"Model"},{"bbox":{"x0":733,"x1":823,"y0":550,"y1":576},"font_size":0.0,"text":"Diamond"},{"bbox":{"x0":850,"x1":912,"y0":553,"y1":575},"font_size":0.0,"text":"Bench"},{"bbox":{"x0":443,"x1":511,"y0":583,"y1":613},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":527,"x1":603,"y0":586,"y1":608},"font_size":0.0,"text":"cons@64"},{"bbox":{"x0":633,"x1":701,"y0":583,"y1":612},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":742,"x1":811,"y0":583,"y1":613},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":846,"x1":914,"y0":584,"y1":613},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":963,"x1":1024,"y0":583,"y1":615},"font_size":0.0,"text":"rating"},{"bbox":{"x0":143,"x1":257,"y0":618,"y1":641},"font_size":0.0,"text":"GPT-4o-0513"},{"bbox":{"x0":462,"x1":495,"y0":619,"y1":641},"font_size":0.0,"text":"9.3"},{"bbox":{"x0":543,"x1":587,"y0":616,"y1":642},"font_size":0.0,"text":"13.4"},{"bbox":{"x0":647,"x1":690,"y0":618,"y1":641},"font_size":0.0,"text":"74.6"},{"bbox":{"x0":756,"x1":799,"y0":616,"y1":642},"font_size":0.0,"text":"49.9"},{"bbox":{"x0":861,"x1":902,"y0":618,"y1":641},"font_size":0.0,"text":"32.9"},{"bbox":{"x0":975,"x1":1012,"y0":618,"y1":641},"font_size":0.0,"text":"759"},{"bbox":{"x0":144,"x1":351,"y0":644,"y1":665},"font_size":0.0,"text":"Claude-3.5-Sonnet-1022"},{"bbox":{"x0":458,"x1":499,"y0":642,"y1":665},"font_size":0.0,"text":"16.0"},{"bbox":{"x0":544,"x1":586,"y0":642,"y1":665},"font_size":0.0,"text":"26.7"},{"bbox":{"x0":648,"x1":690,"y0":642,"y1":665},"font_size":0.0,"text":"78.3"},{"bbox":{"x0":758,"x1":799,"y0":642,"y1":665},"font_size":0.0,"text":"65.0"},{"bbox":{"x0":861,"x1":902,"y0":642,"y1":665},"font_size":0.0,"text":"38.9"},{"bbox":{"x0":975,"x1":1011,"y0":644,"y1":664},"font_size":0.0,"text":"717"},{"bbox":{"x0":144,"x1":291,"y0":666,"y1":691},"font_size":0.0,"text":"OpenAI-o1-mini"},{"bbox":{"x0":457,"x1":499,"y0":666,"y1":688},"font_size":0.0,"text":"63.6"},{"bbox":{"x0":544,"x1":585,"y0":666,"y1":688},"font_size":0.0,"text":"80.0"},{"bbox":{"x0":648,"x1":690,"y0":666,"y1":688},"font_size":0.0,"text":"90.0"},{"bbox":{"x0":757,"x1":799,"y0":666,"y1":688},"font_size":0.0,"text":"60.0"},{"bbox":{"x0":861,"x1":902,"y0":666,"y1":688},"font_size":0.0,"text":"53.8"},{"bbox":{"x0":970,"x1":1017,"y0":665,"y1":690},"font_size":0.0,"text":"1820"},{"bbox":{"x0":144,"x1":311,"y0":687,"y1":715},"font_size":0.0,"text":"QwQ-32B-Preview"},{"bbox":{"x0":458,"x1":499,"y0":690,"y1":713},"font_size":0.0,"text":"50.0"},{"bbox":{"x0":545,"x1":585,"y0":690,"y1":713},"font_size":0.0,"text":"60.0"},{"bbox":{"x0":648,"x1":690,"y0":690,"y1":713},"font_size":0.0,"text":"90.6"},{"bbox":{"x0":758,"x1":799,"y0":689,"y1":713},"font_size":0.0,"text":"54.5"},{"bbox":{"x0":861,"x1":902,"y0":689,"y1":713},"font_size":0.0,"text":"41.9"},{"bbox":{"x0":971,"x1":1016,"y0":690,"y1":713},"font_size":0.0,"text":"1316"},{"bbox":{"x0":145,"x1":429,"y0":723,"y1":747},"font_size":0.0,"text":"DeepSeek-R1-Distill-Qwen-1.5B"},{"bbox":{"x0":457,"x1":499,"y0":722,"y1":744},"font_size":0.0,"text":"28.9"},{"bbox":{"x0":545,"x1":585,"y0":721,"y1":744},"font_size":0.0,"text":"52.7"},{"bbox":{"x0":648,"x1":689,"y0":722,"y1":744},"font_size":0.0,"text":"83.9"},{"bbox":{"x0":757,"x1":799,"y0":722,"y1":745},"font_size":0.0,"text":"33.8"},{"bbox":{"x0":861,"x1":902,"y0":722,"y1":744},"font_size":0.0,"text":"16.9"},{"bbox":{"x0":976,"x1":1011,"y0":723,"y1":743},"font_size":0.0,"text":"954"},{"bbox":{"x0":145,"x1":416,"y0":747,"y1":770},"font_size":0.0,"text":"DeepSeek-R1-Distill-Qwen-7B"},{"bbox":{"x0":456,"x1":499,"y0":743,"y1":769},"font_size":0.0,"text":"55.5"},{"bbox":{"x0":544,"x1":585,"y0":745,"y1":768},"font_size":0.0,"text":"83.3"},{"bbox":{"x0":648,"x1":690,"y0":745,"y1":768},"font_size":0.0,"text":"92.8"},{"bbox":{"x0":757,"x1":798,"y0":745,"y1":768},"font_size":0.0,"text":"49.1"},{"bbox":{"x0":861,"x1":902,"y0":745,"y1":768},"font_size":0.0,"text":"37.6"},{"bbox":{"x0":972,"x1":1015,"y0":745,"y1":768},"font_size":0.0,"text":"1189"},{"bbox":{"x0":145,"x1":425,"y0":770,"y1":793},"font_size":0.0,"text":"DeepSeek-R1-Distill-Qwen-14B"},{"bbox":{"x0":458,"x1":499,"y0":769,"y1":792},"font_size":0.0,"text":"69.7"},{"bbox":{"x0":544,"x1":585,"y0":770,"y1":792},"font_size":0.0,"text":"80.0"},{"bbox":{"x0":648,"x1":689,"y0":769,"y1":792},"font_size":0.0,"text":"93.9"},{"bbox":{"x0":758,"x1":798,"y0":770,"y1":792},"font_size":0.0,"text":"59.1"},{"bbox":{"x0":861,"x1":902,"y0":770,"y1":792},"font_size":0.0,"text":"53.1"},{"bbox":{"x0":972,"x1":1013,"y0":771,"y1":790},"font_size":0.0,"text":"1481"},{"bbox":{"x0":145,"x1":424,"y0":792,"y1":816},"font_size":0.0,"text":"DeepSeek-R1-Distill-Qwen-32B"},{"bbox":{"x0":456,"x1":499,"y0":791,"y1":817},"font_size":0.0,"text":"72.6"},{"bbox":{"x0":544,"x1":585,"y0":792,"y1":815},"font_size":0.0,"text":"83.3"},{"bbox":{"x0":648,"x1":690,"y0":792,"y1":815},"font_size":0.0,"text":"94.3"},{"bbox":{"x0":758,"x1":798,"y0":792,"y1":815},"font_size":0.0,"text":"62.1"},{"bbox":{"x0":861,"x1":902,"y0":792,"y1":815},"font_size":0.0,"text":"57.2"},{"bbox":{"x0":971,"x1":1014,"y0":792,"y1":815},"font_size":0.0,"text":"1691"},{"bbox":{"x0":146,"x1":416,"y0":818,"y1":838},"font_size":0.0,"text":"DeepSeek-R1-Distill-Llama-8B"},{"bbox":{"x0":457,"x1":500,"y0":817,"y1":839},"font_size":0.0,"text":"50.4"},{"bbox":{"x0":544,"x1":585,"y0":817,"y1":839},"font_size":0.0,"text":"80.0"},{"bbox":{"x0":648,"x1":689,"y0":817,"y1":839},"font_size":0.0,"text":"89.1"},{"bbox":{"x0":758,"x1":798,"y0":819,"y1":838},"font_size":0.0,"text":"49.0"},{"bbox":{"x0":861,"x1":902,"y0":817,"y1":839},"font_size":0.0,"text":"39.6"},{"bbox":{"x0":972,"x1":1015,"y0":818,"y1":837},"font_size":0.0,"text":"1205"},{"bbox":{"x0":145,"x1":426,"y0":840,"y1":864},"font_size":0.0,"text":"DeepSeek-R1-Distill-Llama-70B"},{"bbox":{"x0":457,"x1":499,"y0":839,"y1":864},"font_size":0.0,"text":"70.0"},{"bbox":{"x0":545,"x1":586,"y0":839,"y1":863},"font_size":0.0,"text":"86.7"},{"bbox":{"x0":648,"x1":690,"y0":839,"y1":863},"font_size":0.0,"text":"94.5"},{"bbox":{"x0":757,"x1":799,"y0":840,"y1":864},"font_size":0.0,"text":"65.2"},{"bbox":{"x0":861,"x1":903,"y0":839,"y1":864},"font_size":0.0,"text":"57.5"},{"bbox":{"x0":971,"x1":1016,"y0":840,"y1":863},"font_size":0.0,"text":"1633"}],"source":"layout det","text":"<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500</td><td rowspan=\"2\">GPQA Diamond pass@1</td><td rowspan=\"2\">LiveCode Bench</td><td rowspan=\"2\">CodeForces</td></tr><tr><td>pass@1</td><td>cons@64</td></tr><tr><td>GPT-4o-0513</td><td>9.3</td><td>13.4</td><td>pass@1 74.6</td><td>49.9</td><td>pass@1 32.9</td><td>rating 759</td></tr><tr><td>Claude-3.5-Sonnet-1022</td><td>16.0</td><td>26.7</td><td>78.3</td><td>65.0</td><td>38.9</td><td>717</td></tr><tr><td>OpenAI-o1-mini</td><td>63.6</td><td>80.0</td><td>90.0</td><td>60.0</td><td>53.8</td><td>1820</td></tr><tr><td>QwQ-32B-Preview</td><td>50.0</td><td>60.0</td><td>90.6</td><td>54.5</td><td>41.9</td><td>1316</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-1.5B</td><td>28.9</td><td>52.7</td><td>83.9</td><td>33.8</td><td>16.9</td><td>954</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-7B</td><td>55.5</td><td>83.3</td><td>92.8</td><td>49.1</td><td>37.6</td><td>1189</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-14B</td><td>69.7</td><td>80.0</td><td>93.9</td><td>59.1</td><td>53.1</td><td>1481</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-32B</td><td>72.6</td><td>83.3</td><td>94.3</td><td>62.1</td><td>57.2</td><td>1691</td></tr><tr><td>DeepSeek-R1-Distill-Llama-8B</td><td>50.4</td><td>80.0</td><td>89.1</td><td>49.0</td><td>39.6</td><td>1205</td></tr><tr><td>DeepSeek-R1-Distill-Llama-70B</td><td>70.0</td><td>86.7</td><td>94.5</td><td>65.2</td><td>57.5</td><td>1633</td></tr></table></body></html>"},{"bbox":{"x0":161,"x1":1029,"y0":885,"y1":948},"conf":0.5769,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":170,"x1":1019,"y0":888,"y1":912},"font_size":0.0,"text":"Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on"},{"bbox":{"x0":443,"x1":743,"y0":917,"y1":940},"font_size":0.0,"text":"reasoning-related benchmarks."}],"source":"layout det","text":"Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks."},{"bbox":{"x0":133,"x1":1061,"y0":962,"y1":1189},"conf":0.9765,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1051,"y0":967,"y1":993},"font_size":0.0,"text":"As shown in Table 5, simply distilling DeepSeek-R1‚Äôs outputs enables the effcient DeepSeek-i"},{"bbox":{"x0":140,"x1":1053,"y0":995,"y1":1017},"font_size":0.0,"text":"R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-"},{"bbox":{"x0":138,"x1":1051,"y0":1021,"y1":1045},"font_size":0.0,"text":"reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-"},{"bbox":{"x0":140,"x1":1049,"y0":1049,"y1":1074},"font_size":0.0,"text":"Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B signifcantlyi"},{"bbox":{"x0":138,"x1":1051,"y0":1077,"y1":1102},"font_size":0.0,"text":"exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-"},{"bbox":{"x0":138,"x1":1051,"y0":1103,"y1":1128},"font_size":0.0,"text":"tion. Additionally, we found that applying RL to these distilled models yields signifcant furtheri"},{"bbox":{"x0":138,"x1":1051,"y0":1128,"y1":1156},"font_size":0.0,"text":"gains. We believe this warrants further exploration and therefore present only the results of the"},{"bbox":{"x0":138,"x1":463,"y0":1156,"y1":1182},"font_size":0.0,"text":"simple SFT-distilled models here."}],"source":"layout det","text":"As shown in Table 5, simply distilling DeepSeek-R1‚Äôs outputs enables the effcient DeepSeek-i R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform nonreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32BPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B signifcantlyi exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields signifcant furtheri gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here."},{"bbox":{"x0":133,"x1":313,"y0":1222,"y1":1259},"conf":0.8863,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":307,"y0":1224,"y1":1253},"font_size":0.0,"text":"4. Discussion"}],"source":"layout det","text":"4.Discussion"},{"bbox":{"x0":132,"x1":594,"y0":1278,"y1":1313},"conf":0.9203,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":135,"x1":590,"y0":1277,"y1":1310},"font_size":0.0,"text":"4.1. Distillation v.s. Reinforcement Learning"}],"source":"layout det","text":"4.1. Distillation v.s. Reinforcement Learning"},{"bbox":{"x0":131,"x1":1057,"y0":1321,"y1":1413},"conf":0.9582,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1324,"y1":1349},"font_size":0.0,"text":"In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive"},{"bbox":{"x0":140,"x1":1049,"y0":1354,"y1":1379},"font_size":0.0,"text":"results. However, there is still one question left: can the model achieve comparable performance"},{"bbox":{"x0":140,"x1":902,"y0":1382,"y1":1407},"font_size":0.0,"text":"through the large-scale RL training discussed in the paper without distillation?"}],"source":"layout det","text":"In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?"},{"bbox":{"x0":132,"x1":1059,"y0":1417,"y1":1509},"conf":0.9534,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":175,"x1":1049,"y0":1421,"y1":1445},"font_size":0.0,"text":"To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,"},{"bbox":{"x0":140,"x1":1049,"y0":1449,"y1":1474},"font_size":0.0,"text":"code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The"},{"bbox":{"x0":138,"x1":1049,"y0":1473,"y1":1500},"font_size":0.0,"text":"experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale"}],"source":"layout det","text":"To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1061,"y0":962,"y1":1189},"conf":0.9765,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":236,"y1":435},"conf":0.9736,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1053,"y0":515,"y1":874},"conf":0.9736,"label":"Table","label_id":5},{"bbox":{"x0":131,"x1":1057,"y0":1321,"y1":1413},"conf":0.9582,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1417,"y1":1509},"conf":0.9534,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":166,"y1":231},"conf":0.9396,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":594,"y0":1278,"y1":1313},"conf":0.9203,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":313,"y0":1222,"y1":1259},"conf":0.8863,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":465,"y0":464,"y1":499},"conf":0.8572,"label":"Table caption","label_id":6},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.8001,"label":"Abandon","label_id":2},{"bbox":{"x0":161,"x1":1029,"y0":885,"y1":948},"conf":0.5769,"label":"Table footnote","label_id":7},{"bbox":{"x0":161,"x1":1029,"y0":885,"y1":948},"conf":0.409,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1552],[611,1552],[611,1575],[580,1575]],"score":0.8566},{"poly":[[138,1475],[1049,1473],[1049,1498],[138,1500]],"score":0.7861},{"poly":[[140,1449],[1049,1449],[1049,1474],[140,1474]],"score":0.7506},{"poly":[[175,1421],[1049,1421],[1049,1445],[175,1445]],"score":0.7501},{"poly":[[140,1382],[902,1382],[902,1407],[140,1407]],"score":0.7674},{"poly":[[140,1354],[1049,1354],[1049,1379],[140,1379]],"score":0.7528},{"poly":[[140,1324],[1049,1324],[1049,1349],[140,1349]],"score":0.728},{"poly":[[135,1277],[590,1281],[590,1310],[135,1307]],"score":0.7381},{"poly":[[137,1224],[307,1226],[306,1253],[136,1251]],"score":0.7555},{"poly":[[138,1158],[462,1156],[463,1181],[138,1182]],"score":0.8331},{"poly":[[138,1131],[1051,1128],[1051,1152],[138,1156]],"score":0.813},{"poly":[[138,1103],[1051,1103],[1051,1128],[138,1128]],"score":0.7821},{"poly":[[138,1077],[1051,1077],[1051,1102],[138,1102]],"score":0.7791},{"poly":[[140,1049],[1049,1049],[1049,1072],[140,1072]],"score":0.9251},{"poly":[[138,1021],[1051,1021],[1051,1045],[138,1045]],"score":0.7591},{"poly":[[140,995],[1053,995],[1053,1017],[140,1017]],"score":0.9006},{"poly":[[174,967],[1051,967],[1051,991],[174,991]],"score":0.7581},{"poly":[[443,917],[743,917],[743,940],[443,940]],"score":0.9461},{"poly":[[170,888],[1019,888],[1019,912],[170,912]],"score":0.7999},{"poly":[[142,839],[427,837],[427,861],[142,863]],"score":0.7526},{"poly":[[969,837],[1017,837],[1017,865],[969,865]],"score":0.8517},{"poly":[[755,837],[799,837],[799,863],[755,863]],"score":0.9657},{"poly":[[542,837],[588,837],[588,865],[542,865]],"score":0.8438},{"poly":[[858,833],[905,838],[902,866],[855,860]],"score":0.8201},{"poly":[[645,835],[693,835],[693,867],[645,867]],"score":0.7856},{"poly":[[455,835],[502,835],[502,863],[455,863]],"score":0.8318},{"poly":[[144,816],[418,816],[418,838],[144,838]],"score":0.8417},{"poly":[[969,814],[1017,814],[1017,840],[969,840]],"score":0.9315},{"poly":[[860,814],[904,814],[904,840],[860,840]],"score":0.9467},{"poly":[[755,814],[799,814],[799,840],[755,840]],"score":0.964},{"poly":[[645,814],[691,814],[691,840],[645,840]],"score":0.9387},{"poly":[[542,814],[588,814],[588,840],[542,840]],"score":0.9233},{"poly":[[455,814],[502,814],[502,840],[455,840]],"score":0.9003},{"poly":[[142,793],[425,793],[425,816],[142,816]],"score":0.8272},{"poly":[[645,791],[691,791],[691,817],[645,817]],"score":0.9075},{"poly":[[969,789],[1017,789],[1017,816],[969,816]],"score":0.8255},{"poly":[[860,789],[904,789],[904,817],[860,817]],"score":0.8586},{"poly":[[755,789],[799,789],[799,817],[755,817]],"score":0.8295},{"poly":[[542,789],[587,789],[587,817],[542,817]],"score":0.8348},{"poly":[[456,787],[502,792],[499,820],[453,815]],"score":0.8044},{"poly":[[140,768],[423,767],[424,791],[140,793]],"score":0.772},{"poly":[[969,767],[1017,767],[1017,793],[969,793]],"score":0.9111},{"poly":[[860,767],[904,767],[904,793],[860,793]],"score":0.9329},{"poly":[[755,767],[799,767],[799,793],[755,793]],"score":0.9225},{"poly":[[645,767],[691,767],[691,793],[645,793]],"score":0.9414},{"poly":[[542,767],[588,767],[588,793],[542,793]],"score":0.8479},{"poly":[[455,767],[502,767],[502,793],[455,793]],"score":0.8487},{"poly":[[142,746],[416,746],[416,768],[142,768]],"score":0.8576},{"poly":[[971,744],[1017,744],[1017,770],[971,770]],"score":0.9021},{"poly":[[860,744],[904,744],[904,770],[860,770]],"score":0.9571},{"poly":[[755,744],[799,744],[799,770],[755,770]],"score":0.937},{"poly":[[645,744],[691,744],[691,770],[645,770]],"score":0.8629},{"poly":[[542,744],[587,744],[587,770],[542,770]],"score":0.9462},{"poly":[[455,744],[500,744],[500,770],[455,770]],"score":0.8942},{"poly":[[142,721],[429,721],[429,744],[142,744]],"score":0.9221},{"poly":[[973,719],[1016,719],[1016,746],[973,746]],"score":0.9043},{"poly":[[860,719],[904,719],[904,747],[860,747]],"score":0.868},{"poly":[[755,719],[799,719],[799,747],[755,747]],"score":0.8831},{"poly":[[645,719],[691,719],[691,747],[645,747]],"score":0.8496},{"poly":[[542,719],[587,719],[587,747],[542,747]],"score":0.9237},{"poly":[[454,719],[500,719],[500,747],[454,747]],"score":0.7659},{"poly":[[140,689],[310,688],[310,712],[140,714]],"score":0.8275},{"poly":[[971,688],[1019,688],[1019,714],[971,714]],"score":0.9186},{"poly":[[542,688],[588,688],[588,714],[542,714]],"score":0.8988},{"poly":[[860,686],[904,686],[904,714],[860,714]],"score":0.859},{"poly":[[755,686],[799,686],[799,714],[755,714]],"score":0.8683},{"poly":[[646,683],[692,689],[689,716],[642,711]],"score":0.8127},{"poly":[[455,686],[500,686],[500,714],[455,714]],"score":0.8583},{"poly":[[142,665],[294,665],[294,689],[142,689]],"score":0.7594},{"poly":[[969,663],[1017,663],[1017,689],[969,689]],"score":0.9194},{"poly":[[860,663],[904,663],[904,689],[860,689]],"score":0.9633},{"poly":[[755,663],[801,663],[801,689],[755,689]],"score":0.8946},{"poly":[[645,663],[691,663],[691,689],[645,689]],"score":0.9478},{"poly":[[542,663],[587,663],[587,689],[542,689]],"score":0.9427},{"poly":[[455,663],[500,663],[500,689],[455,689]],"score":0.9074},{"poly":[[142,642],[353,642],[353,665],[142,665]],"score":0.9406},{"poly":[[755,640],[801,640],[801,667],[755,667]],"score":0.9076},{"poly":[[973,639],[1014,639],[1014,667],[973,667]],"score":0.9109},{"poly":[[860,639],[904,639],[904,667],[860,667]],"score":0.8626},{"poly":[[647,639],[691,639],[691,667],[647,667]],"score":0.8838},{"poly":[[542,639],[587,639],[587,667],[542,667]],"score":0.893},{"poly":[[457,639],[500,639],[500,667],[457,667]],"score":0.8543},{"poly":[[142,617],[255,617],[255,637],[142,637]],"score":0.7922},{"poly":[[973,616],[1014,616],[1014,644],[973,644]],"score":0.8418},{"poly":[[860,616],[904,616],[904,644],[860,644]],"score":0.8804},{"poly":[[755,616],[801,616],[801,642],[755,642]],"score":0.8695},{"poly":[[645,616],[691,616],[691,644],[645,644]],"score":0.8263},{"poly":[[542,616],[587,616],[587,642],[542,642]],"score":0.9349},{"poly":[[457,616],[498,616],[498,642],[457,642]],"score":0.9096},{"poly":[[965,580],[1025,587],[1022,615],[961,608]],"score":0.779},{"poly":[[845,587],[911,580],[914,606],[847,613]],"score":0.8079},{"poly":[[740,587],[807,579],[811,607],[743,615]],"score":0.7593},{"poly":[[630,587],[698,580],[701,606],[633,613]],"score":0.7837},{"poly":[[523,584],[604,584],[604,609],[523,609]],"score":0.8285},{"poly":[[443,585],[511,580],[513,606],[444,611]],"score":0.8145},{"poly":[[849,553],[913,553],[913,577],[849,577]],"score":0.9322},{"poly":[[733,549],[823,553],[821,579],[732,575]],"score":0.7954},{"poly":[[142,551],[207,551],[207,575],[142,575]],"score":0.9434},{"poly":[[938,536],[1048,541],[1047,565],[937,561]],"score":0.7507},{"poly":[[616,536],[720,541],[719,565],[615,561]],"score":0.8536},{"poly":[[475,542],[574,542],[574,561],[475,561]],"score":0.9623},{"poly":[[835,528],[927,528],[927,553],[835,553]],"score":0.8591},{"poly":[[744,528],[812,528],[812,553],[744,553]],"score":0.9366},{"poly":[[137,467],[459,468],[459,493],[136,491]],"score":0.7823},{"poly":[[140,403],[572,403],[572,426],[140,426]],"score":0.9366},{"poly":[[138,375],[1051,375],[1051,400],[138,400]],"score":0.742},{"poly":[[137,346],[1049,349],[1049,374],[136,370]],"score":0.7931},{"poly":[[138,321],[1049,321],[1049,346],[138,346]],"score":0.7495},{"poly":[[140,295],[1049,295],[1049,318],[140,318]],"score":0.8764},{"poly":[[138,268],[1051,268],[1051,291],[138,291]],"score":0.8671},{"poly":[[174,240],[1051,240],[1051,263],[174,263]],"score":0.8999},{"poly":[[138,200],[484,200],[484,223],[138,223]],"score":0.9355},{"poly":[[137,167],[1051,170],[1051,200],[136,196]],"score":0.6537}],"page_no":13,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.7939,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"15"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":137,"x1":1051,"y0":165,"y1":324},"conf":0.9703,"font_size":0.0,"label":"Table","label_id":5,"lines":[{"bbox":{"x0":468,"x1":571,"y0":173,"y1":197},"font_size":0.0,"text":"AIME 2024"},{"bbox":{"x0":612,"x1":716,"y0":173,"y1":197},"font_size":0.0,"text":"MATH-500"},{"bbox":{"x0":728,"x1":881,"y0":173,"y1":197},"font_size":0.0,"text":"GPQA Diamond"},{"bbox":{"x0":891,"x1":1035,"y0":173,"y1":197},"font_size":0.0,"text":"LiveCodeBench"},{"bbox":{"x0":141,"x1":208,"y0":197,"y1":223},"font_size":0.0,"text":"Model"},{"bbox":{"x0":437,"x1":507,"y0":204,"y1":234},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":519,"x1":600,"y0":207,"y1":231},"font_size":0.0,"text":"cons@64"},{"bbox":{"x0":626,"x1":697,"y0":203,"y1":234},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":767,"x1":838,"y0":204,"y1":234},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":928,"x1":998,"y0":204,"y1":234},"font_size":0.0,"text":"pass@1"},{"bbox":{"x0":144,"x1":311,"y0":239,"y1":265},"font_size":0.0,"text":"QwQ-32B-Preview"},{"bbox":{"x0":452,"x1":495,"y0":239,"y1":262},"font_size":0.0,"text":"50.0"},{"bbox":{"x0":538,"x1":582,"y0":238,"y1":264},"font_size":0.0,"text":"60.0"},{"bbox":{"x0":640,"x1":686,"y0":238,"y1":263},"font_size":0.0,"text":"90.6"},{"bbox":{"x0":781,"x1":826,"y0":238,"y1":264},"font_size":0.0,"text":"54.5"},{"bbox":{"x0":944,"x1":986,"y0":239,"y1":262},"font_size":0.0,"text":"41.9"},{"bbox":{"x0":143,"x1":410,"y0":261,"y1":287},"font_size":0.0,"text":"DeepSeek-R1-Zero-Qwen-32B"},{"bbox":{"x0":451,"x1":496,"y0":261,"y1":287},"font_size":0.0,"text":"47.0"},{"bbox":{"x0":537,"x1":582,"y0":261,"y1":287},"font_size":0.0,"text":"60.0"},{"bbox":{"x0":639,"x1":686,"y0":259,"y1":288},"font_size":0.0,"text":"91.6"},{"bbox":{"x0":781,"x1":826,"y0":261,"y1":287},"font_size":0.0,"text":"55.0"},{"bbox":{"x0":943,"x1":988,"y0":261,"y1":287},"font_size":0.0,"text":"40.2"},{"bbox":{"x0":144,"x1":427,"y0":286,"y1":313},"font_size":0.0,"text":"DeepSeek-R1-Distill-Qwen-32B"},{"bbox":{"x0":449,"x1":496,"y0":284,"y1":313},"font_size":0.0,"text":"72.6"},{"bbox":{"x0":537,"x1":582,"y0":286,"y1":312},"font_size":0.0,"text":"83.3"},{"bbox":{"x0":642,"x1":685,"y0":287,"y1":311},"font_size":0.0,"text":"94.3"},{"bbox":{"x0":782,"x1":824,"y0":287,"y1":311},"font_size":0.0,"text":"62.1"},{"bbox":{"x0":943,"x1":988,"y0":286,"y1":312},"font_size":0.0,"text":"57.2"}],"source":"layout det","text":"<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500 pass@1</td><td rowspan=\"2\">GPQA Diamond pass@1</td><td rowspan=\"2\">LiveCodeBench pass@1</td></tr><tr><td>pass@1</td><td>cons@64</td></tr><tr><td>QwQ-32B-Preview</td><td>50.0</td><td>60.0</td><td>90.6</td><td>54.5</td><td>41.9</td></tr><tr><td>DeepSeek-R1-Zero-Qwen-32B</td><td>47.0</td><td>60.0</td><td>91.6</td><td>55.0</td><td>40.2</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-32B</td><td>72.6</td><td>83.3</td><td>94.3</td><td>62.1</td><td>57.2</td></tr></table></body></html>"},{"bbox":{"x0":179,"x1":1009,"y0":332,"y1":370},"conf":0.6247,"font_size":0.0,"label":"Table footnote","label_id":7,"lines":[{"bbox":{"x0":188,"x1":1000,"y0":335,"y1":361},"font_size":0.0,"text":"Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks."}],"source":"layout det","text":"Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks."},{"bbox":{"x0":132,"x1":1059,"y0":397,"y1":488},"conf":0.9528,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":403,"y1":428},"font_size":0.0,"text":"RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-"},{"bbox":{"x0":138,"x1":1049,"y0":430,"y1":456},"font_size":0.0,"text":"Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs signifcantly better thani"},{"bbox":{"x0":140,"x1":652,"y0":460,"y1":482},"font_size":0.0,"text":"DeepSeek-R1-Zero-Qwen-32B across all benchmarks."}],"source":"layout det","text":"RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs signifcantly better thani DeepSeek-R1-Zero-Qwen-32B across all benchmarks."},{"bbox":{"x0":133,"x1":1059,"y0":495,"y1":667},"conf":0.9747,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":496,"y1":523},"font_size":0.0,"text":"Therefore, we can draw two conclusions: First, distilling more powerful models into smaller"},{"bbox":{"x0":140,"x1":1051,"y0":526,"y1":551},"font_size":0.0,"text":"ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in"},{"bbox":{"x0":140,"x1":1051,"y0":554,"y1":577},"font_size":0.0,"text":"this paper require enormous computational power and may not even achieve the performance"},{"bbox":{"x0":136,"x1":1051,"y0":575,"y1":609},"font_size":0.0,"text":"of distillation. Second, while distillation strategies are both economical and effective, advancing"},{"bbox":{"x0":138,"x1":1049,"y0":607,"y1":633},"font_size":0.0,"text":"beyond the boundaries of intelligence may still require more powerful base models and larger-"},{"bbox":{"x0":138,"x1":420,"y0":633,"y1":661},"font_size":0.0,"text":"scale reinforcement learning."}],"source":"layout det","text":"Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and largerscale reinforcement learning."},{"bbox":{"x0":133,"x1":422,"y0":694,"y1":730},"conf":0.8948,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":416,"y0":700,"y1":723},"font_size":0.0,"text":"4.2. Unsuccessful Attempts"}],"source":"layout det","text":"4.2. Unsuccessful Attempts"},{"bbox":{"x0":132,"x1":1057,"y0":739,"y1":830},"conf":0.9556,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":744,"y1":768},"font_size":0.0,"text":"In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along"},{"bbox":{"x0":140,"x1":1049,"y0":772,"y1":795},"font_size":0.0,"text":"the way. We share our failure experiences here to provide insights, but this does not imply that"},{"bbox":{"x0":140,"x1":847,"y0":798,"y1":823},"font_size":0.0,"text":"these approaches are incapable of developing effective reasoning models."}],"source":"layout det","text":"In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models."},{"bbox":{"x0":133,"x1":1061,"y0":863,"y1":1200},"conf":0.9747,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":868,"y1":891},"font_size":0.0,"text":"Process Reward Model (PRM)PRM is a reasonable method to guide the model toward better"},{"bbox":{"x0":138,"x1":1051,"y0":895,"y1":921},"font_size":0.0,"text":"approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,"},{"bbox":{"x0":140,"x1":1051,"y0":923,"y1":947},"font_size":0.0,"text":"2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-"},{"bbox":{"x0":140,"x1":1051,"y0":951,"y1":975},"font_size":0.0,"text":"cess. First, it is challenging to explicitly defne a fne-grain step in general reasoning. Second,ii"},{"bbox":{"x0":140,"x1":1047,"y0":975,"y1":998},"font_size":0.0,"text":"determining whether the current intermediate step is correct is a challenging task. Automated"},{"bbox":{"x0":138,"x1":1049,"y0":1003,"y1":1026},"font_size":0.0,"text":"annotation using models may not yield satisfactory results, while manual annotation is not con-"},{"bbox":{"x0":140,"x1":1049,"y0":1031,"y1":1056},"font_size":0.0,"text":"ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward"},{"bbox":{"x0":138,"x1":1047,"y0":1056,"y1":1081},"font_size":0.0,"text":"hacking (Gao et al., 2022), and retraining the reward model needs additional training resources"},{"bbox":{"x0":140,"x1":1049,"y0":1086,"y1":1110},"font_size":0.0,"text":"and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good"},{"bbox":{"x0":138,"x1":1049,"y0":1110,"y1":1137},"font_size":0.0,"text":"ability to rerank the top-N responses generated by the model or assist in guided search (Snell"},{"bbox":{"x0":140,"x1":1051,"y0":1140,"y1":1165},"font_size":0.0,"text":"et al., 2024), its advantages are limited compared to the additional computational overhead it"},{"bbox":{"x0":138,"x1":954,"y0":1165,"y1":1191},"font_size":0.0,"text":"introduces during the large-scale reinforcement learning process in our experiments."}],"source":"layout det","text":"Process Reward Model (PRM)PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly defne a fne-grain step in general reasoning. Second,ii determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments."},{"bbox":{"x0":132,"x1":1061,"y0":1231,"y1":1457},"conf":0.976,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1053,"y0":1237,"y1":1259},"font_size":0.0,"text":"Monte Carlo Tree Search (MCTS)Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-"},{"bbox":{"x0":138,"x1":1051,"y0":1265,"y1":1288},"font_size":0.0,"text":"ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time"},{"bbox":{"x0":140,"x1":1049,"y0":1293,"y1":1316},"font_size":0.0,"text":"compute scalability. This approach involves breaking answers into smaller parts to allow the"},{"bbox":{"x0":140,"x1":1049,"y0":1319,"y1":1344},"font_size":0.0,"text":"model to explore the solution space systematically. To facilitate this, we prompt the model to"},{"bbox":{"x0":138,"x1":1051,"y0":1344,"y1":1372},"font_size":0.0,"text":"generate multiple tags that correspond to specifc reasoning steps necessary for the search. Fori"},{"bbox":{"x0":138,"x1":1049,"y0":1374,"y1":1397},"font_size":0.0,"text":"training, we frst use collected prompts to fnd answers via MCTS guided by a pre-trained valueii"},{"bbox":{"x0":138,"x1":1049,"y0":1400,"y1":1424},"font_size":0.0,"text":"model. Subsequently, we use the resulting question-answer pairs to train both the actor model"},{"bbox":{"x0":138,"x1":647,"y0":1426,"y1":1452},"font_size":0.0,"text":"and the value model, iteratively refning the process.i"}],"source":"layout det","text":"Monte Carlo Tree Search (MCTS)Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specifc reasoning steps necessary for the search. Fori training, we frst use collected prompts to fnd answers via MCTS guided by a pre-trained valueii model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refning the process.i"},{"bbox":{"x0":133,"x1":1059,"y0":1463,"y1":1526},"conf":0.9217,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":1468,"y1":1493},"font_size":0.0,"text":"However, this approach encounters several challenges when scaling up the training. First,"},{"bbox":{"x0":140,"x1":1051,"y0":1495,"y1":1519},"font_size":0.0,"text":"unlike chess, where the search space is relatively well-defned, token generation presents ani"}],"source":"layout det","text":"However, this approach encounters several challenges when scaling up the training. First,unlike chess, where the search space is relatively well-defned, token generation presents ani"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1061,"y0":1231,"y1":1457},"conf":0.976,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":495,"y1":667},"conf":0.9747,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1061,"y0":863,"y1":1200},"conf":0.9747,"label":"Text","label_id":1},{"bbox":{"x0":137,"x1":1051,"y0":165,"y1":324},"conf":0.9703,"label":"Table","label_id":5},{"bbox":{"x0":132,"x1":1057,"y0":739,"y1":830},"conf":0.9556,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":397,"y1":488},"conf":0.9528,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":1463,"y1":1526},"conf":0.9217,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":422,"y0":694,"y1":730},"conf":0.8948,"label":"Title","label_id":0},{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1580},"conf":0.7939,"label":"Abandon","label_id":2},{"bbox":{"x0":179,"x1":1009,"y0":332,"y1":370},"conf":0.6247,"label":"Table footnote","label_id":7},{"bbox":{"x0":179,"x1":1009,"y0":332,"y1":370},"conf":0.4631,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.8354},{"poly":[[140,1495],[1051,1495],[1051,1519],[140,1519]],"score":0.7534},{"poly":[[172,1468],[1049,1468],[1049,1493],[172,1493]],"score":0.7713},{"poly":[[138,1426],[647,1428],[647,1452],[138,1451]],"score":0.7867},{"poly":[[138,1400],[1049,1400],[1049,1424],[138,1424]],"score":0.7351},{"poly":[[138,1374],[1049,1374],[1049,1396],[138,1396]],"score":0.8914},{"poly":[[138,1347],[1051,1344],[1051,1368],[138,1372]],"score":0.7076},{"poly":[[140,1319],[1049,1319],[1049,1344],[140,1344]],"score":0.7033},{"poly":[[140,1293],[1049,1293],[1049,1316],[140,1316]],"score":0.8257},{"poly":[[138,1265],[1051,1265],[1051,1288],[138,1288]],"score":0.8247},{"poly":[[140,1237],[1053,1237],[1053,1259],[140,1259]],"score":0.8493},{"poly":[[138,1165],[954,1167],[953,1191],[138,1189]],"score":0.7586},{"poly":[[140,1140],[1051,1140],[1051,1165],[140,1165]],"score":0.7354},{"poly":[[138,1112],[1049,1110],[1049,1135],[138,1137]],"score":0.7863},{"poly":[[140,1086],[1049,1086],[1049,1110],[140,1110]],"score":0.7459},{"poly":[[138,1056],[1047,1056],[1047,1081],[138,1081]],"score":0.7195},{"poly":[[140,1031],[1049,1031],[1049,1056],[140,1056]],"score":0.736},{"poly":[[138,1003],[1049,1003],[1049,1026],[138,1026]],"score":0.8176},{"poly":[[140,975],[1047,975],[1047,998],[140,998]],"score":0.6946},{"poly":[[140,951],[1051,951],[1051,974],[140,974]],"score":0.841},{"poly":[[140,923],[1051,923],[1051,947],[140,947]],"score":0.7328},{"poly":[[138,896],[1051,895],[1051,919],[138,921]],"score":0.7994},{"poly":[[140,868],[1051,868],[1051,891],[140,891]],"score":0.9314},{"poly":[[140,798],[847,798],[847,823],[140,823]],"score":0.733},{"poly":[[140,772],[1049,772],[1049,795],[140,795]],"score":0.8998},{"poly":[[138,744],[1049,744],[1049,768],[138,768]],"score":0.7782},{"poly":[[140,700],[416,700],[416,723],[140,723]],"score":0.8745},{"poly":[[138,633],[420,637],[420,661],[138,658]],"score":0.7577},{"poly":[[138,607],[1049,609],[1049,633],[138,631]],"score":0.7907},{"poly":[[137,575],[1051,579],[1051,609],[136,605]],"score":0.6626},{"poly":[[140,554],[1051,554],[1051,577],[140,577]],"score":0.8419},{"poly":[[140,526],[1051,526],[1051,551],[140,551]],"score":0.7315},{"poly":[[174,496],[1049,498],[1049,523],[174,521]],"score":0.7817},{"poly":[[140,460],[652,460],[652,482],[140,482]],"score":0.9329},{"poly":[[138,430],[1049,432],[1049,456],[138,454]],"score":0.7898},{"poly":[[140,403],[1051,403],[1051,428],[140,428]],"score":0.7411},{"poly":[[188,335],[1000,337],[1000,361],[188,360]],"score":0.7895},{"poly":[[143,288],[423,286],[424,310],[144,312]],"score":0.7541},{"poly":[[943,286],[985,286],[985,314],[943,314]],"score":0.9273},{"poly":[[782,286],[824,286],[824,314],[782,314]],"score":0.9296},{"poly":[[642,286],[684,286],[684,314],[642,314]],"score":0.8727},{"poly":[[539,286],[581,286],[581,312],[539,312]],"score":0.9263},{"poly":[[453,282],[495,287],[492,315],[449,309]],"score":0.7948},{"poly":[[143,263],[409,261],[409,286],[144,288]],"score":0.7524},{"poly":[[943,261],[987,261],[987,288],[943,288]],"score":0.9573},{"poly":[[539,261],[581,261],[581,288],[539,288]],"score":0.982},{"poly":[[452,261],[496,261],[496,288],[452,288]],"score":0.9427},{"poly":[[782,255],[827,261],[823,290],[779,285]],"score":0.7913},{"poly":[[640,257],[687,262],[684,290],[637,285]],"score":0.8136},{"poly":[[144,239],[308,239],[308,261],[144,261]],"score":0.7264},{"poly":[[943,239],[987,239],[987,267],[943,267]],"score":0.8754},{"poly":[[780,239],[826,239],[826,265],[780,265]],"score":0.922},{"poly":[[640,239],[686,239],[686,265],[640,265]],"score":0.9327},{"poly":[[537,239],[583,239],[583,265],[537,265]],"score":0.877},{"poly":[[450,239],[496,239],[496,265],[450,265]],"score":0.9206},{"poly":[[928,210],[994,203],[997,229],[931,236]],"score":0.8209},{"poly":[[516,207],[597,207],[597,232],[516,232]],"score":0.8091},{"poly":[[437,209],[509,203],[511,229],[439,236]],"score":0.7491},{"poly":[[767,210],[835,203],[837,229],[769,236]],"score":0.819},{"poly":[[627,207],[695,203],[697,229],[629,234]],"score":0.7842},{"poly":[[142,198],[207,198],[207,223],[142,223]],"score":0.8818},{"poly":[[709,174],[1037,174],[1037,196],[709,196]],"score":0.8024},{"poly":[[613,175],[718,175],[718,195],[613,195]],"score":0.8624},{"poly":[[467,170],[571,174],[570,199],[466,194]],"score":0.8191}],"page_no":14,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1579},"conf":0.7934,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"16"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":1058,"y0":167,"y1":367},"conf":0.9786,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":174,"y1":198},"font_size":0.0,"text":"exponentially larger search space. To address this, we set a maximum extension limit for each"},{"bbox":{"x0":138,"x1":1047,"y0":200,"y1":225},"font_size":0.0,"text":"node, but this can lead to the model getting stuck in local optima. Second, the value model"},{"bbox":{"x0":140,"x1":1051,"y0":228,"y1":253},"font_size":0.0,"text":"directly infuences the quality of generation since it guides each step of the search process.l"},{"bbox":{"x0":140,"x1":1049,"y0":254,"y1":279},"font_size":0.0,"text":"Training a fne-grained value model is inherently diffcult, which makes it challenging for theii"},{"bbox":{"x0":140,"x1":1049,"y0":282,"y1":307},"font_size":0.0,"text":"model to iteratively improve. While AlphaGo‚Äôs core success relied on training a value model to"},{"bbox":{"x0":136,"x1":1049,"y0":305,"y1":335},"font_size":0.0,"text":"progressively enhance its performance, this principle proves diffcult to replicate in our setupi"},{"bbox":{"x0":140,"x1":567,"y0":337,"y1":361},"font_size":0.0,"text":"due to the complexities of token generation."}],"source":"layout det","text":"exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly infuences the quality of generation since it guides each step of the search process.l Training a fne-grained value model is inherently diffcult, which makes it challenging for theii model to iteratively improve. While AlphaGo‚Äôs core success relied on training a value model to progressively enhance its performance, this principle proves diffcult to replicate in our setupi due to the complexities of token generation."},{"bbox":{"x0":132,"x1":1057,"y0":371,"y1":461},"conf":0.9643,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1051,"y0":372,"y1":403},"font_size":0.0,"text":"In conclusion, while MCTS can improve performance during inference when paired with a"},{"bbox":{"x0":136,"x1":1051,"y0":400,"y1":432},"font_size":0.0,"text":"pre-trained value model, iteratively boosting model performance through self-search remains a"},{"bbox":{"x0":136,"x1":346,"y0":428,"y1":460},"font_size":0.0,"text":"signifcant challenge.i"}],"source":"layout det","text":"In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a signifcant challenge.i"},{"bbox":{"x0":133,"x1":678,"y0":495,"y1":532},"conf":0.9075,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":672,"y0":502,"y1":526},"font_size":0.0,"text":"5. Conclusion, Limitations, and Future Work"}],"source":"layout det","text":"5.Conclusion, Limitations, and Future Work"},{"bbox":{"x0":133,"x1":1059,"y0":548,"y1":693},"conf":0.9683,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":551,"y1":577},"font_size":0.0,"text":"In this work, we share our journey in enhancing model reasoning abilities through reinforcement"},{"bbox":{"x0":136,"x1":1051,"y0":577,"y1":609},"font_size":0.0,"text":"learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start"},{"bbox":{"x0":140,"x1":1051,"y0":609,"y1":633},"font_size":0.0,"text":"data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,"},{"bbox":{"x0":140,"x1":1047,"y0":633,"y1":659},"font_size":0.0,"text":"leveraging cold-start data alongside iterative RL fne-tuning. Ultimately, DeepSeek-R1 achievesi"},{"bbox":{"x0":135,"x1":762,"y0":660,"y1":691},"font_size":0.0,"text":"performance comparable to OpenAI-o1-1217 on a range of tasks."}],"source":"layout det","text":"In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,leveraging cold-start data alongside iterative RL fne-tuning. Ultimately, DeepSeek-R1 achievesi performance comparable to OpenAI-o1-1217 on a range of tasks."},{"bbox":{"x0":133,"x1":1059,"y0":697,"y1":868},"conf":0.9759,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":174,"x1":1049,"y0":700,"y1":726},"font_size":0.0,"text":"We further explore distillation the reasoning capability to small dense models. We use"},{"bbox":{"x0":140,"x1":1049,"y0":730,"y1":754},"font_size":0.0,"text":"DeepSeek-R1 as the teacher model to generate 800K training samples, and fne-tune several smalli"},{"bbox":{"x0":140,"x1":1049,"y0":758,"y1":781},"font_size":0.0,"text":"dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o"},{"bbox":{"x0":138,"x1":1049,"y0":782,"y1":809},"font_size":0.0,"text":"and Claude-3.5-Sonnet on math benchmarks with $28.9\\%$ on AIME and $83.9\\%$ on MATH. Other"},{"bbox":{"x0":140,"x1":1051,"y0":812,"y1":837},"font_size":0.0,"text":"dense models also achieve impressive results, signifcantly outperforming other instruction-i"},{"bbox":{"x0":140,"x1":697,"y0":840,"y1":863},"font_size":0.0,"text":"tuned models based on the same underlying checkpoints."}],"source":"layout det","text":"We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fne-tune several smalli dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with $28.9\\%$ on AIME and $83.9\\%$ on MATH. Other dense models also achieve impressive results, signifcantly outperforming other instruction-i tuned models based on the same underlying checkpoints."},{"bbox":{"x0":266,"x1":1055,"y0":873,"y1":909},"conf":0.7599,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1051,"y0":879,"y1":903},"font_size":0.0,"text":"ure, we plan to invest in research across the following directions for DeepSeek-R1."}],"source":"layout det","text":"ure, we plan to invest in research across the following directions for DeepSeek-R1."},{"bbox":{"x0":187,"x1":1059,"y0":915,"y1":1024},"conf":0.9692,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":170,"x1":1049,"y0":919,"y1":944},"font_size":0.0,"text":"General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3"},{"bbox":{"x0":193,"x1":1051,"y0":947,"y1":972},"font_size":0.0,"text":"in tasks such as function calling, multi-turn, complex role-playing, and JSON output."},{"bbox":{"x0":191,"x1":1051,"y0":972,"y1":998},"font_size":0.0,"text":"Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in"},{"bbox":{"x0":197,"x1":308,"y0":1003,"y1":1025},"font_size":0.0,"text":"these felds.i"}],"source":"layout det","text":"General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output.Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these felds.i"},{"bbox":{"x0":187,"x1":1058,"y0":1026,"y1":1161},"conf":0.9663,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1049,"y0":1028,"y1":1052},"font_size":0.0,"text":"Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which"},{"bbox":{"x0":193,"x1":1049,"y0":1056,"y1":1081},"font_size":0.0,"text":"may result in language mixing issues when handling queries in other languages. For"},{"bbox":{"x0":193,"x1":1049,"y0":1084,"y1":1109},"font_size":0.0,"text":"instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is"},{"bbox":{"x0":193,"x1":1049,"y0":1110,"y1":1135},"font_size":0.0,"text":"in a language other than English or Chinese. We aim to address this limitation in future"},{"bbox":{"x0":191,"x1":282,"y0":1135,"y1":1163},"font_size":0.0,"text":"updates."}],"source":"layout det","text":"Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates."},{"bbox":{"x0":188,"x1":1057,"y0":1162,"y1":1269},"conf":0.9614,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":184,"x1":1051,"y0":1159,"y1":1191},"font_size":0.0,"text":"Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive"},{"bbox":{"x0":193,"x1":1049,"y0":1191,"y1":1216},"font_size":0.0,"text":"to prompts. Few-shot prompting consistently degrades its performance. Therefore, we"},{"bbox":{"x0":190,"x1":1051,"y0":1214,"y1":1245},"font_size":0.0,"text":"recommend users directly describe the problem and specify the output format using a"},{"bbox":{"x0":193,"x1":546,"y0":1245,"y1":1270},"font_size":0.0,"text":"zero-shot setting for optimal results."}],"source":"layout det","text":"Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results."},{"bbox":{"x0":187,"x1":1059,"y0":1271,"y1":1439},"conf":0.9722,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":188,"x1":1051,"y0":1272,"y1":1296},"font_size":0.0,"text":"Software Engineering Tasks: Due to the long evaluation times, which impact the eff-i"},{"bbox":{"x0":193,"x1":1049,"y0":1298,"y1":1323},"font_size":0.0,"text":"ciency of the RL process, large-scale RL has not been applied extensively in software"},{"bbox":{"x0":193,"x1":1051,"y0":1326,"y1":1351},"font_size":0.0,"text":"engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement"},{"bbox":{"x0":195,"x1":1049,"y0":1354,"y1":1379},"font_size":0.0,"text":"over DeepSeek-V3 on software engineering benchmarks. Future versions will address"},{"bbox":{"x0":193,"x1":1049,"y0":1379,"y1":1407},"font_size":0.0,"text":"this by implementing rejection sampling on software engineering data or incorporating"},{"bbox":{"x0":193,"x1":876,"y0":1407,"y1":1433},"font_size":0.0,"text":"asynchronous evaluations during the RL process to improve effciency.i"}],"source":"layout det","text":"Software Engineering Tasks: Due to the long evaluation times, which impact the eff-i ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve effciency.i"}],"formula_dets":[{"bbox":{"x0":622,"x1":682,"y0":782,"y1":806},"conf":0.7628,"label":"print_embedding","label_id":0},{"bbox":{"x0":820,"x1":881,"y0":782,"y1":806},"conf":0.7615,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1058,"y0":167,"y1":367},"conf":0.9786,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":697,"y1":868},"conf":0.9759,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1059,"y0":1271,"y1":1439},"conf":0.9722,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1059,"y0":915,"y1":1024},"conf":0.9692,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":548,"y1":693},"conf":0.9683,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":1058,"y0":1026,"y1":1161},"conf":0.9663,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":371,"y1":461},"conf":0.9643,"label":"Text","label_id":1},{"bbox":{"x0":188,"x1":1057,"y0":1162,"y1":1269},"conf":0.9614,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":678,"y0":495,"y1":532},"conf":0.9075,"label":"Title","label_id":0},{"bbox":{"x0":578,"x1":613,"y0":1550,"y1":1579},"conf":0.7934,"label":"Abandon","label_id":2},{"bbox":{"x0":266,"x1":1055,"y0":873,"y1":909},"conf":0.7599,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.8642},{"poly":[[193,1407],[876,1409],[875,1433],[193,1431]],"score":0.8183},{"poly":[[193,1379],[1049,1382],[1049,1407],[193,1403]],"score":0.7552},{"poly":[[195,1354],[1049,1354],[1049,1379],[195,1379]],"score":0.7465},{"poly":[[193,1326],[1051,1326],[1051,1351],[193,1351]],"score":0.7691},{"poly":[[193,1298],[1049,1298],[1049,1323],[193,1323]],"score":0.7564},{"poly":[[188,1272],[1051,1272],[1051,1296],[188,1296]],"score":0.7727},{"poly":[[193,1245],[546,1245],[546,1270],[193,1270]],"score":0.7828},{"poly":[[190,1214],[1051,1216],[1051,1245],[190,1244]],"score":0.726},{"poly":[[193,1191],[1049,1191],[1049,1216],[193,1216]],"score":0.8025},{"poly":[[184,1161],[1051,1159],[1051,1189],[184,1191]],"score":0.6918},{"poly":[[191,1139],[281,1135],[282,1159],[192,1163]],"score":0.8127},{"poly":[[193,1110],[1049,1110],[1049,1135],[193,1135]],"score":0.7592},{"poly":[[193,1084],[1049,1084],[1049,1109],[193,1109]],"score":0.7486},{"poly":[[193,1056],[1049,1056],[1049,1081],[193,1081]],"score":0.7678},{"poly":[[172,1028],[1049,1028],[1049,1052],[172,1052]],"score":0.7609},{"poly":[[197,1003],[308,1003],[308,1023],[197,1023]],"score":0.8892},{"poly":[[191,972],[1051,974],[1051,998],[191,996]],"score":0.7723},{"poly":[[193,947],[1051,947],[1051,972],[193,972]],"score":0.7601},{"poly":[[170,919],[1049,919],[1049,944],[170,944]],"score":0.7701},{"poly":[[172,879],[1051,879],[1051,903],[172,903]],"score":0.7627},{"poly":[[140,840],[697,840],[697,863],[140,863]],"score":0.8865},{"poly":[[140,812],[1051,812],[1051,837],[140,837]],"score":0.78},{"poly":[[138,782],[1049,784],[1049,809],[138,807]],"score":0.7369},{"poly":[[140,758],[1049,758],[1049,781],[140,781]],"score":0.8577},{"poly":[[140,730],[1049,730],[1049,754],[140,754]],"score":0.7393},{"poly":[[174,700],[1049,702],[1049,726],[174,724]],"score":0.7763},{"poly":[[135,661],[762,660],[762,689],[135,691]],"score":0.7122},{"poly":[[140,633],[1047,633],[1047,658],[140,658]],"score":0.7444},{"poly":[[140,609],[1051,609],[1051,633],[140,633]],"score":0.7793},{"poly":[[137,577],[1051,579],[1051,609],[136,607]],"score":0.6949},{"poly":[[138,551],[1051,553],[1051,577],[138,575]],"score":0.8001},{"poly":[[140,502],[672,502],[672,526],[140,526]],"score":0.8405},{"poly":[[137,428],[346,430],[345,460],[136,458]],"score":0.7471},{"poly":[[136,402],[1051,400],[1051,430],[137,432]],"score":0.689},{"poly":[[170,372],[1051,374],[1051,403],[170,402]],"score":0.7095},{"poly":[[140,337],[567,337],[567,361],[140,361]],"score":0.778},{"poly":[[136,305],[1049,305],[1049,335],[136,335]],"score":0.6632},{"poly":[[140,282],[1049,282],[1049,307],[140,307]],"score":0.7357},{"poly":[[140,254],[1049,254],[1049,279],[140,279]],"score":0.7205},{"poly":[[140,228],[1051,228],[1051,253],[140,253]],"score":0.7335},{"poly":[[138,200],[1047,200],[1047,225],[138,225]],"score":0.7399},{"poly":[[140,174],[1049,174],[1049,198],[140,198]],"score":0.7374}],"page_no":15,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":578,"x1":614,"y0":1550,"y1":1580},"conf":0.8083,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":581,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"17"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":277,"y0":165,"y1":200},"conf":0.8824,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":271,"y0":170,"y1":198},"font_size":0.0,"text":"References"}],"source":"layout det","text":"References"},{"bbox":{"x0":135,"x1":1056,"y0":217,"y1":281},"conf":0.9474,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":223,"y1":249},"font_size":0.0,"text":"AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m"},{"bbox":{"x0":163,"x1":734,"y0":253,"y1":275},"font_size":0.0,"text":"odels/blob/main/models/llama3_1/MODEL_CARD.md."}],"source":"layout det","text":"AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md."},{"bbox":{"x0":133,"x1":1058,"y0":289,"y1":351},"conf":0.9316,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":142,"x1":1049,"y0":298,"y1":321},"font_size":0.0,"text":"Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3"},{"bbox":{"x0":156,"x1":276,"y0":325,"y1":344},"font_size":0.0,"text":"-5-sonnet."}],"source":"layout det","text":"Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet."},{"bbox":{"x0":140,"x1":1060,"y0":361,"y1":615},"conf":0.9759,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":368,"y1":393},"font_size":0.0,"text":"M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,"},{"bbox":{"x0":161,"x1":1051,"y0":396,"y1":419},"font_size":0.0,"text":"N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,"},{"bbox":{"x0":161,"x1":1051,"y0":423,"y1":447},"font_size":0.0,"text":"B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,"},{"bbox":{"x0":160,"x1":1051,"y0":451,"y1":474},"font_size":0.0,"text":"F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,"},{"bbox":{"x0":158,"x1":1051,"y0":474,"y1":502},"font_size":0.0,"text":"A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,"},{"bbox":{"x0":160,"x1":1051,"y0":503,"y1":528},"font_size":0.0,"text":"A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,"},{"bbox":{"x0":159,"x1":1049,"y0":528,"y1":554},"font_size":0.0,"text":"M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and"},{"bbox":{"x0":158,"x1":1053,"y0":554,"y1":586},"font_size":0.0,"text":"W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021."},{"bbox":{"x0":159,"x1":622,"y0":584,"y1":610},"font_size":0.0,"text":"URL https://arxiv.org/abs/2107.03374."}],"source":"layout det","text":"M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.URL https://arxiv.org/abs/2107.03374."},{"bbox":{"x0":135,"x1":1058,"y0":624,"y1":689},"conf":0.8066,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":628,"y1":656},"font_size":0.0,"text":"A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,"},{"bbox":{"x0":160,"x1":1017,"y0":656,"y1":681},"font_size":0.0,"text":"A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024."}],"source":"layout det","text":"A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024."},{"bbox":{"x0":132,"x1":1058,"y0":696,"y1":760},"conf":0.9492,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":700,"y1":726},"font_size":0.0,"text":"Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple"},{"bbox":{"x0":158,"x1":881,"y0":728,"y1":756},"font_size":0.0,"text":"way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024."}],"source":"layout det","text":"Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024."},{"bbox":{"x0":133,"x1":1059,"y0":767,"y1":858},"conf":0.3789,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1047,"y0":775,"y1":800},"font_size":0.0,"text":"X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like"},{"bbox":{"x0":161,"x1":1053,"y0":803,"y1":828},"font_size":0.0,"text":"tree-search can guide large language model decoding and training, 2024. URL https:"},{"bbox":{"x0":161,"x1":494,"y0":830,"y1":854},"font_size":0.0,"text":"//arxiv.org/abs/2309.17179."}],"source":"layout det","text":"X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https://arxiv.org/abs/2309.17179."},{"bbox":{"x0":131,"x1":1058,"y0":866,"y1":931},"conf":0.9471,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":874,"y1":898},"font_size":0.0,"text":"L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL"},{"bbox":{"x0":161,"x1":571,"y0":902,"y1":926},"font_size":0.0,"text":"https://arxiv.org/abs/2210.10760."}],"source":"layout det","text":"L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760."},{"bbox":{"x0":133,"x1":1059,"y0":939,"y1":1058},"conf":0.9624,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":945,"y1":970},"font_size":0.0,"text":"A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,"},{"bbox":{"x0":160,"x1":1049,"y0":974,"y1":996},"font_size":0.0,"text":"X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and"},{"bbox":{"x0":159,"x1":1049,"y0":998,"y1":1024},"font_size":0.0,"text":"P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or"},{"bbox":{"x0":159,"x1":510,"y0":1026,"y1":1053},"font_size":0.0,"text":"g/10.48550/arXiv.2406.04127."}],"source":"layout det","text":"A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127."},{"bbox":{"x0":130,"x1":1057,"y0":1066,"y1":1131},"conf":0.9449,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":1068,"y1":1100},"font_size":0.0,"text":"Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno"},{"bbox":{"x0":159,"x1":892,"y0":1098,"y1":1126},"font_size":0.0,"text":"logy/ai/google-gemini-next-generation-model-february-2024."}],"source":"layout det","text":"Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024."},{"bbox":{"x0":132,"x1":1058,"y0":1137,"y1":1229},"conf":0.9572,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1144,"y1":1168},"font_size":0.0,"text":"Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-"},{"bbox":{"x0":161,"x1":1049,"y0":1174,"y1":1198},"font_size":0.0,"text":"nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint"},{"bbox":{"x0":159,"x1":385,"y0":1198,"y1":1224},"font_size":0.0,"text":"arXiv:2411.07140, 2024."}],"source":"layout det","text":"Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024."},{"bbox":{"x0":131,"x1":1057,"y0":1236,"y1":1302},"conf":0.9573,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1238,"y1":1274},"font_size":0.0,"text":"D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring"},{"bbox":{"x0":161,"x1":959,"y0":1272,"y1":1296},"font_size":0.0,"text":"massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020."}],"source":"layout det","text":"D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020."},{"bbox":{"x0":132,"x1":1058,"y0":1309,"y1":1399},"conf":0.941,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1316,"y1":1340},"font_size":0.0,"text":"Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A"},{"bbox":{"x0":159,"x1":1051,"y0":1342,"y1":1368},"font_size":0.0,"text":"multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint"},{"bbox":{"x0":159,"x1":385,"y0":1368,"y1":1395},"font_size":0.0,"text":"arXiv:2305.08322, 2023."}],"source":"layout det","text":"Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023."},{"bbox":{"x0":132,"x1":1059,"y0":1407,"y1":1502},"conf":0.7883,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1414,"y1":1438},"font_size":0.0,"text":"N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica."},{"bbox":{"x0":161,"x1":1053,"y0":1440,"y1":1470},"font_size":0.0,"text":"Livecodebench: Holistic and contamination free evaluation of large language models for code."},{"bbox":{"x0":163,"x1":1019,"y0":1470,"y1":1495},"font_size":0.0,"text":"CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974."}],"source":"layout det","text":"N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.Livecodebench: Holistic and contamination free evaluation of large language models for code.CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":140,"x1":1060,"y0":361,"y1":615},"conf":0.9759,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":939,"y1":1058},"conf":0.9624,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":1236,"y1":1302},"conf":0.9573,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1137,"y1":1229},"conf":0.9572,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":696,"y1":760},"conf":0.9492,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1056,"y0":217,"y1":281},"conf":0.9474,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":866,"y1":931},"conf":0.9471,"label":"Text","label_id":1},{"bbox":{"x0":130,"x1":1057,"y0":1066,"y1":1131},"conf":0.9449,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1309,"y1":1399},"conf":0.941,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":289,"y1":351},"conf":0.9316,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":277,"y0":165,"y1":200},"conf":0.8824,"label":"Title","label_id":0},{"bbox":{"x0":578,"x1":614,"y0":1550,"y1":1580},"conf":0.8083,"label":"Abandon","label_id":2},{"bbox":{"x0":135,"x1":1058,"y0":624,"y1":689},"conf":0.8066,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":1407,"y1":1502},"conf":0.7883,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":767,"y1":858},"conf":0.7276,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":767,"y1":858},"conf":0.3789,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":1407,"y1":1502},"conf":0.3616,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[581,1554],[610,1554],[610,1577],[581,1577]],"score":0.7673},{"poly":[[163,1470],[1019,1470],[1019,1495],[163,1495]],"score":0.7826},{"poly":[[161,1440],[1053,1440],[1053,1470],[161,1470]],"score":0.7034},{"poly":[[138,1414],[1051,1414],[1051,1438],[138,1438]],"score":0.7719},{"poly":[[159,1370],[384,1368],[385,1393],[160,1395]],"score":0.796},{"poly":[[160,1342],[1051,1344],[1051,1368],[159,1366]],"score":0.7982},{"poly":[[140,1316],[1051,1316],[1051,1340],[140,1340]],"score":0.7628},{"poly":[[161,1272],[959,1272],[959,1296],[161,1296]],"score":0.7695},{"poly":[[137,1238],[1051,1244],[1051,1274],[136,1268]],"score":0.6796},{"poly":[[159,1200],[384,1198],[385,1223],[160,1224]],"score":0.8498},{"poly":[[161,1174],[1049,1174],[1049,1198],[161,1198]],"score":0.7465},{"poly":[[140,1144],[1051,1144],[1051,1168],[140,1168]],"score":0.7779},{"poly":[[159,1102],[891,1098],[892,1123],[160,1126]],"score":0.7949},{"poly":[[137,1068],[1049,1070],[1049,1100],[136,1098]],"score":0.7279},{"poly":[[159,1028],[510,1026],[510,1051],[160,1053]],"score":0.7772},{"poly":[[160,998],[1049,1000],[1049,1024],[159,1023]],"score":0.7499},{"poly":[[160,974],[1049,974],[1049,996],[160,996]],"score":0.8962},{"poly":[[140,945],[1051,945],[1051,970],[140,970]],"score":0.7547},{"poly":[[161,902],[571,902],[571,926],[161,926]],"score":0.7801},{"poly":[[138,874],[1049,874],[1049,898],[138,898]],"score":0.734},{"poly":[[161,830],[494,830],[494,854],[161,854]],"score":0.7784},{"poly":[[161,803],[1053,803],[1053,828],[161,828]],"score":0.7779},{"poly":[[140,775],[1047,775],[1047,800],[140,800]],"score":0.7869},{"poly":[[158,732],[881,728],[881,753],[158,756]],"score":0.761},{"poly":[[138,700],[1049,702],[1049,726],[138,724]],"score":0.782},{"poly":[[160,656],[1017,656],[1017,681],[160,681]],"score":0.7039},{"poly":[[138,628],[1051,632],[1051,656],[138,653]],"score":0.7327},{"poly":[[160,584],[622,586],[622,610],[159,609]],"score":0.7806},{"poly":[[158,554],[1053,556],[1053,586],[158,584]],"score":0.7136},{"poly":[[160,528],[1049,530],[1049,554],[159,553]],"score":0.7739},{"poly":[[160,503],[1051,503],[1051,528],[160,528]],"score":0.7237},{"poly":[[158,474],[1051,477],[1051,502],[158,498]],"score":0.7288},{"poly":[[160,451],[1051,451],[1051,474],[160,474]],"score":0.8484},{"poly":[[161,423],[1051,423],[1051,447],[161,447]],"score":0.7174},{"poly":[[161,396],[1051,396],[1051,419],[161,419]],"score":0.8604},{"poly":[[138,368],[1051,368],[1051,393],[138,393]],"score":0.7268},{"poly":[[156,325],[276,325],[276,344],[156,344]],"score":0.6668},{"poly":[[142,298],[1049,298],[1049,321],[142,321]],"score":0.8551},{"poly":[[163,253],[734,253],[734,275],[163,275]],"score":0.8841},{"poly":[[138,223],[1049,225],[1049,249],[138,247]],"score":0.7882},{"poly":[[139,170],[271,174],[271,198],[138,194]],"score":0.8158}],"page_no":16,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7973,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":580,"x1":610,"y0":1554,"y1":1577},"font_size":0.0,"text":"18"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":1060,"y0":166,"y1":283},"conf":0.9642,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":172,"y1":198},"font_size":0.0,"text":"S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui."},{"bbox":{"x0":158,"x1":1051,"y0":196,"y1":228},"font_size":0.0,"text":"Fact, fetch, and reason: A unifed evaluation of retrieval-augmented generation. CoRR,i"},{"bbox":{"x0":159,"x1":1047,"y0":225,"y1":251},"font_size":0.0,"text":"abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485"},{"bbox":{"x0":163,"x1":408,"y0":256,"y1":279},"font_size":0.0,"text":"50/arXiv.2409.12941."}],"source":"layout det","text":"S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.Fact, fetch, and reason: A unifed evaluation of retrieval-augmented generation. CoRR,i abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941."},{"bbox":{"x0":133,"x1":1060,"y0":290,"y1":383},"conf":0.9587,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":293,"y1":326},"font_size":0.0,"text":"A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,"},{"bbox":{"x0":158,"x1":1051,"y0":321,"y1":353},"font_size":0.0,"text":"R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv"},{"bbox":{"x0":158,"x1":472,"y0":347,"y1":381},"font_size":0.0,"text":"preprint arXiv:2409.12917, 2024."}],"source":"layout det","text":"A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024."},{"bbox":{"x0":132,"x1":1059,"y0":389,"y1":478},"conf":0.9633,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":395,"y1":419},"font_size":0.0,"text":"H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-"},{"bbox":{"x0":160,"x1":1049,"y0":423,"y1":447},"font_size":0.0,"text":"ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,"},{"bbox":{"x0":160,"x1":214,"y0":449,"y1":475},"font_size":0.0,"text":"2023."}],"source":"layout det","text":"H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,2023."},{"bbox":{"x0":131,"x1":1059,"y0":485,"y1":578},"conf":0.9574,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":493,"y1":517},"font_size":0.0,"text":"T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From"},{"bbox":{"x0":161,"x1":1047,"y0":519,"y1":542},"font_size":0.0,"text":"crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv"},{"bbox":{"x0":158,"x1":473,"y0":542,"y1":575},"font_size":0.0,"text":"preprint arXiv:2406.11939, 2024."}],"source":"layout det","text":"T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024."},{"bbox":{"x0":131,"x1":1058,"y0":583,"y1":649},"conf":0.9423,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":589,"y1":614},"font_size":0.0,"text":"H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,"},{"bbox":{"x0":160,"x1":1040,"y0":617,"y1":642},"font_size":0.0,"text":"I. Sutskever, and K. Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050, 2023."}],"source":"layout det","text":"H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,I. Sutskever, and K. Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050, 2023."},{"bbox":{"x0":131,"x1":1059,"y0":653,"y1":716},"conf":0.9532,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":658,"y1":684},"font_size":0.0,"text":"B. Y. Lin. ZeroEval: A Unifed Framework for Evaluating Language Models, July 2024. URLi"},{"bbox":{"x0":160,"x1":624,"y0":688,"y1":710},"font_size":0.0,"text":"https://github.com/WildEval/ZeroEval."}],"source":"layout det","text":"B. Y. Lin. ZeroEval: A Unifed Framework for Evaluating Language Models, July 2024. URLi https://github.com/WildEval/ZeroEval."},{"bbox":{"x0":132,"x1":1058,"y0":724,"y1":813},"conf":0.9642,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":730,"y1":753},"font_size":0.0,"text":"MAA.American invitational mathematics examination - aime.In American Invitational"},{"bbox":{"x0":161,"x1":1049,"y0":758,"y1":781},"font_size":0.0,"text":"Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math"},{"bbox":{"x0":159,"x1":973,"y0":782,"y1":809},"font_size":0.0,"text":"-competitions/american-invitational-mathematics-examination-aime."}],"source":"layout det","text":"MAA.American invitational mathematics examination - aime.In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math-competitions/american-invitational-mathematics-examination-aime."},{"bbox":{"x0":187,"x1":976,"y0":819,"y1":858},"conf":0.856,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":978,"y0":828,"y1":851},"font_size":0.0,"text":"nAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/."}],"source":"layout det","text":"nAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/."},{"bbox":{"x0":131,"x1":1058,"y0":863,"y1":926},"conf":0.9442,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":870,"y1":895},"font_size":0.0,"text":"OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin"},{"bbox":{"x0":159,"x1":452,"y0":896,"y1":923},"font_size":0.0,"text":"g-to-reason-with-llms/."}],"source":"layout det","text":"OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/."},{"bbox":{"x0":132,"x1":1056,"y0":933,"y1":998},"conf":0.9422,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":935,"y1":968},"font_size":0.0,"text":"OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing"},{"bbox":{"x0":158,"x1":289,"y0":970,"y1":989},"font_size":0.0,"text":"-simpleqa/."}],"source":"layout det","text":"OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing-simpleqa/."},{"bbox":{"x0":131,"x1":1057,"y0":1003,"y1":1091},"conf":0.9532,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1010,"y1":1034},"font_size":0.0,"text":"OpenAI. Introducing SWE-bench verifed we‚Äôre releasing a human-validated subset of swe-i"},{"bbox":{"x0":160,"x1":1049,"y0":1037,"y1":1061},"font_size":0.0,"text":"bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench"},{"bbox":{"x0":161,"x1":289,"y0":1067,"y1":1086},"font_size":0.0,"text":"-verified/."}],"source":"layout det","text":"OpenAI. Introducing SWE-bench verifed we‚Äôre releasing a human-validated subset of swe-i bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench-verified/."},{"bbox":{"x0":132,"x1":1057,"y0":1100,"y1":1165},"conf":0.9424,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1047,"y0":1107,"y1":1131},"font_size":0.0,"text":"Qwen. Qwq: Refect deeply on the boundaries of the unknown, 2024a. URL https://qwenlml"},{"bbox":{"x0":163,"x1":578,"y0":1131,"y1":1163},"font_size":0.0,"text":".github.io/blog/qwq-32b-preview/."}],"source":"layout det","text":"Qwen. Qwq: Refect deeply on the boundaries of the unknown, 2024a. URL https://qwenlml.github.io/blog/qwq-32b-preview/."},{"bbox":{"x0":131,"x1":1057,"y0":1170,"y1":1234},"conf":0.9601,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":1177,"y1":1202},"font_size":0.0,"text":"Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b"},{"bbox":{"x0":160,"x1":308,"y0":1205,"y1":1230},"font_size":0.0,"text":"log/qwen2.5."}],"source":"layout det","text":"Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5."},{"bbox":{"x0":132,"x1":1058,"y0":1240,"y1":1305},"conf":0.9512,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1244,"y1":1270},"font_size":0.0,"text":"D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman."},{"bbox":{"x0":161,"x1":1049,"y0":1274,"y1":1298},"font_size":0.0,"text":"GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023."}],"source":"layout det","text":"D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023."},{"bbox":{"x0":131,"x1":1059,"y0":1310,"y1":1401},"conf":0.9554,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1314,"y1":1340},"font_size":0.0,"text":"Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:"},{"bbox":{"x0":158,"x1":1053,"y0":1340,"y1":1372},"font_size":0.0,"text":"Pushing the limits of mathematical reasoning in open language models. arXiv preprint"},{"bbox":{"x0":161,"x1":386,"y0":1372,"y1":1395},"font_size":0.0,"text":"arXiv:2402.03300, 2024."}],"source":"layout det","text":"Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024."},{"bbox":{"x0":134,"x1":1060,"y0":1407,"y1":1527},"conf":0.9471,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1414,"y1":1438},"font_size":0.0,"text":"D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,"},{"bbox":{"x0":161,"x1":1049,"y0":1442,"y1":1466},"font_size":0.0,"text":"D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and"},{"bbox":{"x0":158,"x1":1053,"y0":1465,"y1":1496},"font_size":0.0,"text":"shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815,"},{"bbox":{"x0":161,"x1":677,"y0":1496,"y1":1519},"font_size":0.0,"text":"2017a. URL http://arxiv.org/abs/1712.01815."}],"source":"layout det","text":"D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815,2017a. URL http://arxiv.org/abs/1712.01815."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":1058,"y0":724,"y1":813},"conf":0.9642,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":166,"y1":283},"conf":0.9642,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":389,"y1":478},"conf":0.9633,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":1170,"y1":1234},"conf":0.9601,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1060,"y0":290,"y1":383},"conf":0.9587,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1059,"y0":485,"y1":578},"conf":0.9574,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1059,"y0":1310,"y1":1401},"conf":0.9554,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1059,"y0":653,"y1":716},"conf":0.9532,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":1003,"y1":1091},"conf":0.9532,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1240,"y1":1305},"conf":0.9512,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":1407,"y1":1527},"conf":0.9471,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":863,"y1":926},"conf":0.9442,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":1100,"y1":1165},"conf":0.9424,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":583,"y1":649},"conf":0.9423,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1056,"y0":933,"y1":998},"conf":0.9422,"label":"Text","label_id":1},{"bbox":{"x0":187,"x1":976,"y0":819,"y1":858},"conf":0.856,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":614,"y0":1550,"y1":1580},"conf":0.7973,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[580,1554],[610,1554],[610,1577],[580,1577]],"score":0.8472},{"poly":[[161,1496],[677,1496],[677,1519],[161,1519]],"score":0.921},{"poly":[[158,1466],[1053,1465],[1053,1495],[158,1496]],"score":0.721},{"poly":[[161,1442],[1049,1442],[1049,1466],[161,1466]],"score":0.7787},{"poly":[[140,1414],[1051,1414],[1051,1438],[140,1438]],"score":0.7995},{"poly":[[161,1372],[386,1372],[386,1395],[161,1395]],"score":0.8653},{"poly":[[158,1340],[1053,1342],[1053,1372],[158,1370]],"score":0.7237},{"poly":[[138,1314],[1051,1316],[1051,1340],[138,1338]],"score":0.8018},{"poly":[[161,1274],[1049,1274],[1049,1298],[161,1298]],"score":0.7994},{"poly":[[138,1244],[1051,1245],[1051,1270],[138,1268]],"score":0.8245},{"poly":[[160,1205],[308,1205],[308,1230],[160,1230]],"score":0.853},{"poly":[[140,1177],[1049,1177],[1049,1202],[140,1202]],"score":0.7704},{"poly":[[163,1133],[578,1131],[578,1161],[163,1163]],"score":0.7571},{"poly":[[140,1107],[1047,1107],[1047,1131],[140,1131]],"score":0.8271},{"poly":[[161,1067],[289,1067],[289,1086],[161,1086]],"score":0.9561},{"poly":[[160,1037],[1049,1037],[1049,1061],[160,1061]],"score":0.7638},{"poly":[[140,1010],[1051,1010],[1051,1033],[140,1033]],"score":0.886},{"poly":[[158,970],[289,970],[289,989],[158,989]],"score":0.957},{"poly":[[137,935],[1049,937],[1049,968],[136,967]],"score":0.6737},{"poly":[[159,898],[452,896],[452,921],[160,923]],"score":0.8433},{"poly":[[140,870],[1049,870],[1049,895],[140,895]],"score":0.8081},{"poly":[[140,828],[978,828],[978,851],[140,851]],"score":0.9134},{"poly":[[159,784],[973,782],[973,807],[160,809]],"score":0.7584},{"poly":[[161,758],[1049,758],[1049,781],[161,781]],"score":0.9007},{"poly":[[140,730],[1049,730],[1049,753],[140,753]],"score":0.8522},{"poly":[[160,688],[624,688],[624,710],[160,710]],"score":0.9574},{"poly":[[136,658],[1051,660],[1051,684],[136,682]],"score":0.778},{"poly":[[160,617],[1040,617],[1040,642],[160,642]],"score":0.8011},{"poly":[[138,589],[1051,589],[1051,614],[138,614]],"score":0.7337},{"poly":[[158,546],[473,542],[473,572],[158,575]],"score":0.7251},{"poly":[[161,519],[1047,519],[1047,542],[161,542]],"score":0.8065},{"poly":[[140,493],[1049,493],[1049,517],[140,517]],"score":0.7813},{"poly":[[160,449],[214,449],[214,475],[160,475]],"score":0.8673},{"poly":[[160,423],[1049,423],[1049,447],[160,447]],"score":0.7693},{"poly":[[140,395],[1051,395],[1051,419],[140,419]],"score":0.7367},{"poly":[[158,351],[471,347],[472,377],[158,381]],"score":0.7067},{"poly":[[158,321],[1051,323],[1051,353],[158,351]],"score":0.7027},{"poly":[[137,293],[1051,296],[1051,326],[136,323]],"score":0.6842},{"poly":[[163,256],[408,256],[408,279],[163,279]],"score":0.8362},{"poly":[[159,226],[1047,225],[1047,249],[160,251]],"score":0.7766},{"poly":[[158,198],[1051,196],[1051,226],[158,228]],"score":0.7149},{"poly":[[136,172],[1051,174],[1051,198],[136,196]],"score":0.7863}],"page_no":17,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7834,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":581,"x1":608,"y0":1552,"y1":1575},"font_size":0.0,"text":"19"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":1059,"y0":166,"y1":286},"conf":0.965,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":174,"y1":198},"font_size":0.0,"text":"D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,"},{"bbox":{"x0":161,"x1":1049,"y0":200,"y1":225},"font_size":0.0,"text":"M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and"},{"bbox":{"x0":161,"x1":1051,"y0":228,"y1":253},"font_size":0.0,"text":"D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354‚Äì359,"},{"bbox":{"x0":163,"x1":1016,"y0":254,"y1":279},"font_size":0.0,"text":"2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270."}],"source":"layout det","text":"D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354‚Äì359,2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270."},{"bbox":{"x0":132,"x1":1059,"y0":293,"y1":383},"conf":0.952,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":300,"y1":323},"font_size":0.0,"text":"C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more"},{"bbox":{"x0":161,"x1":1046,"y0":325,"y1":349},"font_size":0.0,"text":"effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033"},{"bbox":{"x0":160,"x1":197,"y0":353,"y1":381},"font_size":0.0,"text":"14."}],"source":"layout det","text":"C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14."},{"bbox":{"x0":131,"x1":1057,"y0":392,"y1":457},"conf":0.9464,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":396,"y1":425},"font_size":0.0,"text":"T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human"},{"bbox":{"x0":161,"x1":783,"y0":426,"y1":449},"font_size":0.0,"text":"demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5."}],"source":"layout det","text":"T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5."},{"bbox":{"x0":131,"x1":1058,"y0":465,"y1":557},"conf":0.9416,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":472,"y1":496},"font_size":0.0,"text":"J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and"},{"bbox":{"x0":160,"x1":1049,"y0":498,"y1":523},"font_size":0.0,"text":"I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv"},{"bbox":{"x0":158,"x1":472,"y0":521,"y1":554},"font_size":0.0,"text":"preprint arXiv:2211.14275, 2022."}],"source":"layout det","text":"J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022."},{"bbox":{"x0":132,"x1":1059,"y0":563,"y1":653},"conf":0.965,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":570,"y1":593},"font_size":0.0,"text":"P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-"},{"bbox":{"x0":160,"x1":1051,"y0":598,"y1":623},"font_size":0.0,"text":"free step-by-step verifer for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935,i"},{"bbox":{"x0":159,"x1":215,"y0":619,"y1":650},"font_size":0.0,"text":"2023."}],"source":"layout det","text":"P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A labelfree step-by-step verifer for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935,i 2023."},{"bbox":{"x0":131,"x1":1060,"y0":661,"y1":755},"conf":0.9691,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":667,"y1":693},"font_size":0.0,"text":"X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou."},{"bbox":{"x0":159,"x1":1053,"y0":693,"y1":724},"font_size":0.0,"text":"Self-consistency improves chain of thought reasoning in language models. arXiv preprint"},{"bbox":{"x0":159,"x1":385,"y0":721,"y1":747},"font_size":0.0,"text":"arXiv:2203.11171, 2022."}],"source":"layout det","text":"X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022."},{"bbox":{"x0":131,"x1":1060,"y0":762,"y1":881},"conf":0.9694,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":768,"y1":793},"font_size":0.0,"text":"Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,"},{"bbox":{"x0":160,"x1":1049,"y0":795,"y1":819},"font_size":0.0,"text":"M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and"},{"bbox":{"x0":160,"x1":1053,"y0":821,"y1":851},"font_size":0.0,"text":"challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024."},{"bbox":{"x0":163,"x1":734,"y0":851,"y1":875},"font_size":0.0,"text":"URL https://doi.org/10.48550/arXiv.2406.01574."}],"source":"layout det","text":"Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.URL https://doi.org/10.48550/arXiv.2406.01574."},{"bbox":{"x0":132,"x1":1057,"y0":888,"y1":955},"conf":0.9385,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":895,"y1":919},"font_size":0.0,"text":"C. S. Xia, Y. Deng, S. Dunn, and L. Zhang.Agentless: Demystifying llm-based software"},{"bbox":{"x0":156,"x1":560,"y0":919,"y1":951},"font_size":0.0,"text":"engineering agents. arXiv preprint, 2024."}],"source":"layout det","text":"C. S. Xia, Y. Deng, S. Dunn, and L. Zhang.Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024."},{"bbox":{"x0":133,"x1":1059,"y0":961,"y1":1080},"conf":0.97,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1053,"y0":963,"y1":995},"font_size":0.0,"text":"H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,"},{"bbox":{"x0":158,"x1":1051,"y0":991,"y1":1023},"font_size":0.0,"text":"Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing"},{"bbox":{"x0":158,"x1":1051,"y0":1017,"y1":1049},"font_size":0.0,"text":"proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL"},{"bbox":{"x0":160,"x1":571,"y0":1047,"y1":1072},"font_size":0.0,"text":"https://arxiv.org/abs/2408.08152."}],"source":"layout det","text":"H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152."},{"bbox":{"x0":131,"x1":1058,"y0":1087,"y1":1154},"conf":0.8668,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":1089,"y1":1121},"font_size":0.0,"text":"J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following"},{"bbox":{"x0":160,"x1":900,"y0":1121,"y1":1145},"font_size":0.0,"text":"evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."}],"source":"layout det","text":"J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":133,"x1":1059,"y0":961,"y1":1080},"conf":0.97,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1060,"y0":762,"y1":881},"conf":0.9694,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1060,"y0":661,"y1":755},"conf":0.9691,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":563,"y1":653},"conf":0.965,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1059,"y0":166,"y1":286},"conf":0.965,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1059,"y0":293,"y1":383},"conf":0.952,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":392,"y1":457},"conf":0.9464,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":465,"y1":557},"conf":0.9416,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":888,"y1":955},"conf":0.9385,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":1087,"y1":1154},"conf":0.8668,"label":"Text","label_id":1},{"bbox":{"x0":577,"x1":613,"y0":1550,"y1":1580},"conf":0.7834,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[581,1552],[608,1552],[608,1575],[581,1575]],"score":0.8276},{"poly":[[160,1121],[900,1121],[900,1145],[160,1145]],"score":0.8421},{"poly":[[135,1089],[1051,1091],[1051,1121],[135,1119]],"score":0.7238},{"poly":[[160,1047],[571,1047],[571,1072],[160,1072]],"score":0.776},{"poly":[[158,1019],[1051,1017],[1051,1047],[158,1049]],"score":0.699},{"poly":[[158,991],[1051,993],[1051,1023],[158,1021]],"score":0.7447},{"poly":[[135,963],[1053,965],[1053,995],[135,993]],"score":0.7381},{"poly":[[156,921],[560,919],[560,949],[156,951]],"score":0.7658},{"poly":[[140,895],[1049,895],[1049,919],[140,919]],"score":0.8022},{"poly":[[163,851],[734,851],[734,875],[163,875]],"score":0.8063},{"poly":[[160,821],[1053,821],[1053,851],[160,851]],"score":0.6571},{"poly":[[160,795],[1049,795],[1049,819],[160,819]],"score":0.7687},{"poly":[[140,768],[1051,768],[1051,793],[140,793]],"score":0.8203},{"poly":[[159,723],[384,721],[385,745],[160,747]],"score":0.7573},{"poly":[[160,693],[1053,695],[1053,724],[159,723]],"score":0.7179},{"poly":[[138,667],[1051,668],[1051,693],[138,691]],"score":0.8099},{"poly":[[161,619],[215,623],[213,650],[159,645]],"score":0.7591},{"poly":[[160,598],[1051,598],[1051,623],[160,623]],"score":0.7973},{"poly":[[138,570],[1051,570],[1051,593],[138,593]],"score":0.957},{"poly":[[158,525],[471,521],[472,551],[158,554]],"score":0.691},{"poly":[[160,498],[1049,498],[1049,523],[160,523]],"score":0.7487},{"poly":[[138,472],[1049,472],[1049,496],[138,496]],"score":0.8031},{"poly":[[161,426],[783,426],[783,449],[161,449]],"score":0.9294},{"poly":[[138,396],[1049,400],[1049,425],[138,421]],"score":0.7817},{"poly":[[160,353],[197,353],[197,381],[160,381]],"score":0.8284},{"poly":[[161,325],[1046,325],[1046,349],[161,349]],"score":0.7237},{"poly":[[140,300],[1049,300],[1049,323],[140,323]],"score":0.9124},{"poly":[[163,254],[1016,254],[1016,279],[163,279]],"score":0.7374},{"poly":[[161,228],[1051,228],[1051,253],[161,253]],"score":0.7537},{"poly":[[161,200],[1049,200],[1049,225],[161,225]],"score":0.7298},{"poly":[[138,174],[1051,174],[1051,198],[138,198]],"score":0.7948}],"page_no":18,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":576,"x1":613,"y0":1550,"y1":1580},"conf":0.7929,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":578,"x1":611,"y0":1549,"y1":1579},"font_size":0.0,"text":"20"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":269,"y0":163,"y1":205},"conf":0.6998,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":134,"x1":264,"y0":163,"y1":204},"font_size":0.0,"text":"Appendix"}],"source":"layout det","text":"Appendix"},{"bbox":{"x0":132,"x1":637,"y0":214,"y1":258},"conf":0.8189,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":631,"y0":217,"y1":249},"font_size":0.0,"text":"A. Contributions and Acknowledgments"}],"source":"layout det","text":"A. Contributions and Acknowledgments"},{"bbox":{"x0":130,"x1":331,"y0":280,"y1":1527},"conf":0.8623,"font_size":0.0,"label":"Reference","label_id":11,"lines":[{"bbox":{"x0":138,"x1":324,"y0":284,"y1":309},"font_size":0.0,"text":"Core Contributors"},{"bbox":{"x0":136,"x1":245,"y0":307,"y1":339},"font_size":0.0,"text":"Daya Guo"},{"bbox":{"x0":134,"x1":263,"y0":333,"y1":369},"font_size":0.0,"text":"Dejian Yang"},{"bbox":{"x0":134,"x1":291,"y0":357,"y1":397},"font_size":0.0,"text":"Haowei Zhang"},{"bbox":{"x0":132,"x1":275,"y0":382,"y1":425},"font_size":0.0,"text":"Junxiao Song"},{"bbox":{"x0":133,"x1":280,"y0":414,"y1":449},"font_size":0.0,"text":"Ruoyu Zhang"},{"bbox":{"x0":135,"x1":252,"y0":444,"y1":474},"font_size":0.0,"text":"Runxin Xu"},{"bbox":{"x0":136,"x1":248,"y0":472,"y1":496},"font_size":0.0,"text":"Qihao Zhu"},{"bbox":{"x0":135,"x1":259,"y0":498,"y1":528},"font_size":0.0,"text":"Shirong Ma"},{"bbox":{"x0":132,"x1":256,"y0":521,"y1":558},"font_size":0.0,"text":"Peiyi Wang"},{"bbox":{"x0":133,"x1":216,"y0":551,"y1":582},"font_size":0.0,"text":"Xiao Bi"},{"bbox":{"x0":133,"x1":305,"y0":575,"y1":613},"font_size":0.0,"text":"Xiaokang Zhang"},{"bbox":{"x0":135,"x1":252,"y0":607,"y1":639},"font_size":0.0,"text":"Xingkai Yu"},{"bbox":{"x0":135,"x1":213,"y0":633,"y1":665},"font_size":0.0,"text":"Yu Wu"},{"bbox":{"x0":136,"x1":220,"y0":661,"y1":688},"font_size":0.0,"text":"Z.F. Wu"},{"bbox":{"x0":133,"x1":259,"y0":684,"y1":720},"font_size":0.0,"text":"Zhibin Gou"},{"bbox":{"x0":134,"x1":280,"y0":712,"y1":748},"font_size":0.0,"text":"Zhihong Shao"},{"bbox":{"x0":134,"x1":259,"y0":738,"y1":774},"font_size":0.0,"text":"Zhuoshu Li"},{"bbox":{"x0":135,"x1":232,"y0":768,"y1":800},"font_size":0.0,"text":"Ziyi Gao"},{"bbox":{"x0":138,"x1":273,"y0":838,"y1":863},"font_size":0.0,"text":"Contributors"},{"bbox":{"x0":134,"x1":238,"y0":861,"y1":895},"font_size":0.0,"text":"Aixin Liu"},{"bbox":{"x0":132,"x1":236,"y0":885,"y1":927},"font_size":0.0,"text":"Bing Xue"},{"bbox":{"x0":133,"x1":302,"y0":915,"y1":951},"font_size":0.0,"text":"Bingxuan Wang"},{"bbox":{"x0":133,"x1":257,"y0":942,"y1":977},"font_size":0.0,"text":"Bochao Wu"},{"bbox":{"x0":132,"x1":231,"y0":966,"y1":1006},"font_size":0.0,"text":"Bei Feng"},{"bbox":{"x0":133,"x1":268,"y0":994,"y1":1035},"font_size":0.0,"text":"Chengda Lu"},{"bbox":{"x0":134,"x1":314,"y0":1024,"y1":1060},"font_size":0.0,"text":"Chenggang Zhao"},{"bbox":{"x0":133,"x1":289,"y0":1049,"y1":1090},"font_size":0.0,"text":"Chengqi Deng"},{"bbox":{"x0":136,"x1":269,"y0":1081,"y1":1112},"font_size":0.0,"text":"Chong Ruan"},{"bbox":{"x0":136,"x1":250,"y0":1107,"y1":1137},"font_size":0.0,"text":"Damai Dai"},{"bbox":{"x0":134,"x1":245,"y0":1129,"y1":1165},"font_size":0.0,"text":"Deli Chen"},{"bbox":{"x0":135,"x1":245,"y0":1160,"y1":1196},"font_size":0.0,"text":"Dongjie Ji"},{"bbox":{"x0":133,"x1":243,"y0":1185,"y1":1223},"font_size":0.0,"text":"Erhang Li"},{"bbox":{"x0":133,"x1":271,"y0":1214,"y1":1251},"font_size":0.0,"text":"Fangyun Lin"},{"bbox":{"x0":133,"x1":259,"y0":1240,"y1":1276},"font_size":0.0,"text":"Fucong Dai"},{"bbox":{"x0":133,"x1":238,"y0":1266,"y1":1300},"font_size":0.0,"text":"Fuli Luo*"},{"bbox":{"x0":136,"x1":282,"y0":1298,"y1":1330},"font_size":0.0,"text":"Guangbo Hao"},{"bbox":{"x0":134,"x1":294,"y0":1322,"y1":1358},"font_size":0.0,"text":"Guanting Chen"},{"bbox":{"x0":134,"x1":249,"y0":1349,"y1":1384},"font_size":0.0,"text":"Guowei Li"},{"bbox":{"x0":132,"x1":242,"y0":1373,"y1":1413},"font_size":0.0,"text":"H. Zhang"},{"bbox":{"x0":136,"x1":253,"y0":1409,"y1":1433},"font_size":0.0,"text":"Hanwei Xu"},{"bbox":{"x0":134,"x1":286,"y0":1429,"y1":1465},"font_size":0.0,"text":"Honghui Ding"},{"bbox":{"x0":134,"x1":270,"y0":1457,"y1":1491},"font_size":0.0,"text":"Huazuo Gao"},{"bbox":{"x0":132,"x1":220,"y0":1484,"y1":1520},"font_size":0.0,"text":"Hui Qu"}],"source":"layout det","text":"Core Contributors  \nDaya Guo  \nDejian Yang  \nHaowei Zhang  \nJunxiao Song  \nRuoyu Zhang  \nRunxin Xu  \nQihao Zhu  \nShirong Ma  \nPeiyi Wang  \nXiao Bi  \nXiaokang Zhang  \nXingkai Yu  \nYu Wu  \nZ.F. Wu  \nZhibin Gou  \nZhihong Shao  \nZhuoshu Li  \nZiyi Gao  \nContributors  \nAixin Liu  \nBing Xue  \nBingxuan Wang  \nBochao Wu  \nBei Feng  \nChengda Lu  \nChenggang Zhao  \nChengqi Deng  \nChong Ruan  \nDamai Dai  \nDeli Chen  \nDongjie Ji  \nErhang Li  \nFangyun Lin  \nFucong Dai  \nFuli Luo*  \nGuangbo Hao  \nGuanting Chen  \nGuowei Li  \nH. Zhang  \nHanwei Xu  \nHonghui Ding  \nHuazuo Gao  \nHui Qu"},{"bbox":{"x0":596,"x1":805,"y0":274,"y1":1513},"conf":0.8613,"font_size":0.0,"label":"Reference","label_id":11,"lines":[{"bbox":{"x0":604,"x1":679,"y0":279,"y1":310},"font_size":0.0,"text":"Hui Li"},{"bbox":{"x0":604,"x1":759,"y0":309,"y1":339},"font_size":0.0,"text":"Jianzhong Guo"},{"bbox":{"x0":602,"x1":693,"y0":331,"y1":367},"font_size":0.0,"text":"Jiashi Li"},{"bbox":{"x0":604,"x1":767,"y0":363,"y1":393},"font_size":0.0,"text":"Jingchang Chen"},{"bbox":{"x0":604,"x1":755,"y0":391,"y1":421},"font_size":0.0,"text":"Jingyang Yuan"},{"bbox":{"x0":603,"x1":705,"y0":417,"y1":447},"font_size":0.0,"text":"Jinhao Tu"},{"bbox":{"x0":604,"x1":712,"y0":444,"y1":474},"font_size":0.0,"text":"Junjie Qiu"},{"bbox":{"x0":603,"x1":716,"y0":472,"y1":502},"font_size":0.0,"text":"Junlong Li"},{"bbox":{"x0":603,"x1":688,"y0":496,"y1":528},"font_size":0.0,"text":"J.L. Cai"},{"bbox":{"x0":600,"x1":688,"y0":520,"y1":560},"font_size":0.0,"text":"Jiaqi Ni"},{"bbox":{"x0":600,"x1":713,"y0":547,"y1":586},"font_size":0.0,"text":"Jian Liang"},{"bbox":{"x0":601,"x1":699,"y0":575,"y1":611},"font_size":0.0,"text":"Jin Chen"},{"bbox":{"x0":602,"x1":708,"y0":601,"y1":641},"font_size":0.0,"text":"Kai Dong"},{"bbox":{"x0":604,"x1":691,"y0":635,"y1":661},"font_size":0.0,"text":"Kai Hu*"},{"bbox":{"x0":602,"x1":738,"y0":658,"y1":691},"font_size":0.0,"text":"Kaichao You"},{"bbox":{"x0":604,"x1":718,"y0":688,"y1":719},"font_size":0.0,"text":"Kaige Gao"},{"bbox":{"x0":602,"x1":727,"y0":712,"y1":748},"font_size":0.0,"text":"Kang Guan"},{"bbox":{"x0":602,"x1":745,"y0":736,"y1":776},"font_size":0.0,"text":"Kexin Huang"},{"bbox":{"x0":604,"x1":693,"y0":768,"y1":800},"font_size":0.0,"text":"Kuai Yu"},{"bbox":{"x0":602,"x1":725,"y0":793,"y1":828},"font_size":0.0,"text":"Lean Wang"},{"bbox":{"x0":600,"x1":755,"y0":817,"y1":858},"font_size":0.0,"text":"Lecong Zhang"},{"bbox":{"x0":604,"x1":728,"y0":851,"y1":881},"font_size":0.0,"text":"Liang Zhao"},{"bbox":{"x0":602,"x1":739,"y0":875,"y1":911},"font_size":0.0,"text":"Litong Wang"},{"bbox":{"x0":600,"x1":743,"y0":898,"y1":941},"font_size":0.0,"text":"Liyue Zhang"},{"bbox":{"x0":606,"x1":679,"y0":933,"y1":960},"font_size":0.0,"text":"Lei Xu"},{"bbox":{"x0":604,"x1":693,"y0":958,"y1":989},"font_size":0.0,"text":"Leyi Xia"},{"bbox":{"x0":601,"x1":796,"y0":980,"y1":1021},"font_size":0.0,"text":"Mingchuan Zhang"},{"bbox":{"x0":600,"x1":775,"y0":1006,"y1":1049},"font_size":0.0,"text":"Minghua Zhang"},{"bbox":{"x0":600,"x1":754,"y0":1033,"y1":1076},"font_size":0.0,"text":"Minghui Tang"},{"bbox":{"x0":602,"x1":748,"y0":1064,"y1":1100},"font_size":0.0,"text":"Mingxu Zhou"},{"bbox":{"x0":604,"x1":697,"y0":1093,"y1":1124},"font_size":0.0,"text":"Meng Li"},{"bbox":{"x0":600,"x1":759,"y0":1113,"y1":1156},"font_size":0.0,"text":"Miaojun Wang"},{"bbox":{"x0":602,"x1":743,"y0":1147,"y1":1183},"font_size":0.0,"text":"Mingming Li"},{"bbox":{"x0":602,"x1":713,"y0":1173,"y1":1209},"font_size":0.0,"text":"Ning Tian"},{"bbox":{"x0":602,"x1":761,"y0":1196,"y1":1239},"font_size":0.0,"text":"Panpan Huang"},{"bbox":{"x0":604,"x1":734,"y0":1228,"y1":1263},"font_size":0.0,"text":"Peng Zhang"},{"bbox":{"x0":604,"x1":780,"y0":1252,"y1":1290},"font_size":0.0,"text":"Qiancheng Wang"},{"bbox":{"x0":606,"x1":734,"y0":1284,"y1":1314},"font_size":0.0,"text":"Qinyu Chen"},{"bbox":{"x0":604,"x1":716,"y0":1310,"y1":1340},"font_size":0.0,"text":"Qiushi Du"},{"bbox":{"x0":604,"x1":709,"y0":1337,"y1":1368},"font_size":0.0,"text":"Ruiqi Ge*"},{"bbox":{"x0":602,"x1":762,"y0":1361,"y1":1398},"font_size":0.0,"text":"Ruisong Zhang"},{"bbox":{"x0":602,"x1":725,"y0":1387,"y1":1423},"font_size":0.0,"text":"Ruizhe Pan"},{"bbox":{"x0":602,"x1":729,"y0":1413,"y1":1455},"font_size":0.0,"text":"Runji Wang"},{"bbox":{"x0":603,"x1":704,"y0":1445,"y1":1477},"font_size":0.0,"text":"R.J. Chen"},{"bbox":{"x0":603,"x1":686,"y0":1474,"y1":1505},"font_size":0.0,"text":"R.L. Jin"}],"source":"layout det","text":"Hui Li  \nJianzhong Guo  \nJiashi Li  \nJingchang Chen  \nJingyang Yuan  \nJinhao Tu  \nJunjie Qiu  \nJunlong Li  \nJ.L. Cai  \nJiaqi Ni  \nJian Liang  \nJin Chen  \nKai Dong  \nKai Hu*  \nKaichao You  \nKaige Gao  \nKang Guan  \nKexin Huang  \nKuai Yu  \nLean Wang  \nLecong Zhang  \nLiang Zhao  \nLitong Wang  \nLiyue Zhang  \nLei Xu  \nLeyi Xia  \nMingchuan Zhang  \nMinghua Zhang  \nMinghui Tang  \nMingxu Zhou  \nMeng Li  \nMiaojun Wang  \nMingming Li  \nNing Tian  \nPanpan Huang  \nPeng Zhang  \nQiancheng Wang  \nQinyu Chen  \nQiushi Du  \nRuiqi Ge*  \nRuisong Zhang  \nRuizhe Pan  \nRunji Wang  \nR.J. Chen  \nR.L. Jin"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":130,"x1":331,"y0":280,"y1":1527},"conf":0.8623,"label":"Reference","label_id":11},{"bbox":{"x0":596,"x1":805,"y0":274,"y1":1513},"conf":0.8613,"label":"Reference","label_id":11},{"bbox":{"x0":132,"x1":637,"y0":214,"y1":258},"conf":0.8189,"label":"Title","label_id":0},{"bbox":{"x0":576,"x1":613,"y0":1550,"y1":1580},"conf":0.7929,"label":"Abandon","label_id":2},{"bbox":{"x0":134,"x1":269,"y0":163,"y1":205},"conf":0.6998,"label":"Title","label_id":0},{"bbox":{"x0":131,"x1":317,"y0":282,"y1":811},"conf":0.322,"label":"Reference","label_id":11}],"ocr_all":false,"ocr_dets":[{"poly":[[578,1549],[611,1549],[611,1579],[578,1579]],"score":0.8714},{"poly":[[134,1484],[220,1488],[219,1520],[132,1515]],"score":0.854},{"poly":[[603,1474],[686,1474],[686,1505],[603,1505]],"score":0.8802},{"poly":[[135,1457],[270,1462],[269,1491],[134,1487]],"score":0.8294},{"poly":[[603,1445],[704,1445],[704,1477],[603,1477]],"score":0.8152},{"poly":[[135,1429],[286,1433],[285,1465],[134,1461]],"score":0.7532},{"poly":[[604,1413],[729,1422],[727,1455],[602,1447]],"score":0.7115},{"poly":[[136,1409],[253,1409],[253,1433],[136,1433]],"score":0.9622},{"poly":[[603,1387],[725,1391],[724,1423],[602,1419]],"score":0.7968},{"poly":[[135,1373],[242,1381],[239,1413],[132,1404]],"score":0.8136},{"poly":[[603,1361],[762,1367],[761,1398],[602,1392]],"score":0.8053},{"poly":[[136,1349],[249,1353],[247,1384],[134,1380]],"score":0.8353},{"poly":[[604,1337],[709,1337],[709,1368],[604,1368]],"score":0.8785},{"poly":[[135,1322],[294,1326],[294,1358],[134,1354]],"score":0.8272},{"poly":[[604,1310],[716,1310],[716,1340],[604,1340]],"score":0.9494},{"poly":[[136,1298],[282,1298],[282,1330],[136,1330]],"score":0.7563},{"poly":[[606,1284],[734,1284],[734,1314],[606,1314]],"score":0.9006},{"poly":[[133,1268],[237,1266],[238,1298],[133,1300]],"score":0.8015},{"poly":[[605,1252],[780,1258],[779,1290],[604,1284]],"score":0.7863},{"poly":[[134,1240],[259,1244],[258,1276],[133,1271]],"score":0.7843},{"poly":[[605,1228],[734,1232],[733,1263],[604,1259]],"score":0.8105},{"poly":[[133,1214],[271,1214],[271,1251],[133,1251]],"score":0.7084},{"poly":[[604,1196],[761,1202],[759,1239],[602,1233]],"score":0.6534},{"poly":[[134,1185],[243,1190],[242,1223],[133,1219]],"score":0.7561},{"poly":[[603,1173],[713,1177],[712,1209],[602,1205]],"score":0.7918},{"poly":[[135,1160],[245,1160],[245,1196],[135,1196]],"score":0.7965},{"poly":[[603,1147],[743,1151],[742,1183],[602,1179]],"score":0.791},{"poly":[[136,1129],[245,1134],[244,1165],[134,1161]],"score":0.7988},{"poly":[[602,1113],[759,1120],[758,1156],[600,1150]],"score":0.6495},{"poly":[[136,1107],[250,1107],[250,1137],[136,1137]],"score":0.8736},{"poly":[[604,1093],[697,1093],[697,1124],[604,1124]],"score":0.815},{"poly":[[136,1081],[269,1081],[269,1112],[136,1112]],"score":0.7961},{"poly":[[603,1064],[748,1069],[747,1100],[602,1096]],"score":0.7584},{"poly":[[134,1049],[289,1053],[288,1090],[133,1086]],"score":0.7112},{"poly":[[602,1033],[754,1039],[752,1076],[600,1070]],"score":0.6895},{"poly":[[135,1024],[314,1028],[313,1060],[134,1056]],"score":0.8092},{"poly":[[602,1006],[775,1013],[774,1049],[600,1043]],"score":0.6696},{"poly":[[134,994],[268,998],[267,1035],[133,1031]],"score":0.7512},{"poly":[[601,980],[796,984],[795,1021],[601,1017]],"score":0.6655},{"poly":[[135,966],[231,974],[228,1006],[132,997]],"score":0.784},{"poly":[[604,958],[693,958],[693,989],[604,989]],"score":0.8334},{"poly":[[134,942],[257,946],[256,977],[133,973]],"score":0.8117},{"poly":[[606,933],[679,933],[679,960],[606,960]],"score":0.9371},{"poly":[[133,915],[302,919],[301,951],[133,947]],"score":0.751},{"poly":[[602,898],[743,904],[741,941],[600,934]],"score":0.7672},{"poly":[[134,885],[236,890],[235,927],[132,922]],"score":0.7939},{"poly":[[603,875],[739,879],[738,911],[602,907]],"score":0.7927},{"poly":[[136,861],[238,865],[237,895],[134,891]],"score":0.8333},{"poly":[[604,851],[728,851],[728,881],[604,881]],"score":0.8601},{"poly":[[138,838],[273,838],[273,863],[138,863]],"score":0.8853},{"poly":[[601,817],[755,821],[754,858],[600,854]],"score":0.7025},{"poly":[[603,793],[725,797],[724,828],[602,824]],"score":0.8071},{"poly":[[604,768],[693,768],[693,800],[604,800]],"score":0.8159},{"poly":[[135,768],[232,768],[232,800],[135,800]],"score":0.8712},{"poly":[[604,736],[745,744],[743,776],[602,768]],"score":0.7774},{"poly":[[135,738],[259,742],[258,774],[134,770]],"score":0.8305},{"poly":[[603,712],[727,716],[726,748],[602,743]],"score":0.7365},{"poly":[[135,712],[280,716],[279,748],[134,744]],"score":0.7842},{"poly":[[604,688],[718,688],[718,719],[604,719]],"score":0.7784},{"poly":[[134,684],[259,688],[258,720],[133,715]],"score":0.7817},{"poly":[[603,658],[738,662],[737,691],[602,687]],"score":0.8219},{"poly":[[136,661],[220,661],[220,688],[136,688]],"score":0.9396},{"poly":[[604,635],[691,635],[691,661],[604,661]],"score":0.9255},{"poly":[[135,633],[213,633],[213,665],[135,665]],"score":0.8839},{"poly":[[604,601],[708,608],[706,641],[602,634]],"score":0.7536},{"poly":[[135,607],[252,607],[252,639],[135,639]],"score":0.8525},{"poly":[[134,575],[305,581],[304,613],[133,607]],"score":0.8256},{"poly":[[601,577],[698,575],[699,609],[601,611]],"score":0.7747},{"poly":[[602,547],[713,553],[711,586],[600,580]],"score":0.7182},{"poly":[[133,551],[216,551],[216,582],[133,582]],"score":0.836},{"poly":[[134,521],[256,527],[254,558],[132,552]],"score":0.8247},{"poly":[[600,527],[686,520],[688,554],[603,560]],"score":0.8215},{"poly":[[135,498],[259,498],[259,528],[135,528]],"score":0.8525},{"poly":[[603,496],[688,496],[688,528],[603,528]],"score":0.7853},{"poly":[[603,472],[716,472],[716,502],[603,502]],"score":0.8553},{"poly":[[136,472],[248,472],[248,496],[136,496]],"score":0.8352},{"poly":[[604,444],[712,444],[712,474],[604,474]],"score":0.8816},{"poly":[[135,444],[252,444],[252,474],[135,474]],"score":0.8413},{"poly":[[603,417],[705,417],[705,447],[603,447]],"score":0.8469},{"poly":[[134,414],[280,418],[279,449],[133,445]],"score":0.7472},{"poly":[[604,391],[755,391],[755,421],[604,421]],"score":0.8728},{"poly":[[134,382],[275,390],[273,425],[132,417]],"score":0.6839},{"poly":[[604,363],[767,363],[767,393],[604,393]],"score":0.8426},{"poly":[[136,357],[291,365],[290,397],[134,389]],"score":0.7282},{"poly":[[602,336],[692,331],[693,363],[604,367]],"score":0.8019},{"poly":[[135,333],[263,337],[262,369],[134,365]],"score":0.7967},{"poly":[[604,309],[759,309],[759,339],[604,339]],"score":0.8588},{"poly":[[136,307],[245,307],[245,339],[136,339]],"score":0.8102},{"poly":[[138,284],[324,284],[324,309],[138,309]],"score":0.8867},{"poly":[[604,279],[679,279],[679,310],[604,310]],"score":0.8176},{"poly":[[137,217],[631,219],[631,249],[136,247]],"score":0.8783},{"poly":[[134,167],[263,163],[264,200],[136,204]],"score":0.8759}],"page_no":19,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":576,"x1":611,"y0":1550,"y1":1581},"conf":0.7659,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":578,"x1":610,"y0":1549,"y1":1579},"font_size":0.0,"text":"21"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":132,"x1":319,"y0":159,"y1":1513},"conf":0.8654,"font_size":0.0,"label":"Reference","label_id":11,"lines":[{"bbox":{"x0":136,"x1":248,"y0":168,"y1":198},"font_size":0.0,"text":"Ruyi Chen"},{"bbox":{"x0":134,"x1":273,"y0":193,"y1":228},"font_size":0.0,"text":"Shanghao Lu"},{"bbox":{"x0":133,"x1":302,"y0":217,"y1":258},"font_size":0.0,"text":"Shangyan Zhou"},{"bbox":{"x0":134,"x1":312,"y0":247,"y1":283},"font_size":0.0,"text":"Shanhuang Chen"},{"bbox":{"x0":134,"x1":277,"y0":272,"y1":313},"font_size":0.0,"text":"Shengfeng Ye"},{"bbox":{"x0":134,"x1":265,"y0":300,"y1":337},"font_size":0.0,"text":"Shiyu Wang"},{"bbox":{"x0":134,"x1":266,"y0":329,"y1":365},"font_size":0.0,"text":"Shuiping Yu"},{"bbox":{"x0":134,"x1":298,"y0":356,"y1":391},"font_size":0.0,"text":"Shunfeng Zhou"},{"bbox":{"x0":135,"x1":264,"y0":386,"y1":417},"font_size":0.0,"text":"Shuting Pan"},{"bbox":{"x0":132,"x1":210,"y0":406,"y1":444},"font_size":0.0,"text":"S.S. Li"},{"bbox":{"x0":133,"x1":279,"y0":435,"y1":476},"font_size":0.0,"text":"Shuang Zhou"},{"bbox":{"x0":134,"x1":275,"y0":465,"y1":500},"font_size":0.0,"text":"Shaoqing Wu"},{"bbox":{"x0":133,"x1":279,"y0":489,"y1":530},"font_size":0.0,"text":"Shengfeng Ye"},{"bbox":{"x0":132,"x1":226,"y0":517,"y1":553},"font_size":0.0,"text":"Tao Yun"},{"bbox":{"x0":136,"x1":222,"y0":549,"y1":575},"font_size":0.0,"text":"Tian Pei"},{"bbox":{"x0":134,"x1":256,"y0":572,"y1":607},"font_size":0.0,"text":"Tianyu Sun"},{"bbox":{"x0":132,"x1":228,"y0":597,"y1":638},"font_size":0.0,"text":"T. Wang"},{"bbox":{"x0":133,"x1":302,"y0":626,"y1":663},"font_size":0.0,"text":"Wangding Zeng"},{"bbox":{"x0":132,"x1":227,"y0":652,"y1":688},"font_size":0.0,"text":"Wen Liu"},{"bbox":{"x0":134,"x1":293,"y0":680,"y1":718},"font_size":0.0,"text":"Wenfeng Liang"},{"bbox":{"x0":135,"x1":264,"y0":710,"y1":740},"font_size":0.0,"text":"Wenjun Gao"},{"bbox":{"x0":135,"x1":259,"y0":737,"y1":768},"font_size":0.0,"text":"Wenqin Yu*"},{"bbox":{"x0":134,"x1":288,"y0":757,"y1":799},"font_size":0.0,"text":"Wentao Zhang"},{"bbox":{"x0":135,"x1":239,"y0":791,"y1":821},"font_size":0.0,"text":"W.L. Xiao"},{"bbox":{"x0":135,"x1":218,"y0":817,"y1":849},"font_size":0.0,"text":"Wei An"},{"bbox":{"x0":133,"x1":279,"y0":843,"y1":879},"font_size":0.0,"text":"Xiaodong Liu"},{"bbox":{"x0":132,"x1":288,"y0":866,"y1":909},"font_size":0.0,"text":"Xiaohan Wang"},{"bbox":{"x0":135,"x1":292,"y0":902,"y1":931},"font_size":0.0,"text":"Xiaokang Chen"},{"bbox":{"x0":133,"x1":259,"y0":924,"y1":960},"font_size":0.0,"text":"Xiaotao Nie"},{"bbox":{"x0":132,"x1":249,"y0":950,"y1":988},"font_size":0.0,"text":"Xin Cheng"},{"bbox":{"x0":132,"x1":219,"y0":977,"y1":1013},"font_size":0.0,"text":"Xin Liu"},{"bbox":{"x0":135,"x1":216,"y0":1007,"y1":1038},"font_size":0.0,"text":"Xin Xie"},{"bbox":{"x0":133,"x1":275,"y0":1033,"y1":1069},"font_size":0.0,"text":"Xingchao Liu"},{"bbox":{"x0":132,"x1":259,"y0":1059,"y1":1097},"font_size":0.0,"text":"Xinyu Yang"},{"bbox":{"x0":135,"x1":253,"y0":1089,"y1":1119},"font_size":0.0,"text":"Xinyuan Li"},{"bbox":{"x0":135,"x1":271,"y0":1117,"y1":1147},"font_size":0.0,"text":"Xuecheng Su"},{"bbox":{"x0":133,"x1":261,"y0":1142,"y1":1177},"font_size":0.0,"text":"Xuheng Lin"},{"bbox":{"x0":132,"x1":217,"y0":1168,"y1":1206},"font_size":0.0,"text":"X.Q. Li"},{"bbox":{"x0":131,"x1":273,"y0":1194,"y1":1235},"font_size":0.0,"text":"Xiangyue Jin"},{"bbox":{"x0":133,"x1":268,"y0":1222,"y1":1258},"font_size":0.0,"text":"Xiaojin Shen"},{"bbox":{"x0":135,"x1":278,"y0":1252,"y1":1282},"font_size":0.0,"text":"Xiaosha Chen"},{"bbox":{"x0":133,"x1":275,"y0":1275,"y1":1311},"font_size":0.0,"text":"Xiaowen Sun"},{"bbox":{"x0":133,"x1":305,"y0":1303,"y1":1341},"font_size":0.0,"text":"Xiaoxiang Wang"},{"bbox":{"x0":132,"x1":270,"y0":1329,"y1":1367},"font_size":0.0,"text":"Xinnan Song"},{"bbox":{"x0":133,"x1":256,"y0":1357,"y1":1393},"font_size":0.0,"text":"Xinyi Zhou"},{"bbox":{"x0":134,"x1":277,"y0":1384,"y1":1421},"font_size":0.0,"text":"Xianzu Wang"},{"bbox":{"x0":136,"x1":262,"y0":1416,"y1":1445},"font_size":0.0,"text":"Xinxia Shan"},{"bbox":{"x0":132,"x1":213,"y0":1438,"y1":1474},"font_size":0.0,"text":"Y.K. Li"},{"bbox":{"x0":134,"x1":249,"y0":1462,"y1":1504},"font_size":0.0,"text":"Y.Q. Wang"}],"source":"layout det","text":"Ruyi Chen  \nShanghao Lu  \nShangyan Zhou  \nShanhuang Chen  \nShengfeng Ye  \nShiyu Wang  \nShuiping Yu  \nShunfeng Zhou  \nShuting Pan  \nS.S. Li  \nShuang Zhou  \nShaoqing Wu  \nShengfeng Ye  \nTao Yun  \nTian Pei  \nTianyu Sun  \nT. Wang  \nWangding Zeng  \nWen Liu  \nWenfeng Liang  \nWenjun Gao  \nWenqin Yu*  \nWentao Zhang  \nW.L. Xiao  \nWei An  \nXiaodong Liu  \nXiaohan Wang  \nXiaokang Chen  \nXiaotao Nie  \nXin Cheng  \nXin Liu  \nXin Xie  \nXingchao Liu  \nXinyu Yang  \nXinyuan Li  \nXuecheng Su  \nXuheng Lin  \nX.Q. Li  \nXiangyue Jin  \nXiaojin Shen  \nXiaosha Chen  \nXiaowen Sun  \nXiaoxiang Wang  \nXinnan Song  \nXinyi Zhou  \nXianzu Wang  \nXinxia Shan  \nY.K. Li  \nY.Q. Wang"},{"bbox":{"x0":595,"x1":788,"y0":169,"y1":1513},"conf":0.8652,"font_size":0.0,"label":"Reference","label_id":11,"lines":[{"bbox":{"x0":606,"x1":693,"y0":170,"y1":196},"font_size":0.0,"text":"Y.X. Wei"},{"bbox":{"x0":604,"x1":731,"y0":193,"y1":228},"font_size":0.0,"text":"Yang Zhang"},{"bbox":{"x0":604,"x1":732,"y0":217,"y1":258},"font_size":0.0,"text":"Yanhong Xu"},{"bbox":{"x0":604,"x1":677,"y0":249,"y1":281},"font_size":0.0,"text":"Yao Li"},{"bbox":{"x0":604,"x1":705,"y0":275,"y1":307},"font_size":0.0,"text":"Yao Zhao"},{"bbox":{"x0":602,"x1":736,"y0":301,"y1":335},"font_size":0.0,"text":"Yaofeng Sun"},{"bbox":{"x0":602,"x1":745,"y0":326,"y1":365},"font_size":0.0,"text":"Yaohui Wang"},{"bbox":{"x0":600,"x1":667,"y0":354,"y1":390},"font_size":0.0,"text":"Yi Yu"},{"bbox":{"x0":602,"x1":748,"y0":380,"y1":420},"font_size":0.0,"text":"Yichao Zhang"},{"bbox":{"x0":604,"x1":700,"y0":412,"y1":442},"font_size":0.0,"text":"Yifan Shi"},{"bbox":{"x0":602,"x1":747,"y0":436,"y1":474},"font_size":0.0,"text":"Yiliang Xiong"},{"bbox":{"x0":600,"x1":695,"y0":461,"y1":504},"font_size":0.0,"text":"Ying He"},{"bbox":{"x0":602,"x1":711,"y0":489,"y1":525},"font_size":0.0,"text":"Yishi Piao"},{"bbox":{"x0":600,"x1":743,"y0":515,"y1":556},"font_size":0.0,"text":"Yisong Wang"},{"bbox":{"x0":604,"x1":723,"y0":547,"y1":577},"font_size":0.0,"text":"Yixuan Tan"},{"bbox":{"x0":604,"x1":725,"y0":574,"y1":605},"font_size":0.0,"text":"Yiyang Ma*"},{"bbox":{"x0":602,"x1":722,"y0":600,"y1":635},"font_size":0.0,"text":"Yiyuan Liu"},{"bbox":{"x0":604,"x1":764,"y0":628,"y1":663},"font_size":0.0,"text":"Yongqiang Guo"},{"bbox":{"x0":603,"x1":698,"y0":654,"y1":684},"font_size":0.0,"text":"Yuan Ou"},{"bbox":{"x0":600,"x1":754,"y0":675,"y1":720},"font_size":0.0,"text":"Yuduan Wang"},{"bbox":{"x0":600,"x1":713,"y0":703,"y1":748},"font_size":0.0,"text":"Yue Gong"},{"bbox":{"x0":602,"x1":734,"y0":735,"y1":770},"font_size":0.0,"text":"Yuheng Zou"},{"bbox":{"x0":602,"x1":699,"y0":761,"y1":797},"font_size":0.0,"text":"Yujia He"},{"bbox":{"x0":602,"x1":747,"y0":787,"y1":825},"font_size":0.0,"text":"Yunfan Xiong"},{"bbox":{"x0":602,"x1":740,"y0":815,"y1":853},"font_size":0.0,"text":"Yuxiang Luo"},{"bbox":{"x0":602,"x1":738,"y0":843,"y1":879},"font_size":0.0,"text":"Yuxiang You"},{"bbox":{"x0":603,"x1":725,"y0":874,"y1":903},"font_size":0.0,"text":"Yuxuan Liu"},{"bbox":{"x0":600,"x1":748,"y0":896,"y1":937},"font_size":0.0,"text":"Yuyang Zhou"},{"bbox":{"x0":602,"x1":699,"y0":922,"y1":958},"font_size":0.0,"text":"Y.X. Zhu"},{"bbox":{"x0":602,"x1":768,"y0":952,"y1":988},"font_size":0.0,"text":"Yanping Huang"},{"bbox":{"x0":604,"x1":705,"y0":981,"y1":1010},"font_size":0.0,"text":"Yaohui Li"},{"bbox":{"x0":602,"x1":704,"y0":1003,"y1":1043},"font_size":0.0,"text":"Yi Zheng"},{"bbox":{"x0":604,"x1":732,"y0":1035,"y1":1065},"font_size":0.0,"text":"Yuchen Zhu"},{"bbox":{"x0":602,"x1":731,"y0":1059,"y1":1093},"font_size":0.0,"text":"Yunxian Ma"},{"bbox":{"x0":602,"x1":715,"y0":1087,"y1":1123},"font_size":0.0,"text":"Ying Tang"},{"bbox":{"x0":602,"x1":722,"y0":1112,"y1":1148},"font_size":0.0,"text":"Yukun Zha"},{"bbox":{"x0":602,"x1":722,"y0":1142,"y1":1177},"font_size":0.0,"text":"Yuting Yan"},{"bbox":{"x0":602,"x1":699,"y0":1168,"y1":1202},"font_size":0.0,"text":"Z.Z. Ren"},{"bbox":{"x0":604,"x1":716,"y0":1198,"y1":1228},"font_size":0.0,"text":"Zehui Ren"},{"bbox":{"x0":602,"x1":732,"y0":1222,"y1":1258},"font_size":0.0,"text":"Zhangli Sha"},{"bbox":{"x0":603,"x1":684,"y0":1251,"y1":1282},"font_size":0.0,"text":"Zhe Fu"},{"bbox":{"x0":602,"x1":709,"y0":1275,"y1":1311},"font_size":0.0,"text":"Zhean Xu"},{"bbox":{"x0":603,"x1":725,"y0":1303,"y1":1333},"font_size":0.0,"text":"Zhenda Xie"},{"bbox":{"x0":602,"x1":782,"y0":1331,"y1":1367},"font_size":0.0,"text":"Zhengyan Zhang"},{"bbox":{"x0":604,"x1":741,"y0":1359,"y1":1389},"font_size":0.0,"text":"Zhewen Hao"},{"bbox":{"x0":602,"x1":741,"y0":1384,"y1":1421},"font_size":0.0,"text":"Zhicheng Ma"},{"bbox":{"x0":602,"x1":738,"y0":1414,"y1":1449},"font_size":0.0,"text":"Zhigang Yan"},{"bbox":{"x0":602,"x1":713,"y0":1440,"y1":1476},"font_size":0.0,"text":"Zhiyu Wu"},{"bbox":{"x0":602,"x1":704,"y0":1466,"y1":1502},"font_size":0.0,"text":"Zihui Gu"}],"source":"layout det","text":"Y.X. Wei  \nYang Zhang  \nYanhong Xu  \nYao Li  \nYao Zhao  \nYaofeng Sun  \nYaohui Wang  \nYi Yu  \nYichao Zhang  \nYifan Shi  \nYiliang Xiong  \nYing He  \nYishi Piao  \nYisong Wang  \nYixuan Tan  \nYiyang Ma*  \nYiyuan Liu  \nYongqiang Guo  \nYuan Ou  \nYuduan Wang  \nYue Gong  \nYuheng Zou  \nYujia He  \nYunfan Xiong  \nYuxiang Luo  \nYuxiang You  \nYuxuan Liu  \nYuyang Zhou  \nY.X. Zhu  \nYanping Huang  \nYaohui Li  \nYi Zheng  \nYuchen Zhu  \nYunxian Ma  \nYing Tang  \nYukun Zha  \nYuting Yan  \nZ.Z. Ren  \nZehui Ren  \nZhangli Sha  \nZhe Fu  \nZhean Xu  \nZhenda Xie  \nZhengyan Zhang  \nZhewen Hao  \nZhicheng Ma  \nZhigang Yan  \nZhiyu Wu  \nZihui Gu"}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":132,"x1":319,"y0":159,"y1":1513},"conf":0.8654,"label":"Reference","label_id":11},{"bbox":{"x0":595,"x1":788,"y0":169,"y1":1513},"conf":0.8652,"label":"Reference","label_id":11},{"bbox":{"x0":576,"x1":611,"y0":1550,"y1":1581},"conf":0.7659,"label":"Abandon","label_id":2},{"bbox":{"x0":598,"x1":787,"y0":159,"y1":1511},"conf":0.2242,"label":"Reference","label_id":11}],"ocr_all":false,"ocr_dets":[{"poly":[[578,1549],[610,1549],[610,1579],[578,1579]],"score":0.8335},{"poly":[[604,1466],[704,1470],[703,1502],[602,1498]],"score":0.8562},{"poly":[[136,1462],[249,1469],[247,1504],[134,1498]],"score":0.7975},{"poly":[[603,1440],[713,1444],[712,1476],[602,1471]],"score":0.8189},{"poly":[[134,1438],[213,1442],[211,1474],[132,1469]],"score":0.8109},{"poly":[[603,1414],[738,1418],[737,1449],[602,1445]],"score":0.8083},{"poly":[[136,1416],[262,1416],[262,1445],[136,1445]],"score":0.8446},{"poly":[[136,1384],[277,1390],[276,1421],[134,1415]],"score":0.7575},{"poly":[[604,1384],[741,1390],[740,1421],[602,1415]],"score":0.778},{"poly":[[604,1359],[741,1359],[741,1389],[604,1389]],"score":0.794},{"poly":[[134,1357],[256,1362],[254,1393],[133,1389]],"score":0.7915},{"poly":[[603,1331],[782,1335],[781,1367],[602,1363]],"score":0.7163},{"poly":[[134,1329],[270,1335],[268,1367],[132,1361]],"score":0.8035},{"poly":[[134,1303],[305,1309],[304,1341],[133,1335]],"score":0.7545},{"poly":[[603,1303],[725,1303],[725,1333],[603,1333]],"score":0.7236},{"poly":[[603,1275],[709,1279],[708,1311],[602,1306]],"score":0.7932},{"poly":[[134,1275],[275,1279],[274,1311],[133,1307]],"score":0.7672},{"poly":[[135,1252],[278,1252],[278,1282],[135,1282]],"score":0.9266},{"poly":[[603,1251],[684,1251],[684,1282],[603,1282]],"score":0.8452},{"poly":[[603,1222],[732,1226],[731,1258],[602,1254]],"score":0.8388},{"poly":[[134,1222],[268,1226],[267,1258],[133,1254]],"score":0.7725},{"poly":[[604,1198],[716,1198],[716,1228],[604,1228]],"score":0.9517},{"poly":[[132,1194],[273,1198],[272,1235],[131,1231]],"score":0.708},{"poly":[[604,1168],[699,1172],[697,1202],[602,1198]],"score":0.8652},{"poly":[[134,1168],[217,1172],[215,1206],[132,1201]],"score":0.7998},{"poly":[[603,1142],[722,1146],[721,1177],[602,1173]],"score":0.8285},{"poly":[[134,1142],[261,1146],[260,1177],[133,1173]],"score":0.8384},{"poly":[[135,1117],[271,1117],[271,1147],[135,1147]],"score":0.8793},{"poly":[[603,1112],[722,1116],[721,1148],[602,1143]],"score":0.7879},{"poly":[[603,1087],[715,1091],[713,1123],[602,1119]],"score":0.7927},{"poly":[[135,1089],[253,1089],[253,1119],[135,1119]],"score":0.85},{"poly":[[603,1059],[731,1063],[730,1093],[602,1089]],"score":0.764},{"poly":[[134,1059],[259,1065],[258,1097],[132,1091]],"score":0.7447},{"poly":[[604,1035],[732,1035],[732,1065],[604,1065]],"score":0.8512},{"poly":[[134,1033],[275,1037],[274,1069],[133,1065]],"score":0.7832},{"poly":[[605,1003],[704,1011],[702,1043],[602,1034]],"score":0.8215},{"poly":[[135,1007],[216,1007],[216,1038],[135,1038]],"score":0.8628},{"poly":[[604,981],[705,981],[705,1010],[604,1010]],"score":0.8997},{"poly":[[134,977],[219,981],[217,1013],[132,1008]],"score":0.8522},{"poly":[[603,952],[768,956],[767,988],[602,984]],"score":0.7441},{"poly":[[134,950],[249,957],[247,988],[132,982]],"score":0.772},{"poly":[[604,922],[699,927],[697,958],[602,954]],"score":0.8105},{"poly":[[134,924],[259,928],[258,960],[133,956]],"score":0.8168},{"poly":[[135,902],[292,902],[292,931],[135,931]],"score":0.8682},{"poly":[[602,896],[748,900],[747,937],[600,933]],"score":0.7336},{"poly":[[603,874],[725,874],[725,903],[603,903]],"score":0.8487},{"poly":[[134,866],[288,872],[286,909],[132,903]],"score":0.7331},{"poly":[[603,843],[738,848],[737,879],[602,875]],"score":0.7898},{"poly":[[134,843],[279,848],[278,879],[133,875]],"score":0.8059},{"poly":[[604,815],[740,821],[738,853],[602,847]],"score":0.7441},{"poly":[[135,817],[218,817],[218,849],[135,849]],"score":0.8383},{"poly":[[604,787],[747,793],[745,825],[602,819]],"score":0.7787},{"poly":[[135,791],[239,791],[239,821],[135,821]],"score":0.9043},{"poly":[[604,761],[699,765],[697,797],[602,792]],"score":0.801},{"poly":[[136,757],[288,765],[286,799],[134,791]],"score":0.6647},{"poly":[[603,735],[734,739],[733,770],[602,766]],"score":0.8411},{"poly":[[135,737],[259,737],[259,768],[135,768]],"score":0.7839},{"poly":[[603,703],[713,711],[710,748],[600,739]],"score":0.7652},{"poly":[[135,710],[264,710],[264,740],[135,740]],"score":0.8474},{"poly":[[602,675],[754,683],[752,720],[600,712]],"score":0.6806},{"poly":[[136,680],[293,686],[292,718],[134,712]],"score":0.7554},{"poly":[[603,654],[698,654],[698,684],[603,684]],"score":0.7403},{"poly":[[134,652],[227,656],[226,688],[132,684]],"score":0.8027},{"poly":[[605,628],[764,632],[763,663],[604,659]],"score":0.8044},{"poly":[[134,626],[302,632],[301,663],[133,657]],"score":0.7457},{"poly":[[603,600],[722,604],[721,635],[602,631]],"score":0.8023},{"poly":[[135,597],[228,606],[225,638],[132,629]],"score":0.7739},{"poly":[[604,574],[725,574],[725,605],[604,605]],"score":0.7755},{"poly":[[135,572],[256,576],[254,607],[134,603]],"score":0.7704},{"poly":[[604,547],[723,547],[723,577],[604,577]],"score":0.866},{"poly":[[136,549],[222,549],[222,575],[136,575]],"score":0.9048},{"poly":[[602,515],[743,520],[742,556],[600,552]],"score":0.7033},{"poly":[[134,517],[226,521],[224,553],[132,549]],"score":0.8224},{"poly":[[603,489],[711,493],[710,525],[602,521]],"score":0.8082},{"poly":[[134,489],[279,493],[278,530],[133,526]],"score":0.7537},{"poly":[[135,465],[275,469],[274,500],[134,496]],"score":0.8576},{"poly":[[602,461],[695,465],[693,504],[600,499]],"score":0.7547},{"poly":[[604,436],[747,442],[745,474],[602,468]],"score":0.7795},{"poly":[[134,435],[279,439],[278,476],[133,472]],"score":0.768},{"poly":[[604,412],[700,412],[700,442],[604,442]],"score":0.9798},{"poly":[[134,406],[210,411],[208,444],[132,440]],"score":0.8298},{"poly":[[135,386],[264,386],[264,417],[135,417]],"score":0.7853},{"poly":[[604,380],[748,388],[747,420],[602,412]],"score":0.7596},{"poly":[[135,356],[298,360],[297,391],[134,387]],"score":0.7835},{"poly":[[603,354],[667,359],[665,390],[600,385]],"score":0.8384},{"poly":[[135,329],[266,334],[265,365],[134,361]],"score":0.8308},{"poly":[[604,326],[745,334],[743,365],[602,357]],"score":0.779},{"poly":[[603,301],[736,306],[735,335],[602,331]],"score":0.8568},{"poly":[[136,300],[265,306],[263,337],[134,331]],"score":0.7497},{"poly":[[604,275],[705,275],[705,307],[604,307]],"score":0.7899},{"poly":[[135,272],[277,276],[276,313],[134,308]],"score":0.7259},{"poly":[[604,249],[677,249],[677,281],[604,281]],"score":0.834},{"poly":[[135,247],[312,251],[311,283],[134,279]],"score":0.7832},{"poly":[[605,217],[732,221],[731,258],[604,254]],"score":0.7409},{"poly":[[134,217],[302,221],[301,258],[133,254]],"score":0.6779},{"poly":[[605,193],[731,197],[729,228],[604,224]],"score":0.7701},{"poly":[[135,193],[273,197],[272,228],[134,224]],"score":0.7956},{"poly":[[606,170],[693,170],[693,196],[606,196]],"score":0.8666},{"poly":[[136,168],[248,168],[248,198],[136,198]],"score":0.9189}],"page_no":20,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":576,"x1":613,"y0":1549,"y1":1581},"conf":0.8065,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":578,"x1":611,"y0":1549,"y1":1579},"font_size":0.0,"text":"22"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":270,"y0":280,"y1":308},"conf":0.2301,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":134,"x1":268,"y0":275,"y1":311},"font_size":0.0,"text":"Ziyang Song"}],"source":"layout det","text":"Ziyang Song"},{"bbox":{"x0":134,"x1":1059,"y0":366,"y1":433},"conf":0.7537,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":172,"x1":1053,"y0":368,"y1":398},"font_size":0.0,"text":"Within each role, authors are listed alphabetically by the frst name. Names marked with *i"},{"bbox":{"x0":136,"x1":668,"y0":395,"y1":426},"font_size":0.0,"text":"denote individuals who have departed from our team."}],"source":"layout det","text":"Within each role, authors are listed alphabetically by the frst name. Names marked with *i denote individuals who have departed from our team."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":576,"x1":613,"y0":1549,"y1":1581},"conf":0.8065,"label":"Abandon","label_id":2},{"bbox":{"x0":134,"x1":1059,"y0":366,"y1":433},"conf":0.7537,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":270,"y0":280,"y1":308},"conf":0.2301,"label":"Text","label_id":1}],"ocr_all":false,"ocr_dets":[{"poly":[[578,1549],[611,1549],[611,1579],[578,1579]],"score":0.8073},{"poly":[[137,395],[668,396],[668,426],[136,424]],"score":0.7947},{"poly":[[172,368],[1053,368],[1053,398],[172,398]],"score":0.7611},{"poly":[[136,301],[268,307],[267,339],[134,333]],"score":0.7806},{"poly":[[135,275],[268,279],[267,311],[134,307]],"score":0.797},{"poly":[[605,245],[734,251],[733,283],[604,277]],"score":0.8098},{"poly":[[136,247],[240,251],[238,283],[134,279]],"score":0.8005},{"poly":[[605,221],[773,225],[772,256],[604,252]],"score":0.7909},{"poly":[[136,221],[218,221],[218,253],[136,253]],"score":0.8711},{"poly":[[605,192],[731,199],[729,230],[604,224]],"score":0.7954},{"poly":[[136,195],[243,195],[243,226],[136,226]],"score":0.8325},{"poly":[[605,164],[738,171],[736,202],[604,196]],"score":0.7549},{"poly":[[136,168],[236,168],[236,198],[136,198]],"score":0.9426}],"page_no":21,"scale":2.0,"width":595}],"pages_success_ratio":0.0,"src_path":"oss://glm-data-ocr-data/services/maas/docs/fae77c5f-572c-4a94-9274-391069cbc16d","text":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our frst-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.i DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fne-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.i Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nFigure 1 | Benchmark performance of DeepSeek-R1.\n\nContents\n1Introduction3\n\n1.1Contributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n\n1.2Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n2Approach\n2.1Overview .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\n\n2.2DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .5\n\n2.2.1Reinforcement Learning Algorithm. . . . . . . . . . . . . . . . . . . . . .5\n\n2.2.2Reward Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\n\n2.2.3Training Template. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\n\n2.2.4Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero6\n\n2.3DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .9\n\n2.3.1Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9\n\n2.3.2Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .10\n\n2.3.3Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .10\n\n2.3.4Reinforcement Learning for all Scenarios. . . . . . . . . . . . . . . . . . .11\n\n2.4Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . .11\n\nExperiment11\n\n3.1DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13\n\n3.2Distilled Model Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14\n\nDiscussion14\n\n4.1Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .14\n\n4.2Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15\n\nConclusion, Limitations, and Future Work16\n\nContributions and Acknowledgments20\n\n1.Introduction\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artifcial General Intelligence (AGI).i\nRecently, post-training has emerged as an important component of the full training pipeline.It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI‚Äôs o1 (OpenAI, 2024b) series models were the frst to introduce inference-time scaling by increasing the length of the Chain-of-i Thought reasoning process. This approach has achieved signifcant improvements in variousi reasoning tasks, such as mathematics, coding, and scientifc reasoning. However, the challengei of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI‚Äôs o1 series models.\nIn this paper, we take the frst step toward improving language model reasoning capabilitiesi using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifcally, we use DeepSeek-V3-Base as the base model and employi GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from $15.6\\%$ to $71.0\\%,$ and with majority voting, the score further improves to $86.7\\%,$ matching the performance of OpenAI-o1-0912.\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifcally, we begin by collecting thousands of cold-start data to fne-tune theii DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.After fne-tuning with the new data, the checkpoint undergoes an additional RL process, takingi into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.532B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.\n\n1.1. Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\nWe directly apply RL to the base model without relying on supervised fne-tuning (SFT) asi a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeekR1-Zero demonstrates capabilities such as self-verifcation, refection, and generatingil long CoTs, marking a signifcant milestone for the research community. Notably, it is thei frst open research to validate that reasoning capabilities of LLMs can be incentivizedi purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\nWe introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model‚Äôs reasoning and non-reasoning capabilities. We believe the pipeline will beneft the industry by creatingi better models.\nDistillation: Smaller Models Can Be Powerful Too\nWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefti the research community to distill better smaller models in the future.\nUsing the reasoning data generated by DeepSeek-R1, we fne-tuned several dense modelsi that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeekR1-Distill-Qwen-7B achieves $55.5\\%$ on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores $72.6\\%$ on AIME 2024, $94.3\\%$ on MATH-500,and $57.2\\%$ on LiveCodeBench. These results signifcantly outperform previous open-i source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n1.2. Summary of Evaluation Results\nReasoning tasks: (1) DeepSeek-R1 achieves a score of $79.8\\%$ Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of $97.3\\%,$ performing on par with OpenAI-o1-1217 and signifcantly outperforming other models. (2)i On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,as it achieves 2,029 Elo rating on Codeforces outperforming $96.3\\%$ human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.\nKnowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeekR1 achieves outstanding results, signifcantly outperforming DeepSeek-V3 with scoresi of $90.8\\%$ on MMLU, $84.0\\%$ on MMLU-Pro, and $71.5\\%$ on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n\nOthers: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of $87.6\\%$ on AlpacaEval 2.0 and a win-rate of $92.3\\%$  on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.\n2.Approach\n2.1. Overview\nPrevious work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be signifcantlyi improved through large-scale reinforcement learning (RL), even without using supervised fne-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced withi the inclusion of a small amount of cold-start data. In the following sections, we present: (1)DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and(2) DeepSeek-R1, which applies RL starting from a checkpoint fne-tuned with thousands ofi long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nReinforcement learning has demonstrated signifcant effectiveness in reasoning tasks, as ev-i idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data,focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.\n2.2.1. Reinforcement Learning Algorithm\nGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.Specifcally, for each questioni $q,$  GRPO samples a group of outputs $\\{o_{1},o_{2},\\cdots,o_{G}\\}$  from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:\n$$\\mathcal{J}_{\\texttt{GRPO}}(\\theta)=\\mathbb{E}[q\\sim P(\\texttt{Q}),\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\texttt{O}|q)]$$\n$$\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\min\\left(\\frac{\\pi_{\\theta}(o_{i}|q)}{\\pi_{\\theta_{old}}(o_{i}|q)}A_{i},\\mathrm{clip}\\left(\\frac{\\pi_{\\theta}(o_{i}|q)}{\\pi_{\\theta_{old}}(o_{i}|q)},1-\\varepsilon,1+\\varepsilon\\right)A_{i}\\right)-\\beta\\text{I D}_{KL}\\left(\\pi_{\\theta}||\\pi_{ref}\\right)\\right),$$\n(1)\n$$\\mathbb{D}_{\\textit{KL}}\\left(\\pi_{\\theta}||\\pi_{\\textit{ref}}\\right)=\\frac{\\pi_{\\textit{ref}}(o_{i}|q)}{\\pi_{\\theta}(o_{i}|q)}-\\log\\frac{\\pi_{\\textit{ref}}(o_{i}|q)}{\\pi_{\\theta}(o_{i}|q)}-1,$$\n(2)\nwhere ùúÄand $\\beta$ are hyper-parameters, and $A_{i}$ is the advantage, computed using a group of rewards $\\{r_{1},r_{2},\\ldots,r_{G}\\}$  corresponding to the outputs within each group:\n$$A_{i}=\\frac{r_{i}-\\mathrm{m}ean(\\{r_{1},r_{2},\\cdots,r_{G}\\})}{\\mathrm{s}td(\\{r_{1},r_{2},\\cdots,r_{G}\\})}.$$\n(3)\n\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specifc reasoningi question during training.\n2.2.2. Reward Modeling\nThe reward is the source of the training signal, which decides the optimization direction of RL.To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:\nAccuracy rewards: The accuracy reward model evaluates whether the response is correct.For example, in the case of math problems with deterministic results, the model is required to provide the fnal answer in a specifed format (e.g., within a box), enabling reliableii rule-based verifcation of correctness. Similarly, for LeetCode problems, a compiler can bei used to generate feedback based on predefned test cases.i\nFormat rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‚Äò<think>‚Äô and $'<1$ think>‚Äôtags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,because we fnd that the neural reward model may suffer from reward hacking in the large-scalei reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.\n2.2.3. Training Template\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specifed instructions. As depicted in Table 1, this templatei requires DeepSeek-R1-Zero to frst produce a reasoning process, followed by the fnal answer.ii We intentionally limit our constraints to this structural format, avoiding any content-specifci biases‚Äîsuch as mandating refective reasoning or promoting particular problem-solving strate-l gies‚Äîto ensure that we can accurately observe the model‚Äôs natural progression during the RL process.\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zer\nPerformance of DeepSeek-R1-ZeroFigure 2 depicts the performance trajectory of DeepSeekR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a signifcanti increase, jumping from an initial $15.6\\%$ to an impressive $71.0\\%,$ reaching performance levels comparable to OpenAI-o1-0912. This signifcant improvement highlights the effcacy of our RLii algorithm in optimizing the model‚Äôs performance over time.\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI‚Äôs o1-0912 models across a variety of reasoning-related benchmarks. The fndings reveal that RL empowersi\n\n\n\n<html><body><table><thead><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500</td><td rowspan=\"2\">GPQA Diamond</td><td rowspan=\"2\">LiveCode Bench</td><td rowspan=\"2\">CodeForces</td></tr><tr><td>pass@1</td><td>cons@64</td></tr></thead><tbody><tr><td>OpenAI-o1-mini</td><td>63.6</td><td>80.0</td><td>pass@1 90.0</td><td>pass@1 60.0</td><td>pass@1 53.8</td><td>rating</td></tr><tr><td>OpenAI-o1-0912</td><td>74.4</td><td>83.3</td><td>94.8</td><td>77.3</td><td>63.4</td><td>1820 1843</td></tr><tr><td>DeepSeek-R1-Zero</td><td>71.0</td><td>86.7</td><td>95.9</td><td>73.3</td><td>50.0</td><td>1444</td></tr></tbody></table></body></html>\n\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fne-tuning data. This is a noteworthy achievement, as it underscores the model‚Äôs ability toi learn and generalize effectively through RL alone. Additionally, the performance of DeepSeekR1-Zero can be further augmented through the application of majority voting. For example,when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero‚Äôs performance escalates from $71.0\\%$ to $86.7\\%,$ thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\nSelf-evolution Process of DeepSeek-R1-ZeroThe self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model‚Äôs progression without the infuence of the supervised fne-tuning stage. This approach providesli a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\n\nFigure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\nment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens,allowing the model to explore and refne its thought processes in greater depth.i\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as refection‚Äîwhere the modell revisits and reevaluates its previous steps‚Äîand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model‚Äôs interaction with the reinforcement learning environment. This spontaneous development signifcantly enhances DeepSeek-R1-Zero‚Äôs reasoning capabilities,i enabling it to tackle more challenging tasks with greater effciency and accuracy.i\nAha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an ‚Äúaha moment‚Äù. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase,DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model‚Äôs growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\nThis moment is not only an ‚Äúaha moment‚Äù for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The‚Äúaha moment‚Äù serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artifcial systems, paving the way for more autonomous and adaptive models ini the future.\n\nQuestion: If then the sum of the real solutions of $\\sqrt{a-\\sqrt{a+x}}=x$ Ô∏Åis equal to $a>1,$ Response: <think>Ô∏Å\nTo solve the equation $\\sqrt{a-\\sqrt{a+x}}=x,$ Ô∏Å let‚Äôs start by squaring both ¬∑ ¬∑ ¬∑\n$$\\left(\\sqrt{a-\\sqrt{a+x}}\\right)^{2}=x^{2}\\ \\Longrightarrow\\ a-\\sqrt{a+x}=x^{2}.$$\nRearrange to isolate the inner square root term:\n$$(a-x^{2})^{2}=a+x\\implies a^{2}-2ax^{2}+(x^{2})^{2}=a+x\\implies x^{4}-2ax^{2}-x+(a^{2}-a)=0$$\nWait, wait. Wait. That‚Äôs an aha moment I can fag here.l\nLet‚Äôs reevaluate this step-by-step to identify if the correct sum can be ¬∑ ¬∑ ¬∑\nWe st‚àöarted with the equation:\n$$\\sqrt{a-\\sqrt{a+x}}=x$$\nFirs‚àöt, let‚Äôs square bot‚àöh sides:\n$$a-\\sqrt{a+x}=x^{2}\\implies\\sqrt{a+x}=a-x^{2}$$\nNext, I could square both sides again, treating the equation: ¬∑ ¬∑ ¬∑\nTable 3 | An interesting ‚Äúaha moment‚Äù of an intermediate version of DeepSeek-R1-Zero. The\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,allowing us to witness the power and beauty of reinforcement learning.\nDrawback of DeepSeek-R1-ZeroAlthough DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\n2.3.1. Cold Start\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fne-tune the model as the initial RL actor. To collect such data, we have explored severali approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with refection and verifcation, gathering DeepSeek-R1-li Zero outputs in a readable format, and refning the results through post-processing by humani annotators.\nIn this work, we collect thousands of cold-start data to fne-tune the DeepSeek-V3-Base asi the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start datainclude:\nReadability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,we design a readable pattern that includes a summary at the end of each response and flters out responses that are not reader-friendly. Here, we defne the output format asii|special_token|<reasoning_process>|special_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.\nPotential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models.\n2.3.2. Reasoning-oriented Reinforcement Learning\nAfter fne-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scalei reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model‚Äôs reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defned problems withi clear solutions. During the training process, we observe that CoT often exhibits language mixing,particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model‚Äôs performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the fnal reward. We then apply RL training on the fne-tuned model until it achieves convergenceii on reasoning tasks.\n2.3.3. Rejection Sampling and Supervised Fine-Tuning\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model‚Äôs capabilities in writing, role-playing, and other general-purpose tasks. Specifcally, wei generate the data and fne-tune the model as described below.i\nReasoning dataWe curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage,we only included data that could be evaluated using rule-based rewards. However, in this stage,we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.Additionally, because the model output is sometimes chaotic and diffcult to read, we havei fltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. Fori each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n\nNon-Reasoning dataFor non-reasoning data, such as writing, factual QA, self-cognition,and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries,such as ‚Äúhello‚Äù we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning.\nWe fne-tune DeepSeek-V3-Base for two epochs using the above curated dataset of abouti 800k samples.\n2.3.4. Reinforcement Learning for all Scenarios\nTo further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model‚Äôs helpfulness and harmlessness while simultaneously refning its reasoning capabilities. Specifcally, we train the model using a combinationii of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the fnal summary, ensuring that thei assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\n2.4. Distillation: Empower Small Models with Reasoning Capability\nTo equip more effcient smaller models with reasoning capabilities like DeepSeek-R1, we directlyi fne-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) usingi the 800k samples curated with DeepSeek-R1, as detailed in ¬ß2.3.3. Our fndings indicate thati this straightforward distillation method signifcantly enhances the reasoning abilities of smalleri models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.514B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\nFor distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.\n3.Experiment\nBenchmarksWe evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verifed (OpenAI,i2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 ‚Äì 2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifcally, wei adhere to the original confgurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Lii et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the fnal summary to evaluation to avoid the length bias. For distilled models, wei report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\nEvaluation PromptsFollowing the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simpleevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, $\\mathrm{C}\\mathrm{+}\\mathrm{+},$ C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,after which the expected ratings and percentages of competitors are calculated. SWE-Bench verifed results are obtained via the agentless framework (Xia et al., 2024). AIDER-relatedi benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.\nBaselinesWe conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on offcial reports. For distilled models, we also compare the open-source modeli QwQ-32B-Preview (Qwen, 2024a).\nEvaluation SetupWe set the maximum generation length to 32,768 tokens for the models.We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and signifcant variability across different checkpoints. Therefore, wei default to pass@ùëòevaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifcally, we use a sampling temperature of 0.6 and a top-ùëùvalue of 0.95 to generatei $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as\n$$\\text{pass@1}=\\frac{1}{k}\\sum_{i=1}^{k}p_{i},$$\nwhere $p_{i}$ denotes the correctness of the ùëñ-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n\n\n\n<html><body><table><tr><td></td><td>Benchmark (Metric)</td><td>Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022</td><td>0513</td><td>V3</td><td>o1-mini </td><td>o1-1217</td><td>R1</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>-</td><td>MoE</td><td>-</td><td>-</td><td>MoE</td></tr><tr><td rowspan=\"2\"></td><td># Activated Params</td><td>-</td><td>-</td><td>37B</td><td>-</td><td>-</td><td>37B</td></tr><tr><td># Total Params</td><td>-</td><td>-</td><td>671B</td><td>-</td><td>-</td><td>671B</td></tr><tr><td rowspan=\"12\">English</td><td>MMLU (Pass@1)</td><td>88.3</td><td>87.2</td><td>88.5</td><td>85.2</td><td>91.8</td><td>90.8</td></tr><tr><td>MMLU-Redux (EM)</td><td>88.9</td><td>88.0</td><td>89.1</td><td>86.7</td><td>-</td><td>92.9</td></tr><tr><td>MMLU-Pro (EM)</td><td>78.0</td><td>72.6</td><td>75.9</td><td>80.3</td><td>-</td><td>84.0</td></tr><tr><td>DROP (3-shot F1)</td><td>88.3</td><td>83.7</td><td>91.6</td><td>83.9</td><td>90.2</td><td>92.2</td></tr><tr><td>IF-Eval (Prompt Strict)</td><td>86.5</td><td>84.3</td><td>86.1</td><td>84.8</td><td>-</td><td>83.3</td></tr><tr><td>GPQA Diamond (Pass@1)</td><td>65.0</td><td>49.9</td><td>59.1</td><td>60.0</td><td>75.7</td><td>71.5</td></tr><tr><td>SimpleQA (Correct)</td><td>28.4</td><td>38.2</td><td>24.9</td><td>7.0</td><td>47.0</td><td>30.1</td></tr><tr><td>FRAMES (Acc.)</td><td>72.5</td><td>80.5</td><td>73.3</td><td>76.9</td><td>-</td><td>82.5</td></tr><tr><td>AlpacaEval2.0 (LC-winrate)</td><td>52.0</td><td>51.1</td><td>70.0</td><td>57.8</td><td>-</td><td>87.6</td></tr><tr><td>ArenaHard (GPT-4-1106)</td><td>85.2</td><td>80.4</td><td>85.5</td><td>92.0</td><td>-</td><td>92.3</td></tr><tr><td>LiveCodeBench (Pass@1-COT)</td><td>38.9</td><td>32.9</td><td>36.2</td><td>53.8</td><td>63.4</td><td>65.9</td></tr><tr><td rowspan=\"5\">Code</td><td>Codeforces (Percentile)</td><td>20.3</td><td>23.6</td><td>58.7</td><td>93.4</td><td>96.6</td><td>96.3</td></tr><tr><td>Codeforces (Rating)</td><td>717</td><td>759</td><td>1134</td><td>1820</td><td>2061</td><td>2029</td></tr><tr><td>SWE Verifed (Resolved)</td><td>50.8</td><td>38.8</td><td>42.0</td><td>41.6</td><td>48.9</td><td>49.2</td></tr><tr><td>Aider-Polyglot (Acc.)</td><td>45.3</td><td>16.0</td><td>49.6</td><td>32.9</td><td>61.7</td><td>53.3</td></tr><tr><td>AIME 2024 (Pass@1)</td><td>16.0</td><td>9.3</td><td>39.2</td><td>63.6</td><td>79.2</td><td>79.8</td></tr><tr><td rowspan=\"3\">Math</td><td>MATH-500 (Pass@1)</td><td>78.3</td><td>74.6</td><td>90.2</td><td>90.0</td><td>96.4</td><td>97.3</td></tr><tr><td>CNMO 2024 (Pass@1)</td><td>13.1</td><td>10.8</td><td>43.2</td><td>67.6</td><td>-</td><td>78.8</td></tr><tr><td>CLUEWSC (EM)</td><td>85.4</td><td>87.9</td><td>90.9</td><td>89.9</td><td></td><td></td></tr><tr><td rowspan=\"3\">ChineseC-Eval (EM)</td><td></td><td>76.7</td><td>76.0</td><td>86.5</td><td>68.9</td><td>-</td><td>92.8 91.8</td></tr><tr><td></td><td></td><td></td><td></td><td>40.3</td><td>-</td><td>63.7</td></tr><tr><td>C-SimpleQA (Correct)</td><td>55.4</td><td>58.7</td><td>68.0</td><td></td><td>-</td><td></td></tr></table></body></html>\n\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over $70\\%.$\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a model‚Äôs ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the fnal stages of supervised fne-tuning (SFT) and RLii training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,indicating DeepSeek-R1‚Äôs strengths in writing tasks and open-domain question answering. Its signifcant outperformance of DeepSeek-V3 underscores the generalization benefts of large-scaleii RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates thatDeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verifed. We believe the engineeringi performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited.\n3.2. Distilled Model Evaluation\n\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500</td><td rowspan=\"2\">GPQA Diamond pass@1</td><td rowspan=\"2\">LiveCode Bench</td><td rowspan=\"2\">CodeForces</td></tr><tr><td>pass@1</td><td>cons@64</td></tr><tr><td>GPT-4o-0513</td><td>9.3</td><td>13.4</td><td>pass@1 74.6</td><td>49.9</td><td>pass@1 32.9</td><td>rating 759</td></tr><tr><td>Claude-3.5-Sonnet-1022</td><td>16.0</td><td>26.7</td><td>78.3</td><td>65.0</td><td>38.9</td><td>717</td></tr><tr><td>OpenAI-o1-mini</td><td>63.6</td><td>80.0</td><td>90.0</td><td>60.0</td><td>53.8</td><td>1820</td></tr><tr><td>QwQ-32B-Preview</td><td>50.0</td><td>60.0</td><td>90.6</td><td>54.5</td><td>41.9</td><td>1316</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-1.5B</td><td>28.9</td><td>52.7</td><td>83.9</td><td>33.8</td><td>16.9</td><td>954</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-7B</td><td>55.5</td><td>83.3</td><td>92.8</td><td>49.1</td><td>37.6</td><td>1189</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-14B</td><td>69.7</td><td>80.0</td><td>93.9</td><td>59.1</td><td>53.1</td><td>1481</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-32B</td><td>72.6</td><td>83.3</td><td>94.3</td><td>62.1</td><td>57.2</td><td>1691</td></tr><tr><td>DeepSeek-R1-Distill-Llama-8B</td><td>50.4</td><td>80.0</td><td>89.1</td><td>49.0</td><td>39.6</td><td>1205</td></tr><tr><td>DeepSeek-R1-Distill-Llama-70B</td><td>70.0</td><td>86.7</td><td>94.5</td><td>65.2</td><td>57.5</td><td>1633</td></tr></table></body></html>\n\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.\nAs shown in Table 5, simply distilling DeepSeek-R1‚Äôs outputs enables the effcient DeepSeek-i R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform nonreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32BPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B signifcantlyi exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields signifcant furtheri gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n4.Discussion\n4.1. Distillation v.s. Reinforcement Learning\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n\n\n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">AIME 2024</td><td rowspan=\"2\">MATH-500 pass@1</td><td rowspan=\"2\">GPQA Diamond pass@1</td><td rowspan=\"2\">LiveCodeBench pass@1</td></tr><tr><td>pass@1</td><td>cons@64</td></tr><tr><td>QwQ-32B-Preview</td><td>50.0</td><td>60.0</td><td>90.6</td><td>54.5</td><td>41.9</td></tr><tr><td>DeepSeek-R1-Zero-Qwen-32B</td><td>47.0</td><td>60.0</td><td>91.6</td><td>55.0</td><td>40.2</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-32B</td><td>72.6</td><td>83.3</td><td>94.3</td><td>62.1</td><td>57.2</td></tr></table></body></html>\n\nTable 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs signifcantly better thani DeepSeek-R1-Zero-Qwen-32B across all benchmarks.\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and largerscale reinforcement learning.\n4.2. Unsuccessful Attempts\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\nProcess Reward Model (PRM)PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly defne a fne-grain step in general reasoning. Second,ii determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\nMonte Carlo Tree Search (MCTS)Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specifc reasoning steps necessary for the search. Fori training, we frst use collected prompts to fnd answers via MCTS guided by a pre-trained valueii model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refning the process.i\nHowever, this approach encounters several challenges when scaling up the training. First,unlike chess, where the search space is relatively well-defned, token generation presents aniexponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly infuences the quality of generation since it guides each step of the search process.l Training a fne-grained value model is inherently diffcult, which makes it challenging for theii model to iteratively improve. While AlphaGo‚Äôs core success relied on training a value model to progressively enhance its performance, this principle proves diffcult to replicate in our setupi due to the complexities of token generation.\nIn conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a signifcant challenge.i\n5.Conclusion, Limitations, and Future Work\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,leveraging cold-start data alongside iterative RL fne-tuning. Ultimately, DeepSeek-R1 achievesi performance comparable to OpenAI-o1-1217 on a range of tasks.\nWe further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fne-tune several smalli dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with $28.9\\%$ on AIME and $83.9\\%$ on MATH. Other dense models also achieve impressive results, signifcantly outperforming other instruction-i tuned models based on the same underlying checkpoints.\nure, we plan to invest in research across the following directions for DeepSeek-R1.\nGeneral Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output.Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these felds.i\nLanguage Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\nPrompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\nSoftware Engineering Tasks: Due to the long evaluation times, which impact the eff-i ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve effciency.i\n\nReferences\nAI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md.\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.URL https://arxiv.org/abs/2107.03374.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https://arxiv.org/abs/2309.17179.\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760.\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127.\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024.\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.Livecodebench: Holistic and contamination free evaluation of large language models for code.CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\n\nS. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.Fact, fetch, and reason: A unifed evaluation of retrieval-augmented generation. CoRR,i abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941.\nA. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,2023.\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024.\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,I. Sutskever, and K. Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nB. Y. Lin. ZeroEval: A Unifed Framework for Evaluating Language Models, July 2024. URLi https://github.com/WildEval/ZeroEval.\nMAA.American invitational mathematics examination - aime.In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math-competitions/american-invitational-mathematics-examination-aime.\nnAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/.\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing-simpleqa/.\nOpenAI. Introducing SWE-bench verifed we‚Äôre releasing a human-validated subset of swe-i bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench-verified/.\nQwen. Qwq: Refect deeply on the boundaries of the unknown, 2024a. URL https://qwenlml.github.io/blog/qwq-32b-preview/.\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5.\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815,2017a. URL http://arxiv.org/abs/1712.01815.\n\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354‚Äì359,2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14.\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A labelfree step-by-step verifer for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935,i 2023.\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.URL https://doi.org/10.48550/arXiv.2406.01574.\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang.Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.\nH. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152.\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\n\nAppendix\nA. Contributions and Acknowledgments\nCore Contributors  \nDaya Guo  \nDejian Yang  \nHaowei Zhang  \nJunxiao Song  \nRuoyu Zhang  \nRunxin Xu  \nQihao Zhu  \nShirong Ma  \nPeiyi Wang  \nXiao Bi  \nXiaokang Zhang  \nXingkai Yu  \nYu Wu  \nZ.F. Wu  \nZhibin Gou  \nZhihong Shao  \nZhuoshu Li  \nZiyi Gao  \nContributors  \nAixin Liu  \nBing Xue  \nBingxuan Wang  \nBochao Wu  \nBei Feng  \nChengda Lu  \nChenggang Zhao  \nChengqi Deng  \nChong Ruan  \nDamai Dai  \nDeli Chen  \nDongjie Ji  \nErhang Li  \nFangyun Lin  \nFucong Dai  \nFuli Luo*  \nGuangbo Hao  \nGuanting Chen  \nGuowei Li  \nH. Zhang  \nHanwei Xu  \nHonghui Ding  \nHuazuo Gao  \nHui Qu\nHui Li  \nJianzhong Guo  \nJiashi Li  \nJingchang Chen  \nJingyang Yuan  \nJinhao Tu  \nJunjie Qiu  \nJunlong Li  \nJ.L. Cai  \nJiaqi Ni  \nJian Liang  \nJin Chen  \nKai Dong  \nKai Hu*  \nKaichao You  \nKaige Gao  \nKang Guan  \nKexin Huang  \nKuai Yu  \nLean Wang  \nLecong Zhang  \nLiang Zhao  \nLitong Wang  \nLiyue Zhang  \nLei Xu  \nLeyi Xia  \nMingchuan Zhang  \nMinghua Zhang  \nMinghui Tang  \nMingxu Zhou  \nMeng Li  \nMiaojun Wang  \nMingming Li  \nNing Tian  \nPanpan Huang  \nPeng Zhang  \nQiancheng Wang  \nQinyu Chen  \nQiushi Du  \nRuiqi Ge*  \nRuisong Zhang  \nRuizhe Pan  \nRunji Wang  \nR.J. Chen  \nR.L. Jin\n\nRuyi Chen  \nShanghao Lu  \nShangyan Zhou  \nShanhuang Chen  \nShengfeng Ye  \nShiyu Wang  \nShuiping Yu  \nShunfeng Zhou  \nShuting Pan  \nS.S. Li  \nShuang Zhou  \nShaoqing Wu  \nShengfeng Ye  \nTao Yun  \nTian Pei  \nTianyu Sun  \nT. Wang  \nWangding Zeng  \nWen Liu  \nWenfeng Liang  \nWenjun Gao  \nWenqin Yu*  \nWentao Zhang  \nW.L. Xiao  \nWei An  \nXiaodong Liu  \nXiaohan Wang  \nXiaokang Chen  \nXiaotao Nie  \nXin Cheng  \nXin Liu  \nXin Xie  \nXingchao Liu  \nXinyu Yang  \nXinyuan Li  \nXuecheng Su  \nXuheng Lin  \nX.Q. Li  \nXiangyue Jin  \nXiaojin Shen  \nXiaosha Chen  \nXiaowen Sun  \nXiaoxiang Wang  \nXinnan Song  \nXinyi Zhou  \nXianzu Wang  \nXinxia Shan  \nY.K. Li  \nY.Q. Wang\nY.X. Wei  \nYang Zhang  \nYanhong Xu  \nYao Li  \nYao Zhao  \nYaofeng Sun  \nYaohui Wang  \nYi Yu  \nYichao Zhang  \nYifan Shi  \nYiliang Xiong  \nYing He  \nYishi Piao  \nYisong Wang  \nYixuan Tan  \nYiyang Ma*  \nYiyuan Liu  \nYongqiang Guo  \nYuan Ou  \nYuduan Wang  \nYue Gong  \nYuheng Zou  \nYujia He  \nYunfan Xiong  \nYuxiang Luo  \nYuxiang You  \nYuxuan Liu  \nYuyang Zhou  \nY.X. Zhu  \nYanping Huang  \nYaohui Li  \nYi Zheng  \nYuchen Zhu  \nYunxian Ma  \nYing Tang  \nYukun Zha  \nYuting Yan  \nZ.Z. Ren  \nZehui Ren  \nZhangli Sha  \nZhe Fu  \nZhean Xu  \nZhenda Xie  \nZhengyan Zhang  \nZhewen Hao  \nZhicheng Ma  \nZhigang Yan  \nZhiyu Wu  \nZihui Gu\n\nZiyang Song\nWithin each role, authors are listed alphabetically by the frst name. Names marked with *i denote individuals who have departed from our team.\n"}