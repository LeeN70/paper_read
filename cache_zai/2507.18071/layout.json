{"code":0,"doc_size":458773,"doc_type":"pdf","dst_path":"oss://glm-data-ocr-data/services/maas/pa/e238704f-b715-4d59-9ab4-893b6eb74c67.tar","markdown":"## Group Sequence Policy Optimization\n\nChujie Zheng∗Shixuan LiuMingze LiXiong-Hui ChenBowen Yu∗Chang GaoKai DangYuqiong LiuRui MenAn YangJingren Zhou Junyang Lin\n\nQwen Team, Alibaba Inc.\n\n## Abstract\n\nThis paper introduces Group Sequence Policy Optimization (GSPO), our stable, effcient,i and performant reinforcement learning algorithm for training large language models.Unlike previous algorithms that adopt token-level importance ratios, GSPO defnes thei importance ratio based on sequence likelihood and performs sequence-level clipping,rewarding, and optimization. We demonstrate that GSPO achieves superior training effciency and performance compared to the GRPO algorithm, notably stabilizes Mixture-i of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.\n\n## 1Introduction\n\nReinforcement learning (RL) has emerged as a pivotal paradigm for scaling language models (OpenAI,2024; DeepSeek-AI, 2025; Qwen, 2025b;a). Through large-scale RL, language models develop the capability to tackle sophisticated problems, such as competition-level mathematics and programming, by undertaking deeper and longer reasoning processes.\n\nTo successfully scale RL with greater computational investment, the foremost prerequisite is maintaining stable and robust training dynamics. However, current state-of-the-art RL algorithms, exemplifed byi GRPO (Shao et al., 2024), exhibit severe stability issues when training gigantic language models, often resulting in catastrophic and irreversible model collapse (Qwen, 2025a; MiniMax, 2025). This instability hinders efforts to push the capability boundaries of language models through continued RL training.\n\nIn this paper, we identify that the instability of GRPO stems from the fundamental misapplication and invalidation of importance sampling weights in its algorithmic design. This introduces high-variance training noise that progressively accumulates with increased response length and is further amplifed byi the clipping mechanism, ultimately precipitating model collapse.\n\nTo address these core limitations, we propose Group Sequence Policy Optimization (GSPO), a new RL algorithm for training large language models. The key innovation of GSPO lies in its theoretically grounded defnition of importance ratio based on sequence likelihood (Zheng et al., 2023), aligning withi the basic principle of importance sampling. Additionally, GSPO computes the normalized rewards as the advantages of multiple responses to a query, ensuring the alignment between sequence-level rewarding and optimization.\n\nOur empirical evaluation demonstrates the signifcant superiority of GSPO over GRPO in trainingi stability, effciency, and performance. Critically, GSPO has inherently resolved the stability challenges ini the RL training of large Mixture-of-Experts (MoE) models, eliminating the need for complex stabilization strategies, and shows the potential for simplifying RL infrastructure. These merits of GSPO ultimately contributed to the exceptional performance improvements in the latest Qwen3 models. We envision GSPO as a robust and scalable algorithmic foundation that will enable the continued advancement of large-scale RL training with language models.\n\n## 2Preliminaries\n\nNotationIn this paper, an autoregressive language model parameterized by $\\theta$ is defned as a policy πθ.i We use x to denote a query and $D$  as the query set. Given a response $y$  to a query $x,$  its likelihood underthe policy $\\pi_{\\theta}$ is denoted as $\\pi_{\\theta}(y|x)=\\begin{matrix}\\prod_{t=1}^{|y|}\\pi_{\\theta}(y_{t}|x,y_{<t}) \\end{matrix}$  where $|y|$  denotes the number of tokens in y. A query-response pair $(x,y)$  can be scored by a veriferi $r,$  resulting in a reward $r(x,y)\\in[0,1].$\n\nProximal Policy Optimization (PPO)Using samples generated from the old policy $\\pi_{\\theta_{\\text{old}}},$  PPO (Schulman et al., 2017) constrains the policy update within a proximal region of the old policy through the clipping mechanism. Specifcally, PPO employs the following objective for policy optimization (we omiti the KL regularization term hereinafter for brevity, as it is not the focus of this paper):\n\n$$\\mathcal{J}_{\\mathrm{PPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, y\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{|y|}\\sum_{t=1}^{|y|}\\min\\left(w_{t}(\\theta)\\widehat{A}_{t}, \\mathrm{clip}\\left(w_{t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{t}\\right)\\right],$$\n\n(1)\n\nwhere the importance ratio of the token $y_{t}$  is defined as $w_{t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{t}|x,y_{<t})}{\\pi_{\\theta_{\\mathrm{old}}} (y_{t}|x,y_{<t})},$  the advantage $\\widehat{A}_{t}$  of $y_{t}$  is estimated by another value model, and $\\epsilon$  is the clipping range of imopldortance ratios.\n\nThe core challenge of PPO in practice lies in its heavy reliance on the value model. Specifcally, thei value model usually has a similar size to the policy model, introducing a considerable memory and computational burden. Furthermore, the algorithmic effectiveness hinges on the reliability of its value estimate. While acquiring a reliable value model is inherently challenging, ensuring its scalability to longer responses and more complex tasks presents an even greater challenge.\n\nGroup Relative Policy Optimization (GRPO)GRPO (Shao et al., 2024) bypasses the need for the value model by computing the relative advantage of each response within a group of responses to the same query. Specifcally, GRPO optimizes the following objective:i\n\n$$\\mathcal{J}_{\\mathrm{GRPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G} \\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\min\\left(w_{i,t}(\\theta)\\widehat{A}_{i,t}, \\mathrm{clip}\\left(w_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right) \\widehat{A}_{i,t}\\right)\\right]$$\n\n(2)\n\nwhere $G$  is the number of generated responses to each query $x$  (i.e., the group size), and the importance ratio $w_{i,t}(\\theta)$  and advantage $\\widehat{A}_{i,t}$ of token $y_{i,t}$ are:\n\n$$w_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})},\\ \\ \\ \\ \\widehat{A}_{i,t}=\\widehat{A}_{i}=\\frac{r(x,y_{i})-\\mathrm{mean}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)}{\\mathrm{std}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)},$$\n\n(3)\n\nrespectively, where all the tokens in $y_{i}$  share the same advantage as $\\widehat{A}_{i}.$\n\n## 3Motivation\n\nThe growth in model size, sparsity $\\rm(e.g.,$  in Mixture-of-Experts models), and response length necessitates a large rollout batch size to maximize hardware utilization during RL. To improve sample effciency, it isi standard practice to partition a large batch of rollout data into multiple mini-batches for gradient updates.This procedure inevitably introduces an off-policy learning setting, where responses y are sampled from an old policy $\\pi_{\\theta_{\\mathrm{old}}}$ rather than the current policy $\\pi_{\\theta}$  being optimized. This also explains the necessity of the clipping mechanism in PPO and GRPO, which prevents overly “off-policy” samples from being involved in gradient estimation.\n\nWhile mechanisms like clipping aim to manage this off-policy discrepancy, we identify a more fundamental issue in GRPO: its objective is ill-posed. This problem becomes particularly acute when training large models on long-response tasks, leading to catastrophic model collapse. The ill-posed nature of the GRPO objective stems from a misapplication of importance sampling weights. The principle of importance sampling is to estimate the expectation of a function $f$ under a target distribution $\\pi_{\\mathrm{tar}}$ by re-weighting samples drawn from a behavior distribution $\\pi_{\\mathrm{beh}} :$\n\n$$\\mathbb{E}_{z\\sim\\pi_{\\mathrm{tar}}}\\left[f(z)\\right]=\\mathbb{E}_{z\\sim\\pi_{\\mathrm{beh}}}\\left[\\frac{\\pi_{\\mathrm{tar}}(z)}{\\pi_{\\mathrm{beh}}(z)} f(z) \\right].$$\n\n(4)\n\nCrucially, this relies on averaging over multiple samples $\\text{( N\\gg1 )}$  from the behavior distribution $\\pi_{\\mathrm{beh}}$  for the importance weight $\\tfrac{\\pi_{\\mathrm{tar}}(z)}{\\pi_{\\mathrm{beh}}(z)}$  to effectively correct for the distributional mismatch.\n\nIn contrast, GRPO applies the importance weight $\\begin{array}{l}\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\rm old}}(y_{i,t}|x,y_{i,<t})}\\end{array}$ at each token position $t.$  Since this weight is based on a single sample $y_{i,t}$ from each next-token distribution $\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x,y_{i,<t}),$  it fails to perform the intended distribution-correction role. Instead, it introduces high-variance noise into the traininggradients, which accumulates over long sequences and is exacerbated by the clipping mechanism. We have empirically observed that this can lead to model collapse that is often irreversible. Once the collapse occurs, resuming training is unavailing, even when reverting to a previous checkpoint and meticulously tuning hyperparameters (e.g., the clipping ranges), extending generation length, or switching the RL queries.\n\nThe above observation suggests a fundamental issue in GRPO’s design. The failure of the token-level importance weight points to a core principle: the unit of optimization objective should match the unit of reward. Since the reward is granted to the entire sequence, applying off-policy correction at the token level appears problematic. This motivates us to forego the token-level objective and explore utilizing importance weights and performing optimization directly at the sequence level.\n\n## 4Algorithm\n\n## 4.1GSPO: Group Sequence Policy Optimization\n\nWhile the token-level importance weight $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$ is problematic in GRPO, we observe that in the context of language generation, the sequence-level importance weigh $\\tfrac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y|x)}$ has a clear theoretical meaning: it refects how far the responsel $y$  sampled from $\\pi_{\\theta_{\\rm old}}(\\cdot|x)$  deviates from $\\pi_{\\theta}(\\cdot|x),$  which naturally aligns with the sequence-level reward and can also serve as a meaningful indicator of the clipping mechanism.\n\nBased on this straightforward observation, we propose the Group Sequence Policy Optimization (GSPO)algorithm. GSPO employs the following sequence-level optimization objective:\n\n$$\\mathcal{J}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G} \\min\\left(s_{i}(\\theta)\\widehat{A}_{i}, \\mathrm{clip}\\left(s_{i}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{i}\\right)\\right],$$\n\n(5)\n\nwhere we adopt the group-based advantage estimation:\n\n$$\\widehat{A}_{i}=\\frac{r(x,y_{i})-\\operatorname{mean}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)}{\\operatorname{std}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)},$$\n\n(6)\n\nand defne the importance ratioi $s_{i}(\\theta)$  based on sequence likelihood (Zheng et al., 2023)\n\n$$s_{i}(\\theta)=\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}( y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}=\\exp\\left(\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|} \\log\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i, t}|x,y_{i,<t})}\\right).$$\n\n(7)\n\nTherefore, GSPO applies clipping to entire responses instead of individual tokens to exclude the overly“off-policy” samples from gradient estimation, which matches both the sequence-level rewarding and optimization. Note that we adopt length normalization in $s_{i}(\\theta)$  to reduce the variance and to control $s_{i}(\\theta)$  within a unifed numerical range. Otherwise, the likelihood changes of a few tokens can result ini dramatic fuctuations of the sequence-level importance ratio, and the importance ratios of responses withl different lengths will require varying clipping ranges. We also note that the clipping ranges in GSPO and in previous algorithms (e.g., GRPO) typically differ in order of magnitude due to the distinct defnitionsi of importance ratios.\n\n## 4.2Gradient Analysis\n\nWe can derive the gradient of the GSPO objective as follows (clipping is omitted for brevity):\n\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GSPO}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x )}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta)\\widehat{A}_{i}\\right]$$\n\n(8)\n\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta) \\widehat{A}_{i}\\cdot\\nabla_{\\theta}\\log s_{i}(\\theta)\\right]$$\n\n(9)\n\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}\\widehat{A}_{i}\\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$\n\n(10)\n\n\n\nFor comparison, the gradient of the GRPO objective is as follows (note that $\\widehat{A}_{i,t}=\\widehat{A}_{i}\\text{):}$\n\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GRPO}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}w_{i,t} (\\theta)\\widehat{A}_{i,t}\\right]$$\n\n(11)\n\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\widehat{A}_{i} \\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$\n\n(12)\n\nTherefore, the fundamental distinction between GSPO and GRPO lies in how they weight the gradients of the log likelihoods of tokens. In GRPO, the tokens are weighted according to their respective “importance weight” $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$  However, these unequal weights, which can vary among $(0,1+\\varepsilon]$  (for $\\widehat{A}_{i}>0\\text{)}$  or $[1-\\varepsilon,+\\infty)$  (for $\\widehat{A}_{i}<0\\text{),}$  are not negligible, and their impact can accumulate and lead to unpredictable consequences as training progresses. In contrast, GSPO weights all the tokens in a response equally,eliminating this instability factor of GRPO.\n\n## 4.3GSPO-token: A Token-level Objective Variant\n\nIn scenarios like multi-turn RL, we may desire a fner-grained advantage adjustment than the sequencei level. To this end, we introduce a token-level objective variant of GSPO, namely GSPO-token, to allow token-wise advantage customization:\n\n$$\\mathcal{J}_{\\mathrm{GSPO\\text{-}token}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\min\\left(s_{i,t}(\\theta)\\widehat{A}_{i,t}, \\mathrm{clip}\\left(s_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{i,t}\\right)\\right],$$\n\n(13)\n\nwhere\n\n$$s_{i,t}(\\theta)=\\mathrm{sg}\\left[s_{i}(\\theta)\\right]\\cdot\\frac{\\pi_{\\theta}( y_{i,t}|x,y_{i,<t})}{\\mathrm{sg}\\left[\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right]},$$\n\n(14)\n\nand $\\mathbf{sg}[\\cdot]$  denotes only taking the numerical value but stopping the gradient, corresponding to the detach operation in PyTorch. The gradient of GSPO-token can be derived as:\n\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GSPO\\text{-}token}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}s_{i,t}(\\theta)\\widehat{A}_{i,t}\\right]$$\n\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta) \\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\widehat{A}_{i,t}\\frac{\\nabla_{\\theta}\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}\\right]$$\n\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}\\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\widehat{A}_{i,t}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$\n\nNote that the term $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\mathrm{sg}\\left[\\pi_{\\theta}(y_{i,t} |x,y_{i,<t})\\right]}$ has a numerical value of 1, so $s_{i,t}(\\theta)$  is numerically equal to $s_{i}(\\theta).$ Comparing Equation (5) and (13), and Equation (10) and (17), GSPO-token and GSPO are numerically identical in the optimization objective, clipping condition, and theoretical gradient when we set the advantages of all the tokens in the responseto the same value (i.e., $\\widehat{A}_{i,t}\\stackrel{{\\bar{\\}}}{{=}}\\widehat{A}_{i}\\widehat{)},$  while GSPO-token $y_{i}$ enjoys the higher fexibility of adjusting the advantages per token.l\n\n## 5Experiments and Discussion\n\n## 5.1Empirical Results\n\nWe experiment with a cold-start model fne-tuned from Qwen3-30B-A3B-Base, and report the trainingi reward curves as well as the model performance curves on the AIME’24 (average Pass@1 over 32 samplings), LiveCodeBench (202410-202502, average Pass@1 over 8 samplings), and CodeForces (Elo Rating) benchmarks. During the RL training, each batch of rollout data is partitioned into four minibatches for gradient updates. In GSPO, we set the left and right clipping ranges in Equation (5) to 3e-4 and 4e-4, respectively. We compare against GRPO as the baseline and set the left and right clipping ranges in Equation (2) to 0.2 and 0.27, respectively, which we have carefully tuned to ensure a fair comparison.Note that GRPO necessitates the Routing Replay training strategy for the normal convergence of MoE RL, which we will additionally discuss in § 5.3, while GSPO has obviated the need for this strategy.\n\nFigure 1 shows that the training with GSPO proceeds stably throughout. We observe that GSPO can deliver continuous performance improvement through increasing the training compute, regularly updating the query set, and extending the generation length. Moreover, GSPO also demonstrates superior training effciency over GRPO, achieving better training accuracy and benchmark performancei under the same training compute and consumed queries. Finally, we have successfully applied GSPO to the RL training of the latest Qwen3 models, strongly proving the effcacy of GSPO in unleashing thei power of RL scaling for large language models.\n\n![00560e354e92a5598c2f85cabb9b86ad](imgs/00560e354e92a5598c2f85cabb9b86ad.jpg)\n\nFigure 1: Training curves of a cold-start model fne-tuned from Qwen3-30B-A3B-Base. GSPO possessesi remarkably higher training effciency than GRPO.i\n\n## 5.2Curious Observation on Clipping Fractions\n\nA key distinction of GSPO compared to GRPO is its practice of clipping entire responses rather than individual tokens. Particularly, as shown in Figure 2, we observe a difference of two orders of magnitude in the fractions of clipped tokens between GSPO and GRPO (while adjusting the clipping ranges does not alter the disparity in magnitude). However, despite clipping signifcantly more tokens and consequentlyi using fewer for training (or gradient estimation), GSPO still achieves higher training effciency thani GRPO. This counter-intuitive fnding — that clipping a much larger fraction of tokens leads to superiori training effciency — further indicates that GRPO’s token-level gradient estimates are inherently noisyi and ineffcient for sample exploitation. In contrast, GSPO’s sequence-level approach provides a morei reliable and effective learning signal.\n\n![aca9a28f05c10a38c729f15bef673a0e](imgs/aca9a28f05c10a38c729f15bef673a0e.jpg)\n\nFigure 2: Average fractions of clipped tokens over the RL training of GSPO and GRPO.\n\n\n\n## 5.3Beneft of GSPO for MoE Trainingi\n\nBackgroundCompared to the RL training of dense models, the sparse activation nature of MoE models introduces unique stability challenges. In particular, we found that when adopting the GRPO algorithm,the expert-activation volatility of MoE models can prevent RL training from converging properly. To be specifc, after one or more gradient updates, the experts activated for the same response can changei signifcantly. For example, with the 48-layer Qwen3-30B-A3B-Base model, after each RL gradient updatei and for the same rollout sample, there are roughly $10\\%$ of the experts activated under the new policy $\\pi_{\\theta}$ that are different from those under the old policy $\\pi_{\\theta_{\\rm old}}.$  This phenomenon, which becomes more  prominent in deeper MoE models, makes the token-level importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$ fuctuate drastically and further invalidates them, as discussed in § 3 and 4.2, consequently hindering thel normal convergence of RL training.\n\nOur Previous ApproachTo tackle this challenge, we previously employed the Routing Replay training strategy. Specifcally, we cache the activated experts ini $\\pi_{\\theta_{\\mathrm{old}}}$ and “replay” these routing modes in $\\pi_{\\theta}$  when computing the importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$ In this way, for each token $y_{i,t},\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})$ and $\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})$  share the same activateθdol dneit,twori,kt, so that we can restore the stability of the tokenlevel importance ratios and ensure optimization of the consistent activated network across gradient updates. Figure 3 demonstrates that Routing Replay serves as an essential technique in the normal convergence of the GRPO training of MoE models.\n\n![7445fe26b3bb73c78a0596cc866330ad](imgs/7445fe26b3bb73c78a0596cc866330ad.jpg)\n\nFigure 3: The Routing Replay strategy plays a critical role in the normal convergence of the GRPO training of MoE models.\n\nBeneft of GSPOAlthough Routing Replay enables the GRPO training of MoE models to convergei properly, its practice of reusing routing modes incurs additional memory and communication overhead and can also limit the actual capacity of the MoE model. In contrast, as shown in Figure 1, GSPO eliminates the dependency on Routing Replay and is fully capable of computing the importance ratios $s_{i}(\\theta)$  conventionally, converging normally, and optimizing stably. The key insight is that GSPO focuses only on the sequence likelihood (i.e., $\\pi_{\\theta}(\\widetilde{y_{i}}|x))$  and is not sensitive to the individual token likelihood (i.e., $\\pi_{\\theta}\\big(^{\\prime}y_{i,t}|x,y_{i,<t}\\big)\\widehat{)}.$  Since the MoE model always maintains its language modeling capability, the sequence likelihood will not fuctuate drastically. In summary, GSPO fundamentally resolves the expert-activationl volatility issue in MoE models, obviating the need for complex workarounds like Routing Replay. This not only simplifes and stabilizes the training process but also allows the model to leverage its fulli capacity without artifcial constraints.i\n\n## 5.4Beneft of GSPO for RL Infrastructurei\n\nGiven the precision discrepancies between training engines (e.g., Megatron) and inference engines (e.g.,SGLang and vLLM), in practice, we typically use the training engine to recompute the likelihoods of sampled responses under the old policy $\\pi_{\\theta_{\\mathrm{old}}}.$  However, GSPO uses only sequence-level, rather than token-level, likelihoods for optimization, and intuitively, the former is much more tolerant of precision discrepancies. Hence, GSPO makes it possible to directly use the likelihoods returned by the inference engine for optimization, thereby avoiding the need for recomputation with the training engine. This can be especially benefcial in scenarios like partial rollout and multi-turn RL and in the training-inferencei disaggregated frameworks.\n\n\n\n## 6Conclusion\n\nWe propose Group Sequence Policy Optimization (GSPO), a new reinforcement learning algorithm for training large language models. Following the basic principle of importance sampling, GSPO defnes importance ratios based on sequence likelihood and performs sequence-level clipping, rewarding,i and optimization. GSPO demonstrates notably superior training stability, effciency, and performancei compared to GRPO and exhibits particular effcacy for the large-scale RL training of MoE models, layingi the foundation for the exceptional improvements in the latest Qwen3 models. With GSPO as a scalable algorithmic cornerstone, we will continue to scale RL and look forward to the resulting fundamental advances in intelligence.\n\n## References\n\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n\nMiniMax. Minimax-m1: Scaling test-time compute effciently with lightning attention. arXiv preprinti arXiv:2506.13585, 2025.\n\nOpenAI.LearningtoreasonwithLLMs,2024.URLhttps://openai.com/index/learning-to-reason-with-llms/.\n\nTeam Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a.\n\nTeam Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/blog/qwq-32b/.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n\nChujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of the Association for Computational Linguistics: ACL 2023,2023. URL https://aclanthology.org/2023.findings-acl.65/.\n\n","msg":"","ocr_all":false,"page_count":7,"pages":[{"abandon_blocks":[{"bbox":{"x0":136,"x1":1056,"y0":47,"y1":91},"conf":0.8531,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":954,"x1":1051,"y0":65,"y1":89},"font_size":0.0,"text":"2025-07-29"},{"bbox":{"x0":138,"x1":248,"y0":37,"y1":89},"font_size":0.0,"text":""}],"source":"layout det","text":""},{"bbox":{"x0":16,"x1":78,"y0":491,"y1":1190},"conf":0.3507,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":32,"x1":69,"y0":838,"y1":1184},"font_size":0.0,"text":"7 2v17081.a052:viXr"},{"bbox":{"x0":35,"x1":67,"y0":702,"y1":851},"font_size":0.0,"text":" [cs.LG]"},{"bbox":{"x0":44,"x1":57,"y0":700,"y1":709},"font_size":0.0,"text":""},{"bbox":{"x0":30,"x1":67,"y0":495,"y1":702},"font_size":0.0,"text":"  28 Jul 2025"}],"source":"layout det","text":""},{"bbox":{"x0":140,"x1":1053,"y0":1488,"y1":1612},"conf":0.2233,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":167,"x1":365,"y0":1491,"y1":1514},"font_size":0.0,"text":"∗Corresponding authors."},{"bbox":{"x0":589,"x1":599,"y0":1587,"y1":1607},"font_size":9.0,"text":"1"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":341,"x1":848,"y0":133,"y1":176},"conf":0.8545,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":351,"x1":840,"y0":137,"y1":167},"font_size":-6.381e+26,"text":"Group Sequence Policy Optimization"}],"source":"layout det","text":"Group Sequence Policy Optimization"},{"bbox":{"x0":222,"x1":968,"y0":210,"y1":286},"conf":0.8195,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":246,"x1":948,"y0":214,"y1":239},"font_size":-6.381e+26,"text":"Chujie Zheng∗Shixuan LiuMingze LiXiong-Hui ChenBowen Yu∗"},{"bbox":{"x0":229,"x1":961,"y0":237,"y1":261},"font_size":-6.381e+26,"text":"Chang GaoKai DangYuqiong LiuRui MenAn YangJingren Zhou"},{"bbox":{"x0":535,"x1":654,"y0":261,"y1":281},"font_size":-6.381e+26,"text":"Junyang Lin"}],"source":"layout det","text":"Chujie Zheng∗Shixuan LiuMingze LiXiong-Hui ChenBowen Yu∗Chang GaoKai DangYuqiong LiuRui MenAn YangJingren Zhou Junyang Lin"},{"bbox":{"x0":480,"x1":713,"y0":288,"y1":316},"conf":0.8345,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":484,"x1":707,"y0":289,"y1":312},"font_size":-6.381e+26,"text":"Qwen Team, Alibaba Inc."}],"source":"layout det","text":"Qwen Team, Alibaba Inc."},{"bbox":{"x0":542,"x1":649,"y0":373,"y1":405},"conf":0.8656,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":548,"x1":643,"y0":375,"y1":400},"font_size":-6.381e+26,"text":"Abstract"}],"source":"layout det","text":"Abstract"},{"bbox":{"x0":204,"x1":988,"y0":422,"y1":632},"conf":0.9738,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":213,"x1":980,"y0":428,"y1":453},"font_size":-6.381e+26,"text":"This paper introduces Group Sequence Policy Optimization (GSPO), our stable, effcient,i"},{"bbox":{"x0":211,"x1":980,"y0":451,"y1":474},"font_size":-6.381e+26,"text":"and performant reinforcement learning algorithm for training large language models."},{"bbox":{"x0":211,"x1":978,"y0":472,"y1":496},"font_size":-6.381e+26,"text":"Unlike previous algorithms that adopt token-level importance ratios, GSPO defnes thei"},{"bbox":{"x0":209,"x1":978,"y0":491,"y1":519},"font_size":-6.381e+26,"text":"importance ratio based on sequence likelihood and performs sequence-level clipping,"},{"bbox":{"x0":207,"x1":980,"y0":512,"y1":544},"font_size":-6.381e+26,"text":"rewarding, and optimization. We demonstrate that GSPO achieves superior training"},{"bbox":{"x0":211,"x1":978,"y0":539,"y1":561},"font_size":-6.381e+26,"text":"effciency and performance compared to the GRPO algorithm, notably stabilizes Mixture-i"},{"bbox":{"x0":211,"x1":977,"y0":560,"y1":582},"font_size":-6.381e+26,"text":"of-Experts (MoE) RL training, and has the potential for simplifying the design of RL"},{"bbox":{"x0":211,"x1":978,"y0":582,"y1":605},"font_size":-6.381e+26,"text":"infrastructure. These merits of GSPO have contributed to the remarkable improvements"},{"bbox":{"x0":211,"x1":461,"y0":603,"y1":626},"font_size":-6.381e+26,"text":"in the latest Qwen3 models."}],"source":"layout det","text":"This paper introduces Group Sequence Policy Optimization (GSPO), our stable, effcient,i and performant reinforcement learning algorithm for training large language models.Unlike previous algorithms that adopt token-level importance ratios, GSPO defnes thei importance ratio based on sequence likelihood and performs sequence-level clipping,rewarding, and optimization. We demonstrate that GSPO achieves superior training effciency and performance compared to the GRPO algorithm, notably stabilizes Mixture-i of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models."},{"bbox":{"x0":133,"x1":323,"y0":666,"y1":700},"conf":0.8946,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":140,"x1":317,"y0":672,"y1":695},"font_size":-6.381e+26,"text":"1Introduction"}],"source":"layout det","text":"1Introduction"},{"bbox":{"x0":133,"x1":1060,"y0":718,"y1":818},"conf":0.3003,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":723,"y1":746},"font_size":-6.381e+26,"text":"Reinforcement learning (RL) has emerged as a pivotal paradigm for scaling language models (OpenAI,"},{"bbox":{"x0":140,"x1":1051,"y0":746,"y1":768},"font_size":-6.381e+26,"text":"2024; DeepSeek-AI, 2025; Qwen, 2025b;a). Through large-scale RL, language models develop the capa-"},{"bbox":{"x0":140,"x1":1049,"y0":767,"y1":791},"font_size":-6.381e+26,"text":"bility to tackle sophisticated problems, such as competition-level mathematics and programming, by"},{"bbox":{"x0":138,"x1":603,"y0":788,"y1":814},"font_size":-6.381e+26,"text":"undertaking deeper and longer reasoning processes."}],"source":"layout det","text":"Reinforcement learning (RL) has emerged as a pivotal paradigm for scaling language models (OpenAI,2024; DeepSeek-AI, 2025; Qwen, 2025b;a). Through large-scale RL, language models develop the capability to tackle sophisticated problems, such as competition-level mathematics and programming, by undertaking deeper and longer reasoning processes."},{"bbox":{"x0":131,"x1":1058,"y0":821,"y1":939},"conf":0.9649,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1053,"y0":817,"y1":851},"font_size":-6.381e+26,"text":"To successfully scale RL with greater computational investment, the foremost prerequisite is maintaining"},{"bbox":{"x0":140,"x1":1047,"y0":846,"y1":869},"font_size":-6.381e+26,"text":"stable and robust training dynamics. However, current state-of-the-art RL algorithms, exemplifed byi"},{"bbox":{"x0":140,"x1":1049,"y0":867,"y1":891},"font_size":-6.381e+26,"text":"GRPO (Shao et al., 2024), exhibit severe stability issues when training gigantic language models, often"},{"bbox":{"x0":140,"x1":1049,"y0":889,"y1":912},"font_size":-6.381e+26,"text":"resulting in catastrophic and irreversible model collapse (Qwen, 2025a; MiniMax, 2025). This instability"},{"bbox":{"x0":138,"x1":1030,"y0":909,"y1":937},"font_size":-6.381e+26,"text":"hinders efforts to push the capability boundaries of language models through continued RL training."}],"source":"layout det","text":"To successfully scale RL with greater computational investment, the foremost prerequisite is maintaining stable and robust training dynamics. However, current state-of-the-art RL algorithms, exemplifed byi GRPO (Shao et al., 2024), exhibit severe stability issues when training gigantic language models, often resulting in catastrophic and irreversible model collapse (Qwen, 2025a; MiniMax, 2025). This instability hinders efforts to push the capability boundaries of language models through continued RL training."},{"bbox":{"x0":131,"x1":1057,"y0":942,"y1":1040},"conf":0.9599,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":945,"y1":970},"font_size":-6.381e+26,"text":"In this paper, we identify that the instability of GRPO stems from the fundamental misapplication and"},{"bbox":{"x0":140,"x1":1049,"y0":968,"y1":991},"font_size":-6.381e+26,"text":"invalidation of importance sampling weights in its algorithmic design. This introduces high-variance"},{"bbox":{"x0":140,"x1":1047,"y0":988,"y1":1013},"font_size":-6.381e+26,"text":"training noise that progressively accumulates with increased response length and is further amplifed byi"},{"bbox":{"x0":140,"x1":714,"y0":1012,"y1":1037},"font_size":-6.381e+26,"text":"the clipping mechanism, ultimately precipitating model collapse."}],"source":"layout det","text":"In this paper, we identify that the instability of GRPO stems from the fundamental misapplication and invalidation of importance sampling weights in its algorithmic design. This introduces high-variance training noise that progressively accumulates with increased response length and is further amplifed byi the clipping mechanism, ultimately precipitating model collapse."},{"bbox":{"x0":133,"x1":1058,"y0":1043,"y1":1185},"conf":0.9735,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1045,"y1":1070},"font_size":-6.381e+26,"text":"To address these core limitations, we propose Group Sequence Policy Optimization (GSPO), a new"},{"bbox":{"x0":140,"x1":1049,"y0":1068,"y1":1093},"font_size":-6.381e+26,"text":"RL algorithm for training large language models. The key innovation of GSPO lies in its theoretically"},{"bbox":{"x0":136,"x1":1049,"y0":1086,"y1":1117},"font_size":-6.381e+26,"text":"grounded defnition of importance ratio based on sequence likelihood (Zheng et al., 2023), aligning withi"},{"bbox":{"x0":140,"x1":1049,"y0":1112,"y1":1137},"font_size":-6.381e+26,"text":"the basic principle of importance sampling. Additionally, GSPO computes the normalized rewards as the"},{"bbox":{"x0":138,"x1":1049,"y0":1135,"y1":1160},"font_size":-6.381e+26,"text":"advantages of multiple responses to a query, ensuring the alignment between sequence-level rewarding"},{"bbox":{"x0":138,"x1":303,"y0":1158,"y1":1181},"font_size":-6.381e+26,"text":"and optimization."}],"source":"layout det","text":"To address these core limitations, we propose Group Sequence Policy Optimization (GSPO), a new RL algorithm for training large language models. The key innovation of GSPO lies in its theoretically grounded defnition of importance ratio based on sequence likelihood (Zheng et al., 2023), aligning withi the basic principle of importance sampling. Additionally, GSPO computes the normalized rewards as the advantages of multiple responses to a query, ensuring the alignment between sequence-level rewarding and optimization."},{"bbox":{"x0":132,"x1":1058,"y0":1189,"y1":1354},"conf":0.9722,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1186,"y1":1219},"font_size":-6.381e+26,"text":"Our empirical evaluation demonstrates the signifcant superiority of GSPO over GRPO in trainingi"},{"bbox":{"x0":140,"x1":1049,"y0":1214,"y1":1237},"font_size":-6.381e+26,"text":"stability, effciency, and performance. Critically, GSPO has inherently resolved the stability challenges ini"},{"bbox":{"x0":140,"x1":1049,"y0":1237,"y1":1259},"font_size":-6.381e+26,"text":"the RL training of large Mixture-of-Experts (MoE) models, eliminating the need for complex stabilization"},{"bbox":{"x0":140,"x1":1049,"y0":1258,"y1":1281},"font_size":-6.381e+26,"text":"strategies, and shows the potential for simplifying RL infrastructure. These merits of GSPO ultimately"},{"bbox":{"x0":140,"x1":1049,"y0":1281,"y1":1303},"font_size":-6.381e+26,"text":"contributed to the exceptional performance improvements in the latest Qwen3 models. We envision"},{"bbox":{"x0":140,"x1":1051,"y0":1302,"y1":1324},"font_size":-6.381e+26,"text":"GSPO as a robust and scalable algorithmic foundation that will enable the continued advancement of"},{"bbox":{"x0":140,"x1":546,"y0":1323,"y1":1347},"font_size":-6.381e+26,"text":"large-scale RL training with language models."}],"source":"layout det","text":"Our empirical evaluation demonstrates the signifcant superiority of GSPO over GRPO in trainingi stability, effciency, and performance. Critically, GSPO has inherently resolved the stability challenges ini the RL training of large Mixture-of-Experts (MoE) models, eliminating the need for complex stabilization strategies, and shows the potential for simplifying RL infrastructure. These merits of GSPO ultimately contributed to the exceptional performance improvements in the latest Qwen3 models. We envision GSPO as a robust and scalable algorithmic foundation that will enable the continued advancement of large-scale RL training with language models."},{"bbox":{"x0":132,"x1":333,"y0":1375,"y1":1410},"conf":0.891,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":326,"y0":1379,"y1":1405},"font_size":-6.381e+26,"text":"2Preliminaries"}],"source":"layout det","text":"2Preliminaries"},{"bbox":{"x0":132,"x1":1059,"y0":1427,"y1":1484},"conf":0.8644,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":1430,"y1":1456},"font_size":-6.381e+26,"text":"NotationIn this paper, an autoregressive language model parameterized by $\\theta$ is defned as a policy πθ.i"},{"bbox":{"x0":136,"x1":1049,"y0":1451,"y1":1478},"font_size":-6.381e+26,"text":"We use x to denote a query and $D$  as the query set. Given a response $y$  to a query $x,$  its likelihood under"}],"source":"layout det","text":"NotationIn this paper, an autoregressive language model parameterized by $\\theta$ is defned as a policy πθ.i We use x to denote a query and $D$  as the query set. Given a response $y$  to a query $x,$  its likelihood under"}],"formula_dets":[{"bbox":{"x0":420,"x1":437,"y0":1458,"y1":1473},"conf":0.776,"label":"print_embedding","label_id":0},{"bbox":{"x0":821,"x1":832,"y0":1436,"y1":1453},"conf":0.7538,"label":"print_embedding","label_id":0},{"bbox":{"x0":744,"x1":756,"y0":1462,"y1":1478},"conf":0.652,"label":"print_embedding","label_id":0},{"bbox":{"x0":853,"x1":871,"y0":1461,"y1":1476},"conf":0.6073,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":204,"x1":988,"y0":422,"y1":632},"conf":0.9738,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":1043,"y1":1185},"conf":0.9735,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":1189,"y1":1354},"conf":0.9722,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":821,"y1":939},"conf":0.9649,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1057,"y0":942,"y1":1040},"conf":0.9599,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":323,"y0":666,"y1":700},"conf":0.8946,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":333,"y0":1375,"y1":1410},"conf":0.891,"label":"Title","label_id":0},{"bbox":{"x0":542,"x1":649,"y0":373,"y1":405},"conf":0.8656,"label":"Title","label_id":0},{"bbox":{"x0":132,"x1":1059,"y0":1427,"y1":1484},"conf":0.8644,"label":"Text","label_id":1},{"bbox":{"x0":341,"x1":848,"y0":133,"y1":176},"conf":0.8545,"label":"Title","label_id":0},{"bbox":{"x0":136,"x1":1056,"y0":47,"y1":91},"conf":0.8531,"label":"Abandon","label_id":2},{"bbox":{"x0":132,"x1":1059,"y0":718,"y1":818},"conf":0.837,"label":"Text","label_id":1},{"bbox":{"x0":480,"x1":713,"y0":288,"y1":316},"conf":0.8345,"label":"Text","label_id":1},{"bbox":{"x0":222,"x1":968,"y0":210,"y1":286},"conf":0.8195,"label":"Text","label_id":1},{"bbox":{"x0":25,"x1":78,"y0":491,"y1":1193},"conf":0.5803,"label":"Abandon","label_id":2},{"bbox":{"x0":584,"x1":603,"y0":1583,"y1":1606},"conf":0.4647,"label":"Abandon","label_id":2},{"bbox":{"x0":16,"x1":78,"y0":491,"y1":1190},"conf":0.3507,"label":"Abandon","label_id":2},{"bbox":{"x0":158,"x1":368,"y0":1490,"y1":1520},"conf":0.3377,"label":"Abandon","label_id":2},{"bbox":{"x0":133,"x1":1060,"y0":718,"y1":818},"conf":0.3003,"label":"Text","label_id":1},{"bbox":{"x0":581,"x1":606,"y0":1581,"y1":1608},"conf":0.2308,"label":"Abandon","label_id":2},{"bbox":{"x0":140,"x1":1053,"y0":1488,"y1":1612},"conf":0.2233,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[167,1491],[365,1491],[365,1514],[167,1514]],"score":0.8133},{"poly":[[137,1451],[1049,1452],[1049,1477],[136,1475]],"score":0.7249},{"poly":[[136,1430],[1049,1431],[1049,1456],[136,1454]],"score":0.752},{"poly":[[137,1379],[326,1381],[326,1405],[136,1403]],"score":0.7973},{"poly":[[140,1323],[546,1323],[546,1347],[140,1347]],"score":0.7385},{"poly":[[140,1302],[1051,1302],[1051,1324],[140,1324]],"score":0.8219},{"poly":[[140,1281],[1049,1281],[1049,1303],[140,1303]],"score":0.8183},{"poly":[[140,1258],[1049,1258],[1049,1281],[140,1281]],"score":0.779},{"poly":[[140,1237],[1049,1237],[1049,1259],[140,1259]],"score":0.8048},{"poly":[[140,1214],[1049,1214],[1049,1237],[140,1237]],"score":0.7803},{"poly":[[137,1186],[1051,1189],[1051,1219],[136,1216]],"score":0.6404},{"poly":[[138,1158],[303,1158],[303,1181],[138,1181]],"score":0.7937},{"poly":[[138,1135],[1049,1135],[1049,1160],[138,1160]],"score":0.7158},{"poly":[[140,1112],[1049,1112],[1049,1137],[140,1137]],"score":0.7276},{"poly":[[136,1088],[1049,1086],[1049,1116],[137,1117]],"score":0.6414},{"poly":[[140,1068],[1049,1068],[1049,1093],[140,1093]],"score":0.6801},{"poly":[[138,1045],[1049,1045],[1049,1070],[138,1070]],"score":0.7136},{"poly":[[140,1012],[714,1012],[714,1037],[140,1037]],"score":0.6719},{"poly":[[140,988],[1047,988],[1047,1012],[140,1012]],"score":0.684},{"poly":[[140,968],[1049,968],[1049,991],[140,991]],"score":0.7951},{"poly":[[140,945],[1049,945],[1049,970],[140,970]],"score":0.6968},{"poly":[[138,909],[1030,912],[1030,937],[138,933]],"score":0.7123},{"poly":[[140,889],[1049,889],[1049,912],[140,912]],"score":0.8327},{"poly":[[140,867],[1049,867],[1049,891],[140,891]],"score":0.66},{"poly":[[140,846],[1047,846],[1047,868],[140,868]],"score":0.8195},{"poly":[[32,838],[69,838],[69,1184],[32,1184]],"score":0.7338},{"poly":[[137,817],[1053,821],[1053,851],[136,847]],"score":0.6255},{"poly":[[138,788],[603,791],[602,814],[138,810]],"score":0.7715},{"poly":[[140,767],[1049,767],[1049,791],[140,791]],"score":0.7181},{"poly":[[140,746],[1051,746],[1051,768],[140,768]],"score":0.8148},{"poly":[[140,723],[1051,723],[1051,746],[140,746]],"score":0.8473},{"poly":[[35,702],[67,702],[67,851],[35,851]],"score":0.7616},{"poly":[[44,700],[57,700],[57,709],[44,709]],"score":0.8253},{"poly":[[140,672],[317,672],[317,695],[140,695]],"score":0.9244},{"poly":[[211,603],[461,603],[461,626],[211,626]],"score":0.8072},{"poly":[[211,582],[978,582],[978,605],[211,605]],"score":0.8271},{"poly":[[211,560],[977,560],[977,582],[211,582]],"score":0.7984},{"poly":[[211,539],[978,539],[978,561],[211,561]],"score":0.8129},{"poly":[[207,512],[980,514],[980,544],[207,542]],"score":0.6319},{"poly":[[30,495],[67,495],[67,702],[30,702]],"score":0.8317},{"poly":[[209,491],[978,495],[978,519],[209,516]],"score":0.7393},{"poly":[[211,472],[978,472],[978,495],[211,495]],"score":0.8072},{"poly":[[211,451],[980,451],[980,474],[211,474]],"score":0.7969},{"poly":[[213,428],[980,428],[980,453],[213,453]],"score":0.7164},{"poly":[[548,375],[643,375],[643,400],[548,400]],"score":0.8569},{"poly":[[484,289],[707,289],[707,312],[484,312]],"score":0.9639},{"poly":[[535,261],[654,261],[654,281],[535,281]],"score":0.9651},{"poly":[[229,237],[961,237],[961,261],[229,261]],"score":0.7374},{"poly":[[822,216],[948,216],[948,235],[822,235]],"score":0.8076},{"poly":[[537,214],[826,214],[826,239],[537,239]],"score":0.7697},{"poly":[[246,214],[544,214],[544,239],[246,239]],"score":0.7657},{"poly":[[351,137],[840,137],[840,167],[351,167]],"score":0.8256},{"poly":[[954,65],[1051,65],[1051,89],[954,89]],"score":0.8035},{"poly":[[138,37],[248,37],[248,89],[138,89]],"score":0.6556}],"page_no":0,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":607,"y0":1582,"y1":1607},"conf":0.4606,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1584,"y1":1603},"font_size":0.0,"text":"2"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":1055,"y0":136,"y1":200},"conf":0.938,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1053,"y0":139,"y1":174},"font_size":0.0,"text":"the policy $\\pi_{\\theta}$ is denoted as $\\pi_{\\theta}(y|x)=\\begin{matrix}\\prod_{t=1}^{|y|}\\pi_{\\theta}(y_{t}|x,y_{<t}) \\end{matrix}$  where $|y|$  denotes the number of tokens in y. A"},{"bbox":{"x0":138,"x1":942,"y0":170,"y1":196},"font_size":0.0,"text":"query-response pair $(x,y)$  can be scored by a veriferi $r,$  resulting in a reward $r(x,y)\\in[0,1].$"}],"source":"layout det","text":"the policy $\\pi_{\\theta}$ is denoted as $\\pi_{\\theta}(y|x)=\\begin{matrix}\\prod_{t=1}^{|y|}\\pi_{\\theta}(y_{t}|x,y_{<t}) \\end{matrix}$  where $|y|$  denotes the number of tokens in y. A query-response pair $(x,y)$  can be scored by a veriferi $r,$  resulting in a reward $r(x,y)\\in[0,1].$"},{"bbox":{"x0":135,"x1":1057,"y0":215,"y1":317},"conf":0.9592,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1053,"y0":218,"y1":247},"font_size":0.0,"text":"Proximal Policy Optimization (PPO)Using samples generated from the old policy $\\pi_{\\theta_{\\text{old}}},$  PPO (Schul-"},{"bbox":{"x0":136,"x1":1049,"y0":242,"y1":268},"font_size":0.0,"text":"man et al., 2017) constrains the policy update within a proximal region of the old policy through the"},{"bbox":{"x0":140,"x1":1049,"y0":265,"y1":289},"font_size":0.0,"text":"clipping mechanism. Specifcally, PPO employs the following objective for policy optimization (we omiti"},{"bbox":{"x0":138,"x1":888,"y0":286,"y1":314},"font_size":0.0,"text":"the KL regularization term hereinafter for brevity, as it is not the focus of this paper):"}],"source":"layout det","text":"Proximal Policy Optimization (PPO)Using samples generated from the old policy $\\pi_{\\theta_{\\text{old}}},$  PPO (Schulman et al., 2017) constrains the policy update within a proximal region of the old policy through the clipping mechanism. Specifcally, PPO employs the following objective for policy optimization (we omiti the KL regularization term hereinafter for brevity, as it is not the focus of this paper):"},{"bbox":{"x0":240,"x1":952,"y0":323,"y1":391},"conf":0.9302,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\mathrm{PPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, y\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{|y|}\\sum_{t=1}^{|y|}\\min\\left(w_{t}(\\theta)\\widehat{A}_{t}, \\mathrm{clip}\\left(w_{t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{t}\\right)\\right],$$"},{"bbox":{"x0":1022,"x1":1055,"y0":345,"y1":372},"conf":0.8142,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1055,"y0":344,"y1":374},"font_size":0.0,"text":"(1)"}],"source":"layout det","text":"(1)"},{"bbox":{"x0":133,"x1":1055,"y0":398,"y1":469},"conf":0.9495,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1048,"y0":400,"y1":442},"font_size":0.0,"text":"where the importance ratio of the token $y_{t}$  is defined as $w_{t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{t}|x,y_{<t})}{\\pi_{\\theta_{\\mathrm{old}}} (y_{t}|x,y_{<t})},$  the advantage $\\widehat{A}_{t}$  of $y_{t}$  is"},{"bbox":{"x0":957,"x1":967,"y0":419,"y1":437},"font_size":0.0,"text":""},{"bbox":{"x0":140,"x1":868,"y0":440,"y1":465},"font_size":0.0,"text":"estimated by another value model, and $\\epsilon$  is the clipping range of imopldortance ratios."}],"source":"layout det","text":"where the importance ratio of the token $y_{t}$  is defined as $w_{t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{t}|x,y_{<t})}{\\pi_{\\theta_{\\mathrm{old}}} (y_{t}|x,y_{<t})},$  the advantage $\\widehat{A}_{t}$  of $y_{t}$  is estimated by another value model, and $\\epsilon$  is the clipping range of imopldortance ratios."},{"bbox":{"x0":134,"x1":1056,"y0":472,"y1":591},"conf":0.9637,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":475,"y1":498},"font_size":0.0,"text":"The core challenge of PPO in practice lies in its heavy reliance on the value model. Specifcally, thei"},{"bbox":{"x0":138,"x1":1049,"y0":498,"y1":521},"font_size":0.0,"text":"value model usually has a similar size to the policy model, introducing a considerable memory and"},{"bbox":{"x0":140,"x1":1049,"y0":519,"y1":544},"font_size":0.0,"text":"computational burden. Furthermore, the algorithmic effectiveness hinges on the reliability of its value"},{"bbox":{"x0":140,"x1":1049,"y0":542,"y1":567},"font_size":0.0,"text":"estimate. While acquiring a reliable value model is inherently challenging, ensuring its scalability to"},{"bbox":{"x0":138,"x1":822,"y0":563,"y1":589},"font_size":0.0,"text":"longer responses and more complex tasks presents an even greater challenge."}],"source":"layout det","text":"The core challenge of PPO in practice lies in its heavy reliance on the value model. Specifcally, thei value model usually has a similar size to the policy model, introducing a considerable memory and computational burden. Furthermore, the algorithmic effectiveness hinges on the reliability of its value estimate. While acquiring a reliable value model is inherently challenging, ensuring its scalability to longer responses and more complex tasks presents an even greater challenge."},{"bbox":{"x0":133,"x1":1057,"y0":607,"y1":683},"conf":0.9594,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":610,"y1":635},"font_size":0.0,"text":"Group Relative Policy Optimization (GRPO)GRPO (Shao et al., 2024) bypasses the need for the value"},{"bbox":{"x0":138,"x1":1049,"y0":631,"y1":658},"font_size":0.0,"text":"model by computing the relative advantage of each response within a group of responses to the same"},{"bbox":{"x0":138,"x1":672,"y0":654,"y1":681},"font_size":0.0,"text":"query. Specifcally, GRPO optimizes the following objective:i"}],"source":"layout det","text":"Group Relative Policy Optimization (GRPO)GRPO (Shao et al., 2024) bypasses the need for the value model by computing the relative advantage of each response within a group of responses to the same query. Specifcally, GRPO optimizes the following objective:i"},{"bbox":{"x0":157,"x1":1010,"y0":690,"y1":758},"conf":0.8802,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\mathrm{GRPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G} \\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\min\\left(w_{i,t}(\\theta)\\widehat{A}_{i,t}, \\mathrm{clip}\\left(w_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right) \\widehat{A}_{i,t}\\right)\\right]$$"},{"bbox":{"x0":1022,"x1":1054,"y0":712,"y1":739},"conf":0.8205,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1027,"x1":1050,"y0":716,"y1":736},"font_size":0.0,"text":"(2)"}],"source":"layout det","text":"(2)"},{"bbox":{"x0":133,"x1":1052,"y0":764,"y1":821},"conf":0.9464,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":763,"y1":795},"font_size":0.0,"text":"where $G$  is the number of generated responses to each query $x$  (i.e., the group size), and the importance"},{"bbox":{"x0":136,"x1":565,"y0":789,"y1":821},"font_size":0.0,"text":"ratio $w_{i,t}(\\theta)$  and advantage $\\widehat{A}_{i,t}$ of token $y_{i,t}$ are:"}],"source":"layout det","text":"where $G$  is the number of generated responses to each query $x$  (i.e., the group size), and the importance ratio $w_{i,t}(\\theta)$  and advantage $\\widehat{A}_{i,t}$ of token $y_{i,t}$ are:"},{"bbox":{"x0":272,"x1":919,"y0":829,"y1":889},"conf":0.9184,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$w_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})},\\ \\ \\ \\ \\widehat{A}_{i,t}=\\widehat{A}_{i}=\\frac{r(x,y_{i})-\\mathrm{mean}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)}{\\mathrm{std}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)},$$"},{"bbox":{"x0":1023,"x1":1055,"y0":846,"y1":873},"conf":0.8398,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1023,"x1":1055,"y0":844,"y1":874},"font_size":0.0,"text":"(3)"}],"source":"layout det","text":"(3)"},{"bbox":{"x0":134,"x1":768,"y0":898,"y1":931},"conf":0.8851,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":762,"y0":900,"y1":928},"font_size":0.0,"text":"respectively, where all the tokens in $y_{i}$  share the same advantage as $\\widehat{A}_{i}.$"}],"source":"layout det","text":"respectively, where all the tokens in $y_{i}$  share the same advantage as $\\widehat{A}_{i}.$"},{"bbox":{"x0":135,"x1":305,"y0":957,"y1":986},"conf":0.8947,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":301,"y0":958,"y1":984},"font_size":0.0,"text":"3Motivation"}],"source":"layout det","text":"3Motivation"},{"bbox":{"x0":135,"x1":1057,"y0":1008,"y1":1172},"conf":0.9702,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":1009,"y1":1036},"font_size":0.0,"text":"The growth in model size, sparsity $\\rm(e.g.,$  in Mixture-of-Experts models), and response length necessitates"},{"bbox":{"x0":138,"x1":1049,"y0":1033,"y1":1056},"font_size":0.0,"text":"a large rollout batch size to maximize hardware utilization during RL. To improve sample effciency, it isi"},{"bbox":{"x0":138,"x1":1051,"y0":1056,"y1":1079},"font_size":0.0,"text":"standard practice to partition a large batch of rollout data into multiple mini-batches for gradient updates."},{"bbox":{"x0":140,"x1":1049,"y0":1077,"y1":1102},"font_size":0.0,"text":"This procedure inevitably introduces an off-policy learning setting, where responses y are sampled from"},{"bbox":{"x0":138,"x1":1051,"y0":1098,"y1":1128},"font_size":0.0,"text":"an old policy $\\pi_{\\theta_{\\mathrm{old}}}$ rather than the current policy $\\pi_{\\theta}$  being optimized. This also explains the necessity"},{"bbox":{"x0":138,"x1":1049,"y0":1121,"y1":1149},"font_size":0.0,"text":"of the clipping mechanism in PPO and GRPO, which prevents overly “off-policy” samples from being"},{"bbox":{"x0":138,"x1":427,"y0":1145,"y1":1168},"font_size":0.0,"text":"involved in gradient estimation."}],"source":"layout det","text":"The growth in model size, sparsity $\\rm(e.g.,$  in Mixture-of-Experts models), and response length necessitates a large rollout batch size to maximize hardware utilization during RL. To improve sample effciency, it isi standard practice to partition a large batch of rollout data into multiple mini-batches for gradient updates.This procedure inevitably introduces an off-policy learning setting, where responses y are sampled from an old policy $\\pi_{\\theta_{\\mathrm{old}}}$ rather than the current policy $\\pi_{\\theta}$  being optimized. This also explains the necessity of the clipping mechanism in PPO and GRPO, which prevents overly “off-policy” samples from being involved in gradient estimation."},{"bbox":{"x0":134,"x1":1057,"y0":1177,"y1":1318},"conf":0.974,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1181,"y1":1203},"font_size":0.0,"text":"While mechanisms like clipping aim to manage this off-policy discrepancy, we identify a more fundamen-"},{"bbox":{"x0":140,"x1":1049,"y0":1203,"y1":1226},"font_size":0.0,"text":"tal issue in GRPO: its objective is ill-posed. This problem becomes particularly acute when training large"},{"bbox":{"x0":140,"x1":1049,"y0":1224,"y1":1249},"font_size":0.0,"text":"models on long-response tasks, leading to catastrophic model collapse. The ill-posed nature of the GRPO"},{"bbox":{"x0":140,"x1":1049,"y0":1247,"y1":1270},"font_size":0.0,"text":"objective stems from a misapplication of importance sampling weights. The principle of importance"},{"bbox":{"x0":136,"x1":1051,"y0":1263,"y1":1296},"font_size":0.0,"text":"sampling is to estimate the expectation of a function $f$ under a target distribution $\\pi_{\\mathrm{tar}}$ by re-weighting"},{"bbox":{"x0":136,"x1":581,"y0":1289,"y1":1316},"font_size":0.0,"text":"samples drawn from a behavior distribution $\\pi_{\\mathrm{beh}} :$"}],"source":"layout det","text":"While mechanisms like clipping aim to manage this off-policy discrepancy, we identify a more fundamental issue in GRPO: its objective is ill-posed. This problem becomes particularly acute when training large models on long-response tasks, leading to catastrophic model collapse. The ill-posed nature of the GRPO objective stems from a misapplication of importance sampling weights. The principle of importance sampling is to estimate the expectation of a function $f$ under a target distribution $\\pi_{\\mathrm{tar}}$ by re-weighting samples drawn from a behavior distribution $\\pi_{\\mathrm{beh}} :$"},{"bbox":{"x0":415,"x1":773,"y0":1323,"y1":1377},"conf":0.941,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathbb{E}_{z\\sim\\pi_{\\mathrm{tar}}}\\left[f(z)\\right]=\\mathbb{E}_{z\\sim\\pi_{\\mathrm{beh}}}\\left[\\frac{\\pi_{\\mathrm{tar}}(z)}{\\pi_{\\mathrm{beh}}(z)} f(z) \\right].$$"},{"bbox":{"x0":1022,"x1":1055,"y0":1337,"y1":1366},"conf":0.451,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1023,"x1":1055,"y0":1337,"y1":1366},"font_size":0.0,"text":"(4)"}],"source":"layout det","text":"(4)"},{"bbox":{"x0":134,"x1":1057,"y0":1383,"y1":1449},"conf":0.96,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":1386,"y1":1412},"font_size":0.0,"text":"Crucially, this relies on averaging over multiple samples $\\text{( N\\gg1 )}$  from the behavior distribution $\\pi_{\\mathrm{beh}}$  for"},{"bbox":{"x0":141,"x1":870,"y0":1412,"y1":1448},"font_size":0.0,"text":"the importance weight $\\tfrac{\\pi_{\\mathrm{tar}}(z)}{\\pi_{\\mathrm{beh}}(z)}$  to effectively correct for the distributional mismatch."}],"source":"layout det","text":"Crucially, this relies on averaging over multiple samples $\\text{( N\\gg1 )}$  from the behavior distribution $\\pi_{\\mathrm{beh}}$  for the importance weight $\\tfrac{\\pi_{\\mathrm{tar}}(z)}{\\pi_{\\mathrm{beh}}(z)}$  to effectively correct for the distributional mismatch."},{"bbox":{"x0":134,"x1":1057,"y0":1455,"y1":1553},"conf":0.9632,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1458,"y1":1501},"font_size":0.0,"text":"In contrast, GRPO applies the importance weight $\\begin{array}{l}\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\rm old}}(y_{i,t}|x,y_{i,<t})}\\end{array}$ at each token position $t.$  Since this weight"},{"bbox":{"x0":135,"x1":1051,"y0":1495,"y1":1528},"font_size":0.0,"text":"is based on a single sample $y_{i,t}$ from each next-token distribution $\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x,y_{i,<t}),$  it fails to perform"},{"bbox":{"x0":136,"x1":1051,"y0":1519,"y1":1552},"font_size":0.0,"text":"the intended distribution-correction role. Instead, it introduces high-variance noise into the training"}],"source":"layout det","text":"In contrast, GRPO applies the importance weight $\\begin{array}{l}\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\rm old}}(y_{i,t}|x,y_{i,<t})}\\end{array}$ at each token position $t.$  Since this weight is based on a single sample $y_{i,t}$ from each next-token distribution $\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x,y_{i,<t}),$  it fails to perform the intended distribution-correction role. Instead, it introduces high-variance noise into the training"}],"formula_dets":[{"bbox":{"x0":415,"x1":773,"y0":1323,"y1":1377},"conf":0.941,"label":"print_isolated","label_id":1},{"bbox":{"x0":637,"x1":825,"y0":400,"y1":442},"conf":0.9364,"label":"print_embedding","label_id":0},{"bbox":{"x0":240,"x1":952,"y0":323,"y1":391},"conf":0.9302,"label":"print_isolated","label_id":1},{"bbox":{"x0":568,"x1":686,"y0":1458,"y1":1501},"conf":0.9255,"label":"print_embedding","label_id":0},{"bbox":{"x0":272,"x1":919,"y0":829,"y1":889},"conf":0.9184,"label":"print_isolated","label_id":1},{"bbox":{"x0":380,"x1":633,"y0":139,"y1":174},"conf":0.9131,"label":"print_embedding","label_id":0},{"bbox":{"x0":346,"x1":400,"y0":1412,"y1":1448},"conf":0.9118,"label":"print_embedding","label_id":0},{"bbox":{"x0":186,"x1":243,"y0":795,"y1":819},"conf":0.9055,"label":"print_embedding","label_id":0},{"bbox":{"x0":745,"x1":879,"y0":1499,"y1":1526},"conf":0.899,"label":"print_embedding","label_id":0},{"bbox":{"x0":814,"x1":942,"y0":172,"y1":196},"conf":0.8914,"label":"print_embedding","label_id":0},{"bbox":{"x0":384,"x1":415,"y0":793,"y1":818},"conf":0.8901,"label":"print_embedding","label_id":0},{"bbox":{"x0":322,"x1":368,"y0":173,"y1":196},"conf":0.8896,"label":"print_embedding","label_id":0},{"bbox":{"x0":157,"x1":1010,"y0":690,"y1":758},"conf":0.8802,"label":"print_isolated","label_id":1},{"bbox":{"x0":697,"x1":720,"y0":149,"y1":172},"conf":0.8604,"label":"print_embedding","label_id":0},{"bbox":{"x0":867,"x1":901,"y0":1275,"y1":1291},"conf":0.858,"label":"print_embedding","label_id":0},{"bbox":{"x0":264,"x1":304,"y0":1106,"y1":1126},"conf":0.8547,"label":"print_embedding","label_id":0},{"bbox":{"x0":496,"x1":522,"y0":801,"y1":818},"conf":0.8518,"label":"print_embedding","label_id":0},{"bbox":{"x0":893,"x1":938,"y0":226,"y1":246},"conf":0.8466,"label":"print_embedding","label_id":0},{"bbox":{"x0":961,"x1":984,"y0":405,"y1":429},"conf":0.8423,"label":"print_embedding","label_id":0},{"bbox":{"x0":978,"x1":1018,"y0":1393,"y1":1412},"conf":0.8371,"label":"print_embedding","label_id":0},{"bbox":{"x0":633,"x1":704,"y0":1386,"y1":1408},"conf":0.8335,"label":"print_embedding","label_id":0},{"bbox":{"x0":200,"x1":215,"y0":771,"y1":787},"conf":0.8167,"label":"print_embedding","label_id":0},{"bbox":{"x0":498,"x1":515,"y0":414,"y1":432},"conf":0.8086,"label":"print_embedding","label_id":0},{"bbox":{"x0":608,"x1":621,"y0":1272,"y1":1293},"conf":0.7854,"label":"print_embedding","label_id":0},{"bbox":{"x0":458,"x1":474,"y0":911,"y1":928},"conf":0.7796,"label":"print_embedding","label_id":0},{"bbox":{"x0":580,"x1":602,"y0":1107,"y1":1122},"conf":0.7787,"label":"print_embedding","label_id":0},{"bbox":{"x0":535,"x1":581,"y0":1296,"y1":1314},"conf":0.7784,"label":"print_embedding","label_id":0},{"bbox":{"x0":232,"x1":254,"y0":155,"y1":171},"conf":0.7782,"label":"print_embedding","label_id":0},{"bbox":{"x0":399,"x1":425,"y0":1508,"y1":1526},"conf":0.7666,"label":"print_embedding","label_id":0},{"bbox":{"x0":1010,"x1":1027,"y0":414,"y1":432},"conf":0.7645,"label":"print_embedding","label_id":0},{"bbox":{"x0":735,"x1":762,"y0":901,"y1":927},"conf":0.7378,"label":"print_embedding","label_id":0},{"bbox":{"x0":607,"x1":621,"y0":179,"y1":193},"conf":0.7111,"label":"print_embedding","label_id":0},{"bbox":{"x0":489,"x1":498,"y0":448,"y1":460},"conf":0.7013,"label":"print_embedding","label_id":0},{"bbox":{"x0":676,"x1":688,"y0":774,"y1":788},"conf":0.6754,"label":"print_embedding","label_id":0},{"bbox":{"x0":445,"x1":488,"y0":1013,"y1":1036},"conf":0.6719,"label":"print_embedding","label_id":0},{"bbox":{"x0":882,"x1":895,"y0":1470,"y1":1486},"conf":0.5963,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1057,"y0":1177,"y1":1318},"conf":0.974,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1057,"y0":1008,"y1":1172},"conf":0.9702,"label":"Text","label_id":1},{"bbox":{"x0":234,"x1":958,"y0":322,"y1":395},"conf":0.9651,"label":"Equation","label_id":8},{"bbox":{"x0":134,"x1":1056,"y0":472,"y1":591},"conf":0.9637,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1057,"y0":1455,"y1":1553},"conf":0.9632,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1057,"y0":1383,"y1":1449},"conf":0.96,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":607,"y1":683},"conf":0.9594,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1057,"y0":215,"y1":317},"conf":0.9592,"label":"Text","label_id":1},{"bbox":{"x0":146,"x1":1011,"y0":690,"y1":761},"conf":0.9584,"label":"Equation","label_id":8},{"bbox":{"x0":264,"x1":925,"y0":827,"y1":893},"conf":0.9529,"label":"Equation","label_id":8},{"bbox":{"x0":133,"x1":1055,"y0":398,"y1":469},"conf":0.9495,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1052,"y0":764,"y1":821},"conf":0.9464,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1055,"y0":136,"y1":200},"conf":0.938,"label":"Text","label_id":1},{"bbox":{"x0":412,"x1":778,"y0":1322,"y1":1380},"conf":0.9371,"label":"Equation","label_id":8},{"bbox":{"x0":135,"x1":305,"y0":957,"y1":986},"conf":0.8947,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":768,"y0":898,"y1":931},"conf":0.8851,"label":"Text","label_id":1},{"bbox":{"x0":1023,"x1":1055,"y0":846,"y1":873},"conf":0.8398,"label":"Equation caption","label_id":9},{"bbox":{"x0":1022,"x1":1054,"y0":712,"y1":739},"conf":0.8205,"label":"Equation caption","label_id":9},{"bbox":{"x0":1022,"x1":1055,"y0":345,"y1":372},"conf":0.8142,"label":"Equation caption","label_id":9},{"bbox":{"x0":582,"x1":607,"y0":1582,"y1":1607},"conf":0.4606,"label":"Abandon","label_id":2},{"bbox":{"x0":1022,"x1":1055,"y0":1337,"y1":1366},"conf":0.451,"label":"Equation caption","label_id":9},{"bbox":{"x0":585,"x1":604,"y0":1583,"y1":1606},"conf":0.3255,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1584],[603,1584],[603,1603],[587,1603]],"score":0.9087},{"poly":[[137,1519],[1051,1523],[1051,1552],[136,1549]],"score":0.6737},{"poly":[[135,1495],[1051,1498],[1051,1528],[135,1524]],"score":0.6504},{"poly":[[562,1479],[689,1475],[690,1500],[562,1504]],"score":0.7593},{"poly":[[684,1467],[1049,1465],[1049,1489],[684,1491]],"score":0.7738},{"poly":[[138,1466],[571,1466],[571,1491],[138,1491]],"score":0.7536},{"poly":[[563,1458],[680,1454],[681,1479],[564,1483]],"score":0.712},{"poly":[[347,1428],[401,1428],[401,1447],[347,1447]],"score":0.7923},{"poly":[[401,1416],[870,1416],[870,1438],[401,1438]],"score":0.8512},{"poly":[[140,1386],[1051,1386],[1051,1410],[140,1410]],"score":0.7721},{"poly":[[631,1351],[712,1351],[712,1377],[631,1377]],"score":0.7454},{"poly":[[412,1329],[633,1335],[632,1372],[411,1366]],"score":0.6253},{"poly":[[1023,1337],[1055,1337],[1055,1366],[1023,1366]],"score":0.8825},{"poly":[[711,1333],[764,1333],[764,1368],[711,1368]],"score":0.852},{"poly":[[640,1323],[714,1323],[714,1354],[640,1354]],"score":0.8263},{"poly":[[137,1289],[581,1291],[581,1316],[136,1314]],"score":0.7408},{"poly":[[137,1263],[1051,1267],[1051,1296],[136,1293]],"score":0.6414},{"poly":[[140,1247],[1049,1247],[1049,1270],[140,1270]],"score":0.8471},{"poly":[[140,1224],[1049,1224],[1049,1249],[140,1249]],"score":0.6926},{"poly":[[140,1203],[1049,1203],[1049,1226],[140,1226]],"score":0.8216},{"poly":[[138,1181],[1051,1181],[1051,1203],[138,1203]],"score":0.8038},{"poly":[[138,1145],[427,1145],[427,1168],[138,1168]],"score":0.8286},{"poly":[[138,1121],[1049,1124],[1049,1149],[138,1145]],"score":0.7081},{"poly":[[138,1098],[1051,1098],[1051,1128],[138,1128]],"score":0.6304},{"poly":[[140,1077],[1049,1077],[1049,1102],[140,1102]],"score":0.6965},{"poly":[[138,1056],[1051,1056],[1051,1079],[138,1079]],"score":0.8151},{"poly":[[138,1033],[1049,1033],[1049,1056],[138,1056]],"score":0.7511},{"poly":[[137,1009],[1049,1010],[1049,1035],[136,1033]],"score":0.7457},{"poly":[[137,958],[301,960],[301,984],[136,982]],"score":0.8241},{"poly":[[138,903],[764,900],[764,924],[138,928]],"score":0.7807},{"poly":[[695,860],[860,860],[860,891],[695,891]],"score":0.7658},{"poly":[[349,858],[513,854],[514,886],[350,890]],"score":0.7291},{"poly":[[266,847],[347,847],[347,874],[266,874]],"score":0.7448},{"poly":[[1023,844],[1055,844],[1055,874],[1023,874]],"score":0.9535},{"poly":[[530,844],[646,840],[647,872],[531,876]],"score":0.7063},{"poly":[[360,831],[498,831],[498,861],[360,861]],"score":0.8412},{"poly":[[640,828],[920,828],[920,863],[640,863]],"score":0.7069},{"poly":[[137,789],[565,791],[565,821],[136,819]],"score":0.7072},{"poly":[[135,763],[1051,765],[1051,795],[135,793]],"score":0.7484},{"poly":[[154,705],[458,720],[456,751],[152,736]],"score":0.6847},{"poly":[[578,707],[1001,707],[1001,742],[578,742]],"score":0.7356},{"poly":[[1003,709],[1051,709],[1051,740],[1003,740]],"score":0.7336},{"poly":[[555,695],[581,695],[581,716],[555,716]],"score":0.9187},{"poly":[[138,656],[672,654],[672,679],[138,681]],"score":0.7694},{"poly":[[138,631],[1049,633],[1049,658],[138,656]],"score":0.723},{"poly":[[140,610],[1049,610],[1049,635],[140,635]],"score":0.7566},{"poly":[[138,563],[822,565],[822,589],[138,588]],"score":0.7502},{"poly":[[140,542],[1049,542],[1049,567],[140,567]],"score":0.7151},{"poly":[[140,519],[1049,519],[1049,544],[140,544]],"score":0.6724},{"poly":[[138,498],[1049,498],[1049,521],[138,521]],"score":0.8266},{"poly":[[138,475],[1049,475],[1049,498],[138,498]],"score":0.8366},{"poly":[[140,440],[868,440],[868,465],[140,465]],"score":0.7261},{"poly":[[499,349],[533,356],[527,387],[493,381]],"score":0.6766},{"poly":[[237,338],[488,351],[486,383],[235,370]],"score":0.6914},{"poly":[[1021,344],[1055,344],[1055,374],[1021,374]],"score":0.8634},{"poly":[[527,337],[566,346],[554,394],[515,385]],"score":0.6671},{"poly":[[548,339],[934,339],[934,374],[548,374]],"score":0.7826},{"poly":[[523,326],[550,320],[556,344],[529,351]],"score":0.652},{"poly":[[138,286],[888,288],[888,314],[138,312]],"score":0.6849},{"poly":[[140,265],[1049,265],[1049,289],[140,289]],"score":0.7228},{"poly":[[137,242],[1049,244],[1049,268],[136,267]],"score":0.7722},{"poly":[[138,218],[1053,218],[1053,247],[138,247]],"score":0.6518},{"poly":[[138,172],[941,170],[941,195],[138,196]],"score":0.7921},{"poly":[[136,144],[1053,144],[1053,174],[136,174]],"score":0.6797}],"page_no":1,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":607,"y0":1582,"y1":1608},"conf":0.3706,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1584,"y1":1603},"font_size":0.0,"text":"3"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":1056,"y0":140,"y1":259},"conf":0.9647,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":144,"y1":168},"font_size":0.0,"text":"gradients, which accumulates over long sequences and is exacerbated by the clipping mechanism. We"},{"bbox":{"x0":138,"x1":1049,"y0":167,"y1":189},"font_size":0.0,"text":"have empirically observed that this can lead to model collapse that is often irreversible. Once the collapse"},{"bbox":{"x0":138,"x1":1049,"y0":188,"y1":212},"font_size":0.0,"text":"occurs, resuming training is unavailing, even when reverting to a previous checkpoint and meticulously"},{"bbox":{"x0":136,"x1":1051,"y0":207,"y1":233},"font_size":0.0,"text":"tuning hyperparameters (e.g., the clipping ranges), extending generation length, or switching the RL"},{"bbox":{"x0":136,"x1":215,"y0":229,"y1":258},"font_size":0.0,"text":"queries."}],"source":"layout det","text":"gradients, which accumulates over long sequences and is exacerbated by the clipping mechanism. We have empirically observed that this can lead to model collapse that is often irreversible. Once the collapse occurs, resuming training is unavailing, even when reverting to a previous checkpoint and meticulously tuning hyperparameters (e.g., the clipping ranges), extending generation length, or switching the RL queries."},{"bbox":{"x0":134,"x1":1056,"y0":263,"y1":382},"conf":0.9616,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":263,"y1":289},"font_size":0.0,"text":"The above observation suggests a fundamental issue in GRPO’s design. The failure of the token-level"},{"bbox":{"x0":138,"x1":1051,"y0":286,"y1":312},"font_size":0.0,"text":"importance weight points to a core principle: the unit of optimization objective should match the unit"},{"bbox":{"x0":140,"x1":1047,"y0":309,"y1":332},"font_size":0.0,"text":"of reward. Since the reward is granted to the entire sequence, applying off-policy correction at the token"},{"bbox":{"x0":138,"x1":1051,"y0":330,"y1":358},"font_size":0.0,"text":"level appears problematic. This motivates us to forego the token-level objective and explore utilizing"},{"bbox":{"x0":140,"x1":828,"y0":354,"y1":379},"font_size":0.0,"text":"importance weights and performing optimization directly at the sequence level."}],"source":"layout det","text":"The above observation suggests a fundamental issue in GRPO’s design. The failure of the token-level importance weight points to a core principle: the unit of optimization objective should match the unit of reward. Since the reward is granted to the entire sequence, applying off-policy correction at the token level appears problematic. This motivates us to forego the token-level objective and explore utilizing importance weights and performing optimization directly at the sequence level."},{"bbox":{"x0":135,"x1":298,"y0":409,"y1":442},"conf":0.8949,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":292,"y0":412,"y1":437},"font_size":0.0,"text":"4Algorithm"}],"source":"layout det","text":"4Algorithm"},{"bbox":{"x0":134,"x1":601,"y0":460,"y1":491},"conf":0.8914,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":594,"y0":461,"y1":484},"font_size":0.0,"text":"4.1GSPO: Group Sequence Policy Optimization"}],"source":"layout det","text":"4.1GSPO: Group Sequence Policy Optimization"},{"bbox":{"x0":135,"x1":1056,"y0":503,"y1":661},"conf":0.9718,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1049,"y0":507,"y1":550},"font_size":0.0,"text":"While the token-level importance weight $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$ is problematic in GRPO, we observe that in"},{"bbox":{"x0":141,"x1":1051,"y0":548,"y1":590},"font_size":0.0,"text":"the context of language generation, the sequence-level importance weigh $\\tfrac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y|x)}$ has a clear theoretical"},{"bbox":{"x0":136,"x1":1051,"y0":586,"y1":616},"font_size":0.0,"text":"meaning: it refects how far the responsel $y$  sampled from $\\pi_{\\theta_{\\rm old}}(\\cdot|x)$  deviates from $\\pi_{\\theta}(\\cdot|x),$  which naturally"},{"bbox":{"x0":135,"x1":1051,"y0":607,"y1":640},"font_size":0.0,"text":"aligns with the sequence-level reward and can also serve as a meaningful indicator of the clipping"},{"bbox":{"x0":140,"x1":250,"y0":637,"y1":656},"font_size":0.0,"text":"mechanism."}],"source":"layout det","text":"While the token-level importance weight $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$ is problematic in GRPO, we observe that in the context of language generation, the sequence-level importance weigh $\\tfrac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y|x)}$ has a clear theoretical meaning: it refects how far the responsel $y$  sampled from $\\pi_{\\theta_{\\rm old}}(\\cdot|x)$  deviates from $\\pi_{\\theta}(\\cdot|x),$  which naturally aligns with the sequence-level reward and can also serve as a meaningful indicator of the clipping mechanism."},{"bbox":{"x0":136,"x1":1055,"y0":679,"y1":733},"conf":0.9309,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":682,"y1":705},"font_size":0.0,"text":"Based on this straightforward observation, we propose the Group Sequence Policy Optimization (GSPO)"},{"bbox":{"x0":138,"x1":838,"y0":705,"y1":730},"font_size":0.0,"text":"algorithm. GSPO employs the following sequence-level optimization objective:"}],"source":"layout det","text":"Based on this straightforward observation, we propose the Group Sequence Policy Optimization (GSPO)algorithm. GSPO employs the following sequence-level optimization objective:"},{"bbox":{"x0":225,"x1":967,"y0":740,"y1":806},"conf":0.9244,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G} \\min\\left(s_{i}(\\theta)\\widehat{A}_{i}, \\mathrm{clip}\\left(s_{i}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{i}\\right)\\right],$$"},{"bbox":{"x0":1022,"x1":1055,"y0":759,"y1":787},"conf":0.586,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1055,"y0":758,"y1":788},"font_size":0.0,"text":"(5)"}],"source":"layout det","text":"(5)"},{"bbox":{"x0":135,"x1":637,"y0":814,"y1":845},"conf":0.9076,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":633,"y0":817,"y1":840},"font_size":0.0,"text":"where we adopt the group-based advantage estimation:"}],"source":"layout det","text":"where we adopt the group-based advantage estimation:"},{"bbox":{"x0":432,"x1":758,"y0":852,"y1":915},"conf":0.952,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\widehat{A}_{i}=\\frac{r(x,y_{i})-\\operatorname{mean}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)}{\\operatorname{std}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)},$$"},{"bbox":{"x0":1022,"x1":1055,"y0":869,"y1":897},"conf":0.8283,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1056,"y0":868,"y1":898},"font_size":0.0,"text":"(6)"}],"source":"layout det","text":"(6)"},{"bbox":{"x0":134,"x1":897,"y0":922,"y1":954},"conf":0.858,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":906,"y0":923,"y1":950},"font_size":0.0,"text":"and defne the importance ratioi $s_{i}(\\theta)$  based on sequence likelihood (Zheng et al., 2023)"}],"source":"layout det","text":"and defne the importance ratioi $s_{i}(\\theta)$  based on sequence likelihood (Zheng et al., 2023)"},{"bbox":{"x0":309,"x1":880,"y0":962,"y1":1029},"conf":0.949,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$s_{i}(\\theta)=\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}( y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}=\\exp\\left(\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|} \\log\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i, t}|x,y_{i,<t})}\\right).$$"},{"bbox":{"x0":1022,"x1":1055,"y0":983,"y1":1011},"conf":0.3526,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1056,"y0":982,"y1":1012},"font_size":0.0,"text":"(7)"}],"source":"layout det","text":"(7)"},{"bbox":{"x0":134,"x1":1056,"y0":1036,"y1":1222},"conf":0.9719,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":1040,"y1":1063},"font_size":0.0,"text":"Therefore, GSPO applies clipping to entire responses instead of individual tokens to exclude the overly"},{"bbox":{"x0":136,"x1":1049,"y0":1059,"y1":1086},"font_size":0.0,"text":"“off-policy” samples from gradient estimation, which matches both the sequence-level rewarding and"},{"bbox":{"x0":136,"x1":1049,"y0":1081,"y1":1109},"font_size":0.0,"text":"optimization. Note that we adopt length normalization in $s_{i}(\\theta)$  to reduce the variance and to control"},{"bbox":{"x0":139,"x1":1051,"y0":1107,"y1":1130},"font_size":0.0,"text":"$s_{i}(\\theta)$  within a unifed numerical range. Otherwise, the likelihood changes of a few tokens can result ini"},{"bbox":{"x0":136,"x1":1049,"y0":1126,"y1":1153},"font_size":0.0,"text":"dramatic fuctuations of the sequence-level importance ratio, and the importance ratios of responses withl"},{"bbox":{"x0":138,"x1":1049,"y0":1151,"y1":1174},"font_size":0.0,"text":"different lengths will require varying clipping ranges. We also note that the clipping ranges in GSPO and"},{"bbox":{"x0":138,"x1":1051,"y0":1172,"y1":1196},"font_size":0.0,"text":"in previous algorithms (e.g., GRPO) typically differ in order of magnitude due to the distinct defnitionsi"},{"bbox":{"x0":138,"x1":328,"y0":1195,"y1":1217},"font_size":0.0,"text":"of importance ratios."}],"source":"layout det","text":"Therefore, GSPO applies clipping to entire responses instead of individual tokens to exclude the overly“off-policy” samples from gradient estimation, which matches both the sequence-level rewarding and optimization. Note that we adopt length normalization in $s_{i}(\\theta)$  to reduce the variance and to control $s_{i}(\\theta)$  within a unifed numerical range. Otherwise, the likelihood changes of a few tokens can result ini dramatic fuctuations of the sequence-level importance ratio, and the importance ratios of responses withl different lengths will require varying clipping ranges. We also note that the clipping ranges in GSPO and in previous algorithms (e.g., GRPO) typically differ in order of magnitude due to the distinct defnitionsi of importance ratios."},{"bbox":{"x0":134,"x1":359,"y0":1243,"y1":1273},"conf":0.8401,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":354,"y0":1245,"y1":1268},"font_size":0.0,"text":"4.2Gradient Analysis"}],"source":"layout det","text":"4.2Gradient Analysis"},{"bbox":{"x0":133,"x1":960,"y0":1284,"y1":1316},"conf":0.8696,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":955,"y0":1282,"y1":1314},"font_size":0.0,"text":"We can derive the gradient of the GSPO objective as follows (clipping is omitted for brevity):"}],"source":"layout det","text":"We can derive the gradient of the GSPO objective as follows (clipping is omitted for brevity):"},{"bbox":{"x0":153,"x1":655,"y0":1324,"y1":1388},"conf":0.8469,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GSPO}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x )}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta)\\widehat{A}_{i}\\right]$$"},{"bbox":{"x0":1023,"x1":1054,"y0":1343,"y1":1370},"conf":0.8538,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1021,"x1":1056,"y0":1342,"y1":1372},"font_size":0.0,"text":"(8)"}],"source":"layout det","text":"(8)"},{"bbox":{"x0":260,"x1":738,"y0":1392,"y1":1457},"conf":0.8772,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta) \\widehat{A}_{i}\\cdot\\nabla_{\\theta}\\log s_{i}(\\theta)\\right]$$"},{"bbox":{"x0":1023,"x1":1055,"y0":1411,"y1":1438},"conf":0.8321,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1023,"x1":1056,"y0":1409,"y1":1440},"font_size":0.0,"text":"(9)"}],"source":"layout det","text":"(9)"},{"bbox":{"x0":260,"x1":1007,"y0":1460,"y1":1531},"conf":0.8905,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}\\widehat{A}_{i}\\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$"},{"bbox":{"x0":1013,"x1":1054,"y0":1481,"y1":1509},"conf":0.7973,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1007,"x1":1055,"y0":1475,"y1":1510},"font_size":0.0,"text":"(10)"}],"source":"layout det","text":"(10)"}],"formula_dets":[{"bbox":{"x0":432,"x1":758,"y0":852,"y1":915},"conf":0.952,"label":"print_isolated","label_id":1},{"bbox":{"x0":309,"x1":880,"y0":962,"y1":1029},"conf":0.949,"label":"print_isolated","label_id":1},{"bbox":{"x0":522,"x1":640,"y0":507,"y1":550},"conf":0.9304,"label":"print_embedding","label_id":0},{"bbox":{"x0":225,"x1":967,"y0":740,"y1":806},"conf":0.9244,"label":"print_isolated","label_id":1},{"bbox":{"x0":779,"x1":852,"y0":548,"y1":590},"conf":0.9213,"label":"print_embedding","label_id":0},{"bbox":{"x0":668,"x1":710,"y0":1087,"y1":1108},"conf":0.9081,"label":"print_embedding","label_id":0},{"bbox":{"x0":422,"x1":463,"y0":928,"y1":950},"conf":0.9011,"label":"print_embedding","label_id":0},{"bbox":{"x0":139,"x1":181,"y0":1109,"y1":1130},"conf":0.8917,"label":"print_embedding","label_id":0},{"bbox":{"x0":260,"x1":1007,"y0":1460,"y1":1531},"conf":0.8905,"label":"print_isolated","label_id":1},{"bbox":{"x0":839,"x1":907,"y0":589,"y1":613},"conf":0.8888,"label":"print_embedding","label_id":0},{"bbox":{"x0":632,"x1":712,"y0":588,"y1":615},"conf":0.8853,"label":"print_embedding","label_id":0},{"bbox":{"x0":260,"x1":738,"y0":1392,"y1":1457},"conf":0.8772,"label":"print_isolated","label_id":1},{"bbox":{"x0":153,"x1":655,"y0":1324,"y1":1388},"conf":0.8469,"label":"print_isolated","label_id":1},{"bbox":{"x0":492,"x1":503,"y0":596,"y1":612},"conf":0.6895,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1056,"y0":1036,"y1":1222},"conf":0.9719,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1056,"y0":503,"y1":661},"conf":0.9718,"label":"Text","label_id":1},{"bbox":{"x0":219,"x1":971,"y0":738,"y1":808},"conf":0.9673,"label":"Equation","label_id":8},{"bbox":{"x0":134,"x1":1056,"y0":140,"y1":259},"conf":0.9647,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1056,"y0":263,"y1":382},"conf":0.9616,"label":"Text","label_id":1},{"bbox":{"x0":305,"x1":886,"y0":961,"y1":1033},"conf":0.9447,"label":"Equation","label_id":8},{"bbox":{"x0":428,"x1":764,"y0":850,"y1":917},"conf":0.9378,"label":"Equation","label_id":8},{"bbox":{"x0":136,"x1":1055,"y0":679,"y1":733},"conf":0.9309,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":637,"y0":814,"y1":845},"conf":0.9076,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":298,"y0":409,"y1":442},"conf":0.8949,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":601,"y0":460,"y1":491},"conf":0.8914,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":960,"y0":1284,"y1":1316},"conf":0.8696,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":897,"y0":922,"y1":954},"conf":0.858,"label":"Text","label_id":1},{"bbox":{"x0":1023,"x1":1054,"y0":1343,"y1":1370},"conf":0.8538,"label":"Equation caption","label_id":9},{"bbox":{"x0":134,"x1":359,"y0":1243,"y1":1273},"conf":0.8401,"label":"Title","label_id":0},{"bbox":{"x0":1023,"x1":1055,"y0":1411,"y1":1438},"conf":0.8321,"label":"Equation caption","label_id":9},{"bbox":{"x0":1022,"x1":1055,"y0":869,"y1":897},"conf":0.8283,"label":"Equation caption","label_id":9},{"bbox":{"x0":1013,"x1":1054,"y0":1481,"y1":1509},"conf":0.7973,"label":"Equation caption","label_id":9},{"bbox":{"x0":148,"x1":1014,"y0":1320,"y1":1534},"conf":0.6607,"label":"Equation","label_id":8},{"bbox":{"x0":1022,"x1":1055,"y0":759,"y1":787},"conf":0.586,"label":"Equation caption","label_id":9},{"bbox":{"x0":582,"x1":607,"y0":1582,"y1":1608},"conf":0.3706,"label":"Abandon","label_id":2},{"bbox":{"x0":1022,"x1":1055,"y0":983,"y1":1011},"conf":0.3526,"label":"Equation caption","label_id":9},{"bbox":{"x0":135,"x1":359,"y0":1243,"y1":1273},"conf":0.304,"label":"Title","label_id":0},{"bbox":{"x0":584,"x1":605,"y0":1583,"y1":1607},"conf":0.2594,"label":"Abandon","label_id":2},{"bbox":{"x0":260,"x1":1022,"y0":1460,"y1":1532},"conf":0.2229,"label":"Equation","label_id":8}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1584],[603,1584],[603,1603],[587,1603]],"score":0.8957},{"poly":[[540,1495],[659,1493],[659,1524],[541,1526]],"score":0.7795},{"poly":[[730,1495],[766,1495],[766,1524],[730,1524]],"score":0.8242},{"poly":[[264,1476],[481,1488],[479,1523],[262,1511]],"score":0.6481},{"poly":[[791,1477],[989,1481],[989,1512],[790,1508]],"score":0.7988},{"poly":[[1007,1475],[1055,1475],[1055,1510],[1007,1510]],"score":0.7925},{"poly":[[704,1468],[734,1498],[710,1522],[680,1492]],"score":0.6027},{"poly":[[560,1466],[654,1466],[654,1496],[560,1496]],"score":0.8826},{"poly":[[737,1468],[762,1468],[762,1491],[737,1491]],"score":0.8433},{"poly":[[487,1463],[514,1463],[514,1521],[487,1521]],"score":0.6945},{"poly":[[509,1459],[542,1459],[542,1526],[509,1526]],"score":0.7327},{"poly":[[661,1458],[690,1455],[694,1490],[666,1494]],"score":0.6471},{"poly":[[264,1404],[481,1418],[479,1455],[262,1441]],"score":0.6252},{"poly":[[1023,1409],[1056,1409],[1056,1440],[1023,1440]],"score":0.8522},{"poly":[[535,1405],[725,1405],[725,1440],[535,1440]],"score":0.7988},{"poly":[[487,1396],[512,1396],[512,1451],[487,1451]],"score":0.7753},{"poly":[[509,1389],[542,1389],[542,1459],[509,1459]],"score":0.6236},{"poly":[[541,1368],[564,1368],[564,1384],[541,1384]],"score":0.7229},{"poly":[[516,1349],[539,1349],[539,1379],[516,1379]],"score":0.7936},{"poly":[[148,1334],[507,1349],[506,1385],[147,1370]],"score":0.6684},{"poly":[[1021,1342],[1056,1342],[1056,1372],[1021,1372]],"score":0.8962},{"poly":[[529,1338],[635,1329],[639,1372],[533,1382]],"score":0.627},{"poly":[[516,1328],[539,1328],[539,1352],[516,1352]],"score":0.7435},{"poly":[[532,1324],[564,1324],[564,1347],[532,1347]],"score":0.6832},{"poly":[[135,1282],[955,1284],[955,1314],[135,1312]],"score":0.7187},{"poly":[[138,1245],[354,1245],[354,1268],[138,1268]],"score":0.875},{"poly":[[138,1195],[328,1195],[328,1217],[138,1217]],"score":0.8561},{"poly":[[138,1172],[1051,1172],[1051,1196],[138,1196]],"score":0.7208},{"poly":[[138,1151],[1049,1151],[1049,1174],[138,1174]],"score":0.7972},{"poly":[[136,1128],[1049,1126],[1049,1151],[136,1153]],"score":0.7407},{"poly":[[138,1107],[1051,1107],[1051,1130],[138,1130]],"score":0.8262},{"poly":[[136,1084],[1049,1081],[1049,1105],[137,1109]],"score":0.6785},{"poly":[[137,1059],[1049,1061],[1049,1086],[136,1084]],"score":0.7559},{"poly":[[138,1040],[1049,1040],[1049,1063],[138,1063]],"score":0.8201},{"poly":[[702,1002],[858,1002],[858,1026],[702,1026]],"score":0.8434},{"poly":[[381,996],[501,994],[502,1026],[381,1028]],"score":0.7285},{"poly":[[602,992],[676,997],[674,1030],[600,1026]],"score":0.6276},{"poly":[[530,980],[593,987],[590,1013],[527,1006]],"score":0.7887},{"poly":[[670,983],[708,993],[702,1014],[664,1004]],"score":0.765},{"poly":[[1021,982],[1056,982],[1056,1012],[1021,1012]],"score":0.8404},{"poly":[[645,984],[670,984],[670,1010],[645,1010]],"score":0.6236},{"poly":[[303,981],[378,981],[378,1012],[303,1012]],"score":0.8104},{"poly":[[709,970],[847,970],[847,1000],[709,1000]],"score":0.8446},{"poly":[[399,972],[484,972],[484,996],[399,996]],"score":0.9276},{"poly":[[611,968],[634,968],[634,993],[611,993]],"score":0.8794},{"poly":[[643,965],[670,965],[670,988],[643,988]],"score":0.8601},{"poly":[[500,961],[530,961],[530,996],[500,996]],"score":0.7084},{"poly":[[140,923],[906,923],[906,947],[140,947]],"score":0.711},{"poly":[[532,884],[695,884],[695,916],[532,916]],"score":0.7394},{"poly":[[1021,868],[1056,868],[1056,898],[1021,898]],"score":0.8142},{"poly":[[429,867],[482,867],[482,895],[429,895]],"score":0.7444},{"poly":[[477,851],[755,851],[755,886],[477,886]],"score":0.7069},{"poly":[[140,817],[633,817],[633,840],[140,840]],"score":0.8739},{"poly":[[533,768],[557,768],[557,798],[533,798]],"score":0.7835},{"poly":[[219,752],[529,767],[527,802],[217,787]],"score":0.6337},{"poly":[[1021,758],[1055,758],[1055,788],[1021,788]],"score":0.8757},{"poly":[[578,754],[950,754],[950,791],[578,791]],"score":0.6741},{"poly":[[537,747],[555,747],[555,767],[537,767]],"score":0.7848},{"poly":[[560,744],[581,744],[581,760],[560,760]],"score":0.789},{"poly":[[138,705],[838,705],[838,730],[138,730]],"score":0.7505},{"poly":[[140,682],[1051,682],[1051,705],[140,705]],"score":0.8532},{"poly":[[140,637],[250,637],[250,656],[140,656]],"score":0.8298},{"poly":[[135,607],[1051,610],[1051,640],[135,637]],"score":0.6596},{"poly":[[136,586],[1051,586],[1051,616],[136,616]],"score":0.6594},{"poly":[[773,565],[854,565],[854,589],[773,589]],"score":0.6782},{"poly":[[849,554],[1051,554],[1051,579],[849,579]],"score":0.7696},{"poly":[[519,526],[643,526],[643,551],[519,551]],"score":0.7002},{"poly":[[636,514],[1049,512],[1049,537],[636,539]],"score":0.7672},{"poly":[[137,512],[523,514],[523,539],[136,537]],"score":0.7783},{"poly":[[516,505],[647,507],[647,530],[516,528]],"score":0.6609},{"poly":[[136,461],[594,461],[594,484],[136,484]],"score":0.8247},{"poly":[[138,412],[292,412],[292,437],[138,437]],"score":0.8809},{"poly":[[140,354],[828,354],[828,379],[140,379]],"score":0.6976},{"poly":[[138,330],[1051,333],[1051,358],[138,354]],"score":0.698},{"poly":[[140,309],[1047,309],[1047,332],[140,332]],"score":0.6994},{"poly":[[138,288],[1051,286],[1051,310],[138,312]],"score":0.7169},{"poly":[[136,265],[1049,263],[1049,288],[137,289]],"score":0.7144},{"poly":[[136,234],[214,229],[215,254],[137,258]],"score":0.7609},{"poly":[[136,209],[1051,207],[1051,232],[136,233]],"score":0.7703},{"poly":[[138,188],[1049,188],[1049,212],[138,212]],"score":0.7267},{"poly":[[138,167],[1049,167],[1049,189],[138,189]],"score":0.8305},{"poly":[[140,144],[1049,144],[1049,168],[140,168]],"score":0.6925}],"page_no":2,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":583,"x1":607,"y0":1582,"y1":1607},"conf":0.4625,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1584,"y1":1603},"font_size":0.0,"text":"4"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":135,"x1":904,"y0":136,"y1":173},"conf":0.7861,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":897,"y0":140,"y1":169},"font_size":0.0,"text":"For comparison, the gradient of the GRPO objective is as follows (note that $\\widehat{A}_{i,t}=\\widehat{A}_{i}\\text{):}$"}],"source":"layout det","text":"For comparison, the gradient of the GRPO objective is as follows (note that $\\widehat{A}_{i,t}=\\widehat{A}_{i}\\text{):}$"},{"bbox":{"x0":153,"x1":735,"y0":181,"y1":246},"conf":0.9265,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GRPO}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}w_{i,t} (\\theta)\\widehat{A}_{i,t}\\right]$$"},{"bbox":{"x0":1012,"x1":1055,"y0":199,"y1":229},"conf":0.8246,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1012,"x1":1056,"y0":200,"y1":226},"font_size":0.0,"text":"(11)"}],"source":"layout det","text":"(11)"},{"bbox":{"x0":265,"x1":1004,"y0":252,"y1":318},"conf":0.907,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\widehat{A}_{i} \\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$"},{"bbox":{"x0":1013,"x1":1054,"y0":271,"y1":299},"conf":0.8467,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1008,"x1":1053,"y0":268,"y1":298},"font_size":0.0,"text":"(12)"}],"source":"layout det","text":"(12)"},{"bbox":{"x0":135,"x1":1058,"y0":323,"y1":490},"conf":0.9217,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":326,"y1":351},"font_size":0.0,"text":"Therefore, the fundamental distinction between GSPO and GRPO lies in how they weight the gradients of"},{"bbox":{"x0":138,"x1":1049,"y0":349,"y1":372},"font_size":0.0,"text":"the log likelihoods of tokens. In GRPO, the tokens are weighted according to their respective “importance"},{"bbox":{"x0":138,"x1":1051,"y0":374,"y1":415},"font_size":0.0,"text":"weight” $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$  However, these unequal weights, which can vary among $(0,1+\\varepsilon]$  (for $\\widehat{A}_{i}>0\\text{)}$  or"},{"bbox":{"x0":140,"x1":1051,"y0":414,"y1":441},"font_size":0.0,"text":"$[1-\\varepsilon,+\\infty)$  (for $\\widehat{A}_{i}<0\\text{),}$  are not negligible, and their impact can accumulate and lead to unpredictable"},{"bbox":{"x0":138,"x1":1051,"y0":440,"y1":463},"font_size":0.0,"text":"consequences as training progresses. In contrast, GSPO weights all the tokens in a response equally,"},{"bbox":{"x0":138,"x1":519,"y0":461,"y1":484},"font_size":0.0,"text":"eliminating this instability factor of GRPO."}],"source":"layout det","text":"Therefore, the fundamental distinction between GSPO and GRPO lies in how they weight the gradients of the log likelihoods of tokens. In GRPO, the tokens are weighted according to their respective “importance weight” $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$  However, these unequal weights, which can vary among $(0,1+\\varepsilon]$  (for $\\widehat{A}_{i}>0\\text{)}$  or $[1-\\varepsilon,+\\infty)$  (for $\\widehat{A}_{i}<0\\text{),}$  are not negligible, and their impact can accumulate and lead to unpredictable consequences as training progresses. In contrast, GSPO weights all the tokens in a response equally,eliminating this instability factor of GRPO."},{"bbox":{"x0":134,"x1":614,"y0":510,"y1":540},"conf":0.8722,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":610,"y0":512,"y1":535},"font_size":0.0,"text":"4.3GSPO-token: A Token-level Objective Variant"}],"source":"layout det","text":"4.3GSPO-token: A Token-level Objective Variant"},{"bbox":{"x0":134,"x1":1056,"y0":552,"y1":628},"conf":0.9561,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":553,"y1":579},"font_size":0.0,"text":"In scenarios like multi-turn RL, we may desire a fner-grained advantage adjustment than the sequencei"},{"bbox":{"x0":138,"x1":1051,"y0":577,"y1":600},"font_size":0.0,"text":"level. To this end, we introduce a token-level objective variant of GSPO, namely GSPO-token, to allow"},{"bbox":{"x0":138,"x1":475,"y0":600,"y1":623},"font_size":0.0,"text":"token-wise advantage customization:"}],"source":"layout det","text":"In scenarios like multi-turn RL, we may desire a fner-grained advantage adjustment than the sequencei level. To this end, we introduce a token-level objective variant of GSPO, namely GSPO-token, to allow token-wise advantage customization:"},{"bbox":{"x0":152,"x1":1037,"y0":633,"y1":703},"conf":0.8787,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\mathcal{J}_{\\mathrm{GSPO\\text{-}token}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\min\\left(s_{i,t}(\\theta)\\widehat{A}_{i,t}, \\mathrm{clip}\\left(s_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{i,t}\\right)\\right],$$"},{"bbox":{"x0":1012,"x1":1054,"y0":698,"y1":725},"conf":0.8395,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1012,"x1":1056,"y0":696,"y1":724},"font_size":0.0,"text":"(13)"}],"source":"layout det","text":"(13)"},{"bbox":{"x0":135,"x1":202,"y0":733,"y1":759},"conf":0.8475,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":200,"y0":733,"y1":758},"font_size":0.0,"text":"where"}],"source":"layout det","text":"where"},{"bbox":{"x0":421,"x1":767,"y0":764,"y1":819},"conf":0.9315,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$s_{i,t}(\\theta)=\\mathrm{sg}\\left[s_{i}(\\theta)\\right]\\cdot\\frac{\\pi_{\\theta}( y_{i,t}|x,y_{i,<t})}{\\mathrm{sg}\\left[\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right]},$$"},{"bbox":{"x0":1012,"x1":1054,"y0":778,"y1":806},"conf":0.8313,"font_size":0.0,"label":"Equation caption","label_id":9,"lines":[{"bbox":{"x0":1012,"x1":1055,"y0":777,"y1":805},"font_size":0.0,"text":"(14)"}],"source":"layout det","text":"(14)"},{"bbox":{"x0":133,"x1":1055,"y0":826,"y1":879},"conf":0.9026,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":830,"y1":856},"font_size":0.0,"text":"and $\\mathbf{sg}[\\cdot]$  denotes only taking the numerical value but stopping the gradient, corresponding to the detach"},{"bbox":{"x0":138,"x1":753,"y0":851,"y1":877},"font_size":0.0,"text":"operation in PyTorch. The gradient of GSPO-token can be derived as:"}],"source":"layout det","text":"and $\\mathbf{sg}[\\cdot]$  denotes only taking the numerical value but stopping the gradient, corresponding to the detach operation in PyTorch. The gradient of GSPO-token can be derived as:"},{"bbox":{"x0":145,"x1":763,"y0":888,"y1":954},"conf":0.8864,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GSPO\\text{-}token}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}s_{i,t}(\\theta)\\widehat{A}_{i,t}\\right]$$"},{"bbox":{"x0":296,"x1":900,"y0":959,"y1":1024},"conf":0.8897,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta) \\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\widehat{A}_{i,t}\\frac{\\nabla_{\\theta}\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}\\right]$$"},{"bbox":{"x0":297,"x1":1047,"y0":1028,"y1":1097},"conf":0.8753,"font_size":0.0,"label":"isolated","label_id":1,"lines":[],"source":"mfd","text":"$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}\\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\widehat{A}_{i,t}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$"},{"bbox":{"x0":134,"x1":1058,"y0":1126,"y1":1272},"conf":0.9669,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1050,"y0":1130,"y1":1173},"font_size":0.0,"text":"Note that the term $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\mathrm{sg}\\left[\\pi_{\\theta}(y_{i,t} |x,y_{i,<t})\\right]}$ has a numerical value of 1, so $s_{i,t}(\\theta)$  is numerically equal to $s_{i}(\\theta).$"},{"bbox":{"x0":138,"x1":1049,"y0":1170,"y1":1195},"font_size":0.0,"text":"Comparing Equation (5) and (13), and Equation (10) and (17), GSPO-token and GSPO are numerically"},{"bbox":{"x0":138,"x1":1049,"y0":1193,"y1":1217},"font_size":0.0,"text":"identical in the optimization objective, clipping condition, and theoretical gradient when we set the"},{"bbox":{"x0":138,"x1":1049,"y0":1216,"y1":1245},"font_size":0.0,"text":"advantages of all the tokens in the responseto the same value (i.e., $\\widehat{A}_{i,t}\\stackrel{{\\bar{\\}}}{{=}}\\widehat{A}_{i}\\widehat{)},$  while GSPO-token"},{"bbox":{"x0":543,"x1":561,"y0":1228,"y1":1245},"font_size":0.0,"text":"$y_{i}$"},{"bbox":{"x0":138,"x1":727,"y0":1242,"y1":1267},"font_size":0.0,"text":"enjoys the higher fexibility of adjusting the advantages per token.l"}],"source":"layout det","text":"Note that the term $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\mathrm{sg}\\left[\\pi_{\\theta}(y_{i,t} |x,y_{i,<t})\\right]}$ has a numerical value of 1, so $s_{i,t}(\\theta)$  is numerically equal to $s_{i}(\\theta).$ Comparing Equation (5) and (13), and Equation (10) and (17), GSPO-token and GSPO are numerically identical in the optimization objective, clipping condition, and theoretical gradient when we set the advantages of all the tokens in the responseto the same value (i.e., $\\widehat{A}_{i,t}\\stackrel{{\\bar{\\}}}{{=}}\\widehat{A}_{i}\\widehat{)},$  while GSPO-token $y_{i}$ enjoys the higher fexibility of adjusting the advantages per token.l"},{"bbox":{"x0":134,"x1":494,"y0":1295,"y1":1330},"conf":0.894,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":487,"y0":1296,"y1":1321},"font_size":0.0,"text":"5Experiments and Discussion"}],"source":"layout det","text":"5Experiments and Discussion"},{"bbox":{"x0":134,"x1":353,"y0":1346,"y1":1377},"conf":0.8922,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":347,"y0":1349,"y1":1372},"font_size":0.0,"text":"5.1Empirical Results"}],"source":"layout det","text":"5.1Empirical Results"},{"bbox":{"x0":134,"x1":1058,"y0":1389,"y1":1553},"conf":0.5774,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":1388,"y1":1419},"font_size":0.0,"text":"We experiment with a cold-start model fne-tuned from Qwen3-30B-A3B-Base, and report the trainingi"},{"bbox":{"x0":135,"x1":1051,"y0":1409,"y1":1440},"font_size":0.0,"text":"reward curves as well as the model performance curves on the AIME’24 (average Pass@1 over 32"},{"bbox":{"x0":138,"x1":1051,"y0":1435,"y1":1459},"font_size":0.0,"text":"samplings), LiveCodeBench (202410-202502, average Pass@1 over 8 samplings), and CodeForces (Elo"},{"bbox":{"x0":138,"x1":1051,"y0":1458,"y1":1481},"font_size":0.0,"text":"Rating) benchmarks. During the RL training, each batch of rollout data is partitioned into four mini-"},{"bbox":{"x0":138,"x1":1051,"y0":1481,"y1":1503},"font_size":0.0,"text":"batches for gradient updates. In GSPO, we set the left and right clipping ranges in Equation (5) to 3e-4"},{"bbox":{"x0":136,"x1":1049,"y0":1500,"y1":1526},"font_size":0.0,"text":"and 4e-4, respectively. We compare against GRPO as the baseline and set the left and right clipping ranges"},{"bbox":{"x0":140,"x1":1053,"y0":1524,"y1":1547},"font_size":0.0,"text":"in Equation (2) to 0.2 and 0.27, respectively, which we have carefully tuned to ensure a fair comparison."}],"source":"layout det","text":"We experiment with a cold-start model fne-tuned from Qwen3-30B-A3B-Base, and report the trainingi reward curves as well as the model performance curves on the AIME’24 (average Pass@1 over 32 samplings), LiveCodeBench (202410-202502, average Pass@1 over 8 samplings), and CodeForces (Elo Rating) benchmarks. During the RL training, each batch of rollout data is partitioned into four minibatches for gradient updates. In GSPO, we set the left and right clipping ranges in Equation (5) to 3e-4 and 4e-4, respectively. We compare against GRPO as the baseline and set the left and right clipping ranges in Equation (2) to 0.2 and 0.27, respectively, which we have carefully tuned to ensure a fair comparison."}],"formula_dets":[{"bbox":{"x0":421,"x1":767,"y0":764,"y1":819},"conf":0.9315,"label":"print_isolated","label_id":1},{"bbox":{"x0":215,"x1":340,"y0":374,"y1":415},"conf":0.928,"label":"print_embedding","label_id":0},{"bbox":{"x0":316,"x1":446,"y0":1130,"y1":1173},"conf":0.9272,"label":"print_embedding","label_id":0},{"bbox":{"x0":153,"x1":735,"y0":181,"y1":246},"conf":0.9265,"label":"print_isolated","label_id":1},{"bbox":{"x0":265,"x1":1004,"y0":252,"y1":318},"conf":0.907,"label":"print_isolated","label_id":1},{"bbox":{"x0":803,"x1":897,"y0":140,"y1":169},"conf":0.9002,"label":"print_embedding","label_id":0},{"bbox":{"x0":731,"x1":782,"y0":1139,"y1":1164},"conf":0.8997,"label":"print_embedding","label_id":0},{"bbox":{"x0":776,"x1":873,"y0":1216,"y1":1245},"conf":0.8931,"label":"print_embedding","label_id":0},{"bbox":{"x0":296,"x1":900,"y0":959,"y1":1024},"conf":0.8897,"label":"print_isolated","label_id":1},{"bbox":{"x0":145,"x1":763,"y0":888,"y1":954},"conf":0.8864,"label":"print_isolated","label_id":1},{"bbox":{"x0":843,"x1":918,"y0":379,"y1":405},"conf":0.8843,"label":"print_embedding","label_id":0},{"bbox":{"x0":283,"x1":352,"y0":415,"y1":441},"conf":0.8842,"label":"print_embedding","label_id":0},{"bbox":{"x0":959,"x1":1023,"y0":377,"y1":404},"conf":0.8797,"label":"print_embedding","label_id":0},{"bbox":{"x0":140,"x1":240,"y0":418,"y1":441},"conf":0.8792,"label":"print_embedding","label_id":0},{"bbox":{"x0":152,"x1":1037,"y0":633,"y1":703},"conf":0.8787,"label":"print_isolated","label_id":1},{"bbox":{"x0":297,"x1":1047,"y0":1028,"y1":1097},"conf":0.8753,"label":"print_isolated","label_id":1},{"bbox":{"x0":1003,"x1":1050,"y0":1139,"y1":1163},"conf":0.8596,"label":"print_embedding","label_id":0},{"bbox":{"x0":177,"x1":217,"y0":832,"y1":856},"conf":0.8183,"label":"print_embedding","label_id":0},{"bbox":{"x0":543,"x1":561,"y0":1228,"y1":1245},"conf":0.7907,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1058,"y0":1126,"y1":1272},"conf":0.9669,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1056,"y0":552,"y1":628},"conf":0.9561,"label":"Text","label_id":1},{"bbox":{"x0":145,"x1":1009,"y0":179,"y1":320},"conf":0.9349,"label":"Equation","label_id":8},{"bbox":{"x0":143,"x1":1045,"y0":633,"y1":705},"conf":0.9292,"label":"Equation","label_id":8},{"bbox":{"x0":418,"x1":772,"y0":764,"y1":822},"conf":0.929,"label":"Equation","label_id":8},{"bbox":{"x0":135,"x1":1058,"y0":323,"y1":490},"conf":0.9217,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1055,"y0":826,"y1":879},"conf":0.9026,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":494,"y0":1295,"y1":1330},"conf":0.894,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":353,"y0":1346,"y1":1377},"conf":0.8922,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":614,"y0":510,"y1":540},"conf":0.8722,"label":"Title","label_id":0},{"bbox":{"x0":135,"x1":202,"y0":733,"y1":759},"conf":0.8475,"label":"Text","label_id":1},{"bbox":{"x0":1013,"x1":1054,"y0":271,"y1":299},"conf":0.8467,"label":"Equation caption","label_id":9},{"bbox":{"x0":1012,"x1":1054,"y0":698,"y1":725},"conf":0.8395,"label":"Equation caption","label_id":9},{"bbox":{"x0":1012,"x1":1055,"y0":978,"y1":1006},"conf":0.8395,"label":"Equation caption","label_id":9},{"bbox":{"x0":1012,"x1":1055,"y0":908,"y1":937},"conf":0.8376,"label":"Equation caption","label_id":9},{"bbox":{"x0":1013,"x1":1054,"y0":1093,"y1":1119},"conf":0.8349,"label":"Equation caption","label_id":9},{"bbox":{"x0":1012,"x1":1054,"y0":778,"y1":806},"conf":0.8313,"label":"Equation caption","label_id":9},{"bbox":{"x0":1012,"x1":1055,"y0":199,"y1":229},"conf":0.8246,"label":"Equation caption","label_id":9},{"bbox":{"x0":135,"x1":904,"y0":136,"y1":173},"conf":0.7861,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1058,"y0":1389,"y1":1553},"conf":0.5774,"label":"Text","label_id":1},{"bbox":{"x0":135,"x1":1057,"y0":886,"y1":1117},"conf":0.5554,"label":"Equation","label_id":8},{"bbox":{"x0":583,"x1":607,"y0":1582,"y1":1607},"conf":0.4625,"label":"Abandon","label_id":2},{"bbox":{"x0":135,"x1":1058,"y0":1389,"y1":1553},"conf":0.3328,"label":"Text","label_id":1},{"bbox":{"x0":585,"x1":604,"y0":1584,"y1":1605},"conf":0.287,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1584],[603,1584],[603,1603],[587,1603]],"score":0.7895},{"poly":[[140,1524],[1053,1524],[1053,1547],[140,1547]],"score":0.8362},{"poly":[[136,1500],[1049,1502],[1049,1526],[136,1524]],"score":0.709},{"poly":[[138,1481],[1051,1481],[1051,1503],[138,1503]],"score":0.8195},{"poly":[[138,1458],[1051,1458],[1051,1481],[138,1481]],"score":0.7757},{"poly":[[138,1435],[1051,1435],[1051,1459],[138,1459]],"score":0.6798},{"poly":[[135,1410],[1051,1409],[1051,1438],[135,1440]],"score":0.6332},{"poly":[[135,1388],[1051,1389],[1051,1419],[135,1417]],"score":0.645},{"poly":[[136,1349],[347,1349],[347,1372],[136,1372]],"score":0.8376},{"poly":[[136,1296],[487,1296],[487,1321],[136,1321]],"score":0.6864},{"poly":[[138,1242],[727,1242],[727,1267],[138,1267]],"score":0.725},{"poly":[[138,1217],[1049,1217],[1049,1242],[138,1242]],"score":0.7169},{"poly":[[138,1193],[1049,1193],[1049,1217],[138,1217]],"score":0.6989},{"poly":[[138,1170],[1049,1170],[1049,1195],[138,1195]],"score":0.7295},{"poly":[[317,1152],[443,1152],[443,1172],[317,1172]],"score":0.8158},{"poly":[[138,1138],[317,1138],[317,1161],[138,1161]],"score":0.8332},{"poly":[[445,1135],[1051,1137],[1051,1161],[445,1159]],"score":0.7854},{"poly":[[330,1131],[447,1131],[447,1151],[330,1151]],"score":0.8174},{"poly":[[1012,1089],[1056,1089],[1056,1119],[1012,1119]],"score":0.7903},{"poly":[[548,1077],[572,1077],[572,1089],[548,1089]],"score":0.9444},{"poly":[[583,1063],[697,1057],[699,1089],[584,1095]],"score":0.7393},{"poly":[[523,1054],[544,1054],[544,1086],[523,1086]],"score":0.7682},{"poly":[[297,1045],[515,1057],[513,1092],[295,1080]],"score":0.6499},{"poly":[[800,1045],[1028,1049],[1028,1079],[799,1075]],"score":0.7833},{"poly":[[548,1047],[571,1047],[571,1077],[548,1077]],"score":0.7634},{"poly":[[739,1035],[767,1035],[767,1058],[739,1058]],"score":0.6817},{"poly":[[525,1035],[541,1035],[541,1058],[525,1058]],"score":0.7706},{"poly":[[546,1031],[572,1031],[572,1054],[546,1054]],"score":0.715},{"poly":[[548,1005],[574,1005],[574,1023],[548,1023]],"score":0.7232},{"poly":[[736,993],[877,993],[877,1023],[736,1023]],"score":0.7626},{"poly":[[523,986],[544,986],[544,1016],[523,1016]],"score":0.7612},{"poly":[[673,986],[693,986],[693,998],[673,998]],"score":0.702},{"poly":[[698,977],[732,987],[725,1009],[691,998]],"score":0.6932},{"poly":[[299,973],[513,986],[511,1022],[297,1008]],"score":0.6803},{"poly":[[1012,977],[1056,977],[1056,1005],[1012,1005]],"score":0.9363},{"poly":[[564,975],[624,975],[624,1009],[564,1009]],"score":0.7132},{"poly":[[548,979],[572,979],[572,1007],[548,1007]],"score":0.7003},{"poly":[[724,958],[892,961],[891,997],[723,993]],"score":0.7382},{"poly":[[640,967],[658,967],[658,986],[640,986]],"score":0.7222},{"poly":[[523,963],[544,963],[544,988],[523,988]],"score":0.7534},{"poly":[[666,960],[691,960],[691,982],[666,982]],"score":0.8695},{"poly":[[544,960],[571,960],[571,982],[544,982]],"score":0.733},{"poly":[[636,933],[661,933],[661,947],[636,947]],"score":0.7495},{"poly":[[597,921],[636,921],[636,949],[597,949]],"score":0.7141},{"poly":[[569,917],[601,917],[601,951],[569,951]],"score":0.6665},{"poly":[[549,917],[571,917],[571,944],[549,944]],"score":0.8999},{"poly":[[139,900],[541,913],[540,948],[138,935]],"score":0.6126},{"poly":[[1012,907],[1055,907],[1055,935],[1012,935]],"score":0.8742},{"poly":[[656,903],[755,903],[755,940],[656,940]],"score":0.7116},{"poly":[[548,893],[571,893],[571,917],[548,917]],"score":0.7831},{"poly":[[634,889],[663,889],[663,910],[634,910]],"score":0.832},{"poly":[[138,853],[753,851],[753,875],[138,877]],"score":0.784},{"poly":[[138,830],[1051,830],[1051,853],[138,853]],"score":0.8631},{"poly":[[586,791],[767,787],[768,817],[587,821]],"score":0.7118},{"poly":[[422,781],[580,781],[580,805],[422,805]],"score":0.8527},{"poly":[[1012,777],[1055,777],[1055,805],[1012,805]],"score":0.8926},{"poly":[[604,765],[746,761],[746,791],[605,795]],"score":0.7581},{"poly":[[136,733],[200,733],[200,758],[136,758]],"score":0.8517},{"poly":[[1012,696],[1056,696],[1056,724],[1012,724]],"score":0.8623},{"poly":[[551,670],[594,670],[594,696],[551,696]],"score":0.6893},{"poly":[[505,661],[528,661],[528,695],[505,695]],"score":0.777},{"poly":[[148,649],[497,660],[495,695],[147,684]],"score":0.6334},{"poly":[[613,647],[1016,649],[1015,686],[613,684]],"score":0.7071},{"poly":[[560,642],[585,642],[585,665],[560,665]],"score":0.8394},{"poly":[[503,640],[528,640],[528,667],[503,667]],"score":0.7478},{"poly":[[594,635],[620,635],[620,660],[594,660]],"score":0.7219},{"poly":[[138,600],[475,600],[475,623],[138,623]],"score":0.8172},{"poly":[[138,577],[1051,577],[1051,600],[138,600]],"score":0.803},{"poly":[[137,553],[1051,554],[1051,579],[136,577]],"score":0.7434},{"poly":[[138,512],[610,512],[610,535],[138,535]],"score":0.7959},{"poly":[[138,461],[519,461],[519,484],[138,484]],"score":0.827},{"poly":[[138,440],[1051,440],[1051,463],[138,463]],"score":0.85},{"poly":[[138,416],[1051,414],[1051,439],[138,440]],"score":0.7536},{"poly":[[213,391],[339,391],[339,416],[213,416]],"score":0.6845},{"poly":[[138,381],[218,381],[218,405],[138,405]],"score":0.8009},{"poly":[[326,377],[1051,379],[1051,403],[326,402]],"score":0.7253},{"poly":[[213,372],[337,376],[336,395],[212,391]],"score":0.728},{"poly":[[138,349],[1049,349],[1049,372],[138,372]],"score":0.8369},{"poly":[[138,326],[1051,326],[1051,351],[138,351]],"score":0.7249},{"poly":[[513,292],[543,298],[540,318],[510,313]],"score":0.715},{"poly":[[577,282],[796,288],[795,320],[576,314]],"score":0.6772},{"poly":[[491,281],[514,281],[514,310],[491,310]],"score":0.7577},{"poly":[[266,266],[485,280],[482,315],[263,301]],"score":0.628},{"poly":[[791,266],[986,272],[985,304],[790,298]],"score":0.7413},{"poly":[[1008,268],[1053,268],[1053,298],[1008,298]],"score":0.7862},{"poly":[[539,267],[574,267],[574,296],[539,296]],"score":0.6752},{"poly":[[640,254],[793,258],[792,288],[640,284]],"score":0.7542},{"poly":[[585,256],[606,256],[606,281],[585,281]],"score":0.8089},{"poly":[[617,254],[642,254],[642,277],[617,277]],"score":0.9063},{"poly":[[535,209],[611,209],[611,247],[535,247]],"score":0.6802},{"poly":[[1012,200],[1056,200],[1056,226],[1012,226]],"score":0.9319},{"poly":[[625,191],[729,195],[727,234],[623,229]],"score":0.6665},{"poly":[[569,188],[599,188],[599,210],[569,210]],"score":0.8062},{"poly":[[514,181],[546,181],[546,244],[514,244]],"score":0.6202},{"poly":[[606,182],[633,182],[633,203],[606,203]],"score":0.8582},{"poly":[[534,180],[571,176],[576,222],[539,225]],"score":0.6236},{"poly":[[140,142],[897,142],[897,167],[140,167]],"score":0.7402}],"page_no":3,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1580,"y1":1609},"conf":0.4482,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1582,"y1":1603},"font_size":0.0,"text":"5"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":1056,"y0":138,"y1":195},"conf":0.9286,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":142,"y1":165},"font_size":0.0,"text":"Note that GRPO necessitates the Routing Replay training strategy for the normal convergence of MoE"},{"bbox":{"x0":136,"x1":978,"y0":163,"y1":189},"font_size":0.0,"text":"RL, which we will additionally discuss in § 5.3, while GSPO has obviated the need for this strategy."}],"source":"layout det","text":"Note that GRPO necessitates the Routing Replay training strategy for the normal convergence of MoE RL, which we will additionally discuss in § 5.3, while GSPO has obviated the need for this strategy."},{"bbox":{"x0":132,"x1":1057,"y0":197,"y1":362},"conf":0.9703,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":200,"y1":223},"font_size":0.0,"text":"Figure 1 shows that the training with GSPO proceeds stably throughout. We observe that GSPO can"},{"bbox":{"x0":136,"x1":1051,"y0":219,"y1":246},"font_size":0.0,"text":"deliver continuous performance improvement through increasing the training compute, regularly"},{"bbox":{"x0":138,"x1":1051,"y0":244,"y1":267},"font_size":0.0,"text":"updating the query set, and extending the generation length. Moreover, GSPO also demonstrates"},{"bbox":{"x0":138,"x1":1051,"y0":265,"y1":290},"font_size":0.0,"text":"superior training effciency over GRPO, achieving better training accuracy and benchmark performancei"},{"bbox":{"x0":136,"x1":1051,"y0":284,"y1":311},"font_size":0.0,"text":"under the same training compute and consumed queries. Finally, we have successfully applied GSPO"},{"bbox":{"x0":138,"x1":1051,"y0":309,"y1":333},"font_size":0.0,"text":"to the RL training of the latest Qwen3 models, strongly proving the effcacy of GSPO in unleashing thei"},{"bbox":{"x0":135,"x1":560,"y0":328,"y1":356},"font_size":0.0,"text":"power of RL scaling for large language models."}],"source":"layout det","text":"Figure 1 shows that the training with GSPO proceeds stably throughout. We observe that GSPO can deliver continuous performance improvement through increasing the training compute, regularly updating the query set, and extending the generation length. Moreover, GSPO also demonstrates superior training effciency over GRPO, achieving better training accuracy and benchmark performancei under the same training compute and consumed queries. Finally, we have successfully applied GSPO to the RL training of the latest Qwen3 models, strongly proving the effcacy of GSPO in unleashing thei power of RL scaling for large language models."},{"bbox":{"x0":131,"x1":1057,"y0":373,"y1":903},"conf":0.9784,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![00560e354e92a5598c2f85cabb9b86ad](imgs/00560e354e92a5598c2f85cabb9b86ad.jpg)"},{"bbox":{"x0":131,"x1":1058,"y0":909,"y1":966},"conf":0.9375,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":140,"x1":1047,"y0":910,"y1":937},"font_size":0.0,"text":"Figure 1: Training curves of a cold-start model fne-tuned from Qwen3-30B-A3B-Base. GSPO possessesi"},{"bbox":{"x0":138,"x1":581,"y0":935,"y1":959},"font_size":0.0,"text":"remarkably higher training effciency than GRPO.i"}],"source":"layout det","text":"Figure 1: Training curves of a cold-start model fne-tuned from Qwen3-30B-A3B-Base. GSPO possessesi remarkably higher training effciency than GRPO.i"},{"bbox":{"x0":133,"x1":588,"y0":995,"y1":1029},"conf":0.888,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":583,"y0":995,"y1":1021},"font_size":0.0,"text":"5.2Curious Observation on Clipping Fractions"}],"source":"layout det","text":"5.2Curious Observation on Clipping Fractions"},{"bbox":{"x0":133,"x1":1057,"y0":1037,"y1":1247},"conf":0.9723,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":1040,"y1":1065},"font_size":0.0,"text":"A key distinction of GSPO compared to GRPO is its practice of clipping entire responses rather than"},{"bbox":{"x0":138,"x1":1051,"y0":1063,"y1":1086},"font_size":0.0,"text":"individual tokens. Particularly, as shown in Figure 2, we observe a difference of two orders of magnitude"},{"bbox":{"x0":138,"x1":1053,"y0":1086,"y1":1109},"font_size":0.0,"text":"in the fractions of clipped tokens between GSPO and GRPO (while adjusting the clipping ranges does not"},{"bbox":{"x0":138,"x1":1051,"y0":1107,"y1":1131},"font_size":0.0,"text":"alter the disparity in magnitude). However, despite clipping signifcantly more tokens and consequentlyi"},{"bbox":{"x0":136,"x1":1051,"y0":1126,"y1":1153},"font_size":0.0,"text":"using fewer for training (or gradient estimation), GSPO still achieves higher training effciency thani"},{"bbox":{"x0":136,"x1":1053,"y0":1147,"y1":1174},"font_size":0.0,"text":"GRPO. This counter-intuitive fnding — that clipping a much larger fraction of tokens leads to superiori"},{"bbox":{"x0":136,"x1":1051,"y0":1170,"y1":1196},"font_size":0.0,"text":"training effciency — further indicates that GRPO’s token-level gradient estimates are inherently noisyi"},{"bbox":{"x0":138,"x1":1051,"y0":1195,"y1":1218},"font_size":0.0,"text":"and ineffcient for sample exploitation. In contrast, GSPO’s sequence-level approach provides a morei"},{"bbox":{"x0":136,"x1":468,"y0":1214,"y1":1240},"font_size":0.0,"text":"reliable and effective learning signal."}],"source":"layout det","text":"A key distinction of GSPO compared to GRPO is its practice of clipping entire responses rather than individual tokens. Particularly, as shown in Figure 2, we observe a difference of two orders of magnitude in the fractions of clipped tokens between GSPO and GRPO (while adjusting the clipping ranges does not alter the disparity in magnitude). However, despite clipping signifcantly more tokens and consequentlyi using fewer for training (or gradient estimation), GSPO still achieves higher training effciency thani GRPO. This counter-intuitive fnding — that clipping a much larger fraction of tokens leads to superiori training effciency — further indicates that GRPO’s token-level gradient estimates are inherently noisyi and ineffcient for sample exploitation. In contrast, GSPO’s sequence-level approach provides a morei reliable and effective learning signal."},{"bbox":{"x0":181,"x1":1007,"y0":1257,"y1":1480},"conf":0.9651,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![aca9a28f05c10a38c729f15bef673a0e](imgs/aca9a28f05c10a38c729f15bef673a0e.jpg)"},{"bbox":{"x0":205,"x1":985,"y0":1489,"y1":1525},"conf":0.6419,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":209,"x1":977,"y0":1489,"y1":1517},"font_size":0.0,"text":"Figure 2: Average fractions of clipped tokens over the RL training of GSPO and GRPO."}],"source":"layout det","text":"Figure 2: Average fractions of clipped tokens over the RL training of GSPO and GRPO."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":131,"x1":1057,"y0":373,"y1":903},"conf":0.9784,"label":"Figure","label_id":3},{"bbox":{"x0":133,"x1":1057,"y0":1037,"y1":1247},"conf":0.9723,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":197,"y1":362},"conf":0.9703,"label":"Text","label_id":1},{"bbox":{"x0":181,"x1":1007,"y0":1257,"y1":1480},"conf":0.9651,"label":"Figure","label_id":3},{"bbox":{"x0":131,"x1":1058,"y0":909,"y1":966},"conf":0.9375,"label":"Figure caption","label_id":4},{"bbox":{"x0":133,"x1":1056,"y0":138,"y1":195},"conf":0.9286,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":588,"y0":995,"y1":1029},"conf":0.888,"label":"Title","label_id":0},{"bbox":{"x0":205,"x1":985,"y0":1489,"y1":1525},"conf":0.6419,"label":"Figure caption","label_id":4},{"bbox":{"x0":581,"x1":608,"y0":1580,"y1":1609},"conf":0.4482,"label":"Abandon","label_id":2},{"bbox":{"x0":583,"x1":605,"y0":1581,"y1":1608},"conf":0.2706,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1582],[603,1582],[603,1603],[587,1603]],"score":0.8479},{"poly":[[209,1493],[976,1489],[977,1514],[209,1517]],"score":0.7818},{"poly":[[553,1452],[695,1452],[695,1475],[553,1475]],"score":0.9033},{"poly":[[693,1428],[743,1428],[743,1454],[693,1454]],"score":0.9268},{"poly":[[599,1428],[647,1428],[647,1454],[599,1454]],"score":0.9562},{"poly":[[503,1428],[553,1428],[553,1454],[503,1454]],"score":0.9117},{"poly":[[409,1428],[459,1428],[459,1454],[409,1454]],"score":0.9239},{"poly":[[315,1428],[365,1428],[365,1454],[315,1454]],"score":0.9446},{"poly":[[222,1428],[271,1428],[271,1454],[222,1454]],"score":0.8941},{"poly":[[883,1426],[930,1426],[930,1454],[883,1454]],"score":0.8628},{"poly":[[787,1426],[837,1426],[837,1454],[787,1454]],"score":0.8614},{"poly":[[183,1368],[243,1368],[243,1395],[183,1395]],"score":0.869},{"poly":[[250,1366],[317,1366],[317,1393],[250,1393]],"score":0.831},{"poly":[[184,1296],[241,1296],[241,1321],[184,1321]],"score":0.9432},{"poly":[[955,1296],[996,1296],[996,1317],[955,1317]],"score":0.8713},{"poly":[[137,1214],[468,1216],[468,1240],[136,1238]],"score":0.7259},{"poly":[[138,1195],[1051,1195],[1051,1217],[138,1217]],"score":0.8405},{"poly":[[136,1170],[1051,1172],[1051,1196],[136,1195]],"score":0.7175},{"poly":[[137,1147],[1053,1149],[1053,1174],[136,1172]],"score":0.7457},{"poly":[[136,1128],[1051,1126],[1051,1151],[137,1153]],"score":0.6999},{"poly":[[138,1107],[1051,1107],[1051,1131],[138,1131]],"score":0.7109},{"poly":[[138,1086],[1053,1086],[1053,1109],[138,1109]],"score":0.8127},{"poly":[[138,1063],[1051,1063],[1051,1086],[138,1086]],"score":0.811},{"poly":[[138,1040],[1051,1040],[1051,1065],[138,1065]],"score":0.7222},{"poly":[[137,995],[583,996],[583,1021],[136,1019]],"score":0.7992},{"poly":[[138,935],[581,935],[581,958],[138,958]],"score":0.8344},{"poly":[[140,910],[1047,910],[1047,935],[140,935]],"score":0.7599},{"poly":[[861,872],[1007,872],[1007,895],[861,895]],"score":0.8266},{"poly":[[555,872],[700,872],[700,895],[555,895]],"score":0.8298},{"poly":[[248,872],[393,872],[393,895],[248,895]],"score":0.8292},{"poly":[[427,861],[436,870],[427,879],[418,870]],"score":0.6649},{"poly":[[484,846],[510,846],[510,867],[484,867]],"score":0.9055},{"poly":[[177,817],[206,817],[206,840],[177,840]],"score":0.8092},{"poly":[[769,814],[819,814],[819,837],[769,837]],"score":0.8188},{"poly":[[482,782],[509,782],[509,803],[482,803]],"score":0.8337},{"poly":[[771,767],[817,767],[817,786],[771,786]],"score":0.9401},{"poly":[[177,763],[204,763],[204,788],[177,788]],"score":0.787},{"poly":[[152,739],[177,739],[177,814],[152,814]],"score":0.8178},{"poly":[[746,726],[771,726],[771,828],[746,828]],"score":0.628},{"poly":[[482,719],[509,719],[509,740],[482,740]],"score":0.8944},{"poly":[[771,717],[817,717],[817,739],[771,739]],"score":0.7963},{"poly":[[461,712],[486,713],[482,844],[457,843]],"score":0.6807},{"poly":[[175,710],[207,710],[207,733],[175,733]],"score":0.7642},{"poly":[[555,647],[700,647],[700,670],[555,670]],"score":0.8405},{"poly":[[160,609],[207,609],[207,628],[160,628]],"score":0.9158},{"poly":[[160,560],[204,560],[204,581],[160,581]],"score":0.8702},{"poly":[[482,530],[500,530],[500,544],[482,544]],"score":0.6099},{"poly":[[158,514],[204,514],[204,535],[158,535]],"score":0.8434},{"poly":[[138,508],[167,509],[165,581],[136,580]],"score":0.6449},{"poly":[[596,503],[611,503],[611,514],[596,514]],"score":0.7821},{"poly":[[549,498],[569,498],[569,514],[549,514]],"score":0.6019},{"poly":[[638,486],[654,486],[654,500],[638,500]],"score":0.6238},{"poly":[[160,467],[204,467],[204,488],[160,488]],"score":0.9452},{"poly":[[138,447],[163,447],[163,519],[138,519]],"score":0.6683},{"poly":[[161,421],[207,421],[207,442],[161,442]],"score":0.8577},{"poly":[[262,414],[475,416],[475,440],[262,438]],"score":0.7674},{"poly":[[264,395],[340,395],[340,414],[264,414]],"score":0.9266},{"poly":[[135,332],[560,328],[560,353],[135,356]],"score":0.6937},{"poly":[[138,309],[1051,309],[1051,333],[138,333]],"score":0.7039},{"poly":[[136,286],[1051,284],[1051,309],[136,311]],"score":0.7422},{"poly":[[138,265],[1051,265],[1051,288],[138,288]],"score":0.8223},{"poly":[[138,244],[1051,244],[1051,267],[138,267]],"score":0.8184},{"poly":[[137,219],[1051,221],[1051,246],[136,244]],"score":0.7576},{"poly":[[138,200],[1051,200],[1051,223],[138,223]],"score":0.8164},{"poly":[[136,163],[978,165],[978,189],[136,188]],"score":0.7623},{"poly":[[140,142],[1049,142],[1049,165],[140,165]],"score":0.8041}],"page_no":4,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":582,"x1":607,"y0":1581,"y1":1609},"conf":0.4964,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1586,"y1":1603},"font_size":0.0,"text":"6"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":134,"x1":507,"y0":140,"y1":173},"conf":0.8928,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":136,"x1":502,"y0":142,"y1":168},"font_size":0.0,"text":"5.3Beneft of GSPO for MoE Trainingi"}],"source":"layout det","text":"5.3Beneft of GSPO for MoE Trainingi"},{"bbox":{"x0":134,"x1":1060,"y0":181,"y1":435},"conf":0.9745,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":184,"y1":209},"font_size":0.0,"text":"BackgroundCompared to the RL training of dense models, the sparse activation nature of MoE models"},{"bbox":{"x0":138,"x1":1051,"y0":207,"y1":232},"font_size":0.0,"text":"introduces unique stability challenges. In particular, we found that when adopting the GRPO algorithm,"},{"bbox":{"x0":140,"x1":1049,"y0":230,"y1":254},"font_size":0.0,"text":"the expert-activation volatility of MoE models can prevent RL training from converging properly. To be"},{"bbox":{"x0":140,"x1":1049,"y0":253,"y1":275},"font_size":0.0,"text":"specifc, after one or more gradient updates, the experts activated for the same response can changei"},{"bbox":{"x0":136,"x1":1049,"y0":272,"y1":298},"font_size":0.0,"text":"signifcantly. For example, with the 48-layer Qwen3-30B-A3B-Base model, after each RL gradient updatei"},{"bbox":{"x0":140,"x1":1049,"y0":296,"y1":319},"font_size":0.0,"text":"and for the same rollout sample, there are roughly $10\\%$ of the experts activated under the new policy"},{"bbox":{"x0":140,"x1":1051,"y0":316,"y1":346},"font_size":0.0,"text":"$\\pi_{\\theta}$ that are different from those under the old policy $\\pi_{\\theta_{\\rm old}}.$  This phenomenon, which becomes more"},{"bbox":{"x0":140,"x1":1047,"y0":344,"y1":385},"font_size":0.0,"text":"prominent in deeper MoE models, makes the token-level importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$"},{"bbox":{"x0":140,"x1":1051,"y0":382,"y1":407},"font_size":0.0,"text":"fuctuate drastically and further invalidates them, as discussed in § 3 and 4.2, consequently hindering thel"},{"bbox":{"x0":136,"x1":456,"y0":403,"y1":430},"font_size":0.0,"text":"normal convergence of RL training."}],"source":"layout det","text":"BackgroundCompared to the RL training of dense models, the sparse activation nature of MoE models introduces unique stability challenges. In particular, we found that when adopting the GRPO algorithm,the expert-activation volatility of MoE models can prevent RL training from converging properly. To be specifc, after one or more gradient updates, the experts activated for the same response can changei signifcantly. For example, with the 48-layer Qwen3-30B-A3B-Base model, after each RL gradient updatei and for the same rollout sample, there are roughly $10\\%$ of the experts activated under the new policy $\\pi_{\\theta}$ that are different from those under the old policy $\\pi_{\\theta_{\\rm old}}.$  This phenomenon, which becomes more  prominent in deeper MoE models, makes the token-level importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$ fuctuate drastically and further invalidates them, as discussed in § 3 and 4.2, consequently hindering thel normal convergence of RL training."},{"bbox":{"x0":133,"x1":1059,"y0":448,"y1":640},"conf":0.9718,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1047,"y0":453,"y1":475},"font_size":0.0,"text":"Our Previous ApproachTo tackle this challenge, we previously employed the Routing Replay training"},{"bbox":{"x0":138,"x1":1051,"y0":477,"y1":502},"font_size":0.0,"text":"strategy. Specifcally, we cache the activated experts ini $\\pi_{\\theta_{\\mathrm{old}}}$ and “replay” these routing modes in $\\pi_{\\theta}$  when"},{"bbox":{"x0":141,"x1":1049,"y0":502,"y1":544},"font_size":0.0,"text":"computing the importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$ In this way, for each token $y_{i,t},\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})$"},{"bbox":{"x0":136,"x1":1053,"y0":537,"y1":570},"font_size":0.0,"text":"and $\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})$  share the same activateθdol dneit,twori,kt, so that we can restore the stability of the token-"},{"bbox":{"x0":138,"x1":1051,"y0":565,"y1":591},"font_size":0.0,"text":"level importance ratios and ensure optimization of the consistent activated network across gradient"},{"bbox":{"x0":140,"x1":1051,"y0":589,"y1":612},"font_size":0.0,"text":"updates. Figure 3 demonstrates that Routing Replay serves as an essential technique in the normal"},{"bbox":{"x0":138,"x1":588,"y0":610,"y1":633},"font_size":0.0,"text":"convergence of the GRPO training of MoE models."}],"source":"layout det","text":"Our Previous ApproachTo tackle this challenge, we previously employed the Routing Replay training strategy. Specifcally, we cache the activated experts ini $\\pi_{\\theta_{\\mathrm{old}}}$ and “replay” these routing modes in $\\pi_{\\theta}$  when computing the importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$ In this way, for each token $y_{i,t},\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})$ and $\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})$  share the same activateθdol dneit,twori,kt, so that we can restore the stability of the tokenlevel importance ratios and ensure optimization of the consistent activated network across gradient updates. Figure 3 demonstrates that Routing Replay serves as an essential technique in the normal convergence of the GRPO training of MoE models."},{"bbox":{"x0":134,"x1":1059,"y0":649,"y1":954},"conf":0.9755,"font_size":0.0,"label":"Figure","label_id":3,"lines":[],"source":"layout det","text":"![7445fe26b3bb73c78a0596cc866330ad](imgs/7445fe26b3bb73c78a0596cc866330ad.jpg)"},{"bbox":{"x0":132,"x1":1056,"y0":962,"y1":1014},"conf":0.924,"font_size":0.0,"label":"Figure caption","label_id":4,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":959,"y1":993},"font_size":0.0,"text":"Figure 3: The Routing Replay strategy plays a critical role in the normal convergence of the GRPO training"},{"bbox":{"x0":140,"x1":284,"y0":988,"y1":1010},"font_size":0.0,"text":"of MoE models."}],"source":"layout det","text":"Figure 3: The Routing Replay strategy plays a critical role in the normal convergence of the GRPO training of MoE models."},{"bbox":{"x0":133,"x1":1058,"y0":1044,"y1":1296},"conf":0.9752,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1051,"y0":1044,"y1":1075},"font_size":0.0,"text":"Beneft of GSPOAlthough Routing Replay enables the GRPO training of MoE models to convergei"},{"bbox":{"x0":140,"x1":1049,"y0":1072,"y1":1095},"font_size":0.0,"text":"properly, its practice of reusing routing modes incurs additional memory and communication overhead"},{"bbox":{"x0":138,"x1":1049,"y0":1093,"y1":1116},"font_size":0.0,"text":"and can also limit the actual capacity of the MoE model. In contrast, as shown in Figure 1, GSPO"},{"bbox":{"x0":138,"x1":1051,"y0":1114,"y1":1137},"font_size":0.0,"text":"eliminates the dependency on Routing Replay and is fully capable of computing the importance ratios"},{"bbox":{"x0":139,"x1":1049,"y0":1137,"y1":1161},"font_size":0.0,"text":"$s_{i}(\\theta)$  conventionally, converging normally, and optimizing stably. The key insight is that GSPO focuses"},{"bbox":{"x0":138,"x1":1053,"y0":1158,"y1":1182},"font_size":0.0,"text":"only on the sequence likelihood (i.e., $\\pi_{\\theta}(\\widetilde{y_{i}}|x))$  and is not sensitive to the individual token likelihood (i.e.,"},{"bbox":{"x0":140,"x1":1051,"y0":1180,"y1":1205},"font_size":0.0,"text":"$\\pi_{\\theta}\\big(^{\\prime}y_{i,t}|x,y_{i,<t}\\big)\\widehat{)}.$  Since the MoE model always maintains its language modeling capability, the sequence"},{"bbox":{"x0":140,"x1":1049,"y0":1202,"y1":1225},"font_size":0.0,"text":"likelihood will not fuctuate drastically. In summary, GSPO fundamentally resolves the expert-activationl"},{"bbox":{"x0":136,"x1":1049,"y0":1221,"y1":1247},"font_size":0.0,"text":"volatility issue in MoE models, obviating the need for complex workarounds like Routing Replay. This"},{"bbox":{"x0":138,"x1":1049,"y0":1245,"y1":1270},"font_size":0.0,"text":"not only simplifes and stabilizes the training process but also allows the model to leverage its fulli"},{"bbox":{"x0":140,"x1":475,"y0":1268,"y1":1291},"font_size":0.0,"text":"capacity without artifcial constraints.i"}],"source":"layout det","text":"Beneft of GSPOAlthough Routing Replay enables the GRPO training of MoE models to convergei properly, its practice of reusing routing modes incurs additional memory and communication overhead and can also limit the actual capacity of the MoE model. In contrast, as shown in Figure 1, GSPO eliminates the dependency on Routing Replay and is fully capable of computing the importance ratios $s_{i}(\\theta)$  conventionally, converging normally, and optimizing stably. The key insight is that GSPO focuses only on the sequence likelihood (i.e., $\\pi_{\\theta}(\\widetilde{y_{i}}|x))$  and is not sensitive to the individual token likelihood (i.e., $\\pi_{\\theta}\\big(^{\\prime}y_{i,t}|x,y_{i,<t}\\big)\\widehat{)}.$  Since the MoE model always maintains its language modeling capability, the sequence likelihood will not fuctuate drastically. In summary, GSPO fundamentally resolves the expert-activationl volatility issue in MoE models, obviating the need for complex workarounds like Routing Replay. This not only simplifes and stabilizes the training process but also allows the model to leverage its fulli capacity without artifcial constraints.i"},{"bbox":{"x0":133,"x1":538,"y0":1316,"y1":1346},"conf":0.9,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":533,"y0":1319,"y1":1343},"font_size":0.0,"text":"5.4Beneft of GSPO for RL Infrastructurei"}],"source":"layout det","text":"5.4Beneft of GSPO for RL Infrastructurei"},{"bbox":{"x0":133,"x1":1058,"y0":1357,"y1":1549},"conf":0.9725,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":136,"x1":1053,"y0":1356,"y1":1389},"font_size":0.0,"text":"Given the precision discrepancies between training engines (e.g., Megatron) and inference engines (e.g.,"},{"bbox":{"x0":140,"x1":1049,"y0":1384,"y1":1407},"font_size":0.0,"text":"SGLang and vLLM), in practice, we typically use the training engine to recompute the likelihoods of"},{"bbox":{"x0":138,"x1":1049,"y0":1407,"y1":1435},"font_size":0.0,"text":"sampled responses under the old policy $\\pi_{\\theta_{\\mathrm{old}}}.$  However, GSPO uses only sequence-level, rather than"},{"bbox":{"x0":138,"x1":1047,"y0":1430,"y1":1452},"font_size":0.0,"text":"token-level, likelihoods for optimization, and intuitively, the former is much more tolerant of precision"},{"bbox":{"x0":140,"x1":1049,"y0":1452,"y1":1475},"font_size":0.0,"text":"discrepancies. Hence, GSPO makes it possible to directly use the likelihoods returned by the inference"},{"bbox":{"x0":140,"x1":1049,"y0":1474,"y1":1498},"font_size":0.0,"text":"engine for optimization, thereby avoiding the need for recomputation with the training engine. This can"},{"bbox":{"x0":140,"x1":1049,"y0":1496,"y1":1519},"font_size":0.0,"text":"be especially benefcial in scenarios like partial rollout and multi-turn RL and in the training-inferencei"},{"bbox":{"x0":138,"x1":385,"y0":1516,"y1":1542},"font_size":0.0,"text":"disaggregated frameworks."}],"source":"layout det","text":"Given the precision discrepancies between training engines (e.g., Megatron) and inference engines (e.g.,SGLang and vLLM), in practice, we typically use the training engine to recompute the likelihoods of sampled responses under the old policy $\\pi_{\\theta_{\\mathrm{old}}}.$  However, GSPO uses only sequence-level, rather than token-level, likelihoods for optimization, and intuitively, the former is much more tolerant of precision discrepancies. Hence, GSPO makes it possible to directly use the likelihoods returned by the inference engine for optimization, thereby avoiding the need for recomputation with the training engine. This can be especially benefcial in scenarios like partial rollout and multi-turn RL and in the training-inferencei disaggregated frameworks."}],"formula_dets":[{"bbox":{"x0":435,"x1":645,"y0":502,"y1":544},"conf":0.9187,"label":"print_embedding","label_id":0},{"bbox":{"x0":836,"x1":1047,"y0":344,"y1":385},"conf":0.9162,"label":"print_embedding","label_id":0},{"bbox":{"x0":139,"x1":181,"y0":1139,"y1":1161},"conf":0.9048,"label":"print_embedding","label_id":0},{"bbox":{"x0":179,"x1":325,"y0":541,"y1":570},"conf":0.8943,"label":"print_embedding","label_id":0},{"bbox":{"x0":140,"x1":280,"y0":1180,"y1":1205},"conf":0.8864,"label":"print_embedding","label_id":0},{"bbox":{"x0":885,"x1":1049,"y0":508,"y1":535},"conf":0.883,"label":"print_embedding","label_id":0},{"bbox":{"x0":462,"x1":541,"y0":1159,"y1":1182},"conf":0.8752,"label":"print_embedding","label_id":0},{"bbox":{"x0":610,"x1":650,"y0":483,"y1":502},"conf":0.8625,"label":"print_embedding","label_id":0},{"bbox":{"x0":597,"x1":637,"y0":296,"y1":316},"conf":0.8188,"label":"print_embedding","label_id":0},{"bbox":{"x0":140,"x1":163,"y0":326,"y1":341},"conf":0.7978,"label":"print_embedding","label_id":0},{"bbox":{"x0":972,"x1":994,"y0":484,"y1":500},"conf":0.7794,"label":"print_embedding","label_id":0},{"bbox":{"x0":506,"x1":552,"y0":1412,"y1":1435},"conf":0.7691,"label":"print_embedding","label_id":0},{"bbox":{"x0":624,"x1":670,"y0":324,"y1":345},"conf":0.7606,"label":"print_embedding","label_id":0}],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1059,"y0":649,"y1":954},"conf":0.9755,"label":"Figure","label_id":3},{"bbox":{"x0":133,"x1":1058,"y0":1044,"y1":1296},"conf":0.9752,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":1060,"y0":181,"y1":435},"conf":0.9745,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1058,"y0":1357,"y1":1549},"conf":0.9725,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1059,"y0":448,"y1":640},"conf":0.9718,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1056,"y0":962,"y1":1014},"conf":0.924,"label":"Figure caption","label_id":4},{"bbox":{"x0":133,"x1":538,"y0":1316,"y1":1346},"conf":0.9,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":507,"y0":140,"y1":173},"conf":0.8928,"label":"Title","label_id":0},{"bbox":{"x0":582,"x1":607,"y0":1581,"y1":1609},"conf":0.4964,"label":"Abandon","label_id":2},{"bbox":{"x0":584,"x1":605,"y0":1583,"y1":1607},"conf":0.2462,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1586],[603,1586],[603,1603],[587,1603]],"score":0.8189},{"poly":[[138,1516],[385,1517],[384,1542],[138,1540]],"score":0.7355},{"poly":[[140,1496],[1049,1496],[1049,1519],[140,1519]],"score":0.8232},{"poly":[[140,1474],[1049,1474],[1049,1498],[140,1498]],"score":0.6794},{"poly":[[140,1452],[1049,1452],[1049,1475],[140,1475]],"score":0.8152},{"poly":[[138,1430],[1047,1430],[1047,1452],[138,1452]],"score":0.7969},{"poly":[[138,1407],[1049,1407],[1049,1431],[138,1431]],"score":0.711},{"poly":[[140,1384],[1049,1384],[1049,1407],[140,1407]],"score":0.8163},{"poly":[[137,1356],[1053,1360],[1053,1389],[136,1386]],"score":0.6428},{"poly":[[138,1319],[533,1319],[533,1342],[138,1342]],"score":0.819},{"poly":[[140,1268],[475,1268],[475,1291],[140,1291]],"score":0.8421},{"poly":[[138,1245],[1049,1245],[1049,1270],[138,1270]],"score":0.7225},{"poly":[[137,1221],[1049,1223],[1049,1247],[136,1245]],"score":0.7535},{"poly":[[140,1202],[1049,1202],[1049,1224],[140,1224]],"score":0.7988},{"poly":[[140,1181],[1051,1181],[1051,1203],[140,1203]],"score":0.7781},{"poly":[[138,1158],[1053,1158],[1053,1182],[138,1182]],"score":0.6917},{"poly":[[138,1137],[1049,1137],[1049,1160],[138,1160]],"score":0.8275},{"poly":[[138,1114],[1051,1114],[1051,1137],[138,1137]],"score":0.8021},{"poly":[[138,1093],[1049,1093],[1049,1116],[138,1116]],"score":0.7417},{"poly":[[140,1072],[1049,1072],[1049,1095],[140,1095]],"score":0.8256},{"poly":[[137,1044],[1051,1046],[1051,1075],[136,1074]],"score":0.6792},{"poly":[[140,988],[284,988],[284,1010],[140,1010]],"score":0.8726},{"poly":[[137,959],[1051,963],[1051,993],[136,989]],"score":0.6837},{"poly":[[555,926],[698,926],[698,949],[555,949]],"score":0.8651},{"poly":[[161,891],[202,891],[202,912],[161,912]],"score":0.9221},{"poly":[[161,858],[204,858],[204,877],[161,877]],"score":0.9415},{"poly":[[161,823],[202,823],[202,844],[161,844]],"score":0.859},{"poly":[[161,753],[202,753],[202,774],[161,774]],"score":0.8525},{"poly":[[490,733],[589,723],[593,767],[494,777]],"score":0.6349},{"poly":[[142,728],[160,728],[160,791],[142,791]],"score":0.8003},{"poly":[[161,719],[206,719],[206,740],[161,740]],"score":0.8674},{"poly":[[266,696],[473,696],[473,719],[266,719]],"score":0.8405},{"poly":[[161,684],[204,684],[204,705],[161,705]],"score":0.9175},{"poly":[[561,706],[1017,656],[1025,727],[569,778]],"score":0.732},{"poly":[[266,670],[464,672],[464,695],[266,693]],"score":0.8287},{"poly":[[138,610],[588,610],[588,633],[138,633]],"score":0.804},{"poly":[[140,589],[1051,589],[1051,612],[140,612]],"score":0.7872},{"poly":[[138,565],[1051,567],[1051,591],[138,589]],"score":0.7604},{"poly":[[136,540],[1053,537],[1053,567],[137,570]],"score":0.656},{"poly":[[138,477],[1051,477],[1051,502],[138,502]],"score":0.7072},{"poly":[[140,453],[1047,453],[1047,475],[140,475]],"score":0.8051},{"poly":[[137,403],[456,405],[455,430],[136,428]],"score":0.7363},{"poly":[[140,382],[1051,382],[1051,407],[140,407]],"score":0.7444},{"poly":[[922,359],[1048,363],[1047,388],[921,384]],"score":0.7223},{"poly":[[140,349],[927,349],[927,374],[140,374]],"score":0.7166},{"poly":[[914,342],[1042,338],[1042,363],[915,367]],"score":0.6536},{"poly":[[138,316],[1051,316],[1051,346],[138,346]],"score":0.6472},{"poly":[[140,296],[1049,296],[1049,319],[140,319]],"score":0.8256},{"poly":[[136,274],[1049,272],[1049,296],[137,298]],"score":0.7363},{"poly":[[140,253],[1049,253],[1049,275],[140,275]],"score":0.7818},{"poly":[[140,230],[1049,230],[1049,254],[140,254]],"score":0.6761},{"poly":[[138,207],[1051,207],[1051,232],[138,232]],"score":0.7093},{"poly":[[140,184],[1049,184],[1049,209],[140,209]],"score":0.7177},{"poly":[[137,142],[502,144],[501,168],[136,167]],"score":0.7499}],"page_no":5,"scale":2.0,"width":595},{"abandon_blocks":[{"bbox":{"x0":581,"x1":608,"y0":1580,"y1":1608},"conf":0.5925,"font_size":0.0,"label":"Abandon","label_id":2,"lines":[{"bbox":{"x0":587,"x1":603,"y0":1582,"y1":1602},"font_size":0.0,"text":"7"}],"source":"layout det","text":""}],"blocks":[{"bbox":{"x0":133,"x1":309,"y0":136,"y1":171},"conf":0.8968,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":303,"y0":140,"y1":165},"font_size":0.0,"text":"6Conclusion"}],"source":"layout det","text":"6Conclusion"},{"bbox":{"x0":134,"x1":1059,"y0":187,"y1":378},"conf":0.9737,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1047,"y0":191,"y1":216},"font_size":0.0,"text":"We propose Group Sequence Policy Optimization (GSPO), a new reinforcement learning algorithm"},{"bbox":{"x0":138,"x1":1049,"y0":214,"y1":239},"font_size":0.0,"text":"for training large language models. Following the basic principle of importance sampling, GSPO"},{"bbox":{"x0":138,"x1":1049,"y0":235,"y1":260},"font_size":0.0,"text":"defnes importance ratios based on sequence likelihood and performs sequence-level clipping, rewarding,i"},{"bbox":{"x0":138,"x1":1051,"y0":258,"y1":282},"font_size":0.0,"text":"and optimization. GSPO demonstrates notably superior training stability, effciency, and performancei"},{"bbox":{"x0":136,"x1":1051,"y0":275,"y1":307},"font_size":0.0,"text":"compared to GRPO and exhibits particular effcacy for the large-scale RL training of MoE models, layingi"},{"bbox":{"x0":138,"x1":1047,"y0":300,"y1":323},"font_size":0.0,"text":"the foundation for the exceptional improvements in the latest Qwen3 models. With GSPO as a scalable"},{"bbox":{"x0":138,"x1":1049,"y0":325,"y1":347},"font_size":0.0,"text":"algorithmic cornerstone, we will continue to scale RL and look forward to the resulting fundamental"},{"bbox":{"x0":138,"x1":360,"y0":347,"y1":370},"font_size":0.0,"text":"advances in intelligence."}],"source":"layout det","text":"We propose Group Sequence Policy Optimization (GSPO), a new reinforcement learning algorithm for training large language models. Following the basic principle of importance sampling, GSPO defnes importance ratios based on sequence likelihood and performs sequence-level clipping, rewarding,i and optimization. GSPO demonstrates notably superior training stability, effciency, and performancei compared to GRPO and exhibits particular effcacy for the large-scale RL training of MoE models, layingi the foundation for the exceptional improvements in the latest Qwen3 models. With GSPO as a scalable algorithmic cornerstone, we will continue to scale RL and look forward to the resulting fundamental advances in intelligence."},{"bbox":{"x0":134,"x1":268,"y0":398,"y1":433},"conf":0.9008,"font_size":0.0,"label":"Title","label_id":0,"lines":[{"bbox":{"x0":138,"x1":264,"y0":401,"y1":430},"font_size":0.0,"text":"References"}],"source":"layout det","text":"References"},{"bbox":{"x0":133,"x1":1056,"y0":437,"y1":493},"conf":0.9276,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":437,"y1":468},"font_size":0.0,"text":"DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv"},{"bbox":{"x0":156,"x1":436,"y0":459,"y1":490},"font_size":0.0,"text":"preprint arXiv:2501.12948, 2025."}],"source":"layout det","text":"DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025."},{"bbox":{"x0":132,"x1":1058,"y0":497,"y1":551},"conf":0.8503,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1049,"y0":502,"y1":526},"font_size":0.0,"text":"MiniMax. Minimax-m1: Scaling test-time compute effciently with lightning attention. arXiv preprinti"},{"bbox":{"x0":154,"x1":362,"y0":521,"y1":544},"font_size":0.0,"text":"arXiv:2506.13585, 2025."}],"source":"layout det","text":"MiniMax. Minimax-m1: Scaling test-time compute effciently with lightning attention. arXiv preprinti arXiv:2506.13585, 2025."},{"bbox":{"x0":133,"x1":1057,"y0":556,"y1":611},"conf":0.9233,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1051,"y0":558,"y1":584},"font_size":0.0,"text":"OpenAI.LearningtoreasonwithLLMs,2024.URLhttps://openai.com/index/"},{"bbox":{"x0":158,"x1":475,"y0":581,"y1":607},"font_size":0.0,"text":"learning-to-reason-with-llms/."}],"source":"layout det","text":"OpenAI.LearningtoreasonwithLLMs,2024.URLhttps://openai.com/index/learning-to-reason-with-llms/."},{"bbox":{"x0":134,"x1":818,"y0":616,"y1":650},"conf":0.8961,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":142,"x1":810,"y0":621,"y1":644},"font_size":0.0,"text":"Team Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a."}],"source":"layout det","text":"Team Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a."},{"bbox":{"x0":131,"x1":1058,"y0":655,"y1":710},"conf":0.9014,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1053,"y0":658,"y1":682},"font_size":0.0,"text":"Team Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https:"},{"bbox":{"x0":158,"x1":502,"y0":679,"y1":705},"font_size":0.0,"text":"//qwenlm.github.io/blog/qwq-32b/."}],"source":"layout det","text":"Team Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/blog/qwq-32b/."},{"bbox":{"x0":132,"x1":1057,"y0":714,"y1":771},"conf":0.9489,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":135,"x1":1051,"y0":714,"y1":746},"font_size":0.0,"text":"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy"},{"bbox":{"x0":156,"x1":713,"y0":738,"y1":765},"font_size":0.0,"text":"optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."}],"source":"layout det","text":"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."},{"bbox":{"x0":133,"x1":1057,"y0":775,"y1":851},"conf":0.9525,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":138,"x1":1049,"y0":775,"y1":803},"font_size":0.0,"text":"Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan"},{"bbox":{"x0":158,"x1":1049,"y0":798,"y1":824},"font_size":0.0,"text":"Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in"},{"bbox":{"x0":158,"x1":698,"y0":819,"y1":847},"font_size":0.0,"text":"open language models. arXiv preprint arXiv:2402.03300, 2024."}],"source":"layout det","text":"Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024."},{"bbox":{"x0":132,"x1":1058,"y0":856,"y1":933},"conf":0.9484,"font_size":0.0,"label":"Text","label_id":1,"lines":[{"bbox":{"x0":140,"x1":1051,"y0":860,"y1":882},"font_size":0.0,"text":"Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence"},{"bbox":{"x0":158,"x1":1053,"y0":882,"y1":905},"font_size":0.0,"text":"likelihood contrastive learning. In Findings of the Association for Computational Linguistics: ACL 2023,"},{"bbox":{"x0":160,"x1":748,"y0":903,"y1":928},"font_size":0.0,"text":"2023. URL https://aclanthology.org/2023.findings-acl.65/."}],"source":"layout det","text":"Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of the Association for Computational Linguistics: ACL 2023,2023. URL https://aclanthology.org/2023.findings-acl.65/."}],"formula_dets":[],"height":841,"layout_dets":[{"bbox":{"x0":134,"x1":1059,"y0":187,"y1":378},"conf":0.9737,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":775,"y1":851},"conf":0.9525,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1057,"y0":714,"y1":771},"conf":0.9489,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":856,"y1":933},"conf":0.9484,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1056,"y0":437,"y1":493},"conf":0.9276,"label":"Text","label_id":1},{"bbox":{"x0":133,"x1":1057,"y0":556,"y1":611},"conf":0.9233,"label":"Text","label_id":1},{"bbox":{"x0":131,"x1":1058,"y0":655,"y1":710},"conf":0.9014,"label":"Text","label_id":1},{"bbox":{"x0":134,"x1":268,"y0":398,"y1":433},"conf":0.9008,"label":"Title","label_id":0},{"bbox":{"x0":133,"x1":309,"y0":136,"y1":171},"conf":0.8968,"label":"Title","label_id":0},{"bbox":{"x0":134,"x1":818,"y0":616,"y1":650},"conf":0.8961,"label":"Text","label_id":1},{"bbox":{"x0":132,"x1":1058,"y0":497,"y1":551},"conf":0.8503,"label":"Text","label_id":1},{"bbox":{"x0":581,"x1":608,"y0":1580,"y1":1608},"conf":0.5925,"label":"Abandon","label_id":2}],"ocr_all":false,"ocr_dets":[{"poly":[[587,1582],[603,1582],[603,1602],[587,1602]],"score":0.7503},{"poly":[[160,903],[748,903],[748,928],[160,928]],"score":0.7254},{"poly":[[158,882],[1053,882],[1053,905],[158,905]],"score":0.851},{"poly":[[140,860],[1051,860],[1051,882],[140,882]],"score":0.8449},{"poly":[[158,823],[698,819],[698,844],[158,847]],"score":0.7257},{"poly":[[158,798],[1049,800],[1049,824],[158,823]],"score":0.7339},{"poly":[[138,775],[1049,777],[1049,803],[138,802]],"score":0.6819},{"poly":[[156,740],[712,738],[713,763],[156,765]],"score":0.8089},{"poly":[[135,714],[1051,716],[1051,746],[135,744]],"score":0.6797},{"poly":[[158,681],[501,679],[502,703],[158,705]],"score":0.774},{"poly":[[140,658],[1053,658],[1053,682],[140,682]],"score":0.7373},{"poly":[[142,621],[810,621],[810,644],[142,644]],"score":0.8424},{"poly":[[158,582],[475,581],[475,605],[158,607]],"score":0.7735},{"poly":[[720,558],[1051,560],[1051,584],[720,582]],"score":0.789},{"poly":[[266,560],[681,558],[681,582],[266,584]],"score":0.7638},{"poly":[[138,560],[222,560],[222,584],[138,584]],"score":0.8233},{"poly":[[154,521],[362,521],[362,544],[154,544]],"score":0.663},{"poly":[[140,502],[1049,502],[1049,526],[140,526]],"score":0.748},{"poly":[[156,465],[436,459],[436,484],[156,490]],"score":0.6913},{"poly":[[135,437],[1051,439],[1051,468],[135,467]],"score":0.6978},{"poly":[[139,401],[264,405],[264,430],[138,426]],"score":0.8276},{"poly":[[138,347],[360,347],[360,370],[138,370]],"score":0.8247},{"poly":[[138,325],[1049,325],[1049,347],[138,347]],"score":0.8315},{"poly":[[138,300],[1047,300],[1047,323],[138,323]],"score":0.7755},{"poly":[[137,275],[1051,277],[1051,307],[136,305]],"score":0.6495},{"poly":[[138,258],[1051,258],[1051,281],[138,281]],"score":0.8084},{"poly":[[138,235],[1049,235],[1049,260],[138,260]],"score":0.6969},{"poly":[[138,214],[1049,214],[1049,239],[138,239]],"score":0.7331},{"poly":[[138,191],[1047,191],[1047,216],[138,216]],"score":0.7414},{"poly":[[138,140],[303,140],[303,165],[138,165]],"score":0.7772}],"page_no":6,"scale":2.0,"width":595}],"pages_success_ratio":0.0,"src_path":"oss://glm-data-ocr-data/services/maas/docs/e238704f-b715-4d59-9ab4-893b6eb74c67","text":"Group Sequence Policy Optimization\nChujie Zheng∗Shixuan LiuMingze LiXiong-Hui ChenBowen Yu∗Chang GaoKai DangYuqiong LiuRui MenAn YangJingren Zhou Junyang Lin\nQwen Team, Alibaba Inc.\nAbstract\nThis paper introduces Group Sequence Policy Optimization (GSPO), our stable, effcient,i and performant reinforcement learning algorithm for training large language models.Unlike previous algorithms that adopt token-level importance ratios, GSPO defnes thei importance ratio based on sequence likelihood and performs sequence-level clipping,rewarding, and optimization. We demonstrate that GSPO achieves superior training effciency and performance compared to the GRPO algorithm, notably stabilizes Mixture-i of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.\n1Introduction\nReinforcement learning (RL) has emerged as a pivotal paradigm for scaling language models (OpenAI,2024; DeepSeek-AI, 2025; Qwen, 2025b;a). Through large-scale RL, language models develop the capability to tackle sophisticated problems, such as competition-level mathematics and programming, by undertaking deeper and longer reasoning processes.\nTo successfully scale RL with greater computational investment, the foremost prerequisite is maintaining stable and robust training dynamics. However, current state-of-the-art RL algorithms, exemplifed byi GRPO (Shao et al., 2024), exhibit severe stability issues when training gigantic language models, often resulting in catastrophic and irreversible model collapse (Qwen, 2025a; MiniMax, 2025). This instability hinders efforts to push the capability boundaries of language models through continued RL training.\nIn this paper, we identify that the instability of GRPO stems from the fundamental misapplication and invalidation of importance sampling weights in its algorithmic design. This introduces high-variance training noise that progressively accumulates with increased response length and is further amplifed byi the clipping mechanism, ultimately precipitating model collapse.\nTo address these core limitations, we propose Group Sequence Policy Optimization (GSPO), a new RL algorithm for training large language models. The key innovation of GSPO lies in its theoretically grounded defnition of importance ratio based on sequence likelihood (Zheng et al., 2023), aligning withi the basic principle of importance sampling. Additionally, GSPO computes the normalized rewards as the advantages of multiple responses to a query, ensuring the alignment between sequence-level rewarding and optimization.\nOur empirical evaluation demonstrates the signifcant superiority of GSPO over GRPO in trainingi stability, effciency, and performance. Critically, GSPO has inherently resolved the stability challenges ini the RL training of large Mixture-of-Experts (MoE) models, eliminating the need for complex stabilization strategies, and shows the potential for simplifying RL infrastructure. These merits of GSPO ultimately contributed to the exceptional performance improvements in the latest Qwen3 models. We envision GSPO as a robust and scalable algorithmic foundation that will enable the continued advancement of large-scale RL training with language models.\n2Preliminaries\nNotationIn this paper, an autoregressive language model parameterized by $\\theta$ is defned as a policy πθ.i We use x to denote a query and $D$  as the query set. Given a response $y$  to a query $x,$  its likelihood underthe policy $\\pi_{\\theta}$ is denoted as $\\pi_{\\theta}(y|x)=\\begin{matrix}\\prod_{t=1}^{|y|}\\pi_{\\theta}(y_{t}|x,y_{<t}) \\end{matrix}$  where $|y|$  denotes the number of tokens in y. A query-response pair $(x,y)$  can be scored by a veriferi $r,$  resulting in a reward $r(x,y)\\in[0,1].$\nProximal Policy Optimization (PPO)Using samples generated from the old policy $\\pi_{\\theta_{\\text{old}}},$  PPO (Schulman et al., 2017) constrains the policy update within a proximal region of the old policy through the clipping mechanism. Specifcally, PPO employs the following objective for policy optimization (we omiti the KL regularization term hereinafter for brevity, as it is not the focus of this paper):\n$$\\mathcal{J}_{\\mathrm{PPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, y\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{|y|}\\sum_{t=1}^{|y|}\\min\\left(w_{t}(\\theta)\\widehat{A}_{t}, \\mathrm{clip}\\left(w_{t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{t}\\right)\\right],$$\n(1)\nwhere the importance ratio of the token $y_{t}$  is defined as $w_{t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{t}|x,y_{<t})}{\\pi_{\\theta_{\\mathrm{old}}} (y_{t}|x,y_{<t})},$  the advantage $\\widehat{A}_{t}$  of $y_{t}$  is estimated by another value model, and $\\epsilon$  is the clipping range of imopldortance ratios.\nThe core challenge of PPO in practice lies in its heavy reliance on the value model. Specifcally, thei value model usually has a similar size to the policy model, introducing a considerable memory and computational burden. Furthermore, the algorithmic effectiveness hinges on the reliability of its value estimate. While acquiring a reliable value model is inherently challenging, ensuring its scalability to longer responses and more complex tasks presents an even greater challenge.\nGroup Relative Policy Optimization (GRPO)GRPO (Shao et al., 2024) bypasses the need for the value model by computing the relative advantage of each response within a group of responses to the same query. Specifcally, GRPO optimizes the following objective:i\n$$\\mathcal{J}_{\\mathrm{GRPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G} \\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\min\\left(w_{i,t}(\\theta)\\widehat{A}_{i,t}, \\mathrm{clip}\\left(w_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right) \\widehat{A}_{i,t}\\right)\\right]$$\n(2)\nwhere $G$  is the number of generated responses to each query $x$  (i.e., the group size), and the importance ratio $w_{i,t}(\\theta)$  and advantage $\\widehat{A}_{i,t}$ of token $y_{i,t}$ are:\n$$w_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})},\\ \\ \\ \\ \\widehat{A}_{i,t}=\\widehat{A}_{i}=\\frac{r(x,y_{i})-\\mathrm{mean}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)}{\\mathrm{std}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)},$$\n(3)\nrespectively, where all the tokens in $y_{i}$  share the same advantage as $\\widehat{A}_{i}.$\n3Motivation\nThe growth in model size, sparsity $\\rm(e.g.,$  in Mixture-of-Experts models), and response length necessitates a large rollout batch size to maximize hardware utilization during RL. To improve sample effciency, it isi standard practice to partition a large batch of rollout data into multiple mini-batches for gradient updates.This procedure inevitably introduces an off-policy learning setting, where responses y are sampled from an old policy $\\pi_{\\theta_{\\mathrm{old}}}$ rather than the current policy $\\pi_{\\theta}$  being optimized. This also explains the necessity of the clipping mechanism in PPO and GRPO, which prevents overly “off-policy” samples from being involved in gradient estimation.\nWhile mechanisms like clipping aim to manage this off-policy discrepancy, we identify a more fundamental issue in GRPO: its objective is ill-posed. This problem becomes particularly acute when training large models on long-response tasks, leading to catastrophic model collapse. The ill-posed nature of the GRPO objective stems from a misapplication of importance sampling weights. The principle of importance sampling is to estimate the expectation of a function $f$ under a target distribution $\\pi_{\\mathrm{tar}}$ by re-weighting samples drawn from a behavior distribution $\\pi_{\\mathrm{beh}} :$\n$$\\mathbb{E}_{z\\sim\\pi_{\\mathrm{tar}}}\\left[f(z)\\right]=\\mathbb{E}_{z\\sim\\pi_{\\mathrm{beh}}}\\left[\\frac{\\pi_{\\mathrm{tar}}(z)}{\\pi_{\\mathrm{beh}}(z)} f(z) \\right].$$\n(4)\nCrucially, this relies on averaging over multiple samples $\\text{( N\\gg1 )}$  from the behavior distribution $\\pi_{\\mathrm{beh}}$  for the importance weight $\\tfrac{\\pi_{\\mathrm{tar}}(z)}{\\pi_{\\mathrm{beh}}(z)}$  to effectively correct for the distributional mismatch.\nIn contrast, GRPO applies the importance weight $\\begin{array}{l}\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\rm old}}(y_{i,t}|x,y_{i,<t})}\\end{array}$ at each token position $t.$  Since this weight is based on a single sample $y_{i,t}$ from each next-token distribution $\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x,y_{i,<t}),$  it fails to perform the intended distribution-correction role. Instead, it introduces high-variance noise into the traininggradients, which accumulates over long sequences and is exacerbated by the clipping mechanism. We have empirically observed that this can lead to model collapse that is often irreversible. Once the collapse occurs, resuming training is unavailing, even when reverting to a previous checkpoint and meticulously tuning hyperparameters (e.g., the clipping ranges), extending generation length, or switching the RL queries.\nThe above observation suggests a fundamental issue in GRPO’s design. The failure of the token-level importance weight points to a core principle: the unit of optimization objective should match the unit of reward. Since the reward is granted to the entire sequence, applying off-policy correction at the token level appears problematic. This motivates us to forego the token-level objective and explore utilizing importance weights and performing optimization directly at the sequence level.\n4Algorithm\n4.1GSPO: Group Sequence Policy Optimization\nWhile the token-level importance weight $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$ is problematic in GRPO, we observe that in the context of language generation, the sequence-level importance weigh $\\tfrac{\\pi_{\\theta}(y|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y|x)}$ has a clear theoretical meaning: it refects how far the responsel $y$  sampled from $\\pi_{\\theta_{\\rm old}}(\\cdot|x)$  deviates from $\\pi_{\\theta}(\\cdot|x),$  which naturally aligns with the sequence-level reward and can also serve as a meaningful indicator of the clipping mechanism.\nBased on this straightforward observation, we propose the Group Sequence Policy Optimization (GSPO)algorithm. GSPO employs the following sequence-level optimization objective:\n$$\\mathcal{J}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G} \\min\\left(s_{i}(\\theta)\\widehat{A}_{i}, \\mathrm{clip}\\left(s_{i}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{i}\\right)\\right],$$\n(5)\nwhere we adopt the group-based advantage estimation:\n$$\\widehat{A}_{i}=\\frac{r(x,y_{i})-\\operatorname{mean}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)}{\\operatorname{std}\\left(\\{r(x,y_{i})\\}_{i=1}^{G}\\right)},$$\n(6)\nand defne the importance ratioi $s_{i}(\\theta)$  based on sequence likelihood (Zheng et al., 2023)\n$$s_{i}(\\theta)=\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}( y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}=\\exp\\left(\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|} \\log\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i, t}|x,y_{i,<t})}\\right).$$\n(7)\nTherefore, GSPO applies clipping to entire responses instead of individual tokens to exclude the overly“off-policy” samples from gradient estimation, which matches both the sequence-level rewarding and optimization. Note that we adopt length normalization in $s_{i}(\\theta)$  to reduce the variance and to control $s_{i}(\\theta)$  within a unifed numerical range. Otherwise, the likelihood changes of a few tokens can result ini dramatic fuctuations of the sequence-level importance ratio, and the importance ratios of responses withl different lengths will require varying clipping ranges. We also note that the clipping ranges in GSPO and in previous algorithms (e.g., GRPO) typically differ in order of magnitude due to the distinct defnitionsi of importance ratios.\n4.2Gradient Analysis\nWe can derive the gradient of the GSPO objective as follows (clipping is omitted for brevity):\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GSPO}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x )}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta)\\widehat{A}_{i}\\right]$$\n(8)\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta) \\widehat{A}_{i}\\cdot\\nabla_{\\theta}\\log s_{i}(\\theta)\\right]$$\n(9)\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}\\widehat{A}_{i}\\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$\n(10)\n\nFor comparison, the gradient of the GRPO objective is as follows (note that $\\widehat{A}_{i,t}=\\widehat{A}_{i}\\text{):}$\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GRPO}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}w_{i,t} (\\theta)\\widehat{A}_{i,t}\\right]$$\n(11)\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\widehat{A}_{i} \\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\frac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$\n(12)\nTherefore, the fundamental distinction between GSPO and GRPO lies in how they weight the gradients of the log likelihoods of tokens. In GRPO, the tokens are weighted according to their respective “importance weight” $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$  However, these unequal weights, which can vary among $(0,1+\\varepsilon]$  (for $\\widehat{A}_{i}>0\\text{)}$  or $[1-\\varepsilon,+\\infty)$  (for $\\widehat{A}_{i}<0\\text{),}$  are not negligible, and their impact can accumulate and lead to unpredictable consequences as training progresses. In contrast, GSPO weights all the tokens in a response equally,eliminating this instability factor of GRPO.\n4.3GSPO-token: A Token-level Objective Variant\nIn scenarios like multi-turn RL, we may desire a fner-grained advantage adjustment than the sequencei level. To this end, we introduce a token-level objective variant of GSPO, namely GSPO-token, to allow token-wise advantage customization:\n$$\\mathcal{J}_{\\mathrm{GSPO\\text{-}token}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\min\\left(s_{i,t}(\\theta)\\widehat{A}_{i,t}, \\mathrm{clip}\\left(s_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\right)\\widehat{A}_{i,t}\\right)\\right],$$\n(13)\nwhere\n$$s_{i,t}(\\theta)=\\mathrm{sg}\\left[s_{i}(\\theta)\\right]\\cdot\\frac{\\pi_{\\theta}( y_{i,t}|x,y_{i,<t})}{\\mathrm{sg}\\left[\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right]},$$\n(14)\nand $\\mathbf{sg}[\\cdot]$  denotes only taking the numerical value but stopping the gradient, corresponding to the detach operation in PyTorch. The gradient of GSPO-token can be derived as:\n$$\\nabla_{\\theta}\\mathcal{J}_{\\mathrm{GSPO\\text{-}token}}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G} \\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}s_{i,t}(\\theta)\\widehat{A}_{i,t}\\right]$$\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}s_{i}(\\theta) \\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\widehat{A}_{i,t}\\frac{\\nabla_{\\theta}\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}\\right]$$\n$$=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^{G}\\left(\\frac{\\pi_{\\theta}(y_{i}|x)}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i}|x)}\\right)^{\\frac{1}{|y_{i}|}}\\cdot\\frac{1}{|y_{i}|}\\sum_{t=1}^{|y_{i}|}\\widehat{A}_{i,t}\\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})\\right].$$\nNote that the term $\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\mathrm{sg}\\left[\\pi_{\\theta}(y_{i,t} |x,y_{i,<t})\\right]}$ has a numerical value of 1, so $s_{i,t}(\\theta)$  is numerically equal to $s_{i}(\\theta).$ Comparing Equation (5) and (13), and Equation (10) and (17), GSPO-token and GSPO are numerically identical in the optimization objective, clipping condition, and theoretical gradient when we set the advantages of all the tokens in the responseto the same value (i.e., $\\widehat{A}_{i,t}\\stackrel{{\\bar{\\}}}{{=}}\\widehat{A}_{i}\\widehat{)},$  while GSPO-token $y_{i}$ enjoys the higher fexibility of adjusting the advantages per token.l\n5Experiments and Discussion\n5.1Empirical Results\nWe experiment with a cold-start model fne-tuned from Qwen3-30B-A3B-Base, and report the trainingi reward curves as well as the model performance curves on the AIME’24 (average Pass@1 over 32 samplings), LiveCodeBench (202410-202502, average Pass@1 over 8 samplings), and CodeForces (Elo Rating) benchmarks. During the RL training, each batch of rollout data is partitioned into four minibatches for gradient updates. In GSPO, we set the left and right clipping ranges in Equation (5) to 3e-4 and 4e-4, respectively. We compare against GRPO as the baseline and set the left and right clipping ranges in Equation (2) to 0.2 and 0.27, respectively, which we have carefully tuned to ensure a fair comparison.Note that GRPO necessitates the Routing Replay training strategy for the normal convergence of MoE RL, which we will additionally discuss in § 5.3, while GSPO has obviated the need for this strategy.\nFigure 1 shows that the training with GSPO proceeds stably throughout. We observe that GSPO can deliver continuous performance improvement through increasing the training compute, regularly updating the query set, and extending the generation length. Moreover, GSPO also demonstrates superior training effciency over GRPO, achieving better training accuracy and benchmark performancei under the same training compute and consumed queries. Finally, we have successfully applied GSPO to the RL training of the latest Qwen3 models, strongly proving the effcacy of GSPO in unleashing thei power of RL scaling for large language models.\nFigure 1: Training curves of a cold-start model fne-tuned from Qwen3-30B-A3B-Base. GSPO possessesi remarkably higher training effciency than GRPO.i\n5.2Curious Observation on Clipping Fractions\nA key distinction of GSPO compared to GRPO is its practice of clipping entire responses rather than individual tokens. Particularly, as shown in Figure 2, we observe a difference of two orders of magnitude in the fractions of clipped tokens between GSPO and GRPO (while adjusting the clipping ranges does not alter the disparity in magnitude). However, despite clipping signifcantly more tokens and consequentlyi using fewer for training (or gradient estimation), GSPO still achieves higher training effciency thani GRPO. This counter-intuitive fnding — that clipping a much larger fraction of tokens leads to superiori training effciency — further indicates that GRPO’s token-level gradient estimates are inherently noisyi and ineffcient for sample exploitation. In contrast, GSPO’s sequence-level approach provides a morei reliable and effective learning signal.\nFigure 2: Average fractions of clipped tokens over the RL training of GSPO and GRPO.\n\n5.3Beneft of GSPO for MoE Trainingi\nBackgroundCompared to the RL training of dense models, the sparse activation nature of MoE models introduces unique stability challenges. In particular, we found that when adopting the GRPO algorithm,the expert-activation volatility of MoE models can prevent RL training from converging properly. To be specifc, after one or more gradient updates, the experts activated for the same response can changei signifcantly. For example, with the 48-layer Qwen3-30B-A3B-Base model, after each RL gradient updatei and for the same rollout sample, there are roughly $10\\%$ of the experts activated under the new policy $\\pi_{\\theta}$ that are different from those under the old policy $\\pi_{\\theta_{\\rm old}}.$  This phenomenon, which becomes more  prominent in deeper MoE models, makes the token-level importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}$ fuctuate drastically and further invalidates them, as discussed in § 3 and 4.2, consequently hindering thel normal convergence of RL training.\nOur Previous ApproachTo tackle this challenge, we previously employed the Routing Replay training strategy. Specifcally, we cache the activated experts ini $\\pi_{\\theta_{\\mathrm{old}}}$ and “replay” these routing modes in $\\pi_{\\theta}$  when computing the importance ratios $w_{i,t}(\\theta)=\\tfrac{\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})}.$ In this way, for each token $y_{i,t},\\pi_{\\theta}(y_{i,t}|x,y_{i,<t})$ and $\\pi_{\\theta_{\\mathrm{old}}}(y_{i,t}|x,y_{i,<t})$  share the same activateθdol dneit,twori,kt, so that we can restore the stability of the tokenlevel importance ratios and ensure optimization of the consistent activated network across gradient updates. Figure 3 demonstrates that Routing Replay serves as an essential technique in the normal convergence of the GRPO training of MoE models.\nFigure 3: The Routing Replay strategy plays a critical role in the normal convergence of the GRPO training of MoE models.\nBeneft of GSPOAlthough Routing Replay enables the GRPO training of MoE models to convergei properly, its practice of reusing routing modes incurs additional memory and communication overhead and can also limit the actual capacity of the MoE model. In contrast, as shown in Figure 1, GSPO eliminates the dependency on Routing Replay and is fully capable of computing the importance ratios $s_{i}(\\theta)$  conventionally, converging normally, and optimizing stably. The key insight is that GSPO focuses only on the sequence likelihood (i.e., $\\pi_{\\theta}(\\widetilde{y_{i}}|x))$  and is not sensitive to the individual token likelihood (i.e., $\\pi_{\\theta}\\big(^{\\prime}y_{i,t}|x,y_{i,<t}\\big)\\widehat{)}.$  Since the MoE model always maintains its language modeling capability, the sequence likelihood will not fuctuate drastically. In summary, GSPO fundamentally resolves the expert-activationl volatility issue in MoE models, obviating the need for complex workarounds like Routing Replay. This not only simplifes and stabilizes the training process but also allows the model to leverage its fulli capacity without artifcial constraints.i\n5.4Beneft of GSPO for RL Infrastructurei\nGiven the precision discrepancies between training engines (e.g., Megatron) and inference engines (e.g.,SGLang and vLLM), in practice, we typically use the training engine to recompute the likelihoods of sampled responses under the old policy $\\pi_{\\theta_{\\mathrm{old}}}.$  However, GSPO uses only sequence-level, rather than token-level, likelihoods for optimization, and intuitively, the former is much more tolerant of precision discrepancies. Hence, GSPO makes it possible to directly use the likelihoods returned by the inference engine for optimization, thereby avoiding the need for recomputation with the training engine. This can be especially benefcial in scenarios like partial rollout and multi-turn RL and in the training-inferencei disaggregated frameworks.\n\n6Conclusion\nWe propose Group Sequence Policy Optimization (GSPO), a new reinforcement learning algorithm for training large language models. Following the basic principle of importance sampling, GSPO defnes importance ratios based on sequence likelihood and performs sequence-level clipping, rewarding,i and optimization. GSPO demonstrates notably superior training stability, effciency, and performancei compared to GRPO and exhibits particular effcacy for the large-scale RL training of MoE models, layingi the foundation for the exceptional improvements in the latest Qwen3 models. With GSPO as a scalable algorithmic cornerstone, we will continue to scale RL and look forward to the resulting fundamental advances in intelligence.\nReferences\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\nMiniMax. Minimax-m1: Scaling test-time compute effciently with lightning attention. arXiv preprinti arXiv:2506.13585, 2025.\nOpenAI.LearningtoreasonwithLLMs,2024.URLhttps://openai.com/index/learning-to-reason-with-llms/.\nTeam Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a.\nTeam Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/blog/qwq-32b/.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nChujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of the Association for Computational Linguistics: ACL 2023,2023. URL https://aclanthology.org/2023.findings-acl.65/.\n"}